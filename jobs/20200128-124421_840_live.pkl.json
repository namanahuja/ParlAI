{
    "assignment_ids": [
        "320DUZ38GASFRIS90PDV0NR23RNJG4"
    ],
    "bad_wizard_worker": "",
    "chosen_topic": "Well Krav Maga is pretty lit. It can be used as an advanced form of self defense that is even as good as doing karate in some cases. ",
    "chosen_topic_passage": {},
    "dialog": [
        {
            "checked_passage": {
                "no_passages_used": "no_passages_used"
            },
            "checked_sentence": {
                "no_passages_used": "no_passages_used"
            },
            "clicked_passages": {},
            "context": {
                "conversation": [
                    "I'm obsessed with filming. ",
                    "Cinematograpy is awesome!  It is the science or art of motion-picture photography by recording light or other electromagnetic radiation",
                    "How do cinematographers record light? ",
                    "They typically do it electronically by means of an image sensor, or chemically by means of a light-sensitive material such as film stock."
                ],
                "index": 29
            },
            "contextCount": 0,
            "full_passages": [
                "Cinematography\n\nCinematography (also called \"Direction of Photography\") is the science or art of motion-picture photography by recording light or other electromagnetic radiation, either electronically by means of an image sensor, or chemically by means of a light-sensitive material such as film stock.\n\nTypically, a lens is used to repeatedly focus the light reflected from objects into real images on the light-sensitive surface inside a camera during a questioned exposure, creating multiple images. With an electronic image sensor, this produces an electrical charge at each pixel, which is electronically processed and stored in a video file for subsequent display or processing. The result with photographic emulsion is a series of invisible latent images on the film stock, which are later chemically \"developed\" into a visible image. The images on the film stock are played back at a rapid speed and projected onto a screen, creating the illusion of motion.\n\nCinematography finds uses in many fields of science and business as well as for entertainment purposes and mass communication.\n\nThe word \"cinematography\" was created from the Greek words (\"kinema\"), meaning \"movement, motion\" and (\"graphein\") meaning \"to record\", together meaning \"recording motion.\" The word used to refer to the art, process, or job of filming movies, but later its meaning was restricted to \"motion picture photography.\"\n\nIn the 1830s, moving images were produced on revolving drums and disks, with independent invention by Simon von Stampfer (stroboscope) in Austria, Joseph Plateau (phenakistoscope) in Belgium, and William Horner (zoetrope) in Britain.\n\nIn 1845, Francis Ronalds invented the first successful camera able to make continuous recordings of the varying indications of meteorological and geomagnetic instruments over time. The cameras were supplied to numerous observatories around the world and some remained in use until well into the 20th century.\n\nWilliam Lincoln patented a device, in 1867, that showed animated pictures called the \"wheel of life\" or \"zoopraxiscope\". In it, moving drawings or photographs were watched through a slit.\n\nOn 19 June 1873, Eadweard Muybridge successfully photographed a horse named \"Sallie Gardner\" in fast motion using a series of 24 stereoscopic cameras. The cameras were arranged along a track parallel to the horse's, and each camera shutter was controlled by a trip wire triggered by the horse's hooves. They were 21 inches apart to cover the 20 feet taken by the horse stride, taking pictures at one thousandth of a second. At the end of the decade, Muybridge had adapted sequences of his photographs to a zoopraxiscope for short, primitive projected \"movies,\" which were sensations on his lecture tours by 1879 or 1880.\n\nNine years later, in 1882, French scientist Étienne-Jules Marey invented a chronophotographic gun, which was capable of taking 12 consecutive frames a second, recording all the frames of the same picture.\n\nThe late nineteenth to the early twentieth century brought rise to the use of film not only for entertainment purposes but for scientific exploration as well. French biologist and filmmaker Jean Painleve lobbied heavily for the use of film in the scientific field, as the new medium was more efficient in capturing and documenting the behavior, movement, and environment of microorganisms, cells, and bacteria, than the naked eye. The introduction of film into scientific fields allowed for not only the viewing \"new images and objects, such as cells and natural objects, but also the viewing of them in real time\", whereas prior to the invention of moving pictures, scientists and doctors alike had to rely on hand drawn sketches of human anatomy and its microorganisms. This posed a great inconvenience in the science and medical worlds. The development of film and increased usage of cameras allowed doctors and scientists to grasp a better understanding and knowledge of their projects.\n\nThe experimental film \"Roundhay Garden Scene\", filmed by Louis Le Prince on 14 October 1888, in Roundhay, Leeds, England, is the earliest surviving motion picture. This movie was shot on paper film.\n\nW. K. L. Dickson, working under the direction of Thomas Alva Edison, was the first to design a successful apparatus, the Kinetograph, patented in 1891. This camera took a series of instantaneous photographs on standard Eastman Kodak photographic emulsion coated onto a transparent celluloid strip 35 mm wide. The results of this work were first shown in public in 1893, using the viewing apparatus also designed by Dickson, the Kinetoscope. Contained within a large box, only one person at a time looking into it through a peephole could view the movie.\n\nIn the following year, Charles Francis Jenkins and his projector, the Phantoscope, made a successful audience viewing while Louis and Auguste Lumière perfected the Cinématographe, an apparatus that took, printed, and projected film, in Paris in December 1895. The Lumière brothers were the first to present projected, moving, photographic, pictures to a paying audience of more than one person.\n\nIn 1896, movie theaters were open in France (Paris, Lyon, Bordeaux, Nice, Marseille); Italy (Rome, Milan, Naples, Genoa, Venice, Bologna, Forlì); Brussels; and London.\n\nIn 1896, Edison showed his improved Vitascope projector, the first commercially successful projector in the U.S.\n\nCooper Hewitt invented mercury lamps which made it practical to shoot films indoors without sunlight in 1905.\n\nThe first animated cartoon was produced in 1906.\n\nCredits began to appear at the beginning of motion pictures in 1911.\n\nThe Bell and Howell 2709 movie camera invented in 1915 allowed directors to make close-ups without physically moving the camera.\n\nBy the late 1920s, most of the movies produced were sound films.\n\nWide screen formats were first experimented with in the 1950s.\n\nBy the 1970s, most movies were color films. IMAX and other 70mm formats gained popularity. Wide distribution of films became commonplace, setting the ground for \"blockbusters.\"\n\nFilm cinematography dominated the motion picture industry from its inception until the 2010s when digital cinematography became dominant. Film cinematography is still used by some directors, especially in specific applications or out of fondness of the format.\n\nFrom its birth in the 1880s, movies were predominantly monochrome. Contrary to popular belief, monochrome doesn't always mean black and white; it means a movie shot in a single tone or color. Since the cost of tinted film bases was substantially higher, most movies were produced in black and white monochrome. Even with the advent of early color experiments, the greater expense of color meant films were mostly made in black and white until the 1950s, when cheaper color processes were introduced, and in some years the percentage of films shot on color film surpassed 51%. By the 1960s, color became by far the dominant film stock. In the coming decades, the usage of color film greatly increased while monochrome films became scarce.\n\nAfter the advent of motion pictures, a tremendous amount of energy was invested in the production of photography in natural color. The invention of the talking picture further increased the demand for the use of color photography. However, in comparison to other technological advances of the time, the arrival of color photography was a relatively slow process.\n\nEarly movies were not actually color movies since they were shot monochrome and hand-colored or machine-colored afterwards. (Such movies are referred to as \"colored\" and not \"color\".) The earliest such example is the hand-tinted Annabelle Serpentine Dance in 1895 by Edison Manufacturing Company. Machine-based tinting later became popular. Tinting continued until the advent of natural color cinematography in the 1910s. Many black and white movies have been colorized recently using digital tinting. This includes footage shot from both world wars, sporting events and political propaganda.\n\nIn 1902, Edward Raymond Turner produced the first films with a natural color process rather than using colorization techniques. In 1908, kinemacolor was introduced. In the same year, the short film \"A Visit to the Seaside\" became the first natural color movie to be publicly presented.\n\nIn 1917, the earliest version of Technicolor was introduced. Kodachrome was introduced in 1935. Eastmancolor was introduced in 1950 and became the color standard for the rest of the century.\n\nIn the 2010s, color films were largely superseded by color digital cinematography.\n\nIn digital cinematography, the movie is shot on digital medium such as flash storage, as well as distributed through a digital medium such as a hard drive.\n\nBeginning in the late 1980s, Sony began marketing the concept of \"electronic cinematography,\" utilizing its analog Sony HDVS professional video cameras. The effort met with very little success. However, this led to one of the earliest digitally shot feature movies, \"Julia and Julia\", being produced in 1987. In 1998, with the introduction of HDCAM recorders and 1920 × 1080 pixel digital professional video cameras based on CCD technology, the idea, now re-branded as \"digital cinematography,\" began to gain traction in the market.\n\nShot and released in 1998, \"The Last Broadcast\" is believed by some to be the first feature-length video shot and edited entirely on consumer-level digital equipment. In May 1999, George Lucas challenged the supremacy of the movie-making medium of film for the first time by including footage filmed with high-definition digital cameras in \"\". In late 2013, Paramount became the first major studio to distribute movies to theaters in digital format, eliminating 35mm film entirely. Since then the demand of movies to be developed onto digital format rather than 35mm has increased drastically.\n\nAs digital technology improved, movie studios began increasingly shifting towards digital cinematography. Since the 2010s, digital cinematography has become the dominant form of cinematography after largely superseding film cinematography.\n\nNumerous aspects contribute to the art of cinematography, including:\n\nThe first film cameras were fastened directly to the head of a tripod or other support, with only the crudest kind of leveling devices provided, in the manner of the still-camera tripod heads of the period. The earliest film cameras were thus effectively fixed during the shot, and hence the first camera movements were the result of mounting a camera on a moving vehicle. The first known of these was a film shot by a Lumière cameraman from the back platform of a train leaving Jerusalem in 1896, and by 1898, there were a number of films shot from moving trains. Although listed under the general heading of \"panoramas\" in the sales catalogues of the time, those films shot straight forward from in front of a railway engine were usually specifically referred to as \"phantom rides\".\n\nIn 1897, Robert W. Paul had the first real rotating camera head made to put on a tripod, so that he could follow the passing processions of Queen Victoria's Diamond Jubilee in one uninterrupted shot. This device had the camera mounted on a vertical axis that could be rotated by a worm gear driven by turning a crank handle, and Paul put it on general sale the next year. Shots taken using such a \"panning\" head were also referred to as \"panoramas\" in the film catalogues of the first decade of the cinema. This eventually led to the creation of a panoramic photo as well.\n\nThe standard pattern for early film studios was provided by the studio which Georges Méliès had built in 1897. This had a glass roof and three glass walls constructed after the model of large studios for still photography, and it was fitted with thin cotton cloths that could be stretched below the roof to diffuse the direct ray of the sun on sunny days. The soft overall light without real shadows that this arrangement produced, and which also exists naturally on lightly overcast days, was to become the basis for film lighting in film studios for the next decade.\n\nCinematography can begin with digital image sensor or rolls of film. Advancements in film emulsion and grain structure provided a wide range of available film stocks. The selection of a film stock is one of the first decisions made in preparing a typical film production.\n\nAside from the film gauge selection – 8 mm (amateur), 16 mm (semi-professional), 35 mm (professional) and 65 mm (epic photography, rarely used except in special event venues) – the cinematographer has a selection of stocks in reversal (which, when developed, create a positive image) and negative formats along with a wide range of film speeds (varying sensitivity to light) from ISO 50 (slow, least sensitive to light) to 800 (very fast, extremely sensitive to light) and differing response to color (low saturation, high saturation) and contrast (varying levels between pure black (no exposure) and pure white (complete overexposure).\nAdvancements and adjustments to nearly all gauges of film create the \"super\" formats wherein the area of the film used to capture a single frame of an image is expanded, although the physical gauge of the film remains the same. Super 8 mm, Super 16 mm, and Super 35 mm all utilize more of the overall film area for the image than their \"regular\" non-super counterparts. The larger the film gauge, the higher the overall image resolution clarity and technical quality. The techniques used by the film laboratory to process the film stock can also offer a considerable variance in the image produced. By controlling the temperature and varying the duration in which the film is soaked in the development chemicals, and by skipping certain chemical processes (or partially skipping all of them), cinematographers can achieve very different looks from a single film stock in the laboratory. Some techniques that can be used are push processing, bleach bypass, and cross processing.\n\nMost of modern cinema uses digital cinematography and has no film stocks , but the cameras themselves can be adjusted in ways that go far beyond the abilities of one particular film stock. They can provide varying degrees of color sensitivity, image contrast, light sensitivity and so on. One camera can achieve all the various looks of different emulsions. Digital image adjustments such as ISO and contrast are executed by estimating the same adjustments that would take place if actual film were in use, and are thus vulnerable to the camera's sensor designers perceptions of various film stocks and image adjustment parameters.\n\nFilters, such as diffusion filters or color effect filters, are also widely used to enhance mood or dramatic effects. Most photographic filters are made up of two pieces of optical glass glued together with some form of image or light manipulation material between the glass. In the case of color filters, there is often a translucent color medium pressed between two planes of optical glass. Color filters work by blocking out certain color wavelengths of light from reaching the film. With color film, this works very intuitively wherein a blue filter will cut down on the passage of red, orange, and yellow light and create a blue tint on the film. In black-and-white photography, color filters are used somewhat counter intuitively; for instance a yellow filter, which cuts down on blue wavelengths of light, can be used to darken a daylight sky (by eliminating blue light from hitting the film, thus greatly underexposing the mostly blue sky) while not biasing most human flesh tone. Certain cinematographers, such as Christopher Doyle, are well known for their innovative use of filters. Filters can be used in front of the lens or, in some cases, behind the lens for different effects. Christopher Doyle was a pioneer for increased usage of filters in movies. He was highly respected throughout the cinema world.\n\nLenses can be attached to the camera to give a certain look, feel, or effect by focus, color, etc.\n\nAs does the human eye, the camera creates perspective and spatial relations with the rest of the world. However, unlike one's eye, a cinematographer can select different lenses for different purposes. Variation in focal length is one of the chief benefits. The focal length of the lens determines the angle of view and, therefore, the field of view. Cinematographers can choose from a range of wide-angle lenses, \"normal\" lenses and long focus lenses, as well as macro lenses and other special effect lens systems such as borescope lenses. Wide-angle lenses have short focal lengths and make spatial distances more obvious. A person in the distance is shown as much smaller while someone in the front will loom large. On the other hand, long focus lenses reduce such exaggerations, depicting far-off objects as seemingly close together and flattening perspective. The differences between the perspective rendering is actually not due to the focal length by itself, but by the distance between the subjects and the camera. Therefore, the use of different focal lengths in combination with different camera to subject distances creates these different rendering. Changing the focal length only while keeping the same camera position doesn't affect perspective but the camera angle of view only.\n\nA zoom lens allows a camera operator to change his focal length within a shot or quickly between setups for shots. As prime lenses offer greater optical quality and are \"faster\" (larger aperture openings, usable in less light) than zoom lenses, they are often employed in professional cinematography over zoom lenses. Certain scenes or even types of filmmaking, however, may require the use of zooms for speed or ease of use, as well as shots involving a zoom move.\n\nAs in other photography, the control of the exposed image is done in the lens with the control of the diaphragm aperture. For proper selection, the cinematographer needs that all lenses be engraved with T-Stop, not f-stop so that the eventual light loss due to the glass doesn't affect the exposure control when setting it using the usual meters. The choice of the aperture also affects image quality (aberrations) and depth of field.\n\nFocal length and diaphragm aperture affect the depth of field of a scene – that is, how much the background, mid-ground and foreground will be rendered in \"acceptable focus\" (only one exact plane of the image is in precise focus) on the film or video target. Depth of field (not to be confused with depth of focus) is determined by the aperture size and the focal distance. A large or deep depth of field is generated with a very small iris aperture and focusing on a point in the distance, whereas a shallow depth of field will be achieved with a large (open) iris aperture and focusing closer to the lens. Depth of field is also governed by the format size. If one considers the field of view and angle of view, the smaller the image is, the shorter the focal length should be, as to keep the same field of view. Then, the smaller the image is, the more depth of field is obtained, for the same field of view. Therefore, 70mm has less depth of field than 35mm for a given field of view, 16mm more than 35mm, and video cameras even more depth of field than 16mm. As videographers try to emulate the look of 35 mm film with digital cameras, this is one issue of frustration – excessive depth of field with digital cameras and using additional optical devices to reduce that depth of field.\n\nIn \"Citizen Kane\" (1941), cinematographer Gregg Toland and director Orson Welles used tighter apertures to create every detail of the foreground and background of the sets in sharp focus. This practice is known as deep focus. Deep focus became a popular cinematographic device from the 1940s onwards in Hollywood. Today, the trend is for more shallow focus.\n\nTo change the plane of focus from one object or character to another within a shot is commonly known as a \"rack focus\".\n\nThe aspect ratio of an image is the ratio of its width to its height. This can be expressed either as a ratio of 2 integers, such as 4:3, or in a decimal format, such as 1.33:1 or simply 1.33.\n\nDifferent ratios provide different aesthetic effects. Standards for aspect ratio have varied significantly over time.\n\nDuring the silent era, aspect ratios varied widely, from square 1:1, all the way up to the extreme widescreen 4:1 Polyvision. However, from the 1910s, silent motion pictures generally settled on the ratio of 4:3 (1.33). The introduction of sound-on-film briefly narrowed the aspect ratio, to allow room for a sound stripe. In 1932, a new standard was introduced, the Academy ratio of 1.37, by means of thickening the frame line.\n\nFor years, mainstream cinematographers were limited to using the Academy ratio, but in the 1950s, thanks to the popularity of Cinerama, widescreen ratios were introduced in an effort to pull audiences back into the theater and away from their home television sets. These new widescreen formats provided cinematographers a wider frame within which to compose their images.\n\nMany different proprietary photographic systems were invented and utilized in the 1950s to create widescreen movies, but one dominated film: the anamorphic process, which optically squeezes the image to photograph twice the horizontal area to the same size vertical as standard \"spherical\" lenses. The first commonly used anamorphic format was CinemaScope, which used a 2.35 aspect ratio, although it was originally 2.55. CinemaScope was used from 1953 to 1967, but due to technical flaws in the design and its ownership by Fox, several third-party companies, led by Panavision's technical improvements in the 1950s, dominated the anamorphic cine lens market. Changes to SMPTE projection standards altered the projected ratio from 2.35 to 2.39 in 1970, although this did not change anything regarding the photographic anamorphic standards; all changes in respect to the aspect ratio of anamorphic 35 mm photography are specific to camera or projector gate sizes, not the optical system. After the \"widescreen wars\" of the 1950s, the motion-picture industry settled into 1.85 as a standard for theatrical projection in the United States and the United Kingdom. This is a cropped version of 1.37. Europe and Asia opted for 1.66 at first, although 1.85 has largely permeated these markets in recent decades. Certain \"epic\" or adventure movies utilized the anamorphic 2.39.\n\nIn the 1990s, with the advent of high-definition video, television engineers created the 1.78 (16:9) ratio as a mathematical compromise between the theatrical standard of 1.85 and television's 1.33, as it was not practical to produce a traditional CRT television tube with a width of 1.85. Until that point, nothing had ever been originated in 1.78. Today, this is a standard for high-definition video and for widescreen television.\n\nLight is necessary to create an image exposure on a frame of film or on a digital target (CCD, etc.). The art of lighting for cinematography goes far beyond basic exposure, however, into the essence of visual storytelling. Lighting contributes considerably to the emotional response an audience has watching a motion picture. The increased usage of filters can greatly impact the final image and affect the lighting.\n\nCinematography can not only depict a moving subject but can use a camera, which represents the audience's viewpoint or perspective, that moves during the course of filming. This movement plays a considerable role in the emotional language of film images and the audience's emotional reaction to the action. Techniques range from the most basic movements of panning (horizontal shift in viewpoint from a fixed position; like turning your head side-to-side) and tilting (vertical shift in viewpoint from a fixed position; like tipping your head back to look at the sky or down to look at the ground) to dollying (placing the camera on a moving platform to move it closer or farther from the subject), tracking (placing the camera on a moving platform to move it to the left or right), craning (moving the camera in a vertical position; being able to lift it off the ground as well as swing it side-to-side from a fixed base position), and combinations of the above. Early cinematographers often faced problems that were not common to other graphic artists because of the element of motion.\nCameras have been mounted to nearly every imaginable form of transportation.\n\nMost cameras can also be handheld, that is held in the hands of the camera operator who moves from one position to another while filming the action. Personal stabilizing platforms came into being in the late 1970s through the invention of Garrett Brown, which became known as the Steadicam. The Steadicam is a body harness and stabilization arm that connects to the camera, supporting the camera while isolating it from the operator's body movements. After the Steadicam patent expired in the early 1990s, many other companies began manufacturing their concept of the personal camera stabilizer. This invention is much more common throughout the cinematic world today. From feature-length films to the evening news, more and more networks have begun to use a personal camera stabilizer.\n\nThe first special effects in the cinema were created while the film was being shot. These came to be known as \"in-camera\" effects. Later, optical and digital effects were developed so that editors and visual effects artists could more tightly control the process by manipulating the film in post-production.\n\nThe 1896 movie The Execution of Mary Stuart shows an actor dressed as the queen placing her head on the execution block in front of a small group of bystanders in Elizabethan dress. The executioner brings his axe down, and the queen's severed head drops onto the ground. This trick was worked by stopping the camera and replacing the actor with a dummy, then restarting the camera before the axe falls. The two pieces of film were then trimmed and cemented together so that the action appeared continuous when the film was shown. Thus creating an overall illusion and successfully laying the foundation for special affects.\n\nThis film was among those exported to Europe with the first Kinetoscope machines in 1895 and was seen by Georges Méliès, who was putting on magic shows in his Theatre Robert-Houdin in Paris at the time. He took up filmmaking in 1896, and after making imitations of other films from Edison, Lumière, and Robert Paul, he made \"Escamotage d'un dame chez Robert-Houdin (The Vanishing Lady)\". This film shows a woman being made to vanish by using the same stop motion technique as the earlier Edison film. After this, Georges Méliès made many single shot films using this trick over the next couple of years.\n\nThe other basic technique for trick cinematography involves double exposure of the film in the camera, which was first done by George Albert Smith in July 1898 in the UK. Smith's \"The Corsican Brothers\" (1898) was described in the catalogue of the Warwick Trading Company, which took up the distribution of Smith's films in 1900, thus:\n\"One of the twin brothers returns home from shooting in the Corsican mountains, and is visited by the ghost of the other twin. By extremely careful photography the ghost appears *quite transparent*. After indicating that he has been killed by a sword-thrust, and appealing for vengeance, he disappears. A 'vision' then appears showing the fatal duel in the snow. To the Corsican's amazement, the duel and death of his brother are vividly depicted in the vision, and overcome by his feelings, he falls to the floor just as his mother enters the room.\"\nThe ghost effect was done by draping the set in black velvet after the main action had been shot, and then re-exposing the negative with the actor playing the ghost going through the actions at the appropriate point. Likewise, the vision, which appeared within a circular vignette or matte, was similarly superimposed over a black area in the backdrop to the scene, rather than over a part of the set with detail in it, so that nothing appeared through the image, which seemed quite solid. Smith used this technique again in \"Santa Claus\" (1898).\n\nGeorges Méliès first used superimposition on a dark background in \"La Caverne maudite (The Cave of the Demons)\" made a couple of months later in 1898, and elaborated it with multiple superimpositions in the one shot in \"Un Homme de têtes (The Four Troublesome Heads)\". He created further variations in subsequent films.\n\nMotion picture images are presented to an audience at a constant speed. In the theater it is 24 frames per second, in NTSC (US) Television it is 30 frames per second (29.97 to be exact), in PAL (Europe) television it is 25 frames per second. This speed of presentation does not vary.\n\nHowever, by varying the speed at which the image is captured, various effects can be created knowing that the faster or slower recorded image will be played at a constant speed. Giving the cinematographer even more freedom for creativity and expression to be made.\n\nFor instance, time-lapse photography is created by exposing an image at an extremely slow rate. If a cinematographer sets a camera to expose one frame every minute for four hours, and then that footage is projected at 24 frames per second, a four-hour event will take 10 seconds to present, and one can present the events of a whole day (24 hours) in just one minute.\n\nThe inverse of this, if an image is captured at speeds above that at which they will be presented, the effect is to greatly slow down (slow motion) the image. If a cinematographer shoots a person diving into a pool at 96 frames per second, and that image is played back at 24 frames per second, the presentation will take 4 times as long as the actual event. Extreme slow motion, capturing many thousands of frames per second can present things normally invisible to the human eye, such as bullets in flight and shockwaves travelling through media, a potentially powerful cinematographical technique.\n\nIn motion pictures, the manipulation of time and space is a considerable contributing factor to the narrative storytelling tools. Film editing plays a much stronger role in this manipulation, but frame rate selection in the photography of the original action is also a contributing factor to altering time. For example, Charlie Chaplin's \"Modern Times\" was shot at \"silent speed\" (18 fps) but projected at \"sound speed\" (24 fps), which makes the slapstick action appear even more frenetic.\n\nSpeed ramping, or simply \"ramping\", is a process whereby the capture frame rate of the camera changes over time. For example, if in the course of 10 seconds of capture, the capture frame rate is adjusted from 60 frames per second to 24 frames per second, when played back at the standard movie rate of 24 frames per second, a unique time-manipulation effect is achieved. For example, someone pushing a door open and walking out into the street would appear to start off in slow-motion, but in a few seconds later within the same shot, the person would appear to walk in \"realtime\" (normal speed). The opposite speed-ramping is done in \"The Matrix\" when Neo re-enters the Matrix for the first time to see the Oracle. As he comes out of the warehouse \"load-point\", the camera zooms into Neo at normal speed but as it gets closer to Neo's face, time seems to slow down, foreshadowing the manipulation of time itself within the Matrix later in the movie.\n\nG.A. Smith initiated the technique of reverse motion and also improved the quality of self-motivating images. This he did by repeating the action a second time while filming it with an inverted camera and then joining the tail of the second negative to that of the first. The first films using this were \"Tipsy, Topsy, Turvy\" and \"The Awkward Sign Painter\", the latter which showed a sign painter lettering a sign, and then the painting on the sign vanishing under the painter's brush. The earliest surviving example of this technique is Smith's \"The House That Jack Built\", made before September 1901. Here, a small boy is shown knocking down a castle just constructed by a little girl out of children's building blocks. A title then appears, saying \"Reversed\", and the action is repeated in reverse so that the castle re-erects itself under his blows.\n\nCecil Hepworth improved upon this technique by printing the negative of the forwards motion backwards frame by frame, so that in the production of the print the original action was exactly reversed. Hepworth made \"The Bathers\" in 1900, in which bathers who have undressed and jumped into the water appear to spring backwards out of it, and have their clothes magically fly back onto their bodies.\n\nThe use of different camera speeds also appeared around 1900. Robert Paul's \"On a Runaway Motor Car through Piccadilly Circus\" (1899), had the camera turn so slowly that when the film was projected at the usual 16 frames per second, the scenery appeared to be passing at great speed. Cecil Hepworth used the opposite effect in \"The Indian Chief and the Seidlitz powder\" (1901), in which a naïve Red Indian eats a lot of the fizzy stomach medicine, causing his stomach to expand and then he then leaps around balloon-like. This was done by cranking the camera faster than the normal 16 frames per second giving the first \"slow motion\" effect.\n\nIn descending order of seniority, the following staff is involved:\n\n\nIn the film industry, the cinematographer is responsible for the technical aspects of the images (lighting, lens choices, composition, exposure, filtration, film selection), but works closely with the director to ensure that the artistic aesthetics are supporting the director's vision of the story being told. The cinematographers are the heads of the camera, grip and lighting crew on a set, and for this reason, they are often called directors of photography or DPs. The ASC defines cinematography as a creative and interpretive process that culminates in the authorship of an original work of art rather than the simple recording of a physical event. Cinematography is not a subcategory of photography. Rather, photography is but one craft that the cinematographer uses in addition to other physical, organizational, managerial, interpretive. and image-manipulating techniques to effect one coherent process.\nIn British tradition, if the DOP actually operates the camera him/herself they are called the \"cinematographer\". On smaller productions, it is common for one person to perform all these functions alone. The career progression usually involves climbing up the ladder from seconding, firsting, eventually to operating the camera.\n\nDirectors of photography make many creative and interpretive decisions during the course of their work, from pre-production to post-production, all of which affect the overall feel and look of the motion picture. Many of these decisions are similar to what a photographer needs to note when taking a picture: the cinematographer controls the film choice itself (from a range of available stocks with varying sensitivities to light and color), the selection of lens focal lengths, aperture exposure and focus. Cinematography, however, has a temporal aspect (see persistence of vision), unlike still photography, which is purely a single still image. It is also bulkier and more strenuous to deal with movie cameras, and it involves a more complex array of choices. As such a cinematographer often needs to work co-operatively with more people than does a photographer, who could frequently function as a single person. As a result, the cinematographer's job also includes personnel management and logistical organization. Given the in-depth knowledge. a cinematographer requires not only of his or her own craft but also that of other personnel, formal tuition in analogue or digital filmmaking can be advantageous.\n\n\n",
                "Photography\n\nPhotography is the science, art, application and practice of creating durable images by recording light or other electromagnetic radiation, either electronically by means of an image sensor, or chemically by means of a light-sensitive material such as photographic film.\n\nTypically, a lens is used to focus the light reflected or emitted from objects into a real image on the light-sensitive surface inside a camera during a timed exposure. With an electronic image sensor, this produces an electrical charge at each pixel, which is electronically processed and stored in a digital image file for subsequent display or processing. The result with photographic emulsion is an invisible latent image, which is later chemically \"developed\" into a visible image, either negative or positive depending on the purpose of the photographic material and the method of processing. A negative image on film is traditionally used to photographically create a positive image on a paper base, known as a print, either by using an enlarger or by contact printing.\n\nPhotography is employed in many fields of science, manufacturing (e.g., photolithography), and business, as well as its more direct uses for art, film and video production, recreational purposes, hobby, and mass communication.\n\nThe word \"photography\" was created from the Greek roots φωτός (\"phōtos\"), genitive of φῶς (\"phōs\"), \"light\" and γραφή (\"graphé\") \"representation by means of lines\" or \"drawing\", together meaning \"drawing with light\".\n\nSeveral people may have coined the same new term from these roots independently. Hercules Florence, a French painter and inventor living in Campinas, Brazil, used the French form of the word, \"photographie\", in private notes which a Brazilian historian believes were written in 1834. This claim is widely reported but apparently has never been independently confirmed as beyond reasonable doubt.\nThe German newspaper \"Vossische Zeitung\" of 25 February 1839 contained an article entitled \"Photographie\", discussing several priority claims - especially Talbot's - regarding Daguerre's claim of invention. The article is the earliest known occurrence of the word in public print. It was signed \"J.M.\", believed to have been Berlin astronomer Johann von Maedler. \nCredit has traditionally been given to Sir John Herschel both for coining the word and for introducing it to the public. His uses of it in private correspondence prior to 25 February 1839 and at his Royal Society lecture on the subject in London on 14 March 1839 have long been amply documented and accepted as settled facts.\n\nThe inventors Niépce, Talbot and Daguerre seem not to have known or used the word \"photography\", but referred to their processes as \"Heliography\" (Niépce), \"Photogenic Drawing\" / \"Talbotype\" / \"Calotype\" (Talbot) and \"Daguerreotype\" (Daguerre).\n\nPhotography is the result of combining several technical discoveries. Long before the first photographs were made, ancient Han Chinese philosopher Mo Di from the Mohist School of Logic was the first to discover and develop the scientific principles of optics, camera obscura, and pinhole camera. Later Greek mathematicians Aristotle and Euclid also independently described a pinhole camera in the 5th and 4th centuries BCE. In the 6th century CE, Byzantine mathematician Anthemius of Tralles used a type of camera obscura in his experiments. Both the Han Chinese polymath Shen Kuo (1031–95) and Arab physicist Ibn al-Haytham (Alhazen) (965–1040) independently invented the camera obscura and pinhole camera, Albertus Magnus (1193–1280) discovered silver nitrate, and Georg Fabricius (1516–71) discovered silver chloride. Shen Kuo explains the science of camera obscura and optical physics in his scientific work Dream Pool Essays while the techniques described in Ibn al-Haytham's Book of Optics are capable of producing primitive photographs using medieval materials.\n\nDaniele Barbaro described a diaphragm in 1566. Wilhelm Homberg described how light darkened some chemicals (photochemical effect) in 1694. The fiction book \"Giphantie\", published in 1760, by French author Tiphaigne de la Roche, described what can be interpreted as photography.\n\nThe discovery of the camera obscura that provides an image of a scene dates back to ancient China. Leonardo da Vinci mentions natural camera obscura that are formed by dark caves on the edge of a sunlit valley. A hole in the cave wall will act as a pinhole camera and project a laterally reversed, upside down image on a piece of paper. So the birth of photography was primarily concerned with inventing means to capture and keep the image produced by the camera obscura.\n\nRenaissance painters used the camera obscura which, in fact, gives the optical rendering in color that dominates Western Art. The camera obscura literally means \"dark chamber\" in Latin. It is a box with a hole in it which allows light to go through and create an image onto the piece of paper.\n\nAround the year 1800, British inventor Thomas Wedgwood made the first known attempt to capture the image in a camera obscura by means of a light-sensitive substance. He used paper or white leather treated with silver nitrate. Although he succeeded in capturing the shadows of objects placed on the surface in direct sunlight, and even made shadow copies of paintings on glass, it was reported in 1802 that \"the images formed by means of a camera obscura have been found too faint to produce, in any moderate time, an effect upon the nitrate of silver.\" The shadow images eventually darkened all over.\n\nThe first permanent photoetching was an image produced in 1822 by the French inventor Nicéphore Niépce, but it was destroyed in a later attempt to make prints from it. Niépce was successful again in 1825. In 1826 or 1827, he made the \"View from the Window at Le Gras\", the earliest surviving photograph from nature (i.e., of the image of a real-world scene, as formed in a camera obscura by a lens).\n\nBecause Niépce's camera photographs required an extremely long exposure (at least eight hours and probably several days), he sought to greatly improve his bitumen process or replace it with one that was more practical. In partnership with Louis Daguerre, he worked out post-exposure processing methods that produced visually superior results and replaced the bitumen with a more light-sensitive resin, but hours of exposure in the camera were still required. With an eye to eventual commercial exploitation, the partners opted for total secrecy.\n\nNiépce died in 1833 and Daguerre then redirected the experiments toward the light-sensitive silver halides, which Niépce had abandoned many years earlier because of his inability to make the images he captured with them light-fast and permanent. Daguerre's efforts culminated in what would later be named the daguerreotype process. The essential elements—a silver-plated surface sensitized by iodine vapor, developed by mercury vapor, and \"fixed\" with hot saturated salt water—were in place in 1837. The required exposure time was measured in minutes instead of hours. Daguerre took the earliest confirmed photograph of a person in 1838 while capturing a view of a Paris street: unlike the other pedestrian and horse-drawn traffic on the busy boulevard, which appears deserted, one man having his boots polished stood sufficiently still throughout the several-minutes-long exposure to be visible. The existence of Daguerre's process was publicly announced, without details, on 7 January 1839. The news created an international sensation. France soon agreed to pay Daguerre a pension in exchange for the right to present his invention to the world as the gift of France, which occurred when complete working instructions were unveiled on 19 August 1839. In that same year, American photographer Robert Cornelius is credited with taking the earliest surviving photographic self-portrait.\n\nIn Brazil, Hercules Florence had apparently started working out a silver-salt-based paper process in 1832, later naming it \"Photographie\".\n\nMeanwhile, a British inventor, William Fox Talbot, had succeeded in making crude but reasonably light-fast silver images on paper as early as 1834 but had kept his work secret. After reading about Daguerre's invention in January 1839, Talbot published his hitherto secret method and set about improving on it. At first, like other pre-daguerreotype processes, Talbot's paper-based photography typically required hours-long exposures in the camera, but in 1840 he created the calotype process, which used the chemical development of a latent image to greatly reduce the exposure needed and compete with the daguerreotype. In both its original and calotype forms, Talbot's process, unlike Daguerre's, created a translucent negative which could be used to print multiple positive copies; this is the basis of most modern chemical photography up to the present day, as Daguerreotypes could only be replicated by rephotographing them with a camera. Talbot's famous tiny paper negative of the Oriel window in Lacock Abbey, one of a number of camera photographs he made in the summer of 1835, may be the oldest camera negative in existence.\n\nBritish chemist John Herschel made many contributions to the new field. He invented the cyanotype process, later familiar as the \"blueprint\". He was the first to use the terms \"photography\", \"negative\" and \"positive\". He had discovered in 1819 that sodium thiosulphate was a solvent of silver halides, and in 1839 he informed Talbot (and, indirectly, Daguerre) that it could be used to \"fix\" silver-halide-based photographs and make them completely light-fast. He made the first glass negative in late 1839.\nIn the March 1851 issue of \"The Chemist\", Frederick Scott Archer published his wet plate collodion process. It became the most widely used photographic medium until the gelatin dry plate, introduced in the 1870s, eventually replaced it. There are three subsets to the collodion process; the Ambrotype (a positive image on glass), the Ferrotype or Tintype (a positive image on metal) and the glass negative, which was used to make positive prints on albumen or salted paper.\n\nMany advances in photographic glass plates and printing were made during the rest of the 19th century. In 1891, Gabriel Lippmann introduced a process for making natural-color photographs based on the optical phenomenon of the interference of light waves. His scientifically elegant and important but ultimately impractical invention earned him the Nobel Prize in Physics in 1908.\n\nGlass plates were the medium for most original camera photography from the late 1850s until the general introduction of flexible plastic films during the 1890s. Although the convenience of the film greatly popularized amateur photography, early films were somewhat more expensive and of markedly lower optical quality than their glass plate equivalents, and until the late 1910s they were not available in the large formats preferred by most professional photographers, so the new medium did not immediately or completely replace the old. Because of the superior dimensional stability of glass, the use of plates for some scientific applications, such as astrophotography, continued into the 1990s, and in the niche field of laser holography, it has persisted into the 2010s.\n\nHurter and Driffield began pioneering work on the light sensitivity of photographic emulsions in 1876. Their work enabled the first quantitative measure of film speed to be devised.\n\nThe first flexible photographic roll film was marketed by George Eastman in 1885, but this original \"film\" was actually a coating on a paper base. As part of the processing, the image-bearing layer was stripped from the paper and transferred to a hardened gelatin support. The first transparent plastic roll film followed in 1889. It was made from highly flammable nitrocellulose (\"celluloid\"), now usually called \"nitrate film\".\n\nAlthough cellulose acetate or \"safety film\" had been introduced by Kodak in 1908, at first it found only a few special applications as an alternative to the hazardous nitrate film, which had the advantages of being considerably tougher, slightly more transparent, and cheaper. The changeover was not completed for X-ray films until 1933, and although safety film was always used for 16 mm and 8 mm home movies, nitrate film remained standard for theatrical 35 mm motion pictures until it was finally discontinued in 1951.\n\nFilms remained the dominant form of photography until the early 21st century when advances in digital photography drew consumers to digital formats. Although modern photography is dominated by digital users, film continues to be used by enthusiasts and professional photographers. The distinctive \"look\" of film based photographs compared to digital images is likely due to a combination of factors, including: (1) differences in spectral and tonal sensitivity (S-shaped density-to-exposure (H&D curve) with film vs. linear response curve for digital CCD sensors) (2) resolution and (3) continuity of tone.\n\nOriginally, all photography was monochrome, or \"black-and-white\". Even after color film was readily available, black-and-white photography continued to dominate for decades, due to its lower cost and its \"classic\" photographic look. The tones and contrast between light and dark areas define black-and-white photography. It is important to note that monochromatic pictures are not necessarily composed of pure blacks, whites, and intermediate shades of gray but can involve shades of one particular hue depending on the process. The cyanotype process, for example, produces an image composed of blue tones. The albumen print process first used more than years ago, produces brownish tones.\n\nMany photographers continue to produce some monochrome images, sometimes because of the established archival permanence of well-processed silver-halide-based materials. Some full-color digital images are processed using a variety of techniques to create black-and-white results, and some manufacturers produce digital cameras that exclusively shoot monochrome. Monochrome printing or electronic display can be used to salvage certain photographs taken in color which are unsatisfactory in their original form; sometimes when presented as black-and-white or single-color-toned images they are found to be more effective. Although color photography has long predominated, monochrome images are still produced, mostly for artistic reasons. Almost all digital cameras have an option to shoot in monochrome, and almost all image editing software can combine or selectively discard RGB color channels to produce a monochrome image from one shot in color.\n\nColor photography was explored beginning in the 1840s. Early experiments in color required extremely long exposures (hours or days for camera images) and could not \"fix\" the photograph to prevent the color from quickly fading when exposed to white light.\n\nThe first permanent color photograph was taken in 1861 using the three-color-separation principle first published by Scottish physicist James Clerk Maxwell in 1855. The foundation of virtually all practical color processes, Maxwell's idea was to take three separate black-and-white photographs through red, green and blue filters. This provides the photographer with the three basic channels required to recreate a color image. Transparent prints of the images could be projected through similar color filters and superimposed on the projection screen, an additive method of color reproduction. A color print on paper could be produced by superimposing carbon prints of the three images made in their complementary colors, a subtractive method of color reproduction pioneered by Louis Ducos du Hauron in the late 1860s.\n\nRussian photographer Sergei Mikhailovich Prokudin-Gorskii made extensive use of this color separation technique, employing a special camera which successively exposed the three color-filtered images on different parts of an oblong plate. Because his exposures were not simultaneous, unsteady subjects exhibited color \"fringes\" or, if rapidly moving through the scene, appeared as brightly colored ghosts in the resulting projected or printed images.\n\nImplementation of color photography was hindered by the limited sensitivity of early photographic materials, which were mostly sensitive to blue, only slightly sensitive to green, and virtually insensitive to red. The discovery of dye sensitization by photochemist Hermann Vogel in 1873 suddenly made it possible to add sensitivity to green, yellow and even red. Improved color sensitizers and ongoing improvements in the overall sensitivity of emulsions steadily reduced the once-prohibitive long exposure times required for color, bringing it ever closer to commercial viability.\n\nAutochrome, the first commercially successful color process, was introduced by the Lumière brothers in 1907. Autochrome plates incorporated a mosaic color filter layer made of dyed grains of potato starch, which allowed the three color components to be recorded as adjacent microscopic image fragments. After an Autochrome plate was reversal processed to produce a positive transparency, the starch grains served to illuminate each fragment with the correct color and the tiny colored points blended together in the eye, synthesizing the color of the subject by the additive method. Autochrome plates were one of several varieties of additive color screen plates and films marketed between the 1890s and the 1950s.\n\nKodachrome, the first modern \"integral tripack\" (or \"monopack\") color film, was introduced by Kodak in 1935. It captured the three color components in a multi-layer emulsion. One layer was sensitized to record the red-dominated part of the spectrum, another layer recorded only the green part and a third recorded only the blue. Without special film processing, the result would simply be three superimposed black-and-white images, but complementary cyan, magenta, and yellow dye images were created in those layers by adding color couplers during a complex processing procedure.\n\nAgfa's similarly structured Agfacolor Neu was introduced in 1936. Unlike Kodachrome, the color couplers in Agfacolor Neu were incorporated into the emulsion layers during manufacture, which greatly simplified the processing. Currently, available color films still employ a multi-layer emulsion and the same principles, most closely resembling Agfa's product.\n\nInstant color film, used in a special camera which yielded a unique finished color print only a minute or two after the exposure, was introduced by Polaroid in 1963.\n\nColor photography may form images as positive transparencies, which can be used in a slide projector, or as color negatives intended for use in creating positive color enlargements on specially coated paper. The latter is now the most common form of film (non-digital) color photography owing to the introduction of automated photo printing equipment. After a transition period centered around 1995–2005, color film was relegated to a niche market by inexpensive multi-megapixel digital cameras. Film continues to be the preference of some photographers because of its distinctive \"look\".\n\nIn 1981, Sony unveiled the first consumer camera to use a charge-coupled device for imaging, eliminating the need for film: the Sony Mavica. While the Mavica saved images to disk, the images were displayed on television, and the camera was not fully digital. In 1991, Kodak unveiled the DCS 100, the first commercially available digital single lens reflex camera. Although its high cost precluded uses other than photojournalism and professional photography, commercial digital photography was born.\n\nDigital imaging uses an electronic image sensor to record the image as a set of electronic data rather than as chemical changes on film. An important difference between digital and chemical photography is that chemical photography resists photo manipulation because it involves film and photographic paper, while digital imaging is a highly manipulative medium. This difference allows for a degree of image post-processing that is comparatively difficult in film-based photography and permits different communicative potentials and applications.\n\nDigital photography dominates the 21st century. More than 99% of photographs taken around the world are through digital cameras, increasingly through smartphones.\n\nSynthesis photography is part of computer-generated imagery (CGI) where the shooting process is modeled on real photography. The CGI, creating digital copies of real universe, requires a visual representation process of these universes. Synthesis photography is the application of analog and digital photography in digital space. With the characteristics of the real photography but not being constrained by the physical limits of real world, synthesis photography allows to get away from real photography.\n\nA large variety of photographic techniques and media are used in the process of capturing images for photography. These include the camera; stereoscopy; dualphotography; full-spectrum, ultraviolet and infrared media; light field photography; and other imaging techniques.\n\nThe camera is the image-forming device, and a photographic plate, photographic film or a silicon electronic image sensor is the capture medium. The respective recording medium can be the plate or film itself, or a digital magnetic or electronic memory.\n\nPhotographers control the camera and lens to \"expose\" the light recording material to the required amount of light to form a \"latent image\" (on plate or film) or RAW file (in digital cameras) which, after appropriate processing, is converted to a usable image. Digital cameras use an electronic image sensor based on light-sensitive electronics such as charge-coupled device (CCD) or complementary metal-oxide-semiconductor (CMOS) technology. The resulting digital image is stored electronically, but can be reproduced on a paper.\n\nThe camera (or 'camera obscura') is a dark room or chamber from which, as far as possible, all light is excluded except the light that forms the image. It was discovered and used in the 16th century by painters. The subject being photographed, however, must be illuminated. Cameras can range from small to very large, a whole room that is kept dark while the object to be photographed is in another room where it is properly illuminated. This was common for reproduction photography of flat copy when large film negatives were used (see Process camera).\n\nAs soon as photographic materials became \"fast\" (sensitive) enough for taking candid or surreptitious pictures, small \"detective\" cameras were made, some actually disguised as a book or handbag or pocket watch (the \"Ticka\" camera) or even worn hidden behind an Ascot necktie with a tie pin that was really the lens.\n\nThe movie camera is a type of photographic camera which takes a rapid sequence of photographs on recording medium. In contrast to a still camera, which captures a single snapshot at a time, the movie camera takes a series of images, each called a \"frame\". This is accomplished through an intermittent mechanism. The frames are later played back in a movie projector at a specific speed, called the \"frame rate\" (number of frames per second). While viewing, a person's eyes and brain merge the separate pictures together to create the illusion of motion.\n\nPhotographs, both monochrome and color, can be captured and displayed through two side-by-side images that emulate human stereoscopic vision. Stereoscopic photography was the first that captured figures in motion. While known colloquially as \"3-D\" photography, the more accurate term is stereoscopy. Such cameras have long been realized by using film and more recently in digital electronic methods (including cell phone cameras).\n\nDualphotography consists of photographing a scene from both sides of a photographic device at once (e.g. camera for back-to-back dualphotography, or two networked cameras for portal-plane dualphotography). The dualphoto apparatus can be used to simultaneously capture both the subject and the photographer, or both sides of a geographical place at once, thus adding a supplementary narrative layer to that of a single image.\n\nUltraviolet and infrared films have been available for many decades and employed in a variety of photographic avenues since the 1960s. New technological trends in digital photography have opened a new direction in full spectrum photography, where careful filtering choices across the ultraviolet, visible and infrared lead to new artistic visions.\n\nModified digital cameras can detect some ultraviolet, all of the visible and much of the near infrared spectrum, as most digital imaging sensors are sensitive from about 350 nm to 1000 nm. An off-the-shelf digital camera contains an infrared hot mirror filter that blocks most of the infrared and a bit of the ultraviolet that would otherwise be detected by the sensor, narrowing the accepted range from about 400 nm to 700 nm.\n\nReplacing a hot mirror or infrared blocking filter with an infrared pass or a wide spectrally transmitting filter allows the camera to detect the wider spectrum light at greater sensitivity. Without the hot-mirror, the red, green and blue (or cyan, yellow and magenta) colored micro-filters placed over the sensor elements pass varying amounts of ultraviolet (blue window) and infrared (primarily red and somewhat lesser the green and blue micro-filters).\n\nUses of full spectrum photography are for fine art photography, geology, forensics and law enforcement.\n\nDigital methods of image capture and display processing have enabled the new technology of \"light field photography\" (also known as synthetic aperture photography). This process allows focusing at various depths of field to be selected \"after\" the photograph has been captured. As explained by Michael Faraday in 1846, the \"light field\" is understood as 5-dimensional, with each point in 3-D space having attributes of two more angles that define the direction of each ray passing through that point.\n\nThese additional vector attributes can be captured optically through the use of microlenses at each pixel point within the 2-dimensional image sensor. Every pixel of the final image is actually a selection from each sub-array located under each microlens, as identified by a post-image capture focus algorithm.\n\nBesides the camera, other methods of forming images with light are available. For instance, a photocopy or xerography machine forms permanent images but uses the transfer of static electrical charges rather than photographic medium, hence the term electrophotography. Photograms are images produced by the shadows of objects cast on the photographic paper, without the use of a camera. Objects can also be placed directly on the glass of an image scanner to produce digital pictures.\n\nAn amateur photographer is one who practices photography as a hobby/passion and not necessarily for profit. The quality of some amateur work is comparable to that of many professionals and may be highly specialized or eclectic in choice of subjects. Amateur photography is often pre-eminent in photographic subjects which have little prospect of commercial use or reward. Amateur photography grew during the late 19th century due to the popularization of the hand-held camera. Nowadays it has spread widely through social media and is carried out throughout different platforms and equipment, switching to the use of cell phone as a key tool for making photography more accessible to everyone.\n\nCommercial photography is probably best defined as any photography for which the photographer is paid for images rather than works of art. In this light, money could be paid for the subject of the photograph or the photograph itself. Wholesale, retail, and professional uses of photography would fall under this definition. The commercial photographic world could include:\nThe market for photographic services demonstrates the aphorism \"A picture is worth a thousand words\", which has an interesting basis in the history of photography. Magazines and newspapers, companies putting up Web sites, advertising agencies and other groups pay for photography.\n\nMany people take photographs for commercial purposes. Organizations with a budget and a need for photography have several options: they can employ a photographer directly, organize a public competition, or obtain rights to stock photographs. Photo stock can be procured through traditional stock giants, such as Getty Images or Corbis; smaller microstock agencies, such as Fotolia; or web marketplaces, such as Cutcaster.\n\nDuring the 20th century, both fine art photography and documentary photography became accepted by the English-speaking art world and the gallery system. In the United States, a handful of photographers, including Alfred Stieglitz, Edward Steichen, John Szarkowski, F. Holland Day, and Edward Weston, spent their lives advocating for photography as a fine art.\nAt first, fine art photographers tried to imitate painting styles. This movement is called Pictorialism, often using soft focus for a dreamy, 'romantic' look. In reaction to that, Weston, Ansel Adams, and others formed the Group f/64 to advocate 'straight photography', the photograph as a (sharply focused) thing in itself and not an imitation of something else.\n\nThe aesthetics of photography is a matter that continues to be discussed regularly, especially in artistic circles. Many artists argued that photography was the mechanical reproduction of an image. If photography is authentically art, then photography in the context of art would need redefinition, such as determining what component of a photograph makes it beautiful to the viewer. The controversy began with the earliest images \"written with light\"; Nicéphore Niépce, Louis Daguerre, and others among the very earliest photographers were met with acclaim, but some questioned if their work met the definitions and purposes of art.\n\nClive Bell in his classic essay \"Art\" states that only \"significant form\" can distinguish art from what is not art.\n\nOn 7 February 2007, Sotheby's London sold the 2001 photograph \"99 Cent II Diptychon\" for an unprecedented $3,346,456 to an anonymous bidder, making it the most expensive at the time.\n\nConceptual photography turns a concept or idea into a photograph. Even though what is depicted in the photographs are real objects, the subject is strictly abstract.\n\nPhotojournalism is a particular form of photography (the collecting, editing, and presenting of news material for publication or broadcast) that employs images in order to tell a news story. It is now usually understood to refer only to still images, but in some cases the term also refers to video used in broadcast journalism. Photojournalism is distinguished from other close branches of photography (e.g., documentary photography, social documentary photography, street photography or celebrity photography) by complying with a rigid ethical framework which demands that the work be both honest and impartial whilst telling the story in strictly journalistic terms. Photojournalists create pictures that contribute to the news media, and help communities connect with one other. Photojournalists must be well informed and knowledgeable about events happening right outside their door. They deliver news in a creative format that is not only informative, but also entertaining.\n\nThe camera has a long and distinguished history as a means of recording scientific phenomena from the first use by Daguerre and Fox-Talbot, such as astronomical events (eclipses for example), small creatures and plants when the camera was attached to the eyepiece of microscopes (in photomicroscopy) and for macro photography of larger specimens. The camera also proved useful in recording crime scenes and the scenes of accidents, such as the Wootton bridge collapse in 1861. The methods used in analysing photographs for use in legal cases are collectively known as forensic photography. Crime scene photos are taken from three vantage point. The vantage points are overview, mid-range, and close-up.\n\nIn 1845 Francis Ronalds, the Honorary Director of the Kew Observatory, invented the first successful camera to make continuous recordings of meteorological and geomagnetic parameters. Different machines produced 12- or 24- hour photographic traces of the minute-by-minute variations of atmospheric pressure, temperature, humidity, atmospheric electricity, and the three components of geomagnetic forces. The cameras were supplied to numerous observatories around the world and some remained in use until well into the 20th century. Charles Brooke a little later developed similar instruments for the Greenwich Observatory.\n\nScience uses image technology that has derived from the design of the Pin Hole camera. X-Ray machines are similar in design to Pin Hole cameras with high-grade filters and laser radiation.\nPhotography has become ubiquitous in recording events and data in science and engineering, and at crime scenes or accident scenes. The method has been much extended by using other wavelengths, such as infrared photography and ultraviolet photography, as well as spectroscopy. Those methods were first used in the Victorian era and improved much further since that time.\n\nThe first photographed atom was discovered in 2012 by physicists at Griffith University, Australia. They used an electric field to trap an \"Ion\" of the element, Ytterbium. The image was recorded on a CCD, an electronic photographic film.\n\nThere are many ongoing questions about different aspects of photography. In her writing \"On Photography\" (1977), Susan Sontag discusses concerns about the objectivity of photography. This is a highly debated subject within the photographic community. Sontag argues, \"To photograph is to appropriate the thing photographed. It means putting one's self into a certain relation to the world that feels like knowledge, and therefore like power.\" Photographers decide what to take a photo of, what elements to exclude and what angle to frame the photo, and these factors may reflect a particular socio-historical context. Along these lines, it can be argued that photography is a subjective form of representation.\n\nModern photography has raised a number of concerns on its effect on society. In Alfred Hitchcock's \"Rear Window\" (1954), the camera is presented as promoting voyeurism. 'Although the camera is an observation station, the act of photographing is more than passive observing'.\nThe camera doesn't rape or even possess, though it may presume, intrude, trespass, distort, exploit, and, at the farthest reach of metaphor, assassinate – all activities that, unlike the sexual push and shove, can be conducted from a distance, and with some detachment.\nDigital imaging has raised ethical concerns because of the ease of manipulating digital photographs in post-processing. Many photojournalists have declared they will not crop their pictures or are forbidden from combining elements of multiple photos to make \"photomontages\", passing them as \"real\" photographs. Today's technology has made image editing relatively simple for even the novice photographer. However, recent changes of in-camera processing allow digital fingerprinting of photos to detect tampering for purposes of forensic photography.\n\nPhotography is one of the new media forms that changes perception and changes the structure of society. Further unease has been caused around cameras in regards to desensitization. Fears that disturbing or explicit images are widely accessible to children and society at large have been raised. Particularly, photos of war and pornography are causing a stir. Sontag is concerned that \"to photograph is to turn people into objects that can be symbolically possessed.\" Desensitization discussion goes hand in hand with debates about censored images. Sontag writes of her concern that the ability to censor pictures means the photographer has the ability to construct reality.\n\nOne of the practices through which photography constitutes society is tourism. Tourism and photography combine to create a \"tourist gaze\"\nin which local inhabitants are positioned and defined by the camera lens. However, it has also been argued that there exists a \"reverse gaze\" through which indigenous photographees can position the tourist photographer as a shallow consumer of images.\n\nAdditionally, photography has been the topic of many songs in popular culture.\n\nPhotography is both restricted as well as protected by the law in many jurisdictions. Protection of photographs is typically achieved through the granting of copyright or moral rights to the photographer. In the United States, photography is protected as a First Amendment right and anyone is free to photograph anything seen in public spaces as long as it is in plain view. In the UK a recent law (Counter-Terrorism Act 2008) increases the power of the police to prevent people, even press photographers, from taking pictures in public places.\n\n\n\n\n\n\n",
                "Digital camera\n\nA digital camera or digicam is a camera that captures photographs in digital memory. Most cameras produced today are digital, and while there are still dedicated compact cameras on the market, the use of dedicated digital cameras is dwindling, as digital cameras are now incorporated into many devices ranging from mobile devices to vehicles. However, expensive, high-end, high-definition dedicated cameras are still commonly used by professionals.\n\nDigital and movie cameras share an optical system, typically using a lens with a variable diaphragm to focus light onto an image pickup device. The diaphragm and shutter admit the correct amount of light to the imager, just as with film but the image pickup device is electronic rather than chemical. However, unlike film cameras, digital cameras can display images on a screen immediately after being recorded, and store and delete images from memory. Many digital cameras can also record moving videos with sound. Some digital cameras can crop and stitch pictures and perform other elementary image editing.\n\nThe history of the digital camera began with Eugene F. Lally of the Jet Propulsion Laboratory, who was thinking about how to use a mosaic photosensor to capture digital images. His 1961 idea was to take pictures of the planets and stars while travelling through space to give information about the astronauts' position. As with Texas Instruments employee Willis Adcock's filmless camera (US patent 4,057,830) in 1972, the technology had yet to catch up with the concept.\nSteven Sasson as an engineer at Eastman Kodak invented and built the first electronic camera using a charge-coupled device image sensor in 1975. Earlier ones used a camera tube; later ones digitized the signal. Early uses were mainly military and scientific; followed by medical and news applications.\n\nIn 1986, Japanese company Nikon introduced the first digital single-lens reflex (DSLR) camera, the Nikon SVC. In the mid-to-late 1990s, DSLR cameras became common among consumers. By the mid-2000s, DSLR cameras had largely replaced film cameras.\n\nIn 2000, Sharp introduced the world's first digital camera phone, the J-SH04 J-Phone, in Japan. By the mid-2000s, higher-end cell phones had an integrated digital camera. By the beginning of the 2010s, almost all smartphones had an integrated digital camera.\n\nThe two major types of digital image sensor are CCD and CMOS. A CCD sensor has one amplifier for all the pixels, while each pixel in a CMOS active-pixel sensor has its own amplifier. Compared to CCDs, CMOS sensors use less power. Cameras with a small sensor use a back-side-illuminated CMOS (BSI-CMOS) sensor. Overall final image quality is more dependent on the image processing capability of the camera, than on sensor type.\n\nThe resolution of a digital camera is often limited by the image sensor that turns light into discrete signals. The brighter the image at a given point on the sensor, the larger the value that is read for that pixel.\nDepending on the physical structure of the sensor, a color filter array may be used, which requires demosaicing to recreate a full-color image.\nThe number of pixels in the sensor determines the camera's \"pixel count\".\nIn a typical sensor, the pixel count is the product of the number of rows and the number of columns. For example, a 1,000 by 1,000 pixel sensor would have 1,000,000 pixels, or 1 megapixel.\n\nFinal quality of an image depends on all optical transformations in the chain of producing the image. Carl Zeiss points out that the weakest link in an optical chain determines the final image quality. In case of a digital camera, a simplistic way of expressing it is that the lens determines the maximum sharpness of the image while the image sensor determines the maximum resolution. The illustration on the right can be said to compare a lens with very poor sharpness on a camera with high resolution, to a lens with good sharpness on a camera with lower resolution.\n\nSince the first digital backs were introduced, there have been three main methods of capturing the image, each based on the hardware configuration of the sensor and color filters.\n\n\"Single-shot\" capture systems use either one sensor chip with a Bayer filter mosaic, or three separate image sensors (one each for the primary additive colors red, green, and blue) which are exposed to the same image via a beam splitter (see Three-CCD camera).\n\n\"Multi-shot\" exposes the sensor to the image in a sequence of three or more openings of the lens aperture. There are several methods of application of the multi-shot technique. The most common originally was to use a single image sensor with three filters passed in front of the sensor in sequence to obtain the additive color information. Another multiple shot method is called Microscanning. This method uses a single sensor chip with a Bayer filter and physically moved the sensor on the focus plane of the lens to construct a higher resolution image than the native resolution of the chip. A third version combined the two methods without a Bayer filter on the chip.\n\nThe third method is called \"scanning\" because the sensor moves across the focal plane much like the sensor of an image scanner. The \"linear\" or \"tri-linear\" sensors in scanning cameras utilize only a single line of photosensors, or three lines for the three colors. Scanning may be accomplished by moving the sensor (for example, when using color co-site sampling) or by rotating the whole camera. A digital rotating line camera offers images of very high total resolution.\n\nThe choice of method for a given capture is determined largely by the subject matter. It is usually inappropriate to attempt to capture a subject that moves with anything but a single-shot system. However, the higher color fidelity and larger file sizes and resolutions available with multi-shot and scanning backs make them attractive for commercial photographers working with stationary subjects and large-format photographs.\n\nImprovements in single-shot cameras and image file processing at the beginning of the 21st century made single shot cameras almost completely dominant, even in high-end commercial photography.\n\nMost current consumer digital cameras use a Bayer filter mosaic in combination with an optical anti-aliasing filter to reduce the aliasing due to the reduced sampling of the different primary-color images.\nA demosaicing algorithm is used to interpolate color information to create a full array of RGB image data.\n\nCameras that use a beam-splitter single-shot 3CCD approach, three-filter multi-shot approach, color co-site sampling or Foveon X3 sensor do not use anti-aliasing filters, nor demosaicing.\n\nFirmware in the camera, or a software in a raw converter program such as Adobe Camera Raw, interprets the raw data from the sensor to obtain a full color image, because the RGB color model requires three intensity values for each pixel: one each for the red, green, and blue (other color models, when used, also require three or more values per pixel).\nA single sensor element cannot simultaneously record these three intensities, and so a color filter array (CFA) must be used to selectively filter a particular color for each pixel.\n\nThe Bayer filter pattern is a repeating 2x2 mosaic pattern of light filters, with green ones at opposite corners and red and blue in the other two positions. The high proportion of green takes advantage of properties of the human visual system, which determines brightness mostly from green and is far more sensitive to brightness than to hue or saturation. Sometimes a 4-color filter pattern is used, often involving two different hues of green. This provides potentially more accurate color, but requires a slightly more complicated interpolation process.\n\nThe color intensity values not captured for each pixel can be interpolated from the values of adjacent pixels which represent the color being calculated.\n\nCameras with digital image sensors that are smaller than the typical 35mm film size have a smaller field or angle of view when used with a lens of the same focal length. This is because angle of view is a function of both focal length and the sensor or film size used.\n\nThe crop factor is relative to the 35mm film format. If a smaller sensor is used, as in most digicams, the field of view is cropped by the sensor to smaller than the 35mm full-frame format's field of view. This narrowing of the field of view may be described as crop factor, a factor by which a longer focal length lens would be needed to get the same field of view on a 35mm film camera. Full-frame digital SLRs utilize a sensor of the same size as a frame of 35mm film.\n\nCommon values for field of view crop in DSLRs using active pixel sensors include 1.3x for some Canon (APS-H) sensors, 1.5x for Sony APS-C sensors used by Nikon, Pentax and Konica Minolta and for Fujifilm sensors, 1.6 (APS-C) for most Canon sensors, ~1.7x for Sigma's Foveon sensors and 2x for Kodak and Panasonic 4/3-inch sensors currently used by Olympus and Panasonic. Crop factors for non-SLR consumer compact and bridge cameras are larger, frequently 4x or more.\n\nDigital cameras come in a wide range of sizes, prices and capabilities. In addition to general purpose digital cameras, specialized cameras including multispectral imaging equipment and astrographs are used for scientific, military, medical and other special purposes.\n\nCompact cameras are intended to be portable (pocketable) and are particularly suitable for casual \"snapshots\".\n\nMany incorporate a retractable lens assembly that provides optical zoom. In most models, an auto actuating lens cover protects the lens from elements. Most ruggedized or water-resistant models do not retract, and most with superzoom capability do not retract fully.\n\nCompact cameras are usually designed to be easy to use. Almost all include an automatic mode, or \"auto mode\", which automatically makes all camera settings for the user. Some also have manual controls. Compact digital cameras typically contain a small sensor which trades-off picture quality for compactness and simplicity; images can usually only be stored using lossy compression (JPEG). Most have a built-in flash usually of low power, sufficient for nearby subjects. A few high end compact digital cameras have a hotshoe for connecting to an external flash. Live preview is almost always used to frame the photo on an integrated LCD. In addition to being able to take still photographs almost all compact cameras have the ability to record video.\n\nCompacts often have macro capability and zoom lenses, but the zoom range (up to 30x) is generally enough for candid photography but less than is available on bridge cameras (more than 60x), or the interchangeable lenses of DSLR cameras available at a much higher cost. Autofocus systems in compact digital cameras generally are based on a contrast-detection methodology using the image data from the live preview feed of the main imager. Some compact digital cameras use a hybrid autofocus system similar to what is commonly available on DSLRs. Some high end travel compact cameras have 30x optical zoom have full manual control with lens ring, electronic viewfinder, Hybrid Optical Image Stabilization, built-in flash, Full HD 60p, RAW, burst shooting up to 10fps, built-in Wi-Fi with NFC and GPS altogether.\n\nTypically, compact digital cameras incorporate a nearly silent leaf shutter into the lens but play a simulated camera sound for skeuomorphic purposes.\n\nFor low cost and small size, these cameras typically use image sensor formats with a diagonal between 6 and 11 mm, corresponding to a crop factor between 7 and 4. This gives them weaker low-light performance, greater depth of field, generally closer focusing ability, and smaller components than cameras using larger sensors. Some cameras use a larger sensor including, at the high end, a pricey full-frame sensor compact camera, such as Sony Cyber-shot DSC-RX1, but have capability near that of a DSLR.\n\nA variety of additional features are available depending on the model of the camera. Such features include ones such as GPS, compass, barometer and altimeter for above mean sea level or under(water) mean sea level. and some are rugged and waterproof.\n\nStarting in 2011, some compact digital cameras can take 3D still photos. These 3D compact stereo cameras can capture 3D panoramic photos with dual lens or even single lens for play back on a 3D TV.\n\nIn 2013, Sony released two add-on camera models without display, to be used with a smartphone or tablet, controlled by a mobile application via WiFi.\n\nRugged compact cameras typically include protection against submersion, hot and cold conditions, shock and pressure. Terms used to describe such properties include waterproof, freezeproof, heatproof, shockproof and crushproof, respectively. Nearly all major camera manufacturers have at least one product in this category. Some are waterproof to a considerable depth up to 82 feet (27 m); others only 10 feet (3m), but only a few will float. Ruggeds often lack some of the features of ordinary compact camera, but they have video capability and the majority can record sound. Most have image stabilization and built-in flash. Touchscreen LCD and GPS do not work under water.\n\nGoPro and other brands offer action cameras which are rugged, small and can be easily attached to helmet, arm, bicycle, etc. Most have wide angle and fixed focus, and can take still pictures and video, typically with sound.\n\nThe rising popularity of action cameras is in line with many people desiring to share photos or videos in social media. Many competitive manufacturers of action cameras results in many options and lowered, competitive prices, and nowadays, cameras are sold bundled with waterproof housings, accessories, and mountings compatible with the popular GoPro.\n\nThe 360-degree camera can take picture or video 360 degrees using two lenses back-to-back and shooting at the same time. Some of the cameras are Ricoh Theta S, Nikon Keymission 360 and Samsung Gear 360. Nico360 was launched in 2016 and claimed as the world's smallest 360-degree camera with size 46 x 46 x 28 mm (1.8 x 1.8 x 1.1 in) and price less than $200. With virtual reality mode built-in stitching, Wifi, and Bluetooth, live streaming can be done. Due to it also being water resistant, the Nico360 can be used as action camera.\n\nThere are tend that action cameras have capabilities to shoot 360 degrees with at least 4K resolution.\n\nBridge cameras physically resemble DSLRs, and are sometimes called DSLR-shape or DSLR-like. They provide some similar features but, like compacts, they use a fixed lens and a small sensor. Some compact cameras have also PSAM mode. Most use live preview to frame the image. Their usual autofocus is by the same contrast-detect mechanism as compacts, but many bridge cameras have a manual focus mode and some have a separate focus ring for greater control.\n\nBig physical size and small sensor allow superzoom and wide aperture. Bridgcams generally include an image stabilization system to enable longer handheld exposures, sometimes better than DSLR for low light condition.\n\nAs of 2014, bridge cameras come in two principal classes in terms of sensor size, firstly the more traditional 1/2.3\" sensor (as measured by image sensor format) which gives more flexibility in lens design and allows for handholdable zoom from 20 to 24mm (35mm equivalent) wide angle all the way up to over 1000 mm supertele, and secondly a 1\" sensor that allows better image quality particularly in low light (higher ISO) but puts greater constraints on lens design, resulting in zoom lenses that stop at 200mm (constant aperture, e.g. Sony RX10) or 400mm (variable aperture, e.g. Panasonic Lumix FZ1000) equivalent, corresponding to an optical zoom factor of roughly 10 to 15.\n\nSome bridge cameras have a lens thread to attach accessories such as wide-angle or telephoto converters as well as filters such as UV or Circular Polarizing filter and lens hoods. The scene is composed by viewing the display or the electronic viewfinder (EVF). Most have a slightly longer shutter lag than a DSLR. Many of these cameras can store images in a raw format in addition to supporting JPEG. The majority have a built-in flash, but only a few have a hotshoe.\n\nIn bright sun, the quality difference between a good compact camera and a digital SLR is minimal but bridge cameras are more portable, cost less and have a greater zoom ability. Thus a bridge camera may better suit outdoor daytime activities, except when seeking professional-quality photos.\n\nIn late 2008, a new type of camera emerged called mirrorless interchangeable-lens camera (MILC), which uses various sensors and offers lens interchangeability. These are simpler and more compact than DSLRs due to not having a lens reflex system. MILC camera models are available with various sensor sizes including: a small 1/2.3 inch sensor, as is commonly used in bridge cameras such as the original Pentax Q (more recent Pentax Q versions have a slightly larger 1/1.7 inch sensor); a 1-inch sensor; a Micro Four Thirds sensor; an APS-C sensor such as the Sony NEX series, Fujifilm X series, Pentax K-01, and Canon EOS M; and some, such as the Sony α7, use a full frame (35 mm) sensor and even Hasselblad X1D is the first medium format MILC. Some MILC cameras have a separate electronic viewfinder. In other cameras the back display is used as a viewfinder in same way as in compact cameras. Disadvantage of MILC over DSLR is battery energy consume due to high energy consume of electronic viewfinder.\n\nOlympus and Panasonic released many Micro Four Thirds cameras with interchangeable lenses which are fully compatible each other without any adapter, while the others have proprietary mounts. In 2014, Kodak released its first Micro Four Third system camera.\n\n, MILC cameras are available which appeal to both amateurs and professionals.\n\nWhile most digital cameras with interchangeable lenses feature a lens-mount of some kind, there are also a number of modular cameras, where the shutter and sensor are incorporated into the lens module.\n\nThe first such modular camera was the Minolta Dimâge V in 1996, followed by the Minolta Dimâge EX 1500 in 1998 and the Minolta MetaFlash 3D 1500 in 1999. In 2009, Ricoh released the Ricoh GXR modular camera.\n\nAt CES 2013, Sakar International announced the Polaroid iM1836, an 18 MP camera with 1\"-sensor with interchangeable sensor-lens. An adapter for Micro Four Thirds, Nikon and K-mount lenses was planned to ship with the camera.\n\nThere are also a number of add-on camera modules for smartphones called lens-style cameras (lens camera). They contain all components of a digital camera in a module, but lack a viewfinder, display and most of the controls. Instead they can be mounted to a smartphone and use its display and controls. Lens-style cameras include:\n\nDigital single-lens reflex cameras (DSLR) use a reflex mirror that can reflect the light and also can swivel from one position to another position and back to initial position. By default, the reflex mirror is set 45 degree from horizontal, blocks the light to the sensor and reflects light from the lens to penta-mirror/prism at the DSLR camera and after some reflections arrives at the viewfinder. The reflex mirror is pulled out horizontally below the penta-mirror/prism when shutter release is fully pressed, so the viewfinder will be dark and the light/image can directly strike the sensor at the time of exposure (speed setting).\n\nAutofocus is accomplished using sensors in the mirror box. Some DSLRs have a \"live view\" mode that allows framing using the screen with image from the sensor.\n\nThese cameras have much larger sensors than the other types, typically 18 mm to 36 mm on the diagonal (crop factor 2, 1.6, or 1). The larger sensor permits more light to be received by each pixel; this, combined with the relatively large lenses provides superior low-light performance. For the same field of view and the same aperture, a larger sensor gives shallower focus.\nThey use interchangeable lenses for versatility. Usually some lenses are made for digital SLR use only, but recent trend the lenses can also be used in detachable lens video camera with or without adapter.\n\nA DSLT uses a fixed translucent mirror instead of a moving reflex mirror as in DSLR. A translucent mirror or transmissive mirror or semi-transparent mirror is a mirror which reflects the light to two things at the same time. It reflects it along the path to a pentaprism/pentamirror which then goes to an optical view finder (OVF) as is done with a reflex mirror in DSLR cameras. The translucent mirror also sends light along a second path to the sensor. The total amount of light is not changed, just some of the light travels one path and some of it travels the other. The consequences are that DSLT cameras should shoot a half stop differently from DSL. One advantage of using a DSLT camera is the blind moments a DSLR user experiences while the reflecting mirror is moved to send the light to the sensor instead of the viewfinder do not exist for DSLT cameras. Because there is no time at which light is not traveling along both paths, DSLT cameras get the benefit of continuous auto-focus tracking. This is especially beneficial for burst mode shooting in low-light conditions and also for tracking when taking video.\n\nUntil early 2014, only Sony had released DSLT cameras. By March 2014, Sony had released more DSLTs than DSLRs with a relatively complete lenses line-up.\n\nA rangefinder is a device to measure subject distance, with the intent to adjust the focus of a camera's objective lens accordingly (open-loop controller). The rangefinder and lens focusing mechanism may or may not be coupled. In common parlance, the term \"rangefinder camera\" is interpreted very narrowly to denote manual-focus cameras with a visually-read out optical rangefinder based on parallax. Most digital cameras achieve focus through analysis of the image captured by the objective lens and distance estimation, if it is provided at all, is only a byproduct of the focusing process (closed-loop controller).\n\nA line-scan camera traditionally has a single row of pixel sensors, instead of a matrix of them. The lines are continuously fed to a computer that joins them to each other and makes an image. This is most commonly done by connecting the camera output to a frame grabber which resides in a PCI slot of an industrial computer. The frame grabber acts to buffer the image and sometimes provide some processing before delivering to the computer software for processing.\n\nMultiple rows of sensors may be used to make colored images, or to increase sensitivity by TDI (Time delay and integration).\nMany industrial applications require a wide field of view. Traditionally maintaining consistent light over large 2D areas is quite difficult. With a line scan camera all that is necessary is to provide even illumination across the “line” currently being viewed by the camera. This makes possible sharp pictures of objects that pass the camera at high speed.\n\nSuch cameras are also commonly used to make photo finishes, to determine the winner when multiple competitors cross the finishing line at nearly the same time. They can also be used as industrial instruments for analyzing fast processes.\n\nLinescan cameras are also extensively used in imaging from satellites (see push broom scanner). In this case the row of sensors is perpendicular to the direction of satellite motion. Linescan cameras are widely used in scanners. In this case, the camera moves horizontally.\n\nStand alone cameras can be used as remote camera. One kind weighs 2.31 ounces (65.5 g), with a periscope shape, IPx7 water-resistance and dust-resistance rating and can be enhanced to IPx8 by using a cap. They have no viewfinder or LCD. Lens is a 146 degree wide angle or standard lens, with fixed focus. It can have a microphone and speaker, And it can take photos and video. As a remote camera, a phone app using Android or iOS is needed to send live video, change settings, take photos, or use time lapse.\n\nMany devices have a built-in digital camera, including, for example, smartphones, mobile phones, PDAs and laptop computers. Built-in cameras generally store the images in the JPEG file format.\n\nMobile phones incorporating digital cameras were introduced in Japan in 2001 by J-Phone. In 2003 camera phones outsold stand-alone digital cameras, and in 2006 they outsold film and digital stand-alone cameras. Five billion camera phones were sold in five years, and by 2007 more than half of the installed base of all mobile phones were camera phones. Sales of separate cameras peaked in 2008.\n\nSales of traditional digital cameras have declined due to the increasing use of smartphones for casual photography, which also enable easier manipulation and sharing of photos through the use of apps and web-based services. \"Bridge cameras\", in contrast, have held their ground with functionality that most smartphone cameras lack, such as optical zoom and other advanced features. DSLRs have also lost ground to Mirrorless interchangeable-lens camera (MILC)s offering the same sensor size in a smaller camera. A few expensive ones use a full-frame sensor as DSLR professional cameras.\n\nIn response to the convenience and flexibility of smartphone cameras, some manufacturers produced \"smart\" digital cameras that combine features of traditional cameras with those of a smartphone. In 2012, Nikon and Samsung released the Coolpix S800c and Galaxy Camera, the first two digital cameras to run the Android operating system. Since this software platform is used in many smartphones, they can integrate with services (such as e-mail attachments, social networks and photo sharing sites) as smartphones do, and use other Android-compatible software as well.\n\nIn an inversion, some phone makers have introduced smartphones with cameras designed to resemble traditional digital cameras. Nokia released the 808 PureView and Lumia 1020 in 2012 and 2013; the two devices respectively run the Symbian and Windows Phone operating systems, and both include a 41-megapixel camera (along with a camera grip attachment for the latter). Similarly, Samsung introduced the Galaxy S4 Zoom, having a 16-megapixel camera and 10x optical zoom, combining traits from the Galaxy S4 Mini with the Galaxy Camera. Furthermore, Panasonic Lumic DMC-CM1 is an Android KitKat 4.4 smartphone with 20MP, 1\" sensor, the largest sensor for a smartphone ever, with Leica fixed lens equivalent of 28mm at F2.8, can take RAW image and 4K video, has 21mm thickness.\n\nLight-field cameras were introduced in 2013 with one consumer product and several professional ones.\n\nAfter a big dip of sales in 2012, consumer digital camera sales declined again in 2013 by 36 percent. In 2011, compact digital cameras sold 10 million per month. In 2013, sales fell to about 4 million per month. DSLR and MILC sales also declined in 2013 by 10–15% after almost ten years of double digit growth.\nWorldwide unit sales of digital cameras is continuously declining from 148 million in 2011 to 58 million in 2015 and tends to decrease more in the following years.\n\nFilm camera sold got the peak at 36.671 million units in 1997 and digital camera sold began in 1999. In 2008, film camera market was dead and digital camera sold got the peak by 121.463 million units in 2010. In 2002, cell phone with camera has been introduced and in 2003 the cell phone with camera sold 80 million units per year. In 2011 the cell phone with camera sold hundreds of millions per year, when digital camera sold initialized to decline. In 2015, digital camera sold is 35.395 million units or only less than a third of digital camera sold number in a peak and also slightly less than film camera sold number in a peak.\n\nMany digital cameras can connect directly to a computer to transfer data:-\n\n\n\nA common alternative is the use of a card reader which may be capable of reading several types of storage media, as well as high speed transfer of data to the computer. Use of a card reader also avoids draining the camera battery during the download process. An external card reader allows convenient direct access to the images on a collection of storage media. But if only one storage card is in use, moving it back and forth between the camera and the reader can be inconvenient. Many computers have a card reader built in, at least for SD cards.\n\nMany modern cameras support the PictBridge standard, which allows them to send data directly to a PictBridge-capable computer printer without the need for a computer.\n\nWireless connectivity can also provide for printing photos without a cable connection.\nAn \"instant-print camera\", is a digital camera with a built-in printer. This confers a similar functionality as an instant camera which uses instant film to quickly generate a physical photograph. Such non-digital cameras were popularized by Polaroid in 1972.\n\nMany digital cameras include a video output port. Usually sVideo, it sends a standard-definition video signal to a television, allowing the user to show one picture at a time. Buttons or menus on the camera allow the user to select the photo, advance from one to another, or automatically send a \"slide show\" to the TV.\n\nHDMI has been adopted by many high-end digital camera makers, to show photos in their high-resolution quality on an HDTV.\n\nIn January 2008, Silicon Image announced a new technology for sending video from mobile devices to a television in digital form. MHL sends pictures as a video stream, up to 1080p resolution, and is compatible with HDMI.\n\nSome DVD recorders and television sets can read memory cards used in cameras; alternatively several types of flash card readers have TV output capability.\n\nCameras can be equipped with a varying amount of environmental sealing to provide protection against splashing water, moisture (humidity and fog), dust and sand, or complete waterproofness to a certain depth and for a certain duration. The latter is one of the approaches to allow underwater photography, the other approach being the use of waterproof housings. Many waterproof digital cameras are also shockproof and resistant to low temperatures.\n\nMany digital cameras have preset modes for different applications. Within the constraints of correct exposure various parameters can be changed, including exposure, aperture, focusing, light metering, white balance, and equivalent sensitivity. For example, a portrait might use a wider aperture to render the background out of focus, and would seek out and focus on a human face rather than other image content.\n\nMany camera phones and most stand alone digital cameras store image data in flash memory cards or other removable media. Most stand-alone cameras use SD format, while a few use CompactFlash or other types. In January 2012, a faster XQD card format was announced. In early 2014, some high end cameras have two hot-swapable memory slots. Photographers can swap one of the memory card with camera-on. Each memory slot can accept either Compact Flash or SD Card. All new Sony cameras also have two memory slots, one for its Memory Stick and one for SD Card, but not hot-swapable.\n\nA few cameras used other removable storage such as Microdrives (very small hard disk drives), CD single (185 MB), and 3.5\" floppy disks. Other unusual formats include:\nMost manufacturers of digital cameras do not provide drivers and software to allow their cameras to work with Linux or other free software. Still, many cameras use the standard USB storage protocol, and are thus easily usable. Other cameras are supported by the gPhoto project.\n\nThe Joint Photography Experts Group standard (JPEG) is the most common file format for storing image data. Other file types include Tagged Image File Format (TIFF) and various Raw image formats.\n\nMany cameras, especially high-end ones, support a raw image format. A raw image is the unprocessed set of pixel data directly from the camera's sensor, often saved in a proprietary format. Adobe Systems has released the DNG format, a royalty-free raw image format used by at least 10 camera manufacturers.\n\nRaw files initially had to be processed in specialized image editing programs, but over time many mainstream editing programs, such as Google's Picasa, have added support for raw images. Rendering to standard images from raw sensor data allows more flexibility in making major adjustments without losing image quality or retaking the picture.\n\nFormats for movies are AVI, DV, MPEG, MOV (often containing motion JPEG), WMV, and ASF (basically the same as WMV). Recent formats include MP4, which is based on the QuickTime format and uses newer compression algorithms to allow longer recording times in the same space.\n\nOther formats that are used in cameras (but not for pictures) are the Design Rule for Camera Format (DCF), an ISO specification, used in almost all camera since 1998, which defines an internal file structure and naming. Also used is the Digital Print Order Format (DPOF), which dictates what order images are to be printed in and how many copies. The DCF 1998 defines a logical file system with 8.3 filenames and makes the usage of either FAT12, FAT16, FAT32 or exFAT mandatory for its physical layer in order to maximize platform interoperability.\n\nMost cameras include Exif data that provides metadata about the picture. Exif data may include aperture, exposure time, focal length, date and time taken, and location.\n\nDigital cameras have become smaller over time, resulting in an ongoing need to develop a battery small enough to fit in the camera and yet able to power it for a reasonable length of time.\n\nDigital cameras utilize either proprietary or standard consumer batteries. , most cameras use proprietary lithium-ion batteries while some use standard AA batteries or primarily use a proprietary Lithium-ion rechargeable battery pack but have an optional AA battery holder available.\n\nThe most common class of battery used in digital cameras is proprietary battery formats. These are built to a manufacturer's custom specifications. Almost all proprietary batteries are lithium-ion. In addition to being available from the OEM, aftermarket replacement batteries are commonly available for most camera models.\n\nDigital cameras that utilize off-the-shelf batteries are typically designed to be able to use both single-use disposable and rechargeable batteries, but not with both types in use at the same time. The most common off-the-shelf battery size used is AA. CR2, CR-V3 batteries, and AAA batteries are also used in some cameras. The CR2 and CR-V3 batteries are lithium based, intended for a single use. Rechargeable RCR-V3 lithium-ion batteries are also available as an alternative to non-rechargeable CR-V3 batteries.\n\nSome battery grips for DSLRs come with a separate holder to accommodate AA cells as an external power source.\n\nWhen digital cameras became common, many photographers asked whether their film cameras could be converted to digital. The answer was yes and no. For the majority of 35 mm film cameras the answer is no, the reworking and cost would be too great, especially as lenses have been evolving as well as cameras. For most a conversion to digital, to give enough space for the electronics and allow a liquid crystal display to preview, would require removing the back of the camera and replacing it with a custom built digital unit.\n\nMany early professional SLR cameras, such as the Kodak DCS series, were developed from 35 mm film cameras. The technology of the time, however, meant that rather than being digital \"backs\" the bodies of these cameras were mounted on large, bulky digital units, often bigger than the camera portion itself. These were factory built cameras, however, not aftermarket conversions.\n\nA notable exception is the Nikon E2 and Nikon E3, using additional optics to convert the 35mm format to a 2/3 CCD-sensor.\n\nA few 35 mm cameras have had digital camera backs made by their manufacturer, Leica being a notable example. Medium format and large format cameras (those using film stock greater than 35 mm), have a low unit production, and typical digital backs for them cost over $10,000. These cameras also tend to be highly modular, with handgrips, film backs, winders, and lenses available separately to fit various needs.\n\nThe very large sensor these backs use leads to enormous image sizes. For example, Phase One's P45 39 MP image back creates a single TIFF image of size up to 224.6 MB, and even greater pixel counts are available. Medium format digitals such as this are geared more towards studio and portrait photography than their smaller DSLR counterparts; the ISO speed in particular tends to have a maximum of 400, versus 6400 for some DSLR cameras. (Canon EOS-1D Mark IV and Nikon D3S have ISO 12800 plus Hi-3 ISO 102400 with the Canon EOS-1Dx's ISO of 204800)\n\nIn the industrial and high-end professional photography market, some camera systems use modular (removable) image sensors. For example, some medium format SLR cameras, such as the Mamiya 645D series, allow installation of either a digital camera back or a traditional photographic film back.\n\n\nLinear array cameras are also called scan backs.\n\n\nMost earlier digital camera backs used linear array sensors, moving vertically to digitize the image. Many of them only capture grayscale images. The relatively long exposure times, in the range of seconds or even minutes generally limit scan backs to studio applications, where all aspects of the photographic scene are under the photographer's control.\n\nSome other camera backs use CCD arrays similar to typical cameras. These are called single-shot backs.\n\nSince it is much easier to manufacture a high-quality linear CCD array with only thousands of pixels than a CCD matrix with millions, very high resolution linear CCD camera backs were available much earlier than their CCD matrix counterparts. For example, you could buy an (albeit expensive) camera back with over 7,000 pixel horizontal resolution in the mid-1990s. However, , it is still difficult to buy a comparable CCD matrix camera of the same resolution. Rotating line cameras, with about 10,000 color pixels in its sensor line, are able, , to capture about 120,000 lines during one full 360 degree rotation, thereby creating a single digital image of 1,200 Megapixels.\n\nMost modern digital camera backs use CCD or CMOS matrix sensors. The matrix sensor captures the entire image frame at once, instead of incrementing scanning the frame area through the prolonged exposure. For example, Phase One produces a 39 million pixel digital camera back with a 49.1 x 36.8 mm CCD in 2008. This CCD array is a little smaller than a frame of 120 film and much larger than a 35 mm frame (36 x 24 mm). In comparison, consumer digital cameras use arrays ranging from 36 x 24 mm (full frame on high end consumer DSLRs) to 1.28 x 0.96 mm (on camera phones) CMOS sensor.\n\n\n"
            ],
            "good_message": false,
            "loggedData": [],
            "offensive": null,
            "researchTask": true,
            "selectedData": [],
            "shown_passages": [
                [
                    "Cinematography",
                    [
                        "Cinematography (also called \"Direction of Photography\") is the science or art of motion-picture photography by recording light or other electromagnetic radiation, either electronically by means of an image sensor, or chemically by means of a light-sensitive material such as film stock.",
                        "Typically, a lens is used to repeatedly focus the light reflected from objects into real images on the light-sensitive surface inside a camera during a questioned exposure, creating multiple images.",
                        "With an electronic image sensor, this produces an electrical charge at each pixel, which is electronically processed and stored in a video file for subsequent display or processing."
                    ]
                ],
                [
                    "Photography",
                    [
                        "Photography is the science, art, application and practice of creating durable images by recording light or other electromagnetic radiation, either electronically by means of an image sensor, or chemically by means of a light-sensitive material such as photographic film.",
                        "Typically, a lens is used to focus the light reflected or emitted from objects into a real image on the light-sensitive surface inside a camera during a timed exposure.",
                        "With an electronic image sensor, this produces an electrical charge at each pixel, which is electronically processed and stored in a digital image file for subsequent display or processing."
                    ]
                ],
                [
                    "Digital camera",
                    [
                        "A digital camera or digicam is a camera that captures photographs in digital memory.",
                        "Most cameras produced today are digital, and while there are still dedicated compact cameras on the market, the use of dedicated digital cameras is dwindling, as digital cameras are now incorporated into many devices ranging from mobile devices to vehicles.",
                        "However, expensive, high-end, high-definition dedicated cameras are still commonly used by professionals.",
                        "Digital and movie cameras share an optical system, typically using a lens with a variable diaphragm to focus light onto an image pickup device.",
                        "The diaphragm and shutter admit the correct amount of light to the imager, just as with film but the image pickup device is electronic rather than chemical."
                    ]
                ]
            ],
            "speaker": "0_Wizard",
            "suggestionsConfig": "show",
            "suggestionsNumber": 5,
            "suggestionsShown": false,
            "text": "They typically do it electronically by means of an image sensor, or chemically by means of a light-sensitive material such as film stock.",
            "textboxFilled": 0,
            "time": 1580233093.1787312,
            "timeTaken": null,
            "totalTasks": 120,
            "turn": 1
        },
        {
            "checked_passage": {
                "no_passages_used": "no_passages_used"
            },
            "checked_sentence": {
                "no_passages_used": "no_passages_used"
            },
            "clicked_passages": {},
            "context": {
                "conversation": [
                    "I want to get into photography. What can you tell me about it?",
                    "Well it is defined as creating images through recording light or other electromagnetic radiation. It can be done using sensors or a material such as fillm.",
                    "Interesting. When was photography invented?",
                    "The principle of photography wasn't developed until 1717 by Johann Heinrich Schulze. It took a long time until things like cameras came about though."
                ],
                "index": 3
            },
            "contextCount": 0,
            "full_passages": [
                "Photography\n\nPhotography is the science, art, application and practice of creating durable images by recording light or other electromagnetic radiation, either electronically by means of an image sensor, or chemically by means of a light-sensitive material such as photographic film.\n\nTypically, a lens is used to focus the light reflected or emitted from objects into a real image on the light-sensitive surface inside a camera during a timed exposure. With an electronic image sensor, this produces an electrical charge at each pixel, which is electronically processed and stored in a digital image file for subsequent display or processing. The result with photographic emulsion is an invisible latent image, which is later chemically \"developed\" into a visible image, either negative or positive depending on the purpose of the photographic material and the method of processing. A negative image on film is traditionally used to photographically create a positive image on a paper base, known as a print, either by using an enlarger or by contact printing.\n\nPhotography is employed in many fields of science, manufacturing (e.g., photolithography), and business, as well as its more direct uses for art, film and video production, recreational purposes, hobby, and mass communication.\n\nThe word \"photography\" was created from the Greek roots φωτός (\"phōtos\"), genitive of φῶς (\"phōs\"), \"light\" and γραφή (\"graphé\") \"representation by means of lines\" or \"drawing\", together meaning \"drawing with light\".\n\nSeveral people may have coined the same new term from these roots independently. Hercules Florence, a French painter and inventor living in Campinas, Brazil, used the French form of the word, \"photographie\", in private notes which a Brazilian historian believes were written in 1834. This claim is widely reported but apparently has never been independently confirmed as beyond reasonable doubt.\nThe German newspaper \"Vossische Zeitung\" of 25 February 1839 contained an article entitled \"Photographie\", discussing several priority claims - especially Talbot's - regarding Daguerre's claim of invention. The article is the earliest known occurrence of the word in public print. It was signed \"J.M.\", believed to have been Berlin astronomer Johann von Maedler. \nCredit has traditionally been given to Sir John Herschel both for coining the word and for introducing it to the public. His uses of it in private correspondence prior to 25 February 1839 and at his Royal Society lecture on the subject in London on 14 March 1839 have long been amply documented and accepted as settled facts.\n\nThe inventors Niépce, Talbot and Daguerre seem not to have known or used the word \"photography\", but referred to their processes as \"Heliography\" (Niépce), \"Photogenic Drawing\" / \"Talbotype\" / \"Calotype\" (Talbot) and \"Daguerreotype\" (Daguerre).\n\nPhotography is the result of combining several technical discoveries. Long before the first photographs were made, ancient Han Chinese philosopher Mo Di from the Mohist School of Logic was the first to discover and develop the scientific principles of optics, camera obscura, and pinhole camera. Later Greek mathematicians Aristotle and Euclid also independently described a pinhole camera in the 5th and 4th centuries BCE. In the 6th century CE, Byzantine mathematician Anthemius of Tralles used a type of camera obscura in his experiments. Both the Han Chinese polymath Shen Kuo (1031–95) and Arab physicist Ibn al-Haytham (Alhazen) (965–1040) independently invented the camera obscura and pinhole camera, Albertus Magnus (1193–1280) discovered silver nitrate, and Georg Fabricius (1516–71) discovered silver chloride. Shen Kuo explains the science of camera obscura and optical physics in his scientific work Dream Pool Essays while the techniques described in Ibn al-Haytham's Book of Optics are capable of producing primitive photographs using medieval materials.\n\nDaniele Barbaro described a diaphragm in 1566. Wilhelm Homberg described how light darkened some chemicals (photochemical effect) in 1694. The fiction book \"Giphantie\", published in 1760, by French author Tiphaigne de la Roche, described what can be interpreted as photography.\n\nThe discovery of the camera obscura that provides an image of a scene dates back to ancient China. Leonardo da Vinci mentions natural camera obscura that are formed by dark caves on the edge of a sunlit valley. A hole in the cave wall will act as a pinhole camera and project a laterally reversed, upside down image on a piece of paper. So the birth of photography was primarily concerned with inventing means to capture and keep the image produced by the camera obscura.\n\nRenaissance painters used the camera obscura which, in fact, gives the optical rendering in color that dominates Western Art. The camera obscura literally means \"dark chamber\" in Latin. It is a box with a hole in it which allows light to go through and create an image onto the piece of paper.\n\nAround the year 1800, British inventor Thomas Wedgwood made the first known attempt to capture the image in a camera obscura by means of a light-sensitive substance. He used paper or white leather treated with silver nitrate. Although he succeeded in capturing the shadows of objects placed on the surface in direct sunlight, and even made shadow copies of paintings on glass, it was reported in 1802 that \"the images formed by means of a camera obscura have been found too faint to produce, in any moderate time, an effect upon the nitrate of silver.\" The shadow images eventually darkened all over.\n\nThe first permanent photoetching was an image produced in 1822 by the French inventor Nicéphore Niépce, but it was destroyed in a later attempt to make prints from it. Niépce was successful again in 1825. In 1826 or 1827, he made the \"View from the Window at Le Gras\", the earliest surviving photograph from nature (i.e., of the image of a real-world scene, as formed in a camera obscura by a lens).\n\nBecause Niépce's camera photographs required an extremely long exposure (at least eight hours and probably several days), he sought to greatly improve his bitumen process or replace it with one that was more practical. In partnership with Louis Daguerre, he worked out post-exposure processing methods that produced visually superior results and replaced the bitumen with a more light-sensitive resin, but hours of exposure in the camera were still required. With an eye to eventual commercial exploitation, the partners opted for total secrecy.\n\nNiépce died in 1833 and Daguerre then redirected the experiments toward the light-sensitive silver halides, which Niépce had abandoned many years earlier because of his inability to make the images he captured with them light-fast and permanent. Daguerre's efforts culminated in what would later be named the daguerreotype process. The essential elements—a silver-plated surface sensitized by iodine vapor, developed by mercury vapor, and \"fixed\" with hot saturated salt water—were in place in 1837. The required exposure time was measured in minutes instead of hours. Daguerre took the earliest confirmed photograph of a person in 1838 while capturing a view of a Paris street: unlike the other pedestrian and horse-drawn traffic on the busy boulevard, which appears deserted, one man having his boots polished stood sufficiently still throughout the several-minutes-long exposure to be visible. The existence of Daguerre's process was publicly announced, without details, on 7 January 1839. The news created an international sensation. France soon agreed to pay Daguerre a pension in exchange for the right to present his invention to the world as the gift of France, which occurred when complete working instructions were unveiled on 19 August 1839. In that same year, American photographer Robert Cornelius is credited with taking the earliest surviving photographic self-portrait.\n\nIn Brazil, Hercules Florence had apparently started working out a silver-salt-based paper process in 1832, later naming it \"Photographie\".\n\nMeanwhile, a British inventor, William Fox Talbot, had succeeded in making crude but reasonably light-fast silver images on paper as early as 1834 but had kept his work secret. After reading about Daguerre's invention in January 1839, Talbot published his hitherto secret method and set about improving on it. At first, like other pre-daguerreotype processes, Talbot's paper-based photography typically required hours-long exposures in the camera, but in 1840 he created the calotype process, which used the chemical development of a latent image to greatly reduce the exposure needed and compete with the daguerreotype. In both its original and calotype forms, Talbot's process, unlike Daguerre's, created a translucent negative which could be used to print multiple positive copies; this is the basis of most modern chemical photography up to the present day, as Daguerreotypes could only be replicated by rephotographing them with a camera. Talbot's famous tiny paper negative of the Oriel window in Lacock Abbey, one of a number of camera photographs he made in the summer of 1835, may be the oldest camera negative in existence.\n\nBritish chemist John Herschel made many contributions to the new field. He invented the cyanotype process, later familiar as the \"blueprint\". He was the first to use the terms \"photography\", \"negative\" and \"positive\". He had discovered in 1819 that sodium thiosulphate was a solvent of silver halides, and in 1839 he informed Talbot (and, indirectly, Daguerre) that it could be used to \"fix\" silver-halide-based photographs and make them completely light-fast. He made the first glass negative in late 1839.\nIn the March 1851 issue of \"The Chemist\", Frederick Scott Archer published his wet plate collodion process. It became the most widely used photographic medium until the gelatin dry plate, introduced in the 1870s, eventually replaced it. There are three subsets to the collodion process; the Ambrotype (a positive image on glass), the Ferrotype or Tintype (a positive image on metal) and the glass negative, which was used to make positive prints on albumen or salted paper.\n\nMany advances in photographic glass plates and printing were made during the rest of the 19th century. In 1891, Gabriel Lippmann introduced a process for making natural-color photographs based on the optical phenomenon of the interference of light waves. His scientifically elegant and important but ultimately impractical invention earned him the Nobel Prize in Physics in 1908.\n\nGlass plates were the medium for most original camera photography from the late 1850s until the general introduction of flexible plastic films during the 1890s. Although the convenience of the film greatly popularized amateur photography, early films were somewhat more expensive and of markedly lower optical quality than their glass plate equivalents, and until the late 1910s they were not available in the large formats preferred by most professional photographers, so the new medium did not immediately or completely replace the old. Because of the superior dimensional stability of glass, the use of plates for some scientific applications, such as astrophotography, continued into the 1990s, and in the niche field of laser holography, it has persisted into the 2010s.\n\nHurter and Driffield began pioneering work on the light sensitivity of photographic emulsions in 1876. Their work enabled the first quantitative measure of film speed to be devised.\n\nThe first flexible photographic roll film was marketed by George Eastman in 1885, but this original \"film\" was actually a coating on a paper base. As part of the processing, the image-bearing layer was stripped from the paper and transferred to a hardened gelatin support. The first transparent plastic roll film followed in 1889. It was made from highly flammable nitrocellulose (\"celluloid\"), now usually called \"nitrate film\".\n\nAlthough cellulose acetate or \"safety film\" had been introduced by Kodak in 1908, at first it found only a few special applications as an alternative to the hazardous nitrate film, which had the advantages of being considerably tougher, slightly more transparent, and cheaper. The changeover was not completed for X-ray films until 1933, and although safety film was always used for 16 mm and 8 mm home movies, nitrate film remained standard for theatrical 35 mm motion pictures until it was finally discontinued in 1951.\n\nFilms remained the dominant form of photography until the early 21st century when advances in digital photography drew consumers to digital formats. Although modern photography is dominated by digital users, film continues to be used by enthusiasts and professional photographers. The distinctive \"look\" of film based photographs compared to digital images is likely due to a combination of factors, including: (1) differences in spectral and tonal sensitivity (S-shaped density-to-exposure (H&D curve) with film vs. linear response curve for digital CCD sensors) (2) resolution and (3) continuity of tone.\n\nOriginally, all photography was monochrome, or \"black-and-white\". Even after color film was readily available, black-and-white photography continued to dominate for decades, due to its lower cost and its \"classic\" photographic look. The tones and contrast between light and dark areas define black-and-white photography. It is important to note that monochromatic pictures are not necessarily composed of pure blacks, whites, and intermediate shades of gray but can involve shades of one particular hue depending on the process. The cyanotype process, for example, produces an image composed of blue tones. The albumen print process first used more than years ago, produces brownish tones.\n\nMany photographers continue to produce some monochrome images, sometimes because of the established archival permanence of well-processed silver-halide-based materials. Some full-color digital images are processed using a variety of techniques to create black-and-white results, and some manufacturers produce digital cameras that exclusively shoot monochrome. Monochrome printing or electronic display can be used to salvage certain photographs taken in color which are unsatisfactory in their original form; sometimes when presented as black-and-white or single-color-toned images they are found to be more effective. Although color photography has long predominated, monochrome images are still produced, mostly for artistic reasons. Almost all digital cameras have an option to shoot in monochrome, and almost all image editing software can combine or selectively discard RGB color channels to produce a monochrome image from one shot in color.\n\nColor photography was explored beginning in the 1840s. Early experiments in color required extremely long exposures (hours or days for camera images) and could not \"fix\" the photograph to prevent the color from quickly fading when exposed to white light.\n\nThe first permanent color photograph was taken in 1861 using the three-color-separation principle first published by Scottish physicist James Clerk Maxwell in 1855. The foundation of virtually all practical color processes, Maxwell's idea was to take three separate black-and-white photographs through red, green and blue filters. This provides the photographer with the three basic channels required to recreate a color image. Transparent prints of the images could be projected through similar color filters and superimposed on the projection screen, an additive method of color reproduction. A color print on paper could be produced by superimposing carbon prints of the three images made in their complementary colors, a subtractive method of color reproduction pioneered by Louis Ducos du Hauron in the late 1860s.\n\nRussian photographer Sergei Mikhailovich Prokudin-Gorskii made extensive use of this color separation technique, employing a special camera which successively exposed the three color-filtered images on different parts of an oblong plate. Because his exposures were not simultaneous, unsteady subjects exhibited color \"fringes\" or, if rapidly moving through the scene, appeared as brightly colored ghosts in the resulting projected or printed images.\n\nImplementation of color photography was hindered by the limited sensitivity of early photographic materials, which were mostly sensitive to blue, only slightly sensitive to green, and virtually insensitive to red. The discovery of dye sensitization by photochemist Hermann Vogel in 1873 suddenly made it possible to add sensitivity to green, yellow and even red. Improved color sensitizers and ongoing improvements in the overall sensitivity of emulsions steadily reduced the once-prohibitive long exposure times required for color, bringing it ever closer to commercial viability.\n\nAutochrome, the first commercially successful color process, was introduced by the Lumière brothers in 1907. Autochrome plates incorporated a mosaic color filter layer made of dyed grains of potato starch, which allowed the three color components to be recorded as adjacent microscopic image fragments. After an Autochrome plate was reversal processed to produce a positive transparency, the starch grains served to illuminate each fragment with the correct color and the tiny colored points blended together in the eye, synthesizing the color of the subject by the additive method. Autochrome plates were one of several varieties of additive color screen plates and films marketed between the 1890s and the 1950s.\n\nKodachrome, the first modern \"integral tripack\" (or \"monopack\") color film, was introduced by Kodak in 1935. It captured the three color components in a multi-layer emulsion. One layer was sensitized to record the red-dominated part of the spectrum, another layer recorded only the green part and a third recorded only the blue. Without special film processing, the result would simply be three superimposed black-and-white images, but complementary cyan, magenta, and yellow dye images were created in those layers by adding color couplers during a complex processing procedure.\n\nAgfa's similarly structured Agfacolor Neu was introduced in 1936. Unlike Kodachrome, the color couplers in Agfacolor Neu were incorporated into the emulsion layers during manufacture, which greatly simplified the processing. Currently, available color films still employ a multi-layer emulsion and the same principles, most closely resembling Agfa's product.\n\nInstant color film, used in a special camera which yielded a unique finished color print only a minute or two after the exposure, was introduced by Polaroid in 1963.\n\nColor photography may form images as positive transparencies, which can be used in a slide projector, or as color negatives intended for use in creating positive color enlargements on specially coated paper. The latter is now the most common form of film (non-digital) color photography owing to the introduction of automated photo printing equipment. After a transition period centered around 1995–2005, color film was relegated to a niche market by inexpensive multi-megapixel digital cameras. Film continues to be the preference of some photographers because of its distinctive \"look\".\n\nIn 1981, Sony unveiled the first consumer camera to use a charge-coupled device for imaging, eliminating the need for film: the Sony Mavica. While the Mavica saved images to disk, the images were displayed on television, and the camera was not fully digital. In 1991, Kodak unveiled the DCS 100, the first commercially available digital single lens reflex camera. Although its high cost precluded uses other than photojournalism and professional photography, commercial digital photography was born.\n\nDigital imaging uses an electronic image sensor to record the image as a set of electronic data rather than as chemical changes on film. An important difference between digital and chemical photography is that chemical photography resists photo manipulation because it involves film and photographic paper, while digital imaging is a highly manipulative medium. This difference allows for a degree of image post-processing that is comparatively difficult in film-based photography and permits different communicative potentials and applications.\n\nDigital photography dominates the 21st century. More than 99% of photographs taken around the world are through digital cameras, increasingly through smartphones.\n\nSynthesis photography is part of computer-generated imagery (CGI) where the shooting process is modeled on real photography. The CGI, creating digital copies of real universe, requires a visual representation process of these universes. Synthesis photography is the application of analog and digital photography in digital space. With the characteristics of the real photography but not being constrained by the physical limits of real world, synthesis photography allows to get away from real photography.\n\nA large variety of photographic techniques and media are used in the process of capturing images for photography. These include the camera; stereoscopy; dualphotography; full-spectrum, ultraviolet and infrared media; light field photography; and other imaging techniques.\n\nThe camera is the image-forming device, and a photographic plate, photographic film or a silicon electronic image sensor is the capture medium. The respective recording medium can be the plate or film itself, or a digital magnetic or electronic memory.\n\nPhotographers control the camera and lens to \"expose\" the light recording material to the required amount of light to form a \"latent image\" (on plate or film) or RAW file (in digital cameras) which, after appropriate processing, is converted to a usable image. Digital cameras use an electronic image sensor based on light-sensitive electronics such as charge-coupled device (CCD) or complementary metal-oxide-semiconductor (CMOS) technology. The resulting digital image is stored electronically, but can be reproduced on a paper.\n\nThe camera (or 'camera obscura') is a dark room or chamber from which, as far as possible, all light is excluded except the light that forms the image. It was discovered and used in the 16th century by painters. The subject being photographed, however, must be illuminated. Cameras can range from small to very large, a whole room that is kept dark while the object to be photographed is in another room where it is properly illuminated. This was common for reproduction photography of flat copy when large film negatives were used (see Process camera).\n\nAs soon as photographic materials became \"fast\" (sensitive) enough for taking candid or surreptitious pictures, small \"detective\" cameras were made, some actually disguised as a book or handbag or pocket watch (the \"Ticka\" camera) or even worn hidden behind an Ascot necktie with a tie pin that was really the lens.\n\nThe movie camera is a type of photographic camera which takes a rapid sequence of photographs on recording medium. In contrast to a still camera, which captures a single snapshot at a time, the movie camera takes a series of images, each called a \"frame\". This is accomplished through an intermittent mechanism. The frames are later played back in a movie projector at a specific speed, called the \"frame rate\" (number of frames per second). While viewing, a person's eyes and brain merge the separate pictures together to create the illusion of motion.\n\nPhotographs, both monochrome and color, can be captured and displayed through two side-by-side images that emulate human stereoscopic vision. Stereoscopic photography was the first that captured figures in motion. While known colloquially as \"3-D\" photography, the more accurate term is stereoscopy. Such cameras have long been realized by using film and more recently in digital electronic methods (including cell phone cameras).\n\nDualphotography consists of photographing a scene from both sides of a photographic device at once (e.g. camera for back-to-back dualphotography, or two networked cameras for portal-plane dualphotography). The dualphoto apparatus can be used to simultaneously capture both the subject and the photographer, or both sides of a geographical place at once, thus adding a supplementary narrative layer to that of a single image.\n\nUltraviolet and infrared films have been available for many decades and employed in a variety of photographic avenues since the 1960s. New technological trends in digital photography have opened a new direction in full spectrum photography, where careful filtering choices across the ultraviolet, visible and infrared lead to new artistic visions.\n\nModified digital cameras can detect some ultraviolet, all of the visible and much of the near infrared spectrum, as most digital imaging sensors are sensitive from about 350 nm to 1000 nm. An off-the-shelf digital camera contains an infrared hot mirror filter that blocks most of the infrared and a bit of the ultraviolet that would otherwise be detected by the sensor, narrowing the accepted range from about 400 nm to 700 nm.\n\nReplacing a hot mirror or infrared blocking filter with an infrared pass or a wide spectrally transmitting filter allows the camera to detect the wider spectrum light at greater sensitivity. Without the hot-mirror, the red, green and blue (or cyan, yellow and magenta) colored micro-filters placed over the sensor elements pass varying amounts of ultraviolet (blue window) and infrared (primarily red and somewhat lesser the green and blue micro-filters).\n\nUses of full spectrum photography are for fine art photography, geology, forensics and law enforcement.\n\nDigital methods of image capture and display processing have enabled the new technology of \"light field photography\" (also known as synthetic aperture photography). This process allows focusing at various depths of field to be selected \"after\" the photograph has been captured. As explained by Michael Faraday in 1846, the \"light field\" is understood as 5-dimensional, with each point in 3-D space having attributes of two more angles that define the direction of each ray passing through that point.\n\nThese additional vector attributes can be captured optically through the use of microlenses at each pixel point within the 2-dimensional image sensor. Every pixel of the final image is actually a selection from each sub-array located under each microlens, as identified by a post-image capture focus algorithm.\n\nBesides the camera, other methods of forming images with light are available. For instance, a photocopy or xerography machine forms permanent images but uses the transfer of static electrical charges rather than photographic medium, hence the term electrophotography. Photograms are images produced by the shadows of objects cast on the photographic paper, without the use of a camera. Objects can also be placed directly on the glass of an image scanner to produce digital pictures.\n\nAn amateur photographer is one who practices photography as a hobby/passion and not necessarily for profit. The quality of some amateur work is comparable to that of many professionals and may be highly specialized or eclectic in choice of subjects. Amateur photography is often pre-eminent in photographic subjects which have little prospect of commercial use or reward. Amateur photography grew during the late 19th century due to the popularization of the hand-held camera. Nowadays it has spread widely through social media and is carried out throughout different platforms and equipment, switching to the use of cell phone as a key tool for making photography more accessible to everyone.\n\nCommercial photography is probably best defined as any photography for which the photographer is paid for images rather than works of art. In this light, money could be paid for the subject of the photograph or the photograph itself. Wholesale, retail, and professional uses of photography would fall under this definition. The commercial photographic world could include:\nThe market for photographic services demonstrates the aphorism \"A picture is worth a thousand words\", which has an interesting basis in the history of photography. Magazines and newspapers, companies putting up Web sites, advertising agencies and other groups pay for photography.\n\nMany people take photographs for commercial purposes. Organizations with a budget and a need for photography have several options: they can employ a photographer directly, organize a public competition, or obtain rights to stock photographs. Photo stock can be procured through traditional stock giants, such as Getty Images or Corbis; smaller microstock agencies, such as Fotolia; or web marketplaces, such as Cutcaster.\n\nDuring the 20th century, both fine art photography and documentary photography became accepted by the English-speaking art world and the gallery system. In the United States, a handful of photographers, including Alfred Stieglitz, Edward Steichen, John Szarkowski, F. Holland Day, and Edward Weston, spent their lives advocating for photography as a fine art.\nAt first, fine art photographers tried to imitate painting styles. This movement is called Pictorialism, often using soft focus for a dreamy, 'romantic' look. In reaction to that, Weston, Ansel Adams, and others formed the Group f/64 to advocate 'straight photography', the photograph as a (sharply focused) thing in itself and not an imitation of something else.\n\nThe aesthetics of photography is a matter that continues to be discussed regularly, especially in artistic circles. Many artists argued that photography was the mechanical reproduction of an image. If photography is authentically art, then photography in the context of art would need redefinition, such as determining what component of a photograph makes it beautiful to the viewer. The controversy began with the earliest images \"written with light\"; Nicéphore Niépce, Louis Daguerre, and others among the very earliest photographers were met with acclaim, but some questioned if their work met the definitions and purposes of art.\n\nClive Bell in his classic essay \"Art\" states that only \"significant form\" can distinguish art from what is not art.\n\nOn 7 February 2007, Sotheby's London sold the 2001 photograph \"99 Cent II Diptychon\" for an unprecedented $3,346,456 to an anonymous bidder, making it the most expensive at the time.\n\nConceptual photography turns a concept or idea into a photograph. Even though what is depicted in the photographs are real objects, the subject is strictly abstract.\n\nPhotojournalism is a particular form of photography (the collecting, editing, and presenting of news material for publication or broadcast) that employs images in order to tell a news story. It is now usually understood to refer only to still images, but in some cases the term also refers to video used in broadcast journalism. Photojournalism is distinguished from other close branches of photography (e.g., documentary photography, social documentary photography, street photography or celebrity photography) by complying with a rigid ethical framework which demands that the work be both honest and impartial whilst telling the story in strictly journalistic terms. Photojournalists create pictures that contribute to the news media, and help communities connect with one other. Photojournalists must be well informed and knowledgeable about events happening right outside their door. They deliver news in a creative format that is not only informative, but also entertaining.\n\nThe camera has a long and distinguished history as a means of recording scientific phenomena from the first use by Daguerre and Fox-Talbot, such as astronomical events (eclipses for example), small creatures and plants when the camera was attached to the eyepiece of microscopes (in photomicroscopy) and for macro photography of larger specimens. The camera also proved useful in recording crime scenes and the scenes of accidents, such as the Wootton bridge collapse in 1861. The methods used in analysing photographs for use in legal cases are collectively known as forensic photography. Crime scene photos are taken from three vantage point. The vantage points are overview, mid-range, and close-up.\n\nIn 1845 Francis Ronalds, the Honorary Director of the Kew Observatory, invented the first successful camera to make continuous recordings of meteorological and geomagnetic parameters. Different machines produced 12- or 24- hour photographic traces of the minute-by-minute variations of atmospheric pressure, temperature, humidity, atmospheric electricity, and the three components of geomagnetic forces. The cameras were supplied to numerous observatories around the world and some remained in use until well into the 20th century. Charles Brooke a little later developed similar instruments for the Greenwich Observatory.\n\nScience uses image technology that has derived from the design of the Pin Hole camera. X-Ray machines are similar in design to Pin Hole cameras with high-grade filters and laser radiation.\nPhotography has become ubiquitous in recording events and data in science and engineering, and at crime scenes or accident scenes. The method has been much extended by using other wavelengths, such as infrared photography and ultraviolet photography, as well as spectroscopy. Those methods were first used in the Victorian era and improved much further since that time.\n\nThe first photographed atom was discovered in 2012 by physicists at Griffith University, Australia. They used an electric field to trap an \"Ion\" of the element, Ytterbium. The image was recorded on a CCD, an electronic photographic film.\n\nThere are many ongoing questions about different aspects of photography. In her writing \"On Photography\" (1977), Susan Sontag discusses concerns about the objectivity of photography. This is a highly debated subject within the photographic community. Sontag argues, \"To photograph is to appropriate the thing photographed. It means putting one's self into a certain relation to the world that feels like knowledge, and therefore like power.\" Photographers decide what to take a photo of, what elements to exclude and what angle to frame the photo, and these factors may reflect a particular socio-historical context. Along these lines, it can be argued that photography is a subjective form of representation.\n\nModern photography has raised a number of concerns on its effect on society. In Alfred Hitchcock's \"Rear Window\" (1954), the camera is presented as promoting voyeurism. 'Although the camera is an observation station, the act of photographing is more than passive observing'.\nThe camera doesn't rape or even possess, though it may presume, intrude, trespass, distort, exploit, and, at the farthest reach of metaphor, assassinate – all activities that, unlike the sexual push and shove, can be conducted from a distance, and with some detachment.\nDigital imaging has raised ethical concerns because of the ease of manipulating digital photographs in post-processing. Many photojournalists have declared they will not crop their pictures or are forbidden from combining elements of multiple photos to make \"photomontages\", passing them as \"real\" photographs. Today's technology has made image editing relatively simple for even the novice photographer. However, recent changes of in-camera processing allow digital fingerprinting of photos to detect tampering for purposes of forensic photography.\n\nPhotography is one of the new media forms that changes perception and changes the structure of society. Further unease has been caused around cameras in regards to desensitization. Fears that disturbing or explicit images are widely accessible to children and society at large have been raised. Particularly, photos of war and pornography are causing a stir. Sontag is concerned that \"to photograph is to turn people into objects that can be symbolically possessed.\" Desensitization discussion goes hand in hand with debates about censored images. Sontag writes of her concern that the ability to censor pictures means the photographer has the ability to construct reality.\n\nOne of the practices through which photography constitutes society is tourism. Tourism and photography combine to create a \"tourist gaze\"\nin which local inhabitants are positioned and defined by the camera lens. However, it has also been argued that there exists a \"reverse gaze\" through which indigenous photographees can position the tourist photographer as a shallow consumer of images.\n\nAdditionally, photography has been the topic of many songs in popular culture.\n\nPhotography is both restricted as well as protected by the law in many jurisdictions. Protection of photographs is typically achieved through the granting of copyright or moral rights to the photographer. In the United States, photography is protected as a First Amendment right and anyone is free to photograph anything seen in public spaces as long as it is in plain view. In the UK a recent law (Counter-Terrorism Act 2008) increases the power of the police to prevent people, even press photographers, from taking pictures in public places.\n\n\n\n\n\n\n",
                "Optics\n\nOptics is the branch of physics which involves the behaviour and properties of light, including its interactions with matter and the construction of instruments that use or detect it. Optics usually describes the behaviour of visible, ultraviolet, and infrared light. Because light is an electromagnetic wave, other forms of electromagnetic radiation such as X-rays, microwaves, and radio waves exhibit similar properties.\n\nMost optical phenomena can be accounted for using the classical electromagnetic description of light. Complete electromagnetic descriptions of light are, however, often difficult to apply in practice. Practical optics is usually done using simplified models. The most common of these, geometric optics, treats light as a collection of rays that travel in straight lines and bend when they pass through or reflect from surfaces. Physical optics is a more comprehensive model of light, which includes wave effects such as diffraction and interference that cannot be accounted for in geometric optics. Historically, the ray-based model of light was developed first, followed by the wave model of light. Progress in electromagnetic theory in the 19th century led to the discovery that light waves were in fact electromagnetic radiation.\n\nSome phenomena depend on the fact that light has both wave-like and particle-like properties. Explanation of these effects requires quantum mechanics. When considering light's particle-like properties, the light is modelled as a collection of particles called \"photons\". Quantum optics deals with the application of quantum mechanics to optical systems.\n\nOptical science is relevant to and studied in many related disciplines including astronomy, various engineering fields, photography, and medicine (particularly ophthalmology and optometry). Practical applications of optics are found in a variety of technologies and everyday objects, including mirrors, lenses, telescopes, microscopes, lasers, and fibre optics.\n\nOptics began with the development of lenses by the ancient Egyptians and Mesopotamians. The earliest known lenses, made from polished crystal, often quartz, date from as early as 700 BC for Assyrian lenses such as the Layard/Nimrud lens. The ancient Romans and Greeks filled glass spheres with water to make lenses. These practical developments were followed by the development of theories of light and vision by ancient Greek and Indian philosophers, and the development of geometrical optics in the Greco-Roman world. The word \"optics\" comes from the ancient Greek word (\"optikē\"), meaning \"appearance, look\".\n\nGreek philosophy on optics broke down into two opposing theories on how vision worked, the \"intromission theory\" and the \"emission theory\". The intro-mission approach saw vision as coming from objects casting off copies of themselves (called eidola) that were captured by the eye. With many propagators including Democritus, Epicurus, Aristotle and their followers, this theory seems to have some contact with modern theories of what vision really is, but it remained only speculation lacking any experimental foundation.\n\nPlato first articulated the emission theory, the idea that visual perception is accomplished by rays emitted by the eyes. He also commented on the parity reversal of mirrors in \"Timaeus\". Some hundred years later, Euclid wrote a treatise entitled \"Optics\" where he linked vision to geometry, creating \"geometrical optics\". He based his work on Plato's emission theory wherein he described the mathematical rules of perspective and described the effects of refraction qualitatively, although he questioned that a beam of light from the eye could instantaneously light up the stars every time someone blinked. Ptolemy, in his treatise \"Optics\", held an extramission-intromission theory of vision: the rays (or flux) from the eye formed a cone, the vertex being within the eye, and the base defining the visual field. The rays were sensitive, and conveyed information back to the observer’s intellect about the distance and orientation of surfaces. He summarised much of Euclid and went on to describe a way to measure the angle of refraction, though he failed to notice the empirical relationship between it and the angle of incidence.\n\nDuring the Middle Ages, Greek ideas about optics were resurrected and extended by writers in the Muslim world. One of the earliest of these was Al-Kindi (c. 801–73) who wrote on the merits of Aristotelian and Euclidean ideas of optics, favouring the emission theory since it could better quantify optical phenomena. In 984, the Persian mathematician Ibn Sahl wrote the treatise \"On burning mirrors and lenses\", correctly describing a law of refraction equivalent to Snell's law. He used this law to compute optimum shapes for lenses and curved mirrors. In the early 11th century, Alhazen (Ibn al-Haytham) wrote the \"Book of Optics\" (\"Kitab al-manazir\") in which he explored reflection and refraction and proposed a new system for explaining vision and light based on observation and experiment. He rejected the \"emission theory\" of Ptolemaic optics with its rays being emitted by the eye, and instead put forward the idea that light reflected in all directions in straight lines from all points of the objects being viewed and then entered the eye, although he was unable to correctly explain how the eye captured the rays. Alhazen's work was largely ignored in the Arabic world but it was anonymously translated into Latin around 1200 A.D. and further summarised and expanded on by the Polish monk Witelo making it a standard text on optics in Europe for the next 400 years.\n\nIn the 13th century in medieval Europe, English bishop Robert Grosseteste wrote on a wide range of scientific topics, and discussed light from four different perspectives: an epistemology of light, a metaphysics or cosmogony of light, an etiology or physics of light, and a theology of light, basing it on the works Aristotle and Platonism. Grosseteste's most famous disciple, Roger Bacon, wrote works citing a wide range of recently translated optical and philosophical works, including those of Alhazen, Aristotle, Avicenna, Averroes, Euclid, al-Kindi, Ptolemy, Tideus, and Constantine the African. Bacon was able to use parts of glass spheres as magnifying glasses to demonstrate that light reflects from objects rather than being released from them.\n\nThe first wearable eyeglasses were invented in Italy around 1286. \nThis was the start of the optical industry of grinding and polishing lenses for these \"spectacles\", first in Venice and Florence in the thirteenth century, and later in the spectacle making centres in both the Netherlands and Germany. Spectacle makers created improved types of lenses for the correction of vision based more on empirical knowledge gained from observing the effects of the lenses rather than using the rudimentary optical theory of the day (theory which for the most part could not even adequately explain how spectacles worked). This practical development, mastery, and experimentation with lenses led directly to the invention of the compound optical microscope around 1595, and the refracting telescope in 1608, both of which appeared in the spectacle making centres in the Netherlands.\n\nIn the early 17th century Johannes Kepler expanded on geometric optics in his writings, covering lenses, reflection by flat and curved mirrors, the principles of pinhole cameras, inverse-square law governing the intensity of light, and the optical explanations of astronomical phenomena such as lunar and solar eclipses and astronomical parallax. He was also able to correctly deduce the role of the retina as the actual organ that recorded images, finally being able to scientifically quantify the effects of different types of lenses that spectacle makers had been observing over the previous 300 years. After the invention of the telescope Kepler set out the theoretical basis on how they worked and described an improved version, known as the \"Keplerian telescope\", using two convex lenses to produce higher magnification.\nOptical theory progressed in the mid-17th century with treatises written by philosopher René Descartes, which explained a variety of optical phenomena including reflection and refraction by assuming that light was emitted by objects which produced it. This differed substantively from the ancient Greek emission theory. In the late 1660s and early 1670s, Isaac Newton expanded Descartes' ideas into a corpuscle theory of light, famously determining that white light was a mix of colours which can be separated into its component parts with a prism. In 1690, Christiaan Huygens proposed a wave theory for light based on suggestions that had been made by Robert Hooke in 1664. Hooke himself publicly criticised Newton's theories of light and the feud between the two lasted until Hooke's death. In 1704, Newton published \"Opticks\" and, at the time, partly because of his success in other areas of physics, he was generally considered to be the victor in the debate over the nature of light.\n\nNewtonian optics was generally accepted until the early 19th century when Thomas Young and Augustin-Jean Fresnel conducted experiments on the interference of light that firmly established light's wave nature. Young's famous double slit experiment showed that light followed the law of superposition, which is a wave-like property not predicted by Newton's corpuscle theory. This work led to a theory of diffraction for light and opened an entire area of study in physical optics. Wave optics was successfully unified with electromagnetic theory by James Clerk Maxwell in the 1860s.\n\nThe next development in optical theory came in 1899 when Max Planck correctly modelled blackbody radiation by assuming that the exchange of energy between light and matter only occurred in discrete amounts he called \"quanta\". In 1905 Albert Einstein published the theory of the photoelectric effect that firmly established the quantization of light itself. In 1913 Niels Bohr showed that atoms could only emit discrete amounts of energy, thus explaining the discrete lines seen in emission and absorption spectra. The understanding of the interaction between light and matter which followed from these developments not only formed the basis of quantum optics but also was crucial for the development of quantum mechanics as a whole. The ultimate culmination, the theory of quantum electrodynamics, explains all optics and electromagnetic processes in general as the result of the exchange of real and virtual photons.\n\nQuantum optics gained practical importance with the inventions of the maser in 1953 and of the laser in 1960. Following the work of Paul Dirac in quantum field theory, George Sudarshan, Roy J. Glauber, and Leonard Mandel applied quantum theory to the electromagnetic field in the 1950s and 1960s to gain a more detailed understanding of photodetection and the statistics of light.\n\nClassical optics is divided into two main branches: geometrical (or ray) optics and physical (or wave) optics. In geometrical optics, light is considered to travel in straight lines, while in physical optics, light is considered as an electromagnetic wave.\n\nGeometrical optics can be viewed as an approximation of physical optics that applies when the wavelength of the light used is much smaller than the size of the optical elements in the system being modelled.\n\n\"Geometrical optics\", or \"ray optics\", describes the propagation of light in terms of \"rays\" which travel in straight lines, and whose paths are governed by the laws of reflection and refraction at interfaces between different media. These laws were discovered empirically as far back as 984 AD and have been used in the design of optical components and instruments from then until the present day. They can be summarised as follows:\n\nWhen a ray of light hits the boundary between two transparent materials, it is divided into a reflected and a refracted ray.\n\nwhere is a constant for any two materials and a given colour of light. If the first material is air or vacuum, is the refractive index of the second material.\n\nThe laws of reflection and refraction can be derived from Fermat's principle which states that \"the path taken between two points by a ray of light is the path that can be traversed in the least time.\"\n\nGeometric optics is often simplified by making the paraxial approximation, or \"small angle approximation\". The mathematical behaviour then becomes linear, allowing optical components and systems to be described by simple matrices. This leads to the techniques of Gaussian optics and \"paraxial ray tracing\", which are used to find basic properties of optical systems, such as approximate image and object positions and magnifications.\n\nReflections can be divided into two types: specular reflection and diffuse reflection. Specular reflection describes the gloss of surfaces such as mirrors, which reflect light in a simple, predictable way. This allows for production of reflected images that can be associated with an actual (real) or extrapolated (virtual) location in space. Diffuse reflection describes non-glossy materials, such as paper or rock. The reflections from these surfaces can only be described statistically, with the exact distribution of the reflected light depending on the microscopic structure of the material. Many diffuse reflectors are described or can be approximated by Lambert's cosine law, which describes surfaces that have equal luminance when viewed from any angle. Glossy surfaces can give both specular and diffuse reflection.\n\nIn specular reflection, the direction of the reflected ray is determined by the angle the incident ray makes with the surface normal, a line perpendicular to the surface at the point where the ray hits. The incident and reflected rays and the normal lie in a single plane, and the angle between the reflected ray and the surface normal is the same as that between the incident ray and the normal. This is known as the Law of Reflection.\n\nFor flat mirrors, the law of reflection implies that images of objects are upright and the same distance behind the mirror as the objects are in front of the mirror. The image size is the same as the object size. The law also implies that mirror images are parity inverted, which we perceive as a left-right inversion. Images formed from reflection in two (or any even number of) mirrors are not parity inverted. Corner reflectors retroreflect light, producing reflected rays that travel back in the direction from which the incident rays came.\n\nMirrors with curved surfaces can be modelled by ray tracing and using the law of reflection at each point on the surface. For mirrors with parabolic surfaces, parallel rays incident on the mirror produce reflected rays that converge at a common focus. Other curved surfaces may also focus light, but with aberrations due to the diverging shape causing the focus to be smeared out in space. In particular, spherical mirrors exhibit spherical aberration. Curved mirrors can form images with magnification greater than or less than one, and the magnification can be negative, indicating that the image is inverted. An upright image formed by reflection in a mirror is always virtual, while an inverted image is real and can be projected onto a screen.\n\nRefraction occurs when light travels through an area of space that has a changing index of refraction; this principle allows for lenses and the focusing of light. The simplest case of refraction occurs when there is an interface between a uniform medium with index of refraction formula_2 and another medium with index of refraction formula_3. In such situations, Snell's Law describes the resulting deflection of the light ray:\n\nwhere formula_5 and formula_6 are the angles between the normal (to the interface) and the incident and refracted waves, respectively.\n\nThe index of refraction of a medium is related to the speed, , of light in that medium by\nwhere is the speed of light in vacuum.\n\nSnell's Law can be used to predict the deflection of light rays as they pass through linear media as long as the indexes of refraction and the geometry of the media are known. For example, the propagation of light through a prism results in the light ray being deflected depending on the shape and orientation of the prism. In most materials, the index of refraction varies with the frequency of the light. Taking this into account, Snell's Law can be used to predict how a prism will disperse light into a spectrum. The discovery of this phenomenon when passing light through a prism is famously attributed to Isaac Newton.\n\nSome media have an index of refraction which varies gradually with position and, thus, light rays in the medium are curved. This effect is responsible for mirages seen on hot days: a change in index of refraction air with height causes light rays to bend, creating the appearance of specular reflections in the distance (as if on the surface of a pool of water). Optical materials with varying index of refraction are called gradient-index (GRIN) materials. Such materials are used to make gradient-index optics.\n\nFor light rays travelling from a material with a high index of refraction to a material with a low index of refraction, Snell's law predicts that there is no formula_6 when formula_5 is large. In this case, no transmission occurs; all the light is reflected. This phenomenon is called total internal reflection and allows for fibre optics technology. As light travels down an optical fibre, it undergoes total internal reflection allowing for essentially no light to be lost over the length of the cable.\n\nA device which produces converging or diverging light rays due to refraction is known as a \"lens\". Lenses are characterized by their focal length: a converging lens has positive focal length, while a diverging lens has negative focal length. Smaller focal length indicates that the lens has a stronger converging or diverging effect. The focal length of a simple lens in air is given by the lensmaker's equation.\n\nRay tracing can be used to show how images are formed by a lens. For a thin lens in air, the location of the image is given by the simple equation\n\nwhere formula_11 is the distance from the object to the lens, formula_12 is the distance from the lens to the image, and formula_13 is the focal length of the lens. In the sign convention used here, the object and image distances are positive if the object and image are on opposite sides of the lens.\nIncoming parallel rays are focused by a converging lens onto a spot one focal length from the lens, on the far side of the lens. This is called the rear focal point of the lens. Rays from an object at finite distance are focused further from the lens than the focal distance; the closer the object is to the lens, the further the image is from the lens.\n\nWith diverging lenses, incoming parallel rays diverge after going through the lens, in such a way that they seem to have originated at a spot one focal length in front of the lens. This is the lens's front focal point. Rays from an object at finite distance are associated with a virtual image that is closer to the lens than the focal point, and on the same side of the lens as the object. The closer the object is to the lens, the closer the virtual image is to the lens. As with mirrors, upright images produced by a single lens are virtual, while inverted images are real.\n\nLenses suffer from aberrations that distort images. \"Monochromatic aberrations\" occur because the geometry of the lens does not perfectly direct rays from each object point to a single point on the image, while chromatic aberration occurs because the index of refraction of the lens varies with the wavelength of the light.\n\nIn physical optics, light is considered to propagate as a wave. This model predicts phenomena such as interference and diffraction, which are not explained by geometric optics. The speed of light waves in air is approximately 3.0×10 m/s (exactly 299,792,458 m/s in vacuum). The wavelength of visible light waves varies between 400 and 700 nm, but the term \"light\" is also often applied to infrared (0.7–300 μm) and ultraviolet radiation (10–400 nm). \nThe wave model can be used to make predictions about how an optical system will behave without requiring an explanation of what is \"waving\" in what medium. Until the middle of the 19th century, most physicists believed in an \"ethereal\" medium in which the light disturbance propagated. The existence of electromagnetic waves was predicted in 1865 by Maxwell's equations. These waves propagate at the speed of light and have varying electric and magnetic fields which are orthogonal to one another, and also to the direction of propagation of the waves. Light waves are now generally treated as electromagnetic waves except when quantum mechanical effects have to be considered.\n\nMany simplified approximations are available for analysing and designing optical systems. Most of these use a single scalar quantity to represent the electric field of the light wave, rather than using a vector model with orthogonal electric and magnetic vectors.\nThe Huygens–Fresnel equation is one such model. This was derived empirically by Fresnel in 1815, based on Huygens' hypothesis that each point on a wavefront generates a secondary spherical wavefront, which Fresnel combined with the principle of superposition of waves. The Kirchhoff diffraction equation, which is derived using Maxwell's equations, puts the Huygens-Fresnel equation on a firmer physical foundation. Examples of the application of Huygens–Fresnel principle can be found in the sections on diffraction and Fraunhofer diffraction.\n\nMore rigorous models, involving the modelling of both electric and magnetic fields of the light wave, are required when dealing with the detailed interaction of light with materials where the interaction depends on their electric and magnetic properties. For instance, the behaviour of a light wave interacting with a metal surface is quite different from what happens when it interacts with a dielectric material. A vector model must also be used to model polarised light.\n\nNumerical modeling techniques such as the finite element method, the boundary element method and the transmission-line matrix method can be used to model the propagation of light in systems which cannot be solved analytically. Such models are computationally demanding and are normally only used to solve small-scale problems that require accuracy beyond that which can be achieved with analytical solutions.\n\nAll of the results from geometrical optics can be recovered using the techniques of Fourier optics which apply many of the same mathematical and analytical techniques used in acoustic engineering and signal processing.\n\nGaussian beam propagation is a simple paraxial physical optics model for the propagation of coherent radiation such as laser beams. This technique partially accounts for diffraction, allowing accurate calculations of the rate at which a laser beam expands with distance, and the minimum size to which the beam can be focused. Gaussian beam propagation thus bridges the gap between geometric and physical optics.\n\nIn the absence of nonlinear effects, the superposition principle can be used to predict the shape of interacting waveforms through the simple addition of the disturbances. This interaction of waves to produce a resulting pattern is generally termed \"interference\" and can result in a variety of outcomes. If two waves of the same wavelength and frequency are \"in phase\", both the wave crests and wave troughs align. This results in constructive interference and an increase in the amplitude of the wave, which for light is associated with a brightening of the waveform in that location. Alternatively, if the two waves of the same wavelength and frequency are out of phase, then the wave crests will align with wave troughs and vice versa. This results in destructive interference and a decrease in the amplitude of the wave, which for light is associated with a dimming of the waveform at that location. See below for an illustration of this effect.\n\nSince the Huygens–Fresnel principle states that every point of a wavefront is associated with the production of a new disturbance, it is possible for a wavefront to interfere with itself constructively or destructively at different locations producing bright and dark fringes in regular and predictable patterns. Interferometry is the science of measuring these patterns, usually as a means of making precise determinations of distances or angular resolutions. The Michelson interferometer was a famous instrument which used interference effects to accurately measure the speed of light.\n\nThe appearance of thin films and coatings is directly affected by interference effects. Antireflective coatings use destructive interference to reduce the reflectivity of the surfaces they coat, and can be used to minimise glare and unwanted reflections. The simplest case is a single layer with thickness one-fourth the wavelength of incident light. The reflected wave from the top of the film and the reflected wave from the film/material interface are then exactly 180° out of phase, causing destructive interference. The waves are only exactly out of phase for one wavelength, which would typically be chosen to be near the centre of the visible spectrum, around 550 nm. More complex designs using multiple layers can achieve low reflectivity over a broad band, or extremely low reflectivity at a single wavelength.\n\nConstructive interference in thin films can create strong reflection of light in a range of wavelengths, which can be narrow or broad depending on the design of the coating. These films are used to make dielectric mirrors, interference filters, heat reflectors, and filters for colour separation in colour television cameras. This interference effect is also what causes the colourful rainbow patterns seen in oil slicks.\n\nDiffraction is the process by which light interference is most commonly observed. The effect was first described in 1665 by Francesco Maria Grimaldi, who also coined the term from the Latin \"diffringere\", 'to break into pieces'. Later that century, Robert Hooke and Isaac Newton also described phenomena now known to be diffraction in Newton's rings while James Gregory recorded his observations of diffraction patterns from bird feathers.\n\nThe first physical optics model of diffraction that relied on the Huygens–Fresnel principle was developed in 1803 by Thomas Young in his interference experiments with the interference patterns of two closely spaced slits. Young showed that his results could only be explained if the two slits acted as two unique sources of waves rather than corpuscles. In 1815 and 1818, Augustin-Jean Fresnel firmly established the mathematics of how wave interference can account for diffraction.\n\nThe simplest physical models of diffraction use equations that describe the angular separation of light and dark fringes due to light of a particular wavelength (λ). In general, the equation takes the form\n\nwhere formula_15 is the separation between two wavefront sources (in the case of Young's experiments, it was two slits), formula_16 is the angular separation between the central fringe and the formula_17th order fringe, where the central maximum is formula_18.\n\nThis equation is modified slightly to take into account a variety of situations such as diffraction through a single gap, diffraction through multiple slits, or diffraction through a diffraction grating that contains a large number of slits at equal spacing. More complicated models of diffraction require working with the mathematics of Fresnel or Fraunhofer diffraction.\n\nX-ray diffraction makes use of the fact that atoms in a crystal have regular spacing at distances that are on the order of one angstrom. To see diffraction patterns, x-rays with similar wavelengths to that spacing are passed through the crystal. Since crystals are three-dimensional objects rather than two-dimensional gratings, the associated diffraction pattern varies in two directions according to Bragg reflection, with the associated bright spots occurring in unique patterns and formula_15 being twice the spacing between atoms.\n\nDiffraction effects limit the ability for an optical detector to optically resolve separate light sources. In general, light that is passing through an aperture will experience diffraction and the best images that can be created (as described in diffraction-limited optics) appear as a central spot with surrounding bright rings, separated by dark nulls; this pattern is known as an Airy pattern, and the central bright lobe as an Airy disk. The size of such a disk is given by\n\nwhere \"θ\" is the angular resolution, \"λ\" is the wavelength of the light, and \"D\" is the diameter of the lens aperture. If the angular separation of the two points is significantly less than the Airy disk angular radius, then the two points cannot be resolved in the image, but if their angular separation is much greater than this, distinct images of the two points are formed and they can therefore be resolved. Rayleigh defined the somewhat arbitrary \"Rayleigh criterion\" that two points whose angular separation is equal to the Airy disk radius (measured to first null, that is, to the first place where no light is seen) can be considered to be resolved. It can be seen that the greater the diameter of the lens or its aperture, the finer the resolution. Interferometry, with its ability to mimic extremely large baseline apertures, allows for the greatest angular resolution possible.\n\nFor astronomical imaging, the atmosphere prevents optimal resolution from being achieved in the visible spectrum due to the atmospheric scattering and dispersion which cause stars to twinkle. Astronomers refer to this effect as the quality of astronomical seeing. Techniques known as adaptive optics have been used to eliminate the atmospheric disruption of images and achieve results that approach the diffraction limit.\n\nRefractive processes take place in the physical optics limit, where the wavelength of light is similar to other distances, as a kind of scattering. The simplest type of scattering is Thomson scattering which occurs when electromagnetic waves are deflected by single particles. In the limit of Thomson scattering, in which the wavelike nature of light is evident, light is dispersed independent of the frequency, in contrast to Compton scattering which is frequency-dependent and strictly a quantum mechanical process, involving the nature of light as particles. In a statistical sense, elastic scattering of light by numerous particles much smaller than the wavelength of the light is a process known as Rayleigh scattering while the similar process for scattering by particles that are similar or larger in wavelength is known as Mie scattering with the Tyndall effect being a commonly observed result. A small proportion of light scattering from atoms or molecules may undergo Raman scattering, wherein the frequency changes due to excitation of the atoms and molecules. Brillouin scattering occurs when the frequency of light changes due to local changes with time and movements of a dense material.\n\nDispersion occurs when different frequencies of light have different phase velocities, due either to material properties (\"material dispersion\") or to the geometry of an optical waveguide (\"waveguide dispersion\"). The most familiar form of dispersion is a decrease in index of refraction with increasing wavelength, which is seen in most transparent materials. This is called \"normal dispersion\". It occurs in all dielectric materials, in wavelength ranges where the material does not absorb light. In wavelength ranges where a medium has significant absorption, the index of refraction can increase with wavelength. This is called \"anomalous dispersion\".\n\nThe separation of colours by a prism is an example of normal dispersion. At the surfaces of the prism, Snell's law predicts that light incident at an angle θ to the normal will be refracted at an angle arcsin(sin (θ) / \"n\"). Thus, blue light, with its higher refractive index, is bent more strongly than red light, resulting in the well-known rainbow pattern.\n\nMaterial dispersion is often characterised by the Abbe number, which gives a simple measure of dispersion based on the index of refraction at three specific wavelengths. Waveguide dispersion is dependent on the propagation constant. Both kinds of dispersion cause changes in the group characteristics of the wave, the features of the wave packet that change with the same frequency as the amplitude of the electromagnetic wave. \"Group velocity dispersion\" manifests as a spreading-out of the signal \"envelope\" of the radiation and can be quantified with a group dispersion delay parameter:\n\nwhere formula_22 is the group velocity. For a uniform medium, the group velocity is\n\nwhere \"n\" is the index of refraction and \"c\" is the speed of light in a vacuum. This gives a simpler form for the dispersion delay parameter:\n\nIf \"D\" is less than zero, the medium is said to have \"positive dispersion\" or normal dispersion. If \"D\" is greater than zero, the medium has \"negative dispersion\". If a light pulse is propagated through a normally dispersive medium, the result is the higher frequency components slow down more than the lower frequency components. The pulse therefore becomes \"positively chirped\", or \"up-chirped\", increasing in frequency with time. This causes the spectrum coming out of a prism to appear with red light the least refracted and blue/violet light the most refracted. Conversely, if a pulse travels through an anomalously (negatively) dispersive medium, high frequency components travel faster than the lower ones, and the pulse becomes \"negatively chirped\", or \"down-chirped\", decreasing in frequency with time.\n\nThe result of group velocity dispersion, whether negative or positive, is ultimately temporal spreading of the pulse. This makes dispersion management extremely important in optical communications systems based on optical fibres, since if dispersion is too high, a group of pulses representing information will each spread in time and merge, making it impossible to extract the signal.\n\nPolarization is a general property of waves that describes the orientation of their oscillations. For transverse waves such as many electromagnetic waves, it describes the orientation of the oscillations in the plane perpendicular to the wave's direction of travel. The oscillations may be oriented in a single direction (linear polarization), or the oscillation direction may rotate as the wave travels (circular or elliptical polarization). Circularly polarised waves can rotate rightward or leftward in the direction of travel, and which of those two rotations is present in a wave is called the wave's chirality.\n\nThe typical way to consider polarization is to keep track of the orientation of the electric field vector as the electromagnetic wave propagates. The electric field vector of a plane wave may be arbitrarily divided into two perpendicular components labeled \"x\" and \"y\" (with z indicating the direction of travel). The shape traced out in the x-y plane by the electric field vector is a Lissajous figure that describes the \"polarization state\". The following figures show some examples of the evolution of the electric field vector (blue), with time (the vertical axes), at a particular point in space, along with its \"x\" and \"y\" components (red/left and green/right), and the path traced by the vector in the plane (purple): The same evolution would occur when looking at the electric field at a particular time while evolving the point in space, along the direction opposite to propagation.\n\nIn the leftmost figure above, the x and y components of the light wave are in phase. In this case, the ratio of their strengths is constant, so the direction of the electric vector (the vector sum of these two components) is constant. Since the tip of the vector traces out a single line in the plane, this special case is called linear polarization. The direction of this line depends on the relative amplitudes of the two components.\n\nIn the middle figure, the two orthogonal components have the same amplitudes and are 90° out of phase. In this case, one component is zero when the other component is at maximum or minimum amplitude. There are two possible phase relationships that satisfy this requirement: the \"x\" component can be 90° ahead of the \"y\" component or it can be 90° behind the \"y\" component. In this special case, the electric vector traces out a circle in the plane, so this polarization is called circular polarization. The rotation direction in the circle depends on which of the two phase relationships exists and corresponds to \"right-hand circular polarization\" and \"left-hand circular polarization\".\n\nIn all other cases, where the two components either do not have the same amplitudes and/or their phase difference is neither zero nor a multiple of 90°, the polarization is called elliptical polarization because the electric vector traces out an ellipse in the plane (the \"polarization ellipse\"). This is shown in the above figure on the right. Detailed mathematics of polarization is done using Jones calculus and is characterised by the Stokes parameters.\n\nMedia that have different indexes of refraction for different polarization modes are called \"birefringent\". Well known manifestations of this effect appear in optical wave plates/retarders (linear modes) and in Faraday rotation/optical rotation (circular modes). If the path length in the birefringent medium is sufficient, plane waves will exit the material with a significantly different propagation direction, due to refraction. For example, this is the case with macroscopic crystals of calcite, which present the viewer with two offset, orthogonally polarised images of whatever is viewed through them. It was this effect that provided the first discovery of polarization, by Erasmus Bartholinus in 1669. In addition, the phase shift, and thus the change in polarization state, is usually frequency dependent, which, in combination with dichroism, often gives rise to bright colours and rainbow-like effects. In mineralogy, such properties, known as pleochroism, are frequently exploited for the purpose of identifying minerals using polarization microscopes. Additionally, many plastics that are not normally birefringent will become so when subject to mechanical stress, a phenomenon which is the basis of photoelasticity. Non-birefringent methods, to rotate the linear polarization of light beams, include the use of prismatic polarization rotators which use total internal reflection in a prism set designed for efficient collinear transmission.\n\nMedia that reduce the amplitude of certain polarization modes are called \"dichroic\", with devices that block nearly all of the radiation in one mode known as \"polarizing filters\" or simply \"polarisers\". Malus' law, which is named after Étienne-Louis Malus, says that when a perfect polariser is placed in a linear polarised beam of light, the intensity, \"I\", of the light that passes through is given by\n\nwhere\n\nA beam of unpolarised light can be thought of as containing a uniform mixture of linear polarizations at all possible angles. Since the average value of formula_26 is 1/2, the transmission coefficient becomes\n\nIn practice, some light is lost in the polariser and the actual transmission of unpolarised light will be somewhat lower than this, around 38% for Polaroid-type polarisers but considerably higher (>49.9%) for some birefringent prism types.\n\nIn addition to birefringence and dichroism in extended media, polarization effects can also occur at the (reflective) interface between two materials of different refractive index. These effects are treated by the Fresnel equations. Part of the wave is transmitted and part is reflected, with the ratio depending on angle of incidence and the angle of refraction. In this way, physical optics recovers Brewster's angle. When light reflects from a thin film on a surface, interference between the reflections from the film's surfaces can produce polarization in the reflected and transmitted light.\n\nMost sources of electromagnetic radiation contain a large number of atoms or molecules that emit light. The orientation of the electric fields produced by these emitters may not be correlated, in which case the light is said to be \"unpolarised\". If there is partial correlation between the emitters, the light is \"partially polarised\". If the polarization is consistent across the spectrum of the source, partially polarised light can be described as a superposition of a completely unpolarised component, and a completely polarised one. One may then describe the light in terms of the degree of polarization, and the parameters of the polarization ellipse.\n\nLight reflected by shiny transparent materials is partly or fully polarised, except when the light is normal (perpendicular) to the surface. It was this effect that allowed the mathematician Étienne-Louis Malus to make the measurements that allowed for his development of the first mathematical models for polarised light. Polarization occurs when light is scattered in the atmosphere. The scattered light produces the brightness and colour in clear skies. This partial polarization of scattered light can be taken advantage of using polarizing filters to darken the sky in photographs. Optical polarization is principally of importance in chemistry due to circular dichroism and optical rotation (\"\"circular birefringence\"\") exhibited by optically active (chiral) molecules.\n\n\"Modern optics\" encompasses the areas of optical science and engineering that became popular in the 20th century. These areas of optical science typically relate to the electromagnetic or quantum properties of light but do include other topics. A major subfield of modern optics, quantum optics, deals with specifically quantum mechanical properties of light. Quantum optics is not just theoretical; some modern devices, such as lasers, have principles of operation that depend on quantum mechanics. Light detectors, such as photomultipliers and channeltrons, respond to individual photons. Electronic image sensors, such as CCDs, exhibit shot noise corresponding to the statistics of individual photon events. Light-emitting diodes and photovoltaic cells, too, cannot be understood without quantum mechanics. In the study of these devices, quantum optics often overlaps with quantum electronics.\n\nSpecialty areas of optics research include the study of how light interacts with specific materials as in crystal optics and metamaterials. Other research focuses on the phenomenology of electromagnetic waves as in singular optics, non-imaging optics, non-linear optics, statistical optics, and radiometry. Additionally, computer engineers have taken an interest in integrated optics, machine vision, and photonic computing as possible components of the \"next generation\" of computers.\n\nToday, the pure science of optics is called optical science or optical physics to distinguish it from applied optical sciences, which are referred to as optical engineering. Prominent subfields of optical engineering include illumination engineering, photonics, and optoelectronics with practical applications like lens design, fabrication and testing of optical components, and image processing. Some of these fields overlap, with nebulous boundaries between the subjects terms that mean slightly different things in different parts of the world and in different areas of industry. A professional community of researchers in nonlinear optics has developed in the last several decades due to advances in laser technology.\n\nA laser is a device that emits light (electromagnetic radiation) through a process called \"stimulated emission\". The term \"laser\" is an acronym for \"Light Amplification by Stimulated Emission of Radiation\". Laser light is usually spatially coherent, which means that the light either is emitted in a narrow, low-divergence beam, or can be converted into one with the help of optical components such as lenses. Because the microwave equivalent of the laser, the \"maser\", was developed first, devices that emit microwave and radio frequencies are usually called \"masers\".\n\nThe first working laser was demonstrated on 16 May 1960 by Theodore Maiman at Hughes Research Laboratories. When first invented, they were called \"a solution looking for a problem\". Since then, lasers have become a multibillion-dollar industry, finding utility in thousands of highly varied applications. The first application of lasers visible in the daily lives of the general population was the supermarket barcode scanner, introduced in 1974. The laserdisc player, introduced in 1978, was the first successful consumer product to include a laser, but the compact disc player was the first laser-equipped device to become truly common in consumers' homes, beginning in 1982. These optical storage devices use a semiconductor laser less than a millimetre wide to scan the surface of the disc for data retrieval. Fibre-optic communication relies on lasers to transmit large amounts of information at the speed of light. Other common applications of lasers include laser printers and laser pointers. Lasers are used in medicine in areas such as bloodless surgery, laser eye surgery, and laser capture microdissection and in military applications such as missile defence systems, electro-optical countermeasures (EOCM), and lidar. Lasers are also used in holograms, bubblegrams, laser light shows, and laser hair removal.\n\nThe Kapitsa–Dirac effect causes beams of particles to diffract as the result of meeting a standing wave of light. Light can be used to position matter using various phenomena (see optical tweezers).\n\nOptics is part of everyday life. The ubiquity of visual systems in biology indicates the central role optics plays as the science of one of the five senses. Many people benefit from eyeglasses or contact lenses, and optics are integral to the functioning of many consumer goods including cameras. Rainbows and mirages are examples of optical phenomena. Optical communication provides the backbone for both the Internet and modern telephony.\n\nThe human eye functions by focusing light onto a layer of photoreceptor cells called the retina, which forms the inner lining of the back of the eye. The focusing is accomplished by a series of transparent media. Light entering the eye passes first through the cornea, which provides much of the eye's optical power. The light then continues through the fluid just behind the cornea—the anterior chamber, then passes through the pupil. The light then passes through the lens, which focuses the light further and allows adjustment of focus. The light then passes through the main body of fluid in the eye—the vitreous humour, and reaches the retina. The cells in the retina line the back of the eye, except for where the optic nerve exits; this results in a blind spot.\n\nThere are two types of photoreceptor cells, rods and cones, which are sensitive to different aspects of light. Rod cells are sensitive to the intensity of light over a wide frequency range, thus are responsible for black-and-white vision. Rod cells are not present on the fovea, the area of the retina responsible for central vision, and are not as responsive as cone cells to spatial and temporal changes in light. There are, however, twenty times more rod cells than cone cells in the retina because the rod cells are present across a wider area. Because of their wider distribution, rods are responsible for peripheral vision.\n\nIn contrast, cone cells are less sensitive to the overall intensity of light, but come in three varieties that are sensitive to different frequency-ranges and thus are used in the perception of colour and photopic vision. Cone cells are highly concentrated in the fovea and have a high visual acuity meaning that they are better at spatial resolution than rod cells. Since cone cells are not as sensitive to dim light as rod cells, most night vision is limited to rod cells. Likewise, since cone cells are in the fovea, central vision (including the vision needed to do most reading, fine detail work such as sewing, or careful examination of objects) is done by cone cells.\n\nCiliary muscles around the lens allow the eye's focus to be adjusted. This process is known as accommodation. The near point and far point define the nearest and farthest distances from the eye at which an object can be brought into sharp focus. For a person with normal vision, the far point is located at infinity. The near point's location depends on how much the muscles can increase the curvature of the lens, and how inflexible the lens has become with age. Optometrists, ophthalmologists, and opticians usually consider an appropriate near point to be closer than normal reading distance—approximately 25 cm.\n\nDefects in vision can be explained using optical principles. As people age, the lens becomes less flexible and the near point recedes from the eye, a condition known as presbyopia. Similarly, people suffering from hyperopia cannot decrease the focal length of their lens enough to allow for nearby objects to be imaged on their retina. Conversely, people who cannot increase the focal length of their lens enough to allow for distant objects to be imaged on the retina suffer from myopia and have a far point that is considerably closer than infinity. A condition known as astigmatism results when the cornea is not spherical but instead is more curved in one direction. This causes horizontally extended objects to be focused on different parts of the retina than vertically extended objects, and results in distorted images.\n\nAll of these conditions can be corrected using corrective lenses. For presbyopia and hyperopia, a converging lens provides the extra curvature necessary to bring the near point closer to the eye while for myopia a diverging lens provides the curvature necessary to send the far point to infinity. Astigmatism is corrected with a cylindrical surface lens that curves more strongly in one direction than in another, compensating for the non-uniformity of the cornea.\n\nThe optical power of corrective lenses is measured in diopters, a value equal to the reciprocal of the focal length measured in metres; with a positive focal length corresponding to a converging lens and a negative focal length corresponding to a diverging lens. For lenses that correct for astigmatism as well, three numbers are given: one for the spherical power, one for the cylindrical power, and one for the angle of orientation of the astigmatism.\n\nOptical illusions (also called visual illusions) are characterized by visually perceived images that differ from objective reality. The information gathered by the eye is processed in the brain to give a percept that differs from the object being imaged. Optical illusions can be the result of a variety of phenomena including physical effects that create images that are different from the objects that make them, the physiological effects on the eyes and brain of excessive stimulation (e.g. brightness, tilt, colour, movement), and cognitive illusions where the eye and brain make unconscious inferences.\n\nCognitive illusions include some which result from the unconscious misapplication of certain optical principles. For example, the Ames room, Hering, Müller-Lyer, Orbison, Ponzo, Sander, and Wundt illusions all rely on the suggestion of the appearance of distance by using converging and diverging lines, in the same way that parallel light rays (or indeed any set of parallel lines) appear to converge at a vanishing point at infinity in two-dimensionally rendered images with artistic perspective. This suggestion is also responsible for the famous moon illusion where the moon, despite having essentially the same angular size, appears much larger near the horizon than it does at zenith. This illusion so confounded Ptolemy that he incorrectly attributed it to atmospheric refraction when he described it in his treatise, \"Optics\".\n\nAnother type of optical illusion exploits broken patterns to trick the mind into perceiving symmetries or asymmetries that are not present. Examples include the café wall, Ehrenstein, Fraser spiral, Poggendorff, and Zöllner illusions. Related, but not strictly illusions, are patterns that occur due to the superimposition of periodic structures. For example, transparent tissues with a grid structure produce shapes known as moiré patterns, while the superimposition of periodic transparent patterns comprising parallel opaque lines or curves produces line moiré patterns.\n\nSingle lenses have a variety of applications including photographic lenses, corrective lenses, and magnifying glasses while single mirrors are used in parabolic reflectors and rear-view mirrors. Combining a number of mirrors, prisms, and lenses produces compound optical instruments which have practical uses. For example, a periscope is simply two plane mirrors aligned to allow for viewing around obstructions. The most famous compound optical instruments in science are the microscope and the telescope which were both invented by the Dutch in the late 16th century.\n\nMicroscopes were first developed with just two lenses: an objective lens and an eyepiece. The objective lens is essentially a magnifying glass and was designed with a very small focal length while the eyepiece generally has a longer focal length. This has the effect of producing magnified images of close objects. Generally, an additional source of illumination is used since magnified images are dimmer due to the conservation of energy and the spreading of light rays over a larger surface area. Modern microscopes, known as \"compound microscopes\" have many lenses in them (typically four) to optimize the functionality and enhance image stability. A slightly different variety of microscope, the comparison microscope, looks at side-by-side images to produce a stereoscopic binocular view that appears three dimensional when used by humans.\n\nThe first telescopes, called \"refracting telescopes\" were also developed with a single objective and eyepiece lens. In contrast to the microscope, the objective lens of the telescope was designed with a large focal length to avoid optical aberrations. The objective focuses an image of a distant object at its focal point which is adjusted to be at the focal point of an eyepiece of a much smaller focal length. The main goal of a telescope is not necessarily magnification, but rather collection of light which is determined by the physical size of the objective lens. Thus, telescopes are normally indicated by the diameters of their objectives rather than by the magnification which can be changed by switching eyepieces. Because the magnification of a telescope is equal to the focal length of the objective divided by the focal length of the eyepiece, smaller focal-length eyepieces cause greater magnification.\n\nSince crafting large lenses is much more difficult than crafting large mirrors, most modern telescopes are \"reflecting telescopes\", that is, telescopes that use a primary mirror rather than an objective lens. The same general optical considerations apply to reflecting telescopes that applied to refracting telescopes, namely, the larger the primary mirror, the more light collected, and the magnification is still equal to the focal length of the primary mirror divided by the focal length of the eyepiece. Professional telescopes generally do not have eyepieces and instead place an instrument (often a charge-coupled device) at the focal point instead.\n\nThe optics of photography involves both lenses and the medium in which the electromagnetic radiation is recorded, whether it be a plate, film, or charge-coupled device. Photographers must consider the reciprocity of the camera and the shot which is summarized by the relation\n\nIn other words, the smaller the aperture (giving greater depth of focus), the less light coming in, so the length of time has to be increased (leading to possible blurriness if motion occurs). An example of the use of the law of reciprocity is the Sunny 16 rule which gives a rough estimate for the settings needed to estimate the proper exposure in daylight.\n\nA camera's aperture is measured by a unitless number called the f-number or f-stop, #, often notated as formula_28, and given by\nwhere formula_13 is the focal length, and formula_31 is the diameter of the entrance pupil. By convention, \"#\" is treated as a single symbol, and specific values of # are written by replacing the number sign with the value. The two ways to increase the f-stop are to either decrease the diameter of the entrance pupil or change to a longer focal length (in the case of a zoom lens, this can be done by simply adjusting the lens). Higher f-numbers also have a larger depth of field due to the lens approaching the limit of a pinhole camera which is able to focus all images perfectly, regardless of distance, but requires very long exposure times.\n\nThe field of view that the lens will provide changes with the focal length of the lens. There are three basic classifications based on the relationship to the diagonal size of the film or sensor size of the camera to the focal length of the lens:\n\n\nModern zoom lenses may have some or all of these attributes.\n\nThe absolute value for the exposure time required depends on how sensitive to light the medium being used is (measured by the film speed, or, for digital media, by the quantum efficiency). Early photography used media that had very low light sensitivity, and so exposure times had to be long even for very bright shots. As technology has improved, so has the sensitivity through film cameras and digital cameras.\n\nOther results from physical and geometrical optics apply to camera optics. For example, the maximum resolution capability of a particular camera set-up is determined by the diffraction limit associated with the pupil size and given, roughly, by the Rayleigh criterion.\n\nThe unique optical properties of the atmosphere cause a wide range of spectacular optical phenomena. The blue colour of the sky is a direct result of Rayleigh scattering which redirects higher frequency (blue) sunlight back into the field of view of the observer. Because blue light is scattered more easily than red light, the sun takes on a reddish hue when it is observed through a thick atmosphere, as during a sunrise or sunset. Additional particulate matter in the sky can scatter different colours at different angles creating colourful glowing skies at dusk and dawn. Scattering off of ice crystals and other particles in the atmosphere are responsible for halos, afterglows, coronas, rays of sunlight, and sun dogs. The variation in these kinds of phenomena is due to different particle sizes and geometries.\n\nMirages are optical phenomena in which light rays are bent due to thermal variations in the refraction index of air, producing displaced or heavily distorted images of distant objects. Other dramatic optical phenomena associated with this include the Novaya Zemlya effect where the sun appears to rise earlier than predicted with a distorted shape. A spectacular form of refraction occurs with a temperature inversion called the Fata Morgana where objects on the horizon or even beyond the horizon, such as islands, cliffs, ships or icebergs, appear elongated and elevated, like \"fairy tale castles\".\n\nRainbows are the result of a combination of internal reflection and dispersive refraction of light in raindrops. A single reflection off the backs of an array of raindrops produces a rainbow with an angular size on the sky that ranges from 40° to 42° with red on the outside. Double rainbows are produced by two internal reflections with angular size of 50.5° to 54° with violet on the outside. Because rainbows are seen with the sun 180° away from the centre of the rainbow, rainbows are more prominent the closer the sun is to the horizon.\n\n\n\n\n\n\n",
                "Cinematography\n\nCinematography (also called \"Direction of Photography\") is the science or art of motion-picture photography by recording light or other electromagnetic radiation, either electronically by means of an image sensor, or chemically by means of a light-sensitive material such as film stock.\n\nTypically, a lens is used to repeatedly focus the light reflected from objects into real images on the light-sensitive surface inside a camera during a questioned exposure, creating multiple images. With an electronic image sensor, this produces an electrical charge at each pixel, which is electronically processed and stored in a video file for subsequent display or processing. The result with photographic emulsion is a series of invisible latent images on the film stock, which are later chemically \"developed\" into a visible image. The images on the film stock are played back at a rapid speed and projected onto a screen, creating the illusion of motion.\n\nCinematography finds uses in many fields of science and business as well as for entertainment purposes and mass communication.\n\nThe word \"cinematography\" was created from the Greek words (\"kinema\"), meaning \"movement, motion\" and (\"graphein\") meaning \"to record\", together meaning \"recording motion.\" The word used to refer to the art, process, or job of filming movies, but later its meaning was restricted to \"motion picture photography.\"\n\nIn the 1830s, moving images were produced on revolving drums and disks, with independent invention by Simon von Stampfer (stroboscope) in Austria, Joseph Plateau (phenakistoscope) in Belgium, and William Horner (zoetrope) in Britain.\n\nIn 1845, Francis Ronalds invented the first successful camera able to make continuous recordings of the varying indications of meteorological and geomagnetic instruments over time. The cameras were supplied to numerous observatories around the world and some remained in use until well into the 20th century.\n\nWilliam Lincoln patented a device, in 1867, that showed animated pictures called the \"wheel of life\" or \"zoopraxiscope\". In it, moving drawings or photographs were watched through a slit.\n\nOn 19 June 1873, Eadweard Muybridge successfully photographed a horse named \"Sallie Gardner\" in fast motion using a series of 24 stereoscopic cameras. The cameras were arranged along a track parallel to the horse's, and each camera shutter was controlled by a trip wire triggered by the horse's hooves. They were 21 inches apart to cover the 20 feet taken by the horse stride, taking pictures at one thousandth of a second. At the end of the decade, Muybridge had adapted sequences of his photographs to a zoopraxiscope for short, primitive projected \"movies,\" which were sensations on his lecture tours by 1879 or 1880.\n\nNine years later, in 1882, French scientist Étienne-Jules Marey invented a chronophotographic gun, which was capable of taking 12 consecutive frames a second, recording all the frames of the same picture.\n\nThe late nineteenth to the early twentieth century brought rise to the use of film not only for entertainment purposes but for scientific exploration as well. French biologist and filmmaker Jean Painleve lobbied heavily for the use of film in the scientific field, as the new medium was more efficient in capturing and documenting the behavior, movement, and environment of microorganisms, cells, and bacteria, than the naked eye. The introduction of film into scientific fields allowed for not only the viewing \"new images and objects, such as cells and natural objects, but also the viewing of them in real time\", whereas prior to the invention of moving pictures, scientists and doctors alike had to rely on hand drawn sketches of human anatomy and its microorganisms. This posed a great inconvenience in the science and medical worlds. The development of film and increased usage of cameras allowed doctors and scientists to grasp a better understanding and knowledge of their projects.\n\nThe experimental film \"Roundhay Garden Scene\", filmed by Louis Le Prince on 14 October 1888, in Roundhay, Leeds, England, is the earliest surviving motion picture. This movie was shot on paper film.\n\nW. K. L. Dickson, working under the direction of Thomas Alva Edison, was the first to design a successful apparatus, the Kinetograph, patented in 1891. This camera took a series of instantaneous photographs on standard Eastman Kodak photographic emulsion coated onto a transparent celluloid strip 35 mm wide. The results of this work were first shown in public in 1893, using the viewing apparatus also designed by Dickson, the Kinetoscope. Contained within a large box, only one person at a time looking into it through a peephole could view the movie.\n\nIn the following year, Charles Francis Jenkins and his projector, the Phantoscope, made a successful audience viewing while Louis and Auguste Lumière perfected the Cinématographe, an apparatus that took, printed, and projected film, in Paris in December 1895. The Lumière brothers were the first to present projected, moving, photographic, pictures to a paying audience of more than one person.\n\nIn 1896, movie theaters were open in France (Paris, Lyon, Bordeaux, Nice, Marseille); Italy (Rome, Milan, Naples, Genoa, Venice, Bologna, Forlì); Brussels; and London.\n\nIn 1896, Edison showed his improved Vitascope projector, the first commercially successful projector in the U.S.\n\nCooper Hewitt invented mercury lamps which made it practical to shoot films indoors without sunlight in 1905.\n\nThe first animated cartoon was produced in 1906.\n\nCredits began to appear at the beginning of motion pictures in 1911.\n\nThe Bell and Howell 2709 movie camera invented in 1915 allowed directors to make close-ups without physically moving the camera.\n\nBy the late 1920s, most of the movies produced were sound films.\n\nWide screen formats were first experimented with in the 1950s.\n\nBy the 1970s, most movies were color films. IMAX and other 70mm formats gained popularity. Wide distribution of films became commonplace, setting the ground for \"blockbusters.\"\n\nFilm cinematography dominated the motion picture industry from its inception until the 2010s when digital cinematography became dominant. Film cinematography is still used by some directors, especially in specific applications or out of fondness of the format.\n\nFrom its birth in the 1880s, movies were predominantly monochrome. Contrary to popular belief, monochrome doesn't always mean black and white; it means a movie shot in a single tone or color. Since the cost of tinted film bases was substantially higher, most movies were produced in black and white monochrome. Even with the advent of early color experiments, the greater expense of color meant films were mostly made in black and white until the 1950s, when cheaper color processes were introduced, and in some years the percentage of films shot on color film surpassed 51%. By the 1960s, color became by far the dominant film stock. In the coming decades, the usage of color film greatly increased while monochrome films became scarce.\n\nAfter the advent of motion pictures, a tremendous amount of energy was invested in the production of photography in natural color. The invention of the talking picture further increased the demand for the use of color photography. However, in comparison to other technological advances of the time, the arrival of color photography was a relatively slow process.\n\nEarly movies were not actually color movies since they were shot monochrome and hand-colored or machine-colored afterwards. (Such movies are referred to as \"colored\" and not \"color\".) The earliest such example is the hand-tinted Annabelle Serpentine Dance in 1895 by Edison Manufacturing Company. Machine-based tinting later became popular. Tinting continued until the advent of natural color cinematography in the 1910s. Many black and white movies have been colorized recently using digital tinting. This includes footage shot from both world wars, sporting events and political propaganda.\n\nIn 1902, Edward Raymond Turner produced the first films with a natural color process rather than using colorization techniques. In 1908, kinemacolor was introduced. In the same year, the short film \"A Visit to the Seaside\" became the first natural color movie to be publicly presented.\n\nIn 1917, the earliest version of Technicolor was introduced. Kodachrome was introduced in 1935. Eastmancolor was introduced in 1950 and became the color standard for the rest of the century.\n\nIn the 2010s, color films were largely superseded by color digital cinematography.\n\nIn digital cinematography, the movie is shot on digital medium such as flash storage, as well as distributed through a digital medium such as a hard drive.\n\nBeginning in the late 1980s, Sony began marketing the concept of \"electronic cinematography,\" utilizing its analog Sony HDVS professional video cameras. The effort met with very little success. However, this led to one of the earliest digitally shot feature movies, \"Julia and Julia\", being produced in 1987. In 1998, with the introduction of HDCAM recorders and 1920 × 1080 pixel digital professional video cameras based on CCD technology, the idea, now re-branded as \"digital cinematography,\" began to gain traction in the market.\n\nShot and released in 1998, \"The Last Broadcast\" is believed by some to be the first feature-length video shot and edited entirely on consumer-level digital equipment. In May 1999, George Lucas challenged the supremacy of the movie-making medium of film for the first time by including footage filmed with high-definition digital cameras in \"\". In late 2013, Paramount became the first major studio to distribute movies to theaters in digital format, eliminating 35mm film entirely. Since then the demand of movies to be developed onto digital format rather than 35mm has increased drastically.\n\nAs digital technology improved, movie studios began increasingly shifting towards digital cinematography. Since the 2010s, digital cinematography has become the dominant form of cinematography after largely superseding film cinematography.\n\nNumerous aspects contribute to the art of cinematography, including:\n\nThe first film cameras were fastened directly to the head of a tripod or other support, with only the crudest kind of leveling devices provided, in the manner of the still-camera tripod heads of the period. The earliest film cameras were thus effectively fixed during the shot, and hence the first camera movements were the result of mounting a camera on a moving vehicle. The first known of these was a film shot by a Lumière cameraman from the back platform of a train leaving Jerusalem in 1896, and by 1898, there were a number of films shot from moving trains. Although listed under the general heading of \"panoramas\" in the sales catalogues of the time, those films shot straight forward from in front of a railway engine were usually specifically referred to as \"phantom rides\".\n\nIn 1897, Robert W. Paul had the first real rotating camera head made to put on a tripod, so that he could follow the passing processions of Queen Victoria's Diamond Jubilee in one uninterrupted shot. This device had the camera mounted on a vertical axis that could be rotated by a worm gear driven by turning a crank handle, and Paul put it on general sale the next year. Shots taken using such a \"panning\" head were also referred to as \"panoramas\" in the film catalogues of the first decade of the cinema. This eventually led to the creation of a panoramic photo as well.\n\nThe standard pattern for early film studios was provided by the studio which Georges Méliès had built in 1897. This had a glass roof and three glass walls constructed after the model of large studios for still photography, and it was fitted with thin cotton cloths that could be stretched below the roof to diffuse the direct ray of the sun on sunny days. The soft overall light without real shadows that this arrangement produced, and which also exists naturally on lightly overcast days, was to become the basis for film lighting in film studios for the next decade.\n\nCinematography can begin with digital image sensor or rolls of film. Advancements in film emulsion and grain structure provided a wide range of available film stocks. The selection of a film stock is one of the first decisions made in preparing a typical film production.\n\nAside from the film gauge selection – 8 mm (amateur), 16 mm (semi-professional), 35 mm (professional) and 65 mm (epic photography, rarely used except in special event venues) – the cinematographer has a selection of stocks in reversal (which, when developed, create a positive image) and negative formats along with a wide range of film speeds (varying sensitivity to light) from ISO 50 (slow, least sensitive to light) to 800 (very fast, extremely sensitive to light) and differing response to color (low saturation, high saturation) and contrast (varying levels between pure black (no exposure) and pure white (complete overexposure).\nAdvancements and adjustments to nearly all gauges of film create the \"super\" formats wherein the area of the film used to capture a single frame of an image is expanded, although the physical gauge of the film remains the same. Super 8 mm, Super 16 mm, and Super 35 mm all utilize more of the overall film area for the image than their \"regular\" non-super counterparts. The larger the film gauge, the higher the overall image resolution clarity and technical quality. The techniques used by the film laboratory to process the film stock can also offer a considerable variance in the image produced. By controlling the temperature and varying the duration in which the film is soaked in the development chemicals, and by skipping certain chemical processes (or partially skipping all of them), cinematographers can achieve very different looks from a single film stock in the laboratory. Some techniques that can be used are push processing, bleach bypass, and cross processing.\n\nMost of modern cinema uses digital cinematography and has no film stocks , but the cameras themselves can be adjusted in ways that go far beyond the abilities of one particular film stock. They can provide varying degrees of color sensitivity, image contrast, light sensitivity and so on. One camera can achieve all the various looks of different emulsions. Digital image adjustments such as ISO and contrast are executed by estimating the same adjustments that would take place if actual film were in use, and are thus vulnerable to the camera's sensor designers perceptions of various film stocks and image adjustment parameters.\n\nFilters, such as diffusion filters or color effect filters, are also widely used to enhance mood or dramatic effects. Most photographic filters are made up of two pieces of optical glass glued together with some form of image or light manipulation material between the glass. In the case of color filters, there is often a translucent color medium pressed between two planes of optical glass. Color filters work by blocking out certain color wavelengths of light from reaching the film. With color film, this works very intuitively wherein a blue filter will cut down on the passage of red, orange, and yellow light and create a blue tint on the film. In black-and-white photography, color filters are used somewhat counter intuitively; for instance a yellow filter, which cuts down on blue wavelengths of light, can be used to darken a daylight sky (by eliminating blue light from hitting the film, thus greatly underexposing the mostly blue sky) while not biasing most human flesh tone. Certain cinematographers, such as Christopher Doyle, are well known for their innovative use of filters. Filters can be used in front of the lens or, in some cases, behind the lens for different effects. Christopher Doyle was a pioneer for increased usage of filters in movies. He was highly respected throughout the cinema world.\n\nLenses can be attached to the camera to give a certain look, feel, or effect by focus, color, etc.\n\nAs does the human eye, the camera creates perspective and spatial relations with the rest of the world. However, unlike one's eye, a cinematographer can select different lenses for different purposes. Variation in focal length is one of the chief benefits. The focal length of the lens determines the angle of view and, therefore, the field of view. Cinematographers can choose from a range of wide-angle lenses, \"normal\" lenses and long focus lenses, as well as macro lenses and other special effect lens systems such as borescope lenses. Wide-angle lenses have short focal lengths and make spatial distances more obvious. A person in the distance is shown as much smaller while someone in the front will loom large. On the other hand, long focus lenses reduce such exaggerations, depicting far-off objects as seemingly close together and flattening perspective. The differences between the perspective rendering is actually not due to the focal length by itself, but by the distance between the subjects and the camera. Therefore, the use of different focal lengths in combination with different camera to subject distances creates these different rendering. Changing the focal length only while keeping the same camera position doesn't affect perspective but the camera angle of view only.\n\nA zoom lens allows a camera operator to change his focal length within a shot or quickly between setups for shots. As prime lenses offer greater optical quality and are \"faster\" (larger aperture openings, usable in less light) than zoom lenses, they are often employed in professional cinematography over zoom lenses. Certain scenes or even types of filmmaking, however, may require the use of zooms for speed or ease of use, as well as shots involving a zoom move.\n\nAs in other photography, the control of the exposed image is done in the lens with the control of the diaphragm aperture. For proper selection, the cinematographer needs that all lenses be engraved with T-Stop, not f-stop so that the eventual light loss due to the glass doesn't affect the exposure control when setting it using the usual meters. The choice of the aperture also affects image quality (aberrations) and depth of field.\n\nFocal length and diaphragm aperture affect the depth of field of a scene – that is, how much the background, mid-ground and foreground will be rendered in \"acceptable focus\" (only one exact plane of the image is in precise focus) on the film or video target. Depth of field (not to be confused with depth of focus) is determined by the aperture size and the focal distance. A large or deep depth of field is generated with a very small iris aperture and focusing on a point in the distance, whereas a shallow depth of field will be achieved with a large (open) iris aperture and focusing closer to the lens. Depth of field is also governed by the format size. If one considers the field of view and angle of view, the smaller the image is, the shorter the focal length should be, as to keep the same field of view. Then, the smaller the image is, the more depth of field is obtained, for the same field of view. Therefore, 70mm has less depth of field than 35mm for a given field of view, 16mm more than 35mm, and video cameras even more depth of field than 16mm. As videographers try to emulate the look of 35 mm film with digital cameras, this is one issue of frustration – excessive depth of field with digital cameras and using additional optical devices to reduce that depth of field.\n\nIn \"Citizen Kane\" (1941), cinematographer Gregg Toland and director Orson Welles used tighter apertures to create every detail of the foreground and background of the sets in sharp focus. This practice is known as deep focus. Deep focus became a popular cinematographic device from the 1940s onwards in Hollywood. Today, the trend is for more shallow focus.\n\nTo change the plane of focus from one object or character to another within a shot is commonly known as a \"rack focus\".\n\nThe aspect ratio of an image is the ratio of its width to its height. This can be expressed either as a ratio of 2 integers, such as 4:3, or in a decimal format, such as 1.33:1 or simply 1.33.\n\nDifferent ratios provide different aesthetic effects. Standards for aspect ratio have varied significantly over time.\n\nDuring the silent era, aspect ratios varied widely, from square 1:1, all the way up to the extreme widescreen 4:1 Polyvision. However, from the 1910s, silent motion pictures generally settled on the ratio of 4:3 (1.33). The introduction of sound-on-film briefly narrowed the aspect ratio, to allow room for a sound stripe. In 1932, a new standard was introduced, the Academy ratio of 1.37, by means of thickening the frame line.\n\nFor years, mainstream cinematographers were limited to using the Academy ratio, but in the 1950s, thanks to the popularity of Cinerama, widescreen ratios were introduced in an effort to pull audiences back into the theater and away from their home television sets. These new widescreen formats provided cinematographers a wider frame within which to compose their images.\n\nMany different proprietary photographic systems were invented and utilized in the 1950s to create widescreen movies, but one dominated film: the anamorphic process, which optically squeezes the image to photograph twice the horizontal area to the same size vertical as standard \"spherical\" lenses. The first commonly used anamorphic format was CinemaScope, which used a 2.35 aspect ratio, although it was originally 2.55. CinemaScope was used from 1953 to 1967, but due to technical flaws in the design and its ownership by Fox, several third-party companies, led by Panavision's technical improvements in the 1950s, dominated the anamorphic cine lens market. Changes to SMPTE projection standards altered the projected ratio from 2.35 to 2.39 in 1970, although this did not change anything regarding the photographic anamorphic standards; all changes in respect to the aspect ratio of anamorphic 35 mm photography are specific to camera or projector gate sizes, not the optical system. After the \"widescreen wars\" of the 1950s, the motion-picture industry settled into 1.85 as a standard for theatrical projection in the United States and the United Kingdom. This is a cropped version of 1.37. Europe and Asia opted for 1.66 at first, although 1.85 has largely permeated these markets in recent decades. Certain \"epic\" or adventure movies utilized the anamorphic 2.39.\n\nIn the 1990s, with the advent of high-definition video, television engineers created the 1.78 (16:9) ratio as a mathematical compromise between the theatrical standard of 1.85 and television's 1.33, as it was not practical to produce a traditional CRT television tube with a width of 1.85. Until that point, nothing had ever been originated in 1.78. Today, this is a standard for high-definition video and for widescreen television.\n\nLight is necessary to create an image exposure on a frame of film or on a digital target (CCD, etc.). The art of lighting for cinematography goes far beyond basic exposure, however, into the essence of visual storytelling. Lighting contributes considerably to the emotional response an audience has watching a motion picture. The increased usage of filters can greatly impact the final image and affect the lighting.\n\nCinematography can not only depict a moving subject but can use a camera, which represents the audience's viewpoint or perspective, that moves during the course of filming. This movement plays a considerable role in the emotional language of film images and the audience's emotional reaction to the action. Techniques range from the most basic movements of panning (horizontal shift in viewpoint from a fixed position; like turning your head side-to-side) and tilting (vertical shift in viewpoint from a fixed position; like tipping your head back to look at the sky or down to look at the ground) to dollying (placing the camera on a moving platform to move it closer or farther from the subject), tracking (placing the camera on a moving platform to move it to the left or right), craning (moving the camera in a vertical position; being able to lift it off the ground as well as swing it side-to-side from a fixed base position), and combinations of the above. Early cinematographers often faced problems that were not common to other graphic artists because of the element of motion.\nCameras have been mounted to nearly every imaginable form of transportation.\n\nMost cameras can also be handheld, that is held in the hands of the camera operator who moves from one position to another while filming the action. Personal stabilizing platforms came into being in the late 1970s through the invention of Garrett Brown, which became known as the Steadicam. The Steadicam is a body harness and stabilization arm that connects to the camera, supporting the camera while isolating it from the operator's body movements. After the Steadicam patent expired in the early 1990s, many other companies began manufacturing their concept of the personal camera stabilizer. This invention is much more common throughout the cinematic world today. From feature-length films to the evening news, more and more networks have begun to use a personal camera stabilizer.\n\nThe first special effects in the cinema were created while the film was being shot. These came to be known as \"in-camera\" effects. Later, optical and digital effects were developed so that editors and visual effects artists could more tightly control the process by manipulating the film in post-production.\n\nThe 1896 movie The Execution of Mary Stuart shows an actor dressed as the queen placing her head on the execution block in front of a small group of bystanders in Elizabethan dress. The executioner brings his axe down, and the queen's severed head drops onto the ground. This trick was worked by stopping the camera and replacing the actor with a dummy, then restarting the camera before the axe falls. The two pieces of film were then trimmed and cemented together so that the action appeared continuous when the film was shown. Thus creating an overall illusion and successfully laying the foundation for special affects.\n\nThis film was among those exported to Europe with the first Kinetoscope machines in 1895 and was seen by Georges Méliès, who was putting on magic shows in his Theatre Robert-Houdin in Paris at the time. He took up filmmaking in 1896, and after making imitations of other films from Edison, Lumière, and Robert Paul, he made \"Escamotage d'un dame chez Robert-Houdin (The Vanishing Lady)\". This film shows a woman being made to vanish by using the same stop motion technique as the earlier Edison film. After this, Georges Méliès made many single shot films using this trick over the next couple of years.\n\nThe other basic technique for trick cinematography involves double exposure of the film in the camera, which was first done by George Albert Smith in July 1898 in the UK. Smith's \"The Corsican Brothers\" (1898) was described in the catalogue of the Warwick Trading Company, which took up the distribution of Smith's films in 1900, thus:\n\"One of the twin brothers returns home from shooting in the Corsican mountains, and is visited by the ghost of the other twin. By extremely careful photography the ghost appears *quite transparent*. After indicating that he has been killed by a sword-thrust, and appealing for vengeance, he disappears. A 'vision' then appears showing the fatal duel in the snow. To the Corsican's amazement, the duel and death of his brother are vividly depicted in the vision, and overcome by his feelings, he falls to the floor just as his mother enters the room.\"\nThe ghost effect was done by draping the set in black velvet after the main action had been shot, and then re-exposing the negative with the actor playing the ghost going through the actions at the appropriate point. Likewise, the vision, which appeared within a circular vignette or matte, was similarly superimposed over a black area in the backdrop to the scene, rather than over a part of the set with detail in it, so that nothing appeared through the image, which seemed quite solid. Smith used this technique again in \"Santa Claus\" (1898).\n\nGeorges Méliès first used superimposition on a dark background in \"La Caverne maudite (The Cave of the Demons)\" made a couple of months later in 1898, and elaborated it with multiple superimpositions in the one shot in \"Un Homme de têtes (The Four Troublesome Heads)\". He created further variations in subsequent films.\n\nMotion picture images are presented to an audience at a constant speed. In the theater it is 24 frames per second, in NTSC (US) Television it is 30 frames per second (29.97 to be exact), in PAL (Europe) television it is 25 frames per second. This speed of presentation does not vary.\n\nHowever, by varying the speed at which the image is captured, various effects can be created knowing that the faster or slower recorded image will be played at a constant speed. Giving the cinematographer even more freedom for creativity and expression to be made.\n\nFor instance, time-lapse photography is created by exposing an image at an extremely slow rate. If a cinematographer sets a camera to expose one frame every minute for four hours, and then that footage is projected at 24 frames per second, a four-hour event will take 10 seconds to present, and one can present the events of a whole day (24 hours) in just one minute.\n\nThe inverse of this, if an image is captured at speeds above that at which they will be presented, the effect is to greatly slow down (slow motion) the image. If a cinematographer shoots a person diving into a pool at 96 frames per second, and that image is played back at 24 frames per second, the presentation will take 4 times as long as the actual event. Extreme slow motion, capturing many thousands of frames per second can present things normally invisible to the human eye, such as bullets in flight and shockwaves travelling through media, a potentially powerful cinematographical technique.\n\nIn motion pictures, the manipulation of time and space is a considerable contributing factor to the narrative storytelling tools. Film editing plays a much stronger role in this manipulation, but frame rate selection in the photography of the original action is also a contributing factor to altering time. For example, Charlie Chaplin's \"Modern Times\" was shot at \"silent speed\" (18 fps) but projected at \"sound speed\" (24 fps), which makes the slapstick action appear even more frenetic.\n\nSpeed ramping, or simply \"ramping\", is a process whereby the capture frame rate of the camera changes over time. For example, if in the course of 10 seconds of capture, the capture frame rate is adjusted from 60 frames per second to 24 frames per second, when played back at the standard movie rate of 24 frames per second, a unique time-manipulation effect is achieved. For example, someone pushing a door open and walking out into the street would appear to start off in slow-motion, but in a few seconds later within the same shot, the person would appear to walk in \"realtime\" (normal speed). The opposite speed-ramping is done in \"The Matrix\" when Neo re-enters the Matrix for the first time to see the Oracle. As he comes out of the warehouse \"load-point\", the camera zooms into Neo at normal speed but as it gets closer to Neo's face, time seems to slow down, foreshadowing the manipulation of time itself within the Matrix later in the movie.\n\nG.A. Smith initiated the technique of reverse motion and also improved the quality of self-motivating images. This he did by repeating the action a second time while filming it with an inverted camera and then joining the tail of the second negative to that of the first. The first films using this were \"Tipsy, Topsy, Turvy\" and \"The Awkward Sign Painter\", the latter which showed a sign painter lettering a sign, and then the painting on the sign vanishing under the painter's brush. The earliest surviving example of this technique is Smith's \"The House That Jack Built\", made before September 1901. Here, a small boy is shown knocking down a castle just constructed by a little girl out of children's building blocks. A title then appears, saying \"Reversed\", and the action is repeated in reverse so that the castle re-erects itself under his blows.\n\nCecil Hepworth improved upon this technique by printing the negative of the forwards motion backwards frame by frame, so that in the production of the print the original action was exactly reversed. Hepworth made \"The Bathers\" in 1900, in which bathers who have undressed and jumped into the water appear to spring backwards out of it, and have their clothes magically fly back onto their bodies.\n\nThe use of different camera speeds also appeared around 1900. Robert Paul's \"On a Runaway Motor Car through Piccadilly Circus\" (1899), had the camera turn so slowly that when the film was projected at the usual 16 frames per second, the scenery appeared to be passing at great speed. Cecil Hepworth used the opposite effect in \"The Indian Chief and the Seidlitz powder\" (1901), in which a naïve Red Indian eats a lot of the fizzy stomach medicine, causing his stomach to expand and then he then leaps around balloon-like. This was done by cranking the camera faster than the normal 16 frames per second giving the first \"slow motion\" effect.\n\nIn descending order of seniority, the following staff is involved:\n\n\nIn the film industry, the cinematographer is responsible for the technical aspects of the images (lighting, lens choices, composition, exposure, filtration, film selection), but works closely with the director to ensure that the artistic aesthetics are supporting the director's vision of the story being told. The cinematographers are the heads of the camera, grip and lighting crew on a set, and for this reason, they are often called directors of photography or DPs. The ASC defines cinematography as a creative and interpretive process that culminates in the authorship of an original work of art rather than the simple recording of a physical event. Cinematography is not a subcategory of photography. Rather, photography is but one craft that the cinematographer uses in addition to other physical, organizational, managerial, interpretive. and image-manipulating techniques to effect one coherent process.\nIn British tradition, if the DOP actually operates the camera him/herself they are called the \"cinematographer\". On smaller productions, it is common for one person to perform all these functions alone. The career progression usually involves climbing up the ladder from seconding, firsting, eventually to operating the camera.\n\nDirectors of photography make many creative and interpretive decisions during the course of their work, from pre-production to post-production, all of which affect the overall feel and look of the motion picture. Many of these decisions are similar to what a photographer needs to note when taking a picture: the cinematographer controls the film choice itself (from a range of available stocks with varying sensitivities to light and color), the selection of lens focal lengths, aperture exposure and focus. Cinematography, however, has a temporal aspect (see persistence of vision), unlike still photography, which is purely a single still image. It is also bulkier and more strenuous to deal with movie cameras, and it involves a more complex array of choices. As such a cinematographer often needs to work co-operatively with more people than does a photographer, who could frequently function as a single person. As a result, the cinematographer's job also includes personnel management and logistical organization. Given the in-depth knowledge. a cinematographer requires not only of his or her own craft but also that of other personnel, formal tuition in analogue or digital filmmaking can be advantageous.\n\n\n"
            ],
            "good_message": false,
            "loggedData": [
                {
                    "curr_message": " Typically, a lens is used to focus the light reflected or emitted from objects into a real image on the light-sensitive surface inside a camera during a timed exposure.",
                    "last_key_pressed": "<<selected>>"
                }
            ],
            "offensive": null,
            "researchTask": true,
            "selectedData": [
                "Typically, a lens is used to focus the light reflected or emitted from objects into a real image on the light-sensitive surface inside a camera during a timed exposure."
            ],
            "shown_passages": [
                [
                    "Photography",
                    [
                        "Photography is the science, art, application and practice of creating durable images by recording light or other electromagnetic radiation, either electronically by means of an image sensor, or chemically by means of a light-sensitive material such as photographic film.",
                        "Typically, a lens is used to focus the light reflected or emitted from objects into a real image on the light-sensitive surface inside a camera during a timed exposure.",
                        "With an electronic image sensor, this produces an electrical charge at each pixel, which is electronically processed and stored in a digital image file for subsequent display or processing."
                    ]
                ],
                [
                    "Optics",
                    [
                        "Optics is the branch of physics which involves the behaviour and properties of light, including its interactions with matter and the construction of instruments that use or detect it.",
                        "Optics usually describes the behaviour of visible, ultraviolet, and infrared light.",
                        "Because light is an electromagnetic wave, other forms of electromagnetic radiation such as X-rays, microwaves, and radio waves exhibit similar properties.",
                        "Most optical phenomena can be accounted for using the classical electromagnetic description of light.",
                        "Complete electromagnetic descriptions of light are, however, often difficult to apply in practice."
                    ]
                ],
                [
                    "Cinematography",
                    [
                        "Cinematography (also called \"Direction of Photography\") is the science or art of motion-picture photography by recording light or other electromagnetic radiation, either electronically by means of an image sensor, or chemically by means of a light-sensitive material such as film stock.",
                        "Typically, a lens is used to repeatedly focus the light reflected from objects into real images on the light-sensitive surface inside a camera during a questioned exposure, creating multiple images.",
                        "With an electronic image sensor, this produces an electrical charge at each pixel, which is electronically processed and stored in a video file for subsequent display or processing."
                    ]
                ]
            ],
            "speaker": "0_Wizard",
            "suggestionsConfig": "show",
            "suggestionsNumber": 5,
            "suggestionsShown": true,
            "text": " Typically, a lens is used to focus the light reflected or emitted from objects into a real image on the light-sensitive surface inside a camera during a timed exposure.",
            "textboxFilled": 0,
            "time": 1580233156.6413627,
            "timeTaken": 63.546,
            "totalTasks": 120,
            "turn": 2
        },
        {
            "checked_passage": {
                "no_passages_used": "no_passages_used"
            },
            "checked_sentence": {
                "no_passages_used": "no_passages_used"
            },
            "clicked_passages": {},
            "context": {
                "conversation": [
                    "I love everything about pizza, the taste and the shape. Yum! Do you like pizza too?",
                    "I love pizza its a great italian dish",
                    "Oh yeah, I forgot they originated from Italy. What else can you tell me about these delicious circle of goodness?",
                    "Well pizza goes great with almost any meat or vegetable topping, what do you like"
                ],
                "index": 12
            },
            "contextCount": 0,
            "full_passages": [
                "List of pizza varieties by country\n\nDuring the latter half of the 20th century, pizza became a globally accessible dish, mainly due to Italian immigrants that had brought their dishes to new people with resounding success, often in racially and culturally resistive environments.\n\nA survey from 2004 showed that Norwegians eat the most frozen pizza (5.4 kg/person*year), followed by Germans.\n\nPizza became a popular fast food in Bangladeshi urban areas. Since the introduction of various pizza brands such as Domino's and Pizza Hut in the early to mid-2000s, it has reached almost all classes of urban peoples.\n\nPizza is an emerging fast food in Indian urban areas. American pizza chains Domino's Pizza and Pizza Hut opened their first outlets in India in 1996. Domestic pizza brands include U.S.Pizza, Smokin' Joes and Pizza Corner. Branded pizza is available in most cities in India.\n\nPizzas served in India by foreign pizza brands feature greater \"recipe localization\" from pizza makers than many other markets such as Latin America and Europe, but similar to other Asian pizza markets. Indian pizzas are generally spicier and more vegetable-oriented than those in other countries. For instance, oregano spice packs are included with a typical pizza order in India instead of Parmesan cheese. In addition to spicier and more vegetable-oriented ingredients, Indian pizza also utilized unique toppings. For example, a pizza topping unique to India would be pickled ginger.\n\nPizza outlets serve pizzas with several Indian-style toppings, such as tandoori chicken and paneer. More conventional pizzas are also eaten. Pizzas available in India range from localized basic variants, available in neighborhood bakeries, to gourmet pizzas with exotic and imported ingredients available at specialty restaurants.\n\nIn Indonesia, Pizza Hut is the largest pizza chain restaurant who entered Indonesia in 1984, followed by Domino Pizza and Papa Ron's Pizza. Popular pizza recipes such as meat lover with pepperoni, tuna with melt cheese, and beef blackpepper exist in Indonesia. Those recipes are originated either from United States or Italy, thus derived from western counterpart. \n\nHowever, there are also Asian eastern pizza which includes Indonesian fusion pizza that combine Indonesian favourite as pizza toppings — such as satay, balado and rendang.\nOther than Indonesian fusion, other Asian fusion pizza are also known in Indonesia, including:\n\nAmerican pizza chains entered Japan in the 1970s (e.g. Shakey's Pizza and Pizza Hut 1973, Domino's pizza in 1985). The largest Japanese pizza chain is Pizza-La. Local types of pizza are popular, with many using mayonnaise sauces, and sometimes other ingredients such as corn, potatoes, avocado, eel, or even honey or chocolate (as in dessert). \"Side orders\" also often include items such as french fries, fried chicken, baked pasta, as well as vegetable soups, green salads, desserts, and soda or Japanese tea. There is also a strong tradition of using Tabasco sauce on cooked pizzas.\n\nPizza toppings in Japan also differ from that found in the United States. One of the unique pizza toppings found in Japan is squid. Seafood may be found on pizza everywhere, but having squid as the focal ingredient is unique to Japan.\n\nLocal crust variants also exist, for instance mochi pizza (crust made with Japanese mochi cakes). Traditional pizza served in Italian-style restaurants are also popular, and the most popular pizza chain promoting Italian style artisanal pizza is Salvatore Cuomo. The Italian association Associazione Verace Pizza Napoletana also has an independent branch in Japan.\n\nPizza is a popular snack food in South Korea, especially among younger people. Major American brands such as Domino's, Pizza Hut, and Papa John's Pizza compete against domestic brands such as Mr. Pizza and Pizza Etang, offering traditional as well as local varieties which may include toppings such as bulgogi and dak galbi. Korean-style pizza tends to be complicated, and often has nontraditional toppings such as corn, potato wedges, sweet potato, shrimp, or crab. Traditional Italian-style thin-crust pizza is served in the many Italian restaurants in Seoul and other major cities. \nNorth Korea's first pizzeria opened in its capital Pyongyang in 2009.\n\nPizza restaurants in Malaysia include Domino's, Pizza Hut, Papa John's, Jom Pizza, and Sure Pizza.\n\nPizza is becoming more popular as a fast food in the urban areas of Nepal, particularly in the capital city, Kathmandu. There are a number of restaurants that serve pizzas in Kathmandu. With the opening of number of international pizza restaurants, the popularity as well as consumption has markedly increased in recent times.They are many types of pizza are there. Some are listed below\n1.mushroom pizza\n2.chicken pizza\n3. pane-er pizza\n\nThe first pizzerias opened in Karachi and Islamabad in the late 1980s, with Pappasallis serving pizza in Islamabad since 1990. Pizza has gained a measure of popularity in the eastern regions of Pakistan—namely, the provinces of Sindh, Punjab, and P.O.K, as well as the autonomous territory of Gilgit-Baltistan. Pizza has not penetrated into western Pakistan; of the remaining provinces and territories of Pakistan, only one (Khyber Pakhtunkhwa) has seen much of the dish, in the form of a single Pizza Hut in Peshawar. Chicken Tikka and achari chicken pizzas are popular. In the regions where pizza is known, spicy chicken and sausage-based pizzas are also very popular, as they cater to the local palate.\n\nAuthentic Neapolitan pizzas (\"pizza napoletana\") are typically made with tomatoes and mozzarella cheese. They can be made with ingredients like San Marzano tomatoes, which grow on the volcanic plains to the south of Mount Vesuvius, and mozzarella di bufala Campana, made with the milk from water buffalo raised in the marshlands of Campania and Lazio in a semi-wild state (this mozzarella is protected with its own European protected designation of origin).\n\nAccording to the rules proposed by the \"Associazione Verace Pizza Napoletana\", the genuine Neapolitan pizza dough consists of wheat flour (type \"0\" or \"00\", or a mixture of both), natural Neapolitan yeast or brewer's yeast, salt and water. For proper results, strong flour with high protein content (as used for bread-making rather than cakes) must be used. The dough must be kneaded by hand or with a low-speed mixer. After the rising process, the dough must be formed by hand without the help of a rolling pin or other machine, and may be no more than thick. The pizza must be baked for 60–90 seconds in a stone oven with an oak-wood fire. When cooked, it should be crispy, tender and fragrant. There are three official variants: \"pizza marinara\", which is made with tomato, garlic, oregano and extra virgin olive oil, \"pizza Margherita\", made with tomato, sliced mozzarella, basil and extra-virgin olive oil, and \"pizza Margherita extra\" made with tomato, mozzarella from Campania in fillets, basil and extra virgin olive oil. The pizza napoletana is a Traditional Speciality Guaranteed (\"Specialità Tradizionale Garantita\", STG) product in Europe.\n\nPizza in Lazio (Rome), as well as in many other parts of Italy, is available in two different styles. Take-away shops sell \"pizza rustica\" or \"pizza al taglio\". This pizza is cooked in long, rectangular baking pans and relatively thick (1–2 cm). The pizza is often cooked in an electric oven. It is usually cut with scissors or a knife and sold by weight. In pizzerias, pizza is served in a dish in its traditional round shape. It has a thin, crisp base quite different from the thicker and softer Neapolitan style base. It is usually cooked in a wood-fired oven, giving the pizza its unique flavor and texture. In Rome, a \"pizza napoletana\" is topped with tomato, mozzarella, anchovies and oil (thus, what in Naples is called \"pizza romana\", in Rome is called \"pizza napoletana\"). Other types of Lazio-style pizza include\n\nPizza quattro stagioni is a popular style prepared with various ingredients in four sections, with each section representing a season of the year.\n\nPizza pugliese is prepared with tomato, mozzarella and onion.\n\nPizzetta a small pizza that can range in size from around three inches in diameter to the size of a small personal-sized pizza. It may be served as an hors d'oeuvre.\n\nSicilian pizza is prepared in a manner originating in Sicily, Italy. Just in the US, the phrase \"Sicilian pizza\" is often synonymous with thick-crust or deep-dish pizza derived from the Sicilian \"Sfincione\". In Sicily, there is a variety of pizza called \"Sfincione\". It is that believed Sicilian pizza, Sfincione, or focaccia with toppings, was popular on the western portion of the island as far back as the 1860s.\n\nThere was a bill before the Italian Parliament in 2002 to safeguard the \"traditional Italian pizza\", specifying permissible ingredients and methods of processing (e.g., excluding frozen pizzas). Only pizzas which followed these guidelines could be called \"traditional Italian pizzas\" in Italy. On 9 December 2009, the European Union, upon Italian request, granted Traditional Speciality Guaranteed (TSG) safeguard to traditional Neapolitan pizza, in particular to \"Margherita\" and \"Marinara\". The European Union enacted a protected designation of origin system in the 1990s.\n\nThe Maltese enjoy eating Italian style pizza and fast-food pizzas, as well as experimenting with various toppings, including local produce. One style of fast-food pizza is the typical \"pizza kwadra\" (square pizza), which is found in Pastizzi shops (\"pastizzeriji\"), a deep-pan pizza cut into squares, generally topped with either green olives (\"taż-żebbuġ\"), hard boiled egg and cocktail sausage (\"bajd u zalzett\"), or chicken and barbecue sauce (\"tat-tiġieġ\"). A typical \"Pizzerija\" restaurant will offer a vast number of different pizza recipes, mostly based on the Italian style ones. A typical menu would include:\n\n\nPizza has become a household dish. Nevertheless, the traditional Maltese pizza consists of a typical Maltese ftira covered in cheese (mainly local gbejna), onions and potatoes. In fact, it is most often known simply as \"ftira\" and is mainly sold on the island of Gozo. Different toppings can be added, including tuna, olives, anchovies, sundried tomatoes, and even traditional Maltese sausage.\n\nNorwegians eat the most pizza in the world according to a 2004 survey by ACNielsen 2004, 5,4 kg/year per capita. 50 million frozen pizzas were sold that year, with consumption being 22,000 tons of frozen pizza, 15,000 tons of home-baked and 13,000 tons of restaurant-made pizzas. By far the most popular is the frozen pizza Grandiosa, every other pizza sold, frozen or fresh is a Pizza Grandiosa. Since its start in 1980 the Grandiosa has been part of Norwegian modern culture and trends, going so far to be unofficial called \"The national dish of Norway\".\n\nNorway also has a traditional home-made pizza called \"lørdagspizza\" (literally translates to \"Saturday pizza\"). The dough is shaped to the pan (usually rectangular), then a mix of minced meat and tomato sauce follows. Finally it is gratinated with a generous amount of cheese.\n\nPizza arrived in Sweden with Italian guest workers and became popular around 1970. Swedish pizza is mainly of the Neapolitan type and most pizzerias in Sweden have Margherita, Capricciosa and Quattro Stagioni pizzas at the top of the menu, although with altered recipes. For example, a Swedish Margherita uses Swedish hard cheese instead of mozzarella and dried oregano instead of fresh basil. The Swedish pizza has been developed with lots of innovations and styles, creating a tradition distinct from the Italian one, although some names may overlap. Occasionally pizzerias offer \"Italian pizza\" imitating Italian recipes in addition to the Swedish ones.\n\nA typical Swedish pizzeria offers 40-50 different named varieties on the menu, even up to 100, and personal modifications are allowed. Also, many pizzerias also serve salads, lasagne, kebab and hamburgers, especially if there is a facility to sit and eat. Italian style restaurants often combine a restaurant menu with a pizza menu.\n\nSome popular varieties common in most of Sweden, mostly with the same name, all having tomato sauce and cheese to start with and additional toppings:\n\nPerhaps the most extreme pizza sort heard of in Sweden is the Calskrove or Calzskrove (a portmanteau of calzone and \"skrovmål\" meaning \"big meal\" but also Northern slang for \"hamburger meal\"), sold at some pizzerias in northern Sweden, a complete meal of a 150 or 250 grams hamburger with bread and all regular toppings, and chips (french fries), baked into a regular Calzone with ham as well.\n\nOne of the most popular types of pizza in Sweden since the 1990s is kebab-pizza, and a song in the Swedish Melodifestivalen 2008 was \"Kebabpizza slivovitza\". The invention is most likely the result of the common tendency of pizza bakers to create their own flagship compositions and novel flavors, using whatever might be available in their kitchen. In recent years one can find pizza with fresh lettuce or chips (French fries) put on top after baking. The amount of topping compared to the crust is rather high by international standards.\n\nThe typical side order with Swedish pizza is a free \"pizza salad\". 1969 Giuseppe \"Peppino\" Sperandio opened \"Pizzeria Piazza Opera\", one of the first restaurants only serving pizza in Stockholm, Sweden. Sperandio was born in northeast Italy where a cabbage salad called \"kupus salata\" was a very common dish, from bordering country Croatia. This salad from his childhood, was offered as a free side dish. Eaten, while waiting for the pizza to be baked. Sperandio became Stockholm's pizza king and had during his hey day more than 30 pizza restaurants. Today this Balkan salad (renamed to pizza salad), is as Swedish as the Dala horse.\nThe pizza salad is made with shredded cabbage, coarse pepper and sometimes red bell pepper, slightly pickled (fermented) in vinaigrette for a few days.\n\nIn general, Swedish pizzerias are private enterprises and not franchise, often owned as a family business by immigrants, but very seldom Italians. Of international restaurant chains only Pizza Hut is well established, although Vapiano has a few restaurants in Stockholm and Domino's have been trying to establish itself in southern Sweden since 2008. Many pizzerias offer affordable (about 1-2 € total, or free with large order) home delivery in less than 30 minutes and many are connected to an on-line ordering service. The take-away price of one standard size (30 cm) pizza is 5 to 8 € depending on topping, about the double for a \"family pizza\" of double size (weight), and about the half for a \"children's pizza\" (mostly served in restaurants). Pizza has become a staple food in Sweden (1,1 kg/year), although most people prepare their own food, as home cooking skills generally are good, and is largely considered as an acceptable occasional fast food alternative to a proper meal.\n\nSince the 1980s, a wide variety of pizzas ranging from fairly authentic Italian to American style to the mass-processed varieties are widely available and pizzas are also commonly made at home with children using local substitutions such as bacon for prosciutto and cheddar for mozzarella. Dough bases vary widely from homemade scone doughs to thin Roman-style and thick American stuffed-crust types. The typical British high-street now has a variety of international Italian- and American-style pizza chains, including homegrown chains PizzaExpress, Strada and Prezzo as well as Dominos, Pizza Hut, and Papa John's alongside much more authentic independent Italian-run restaurants with wood-fired ovens particularly in large cities such as London. Unique spicy varieties enjoy some popularity, including Chicken tikka masala or other curry toppings, chilli pizzas and a typical mid-range restaurant or takeaway will usually have versions of such standard \"Italian-American\" combinations as 'Hawaiian' (ham and pineapple); 'Peperroni' (spicy salami) and 'Meat Feast' (a mix of meats and salami) and a 'Vegeteriana' options. Non-Italian varieties are common too, for example, lahmacun called 'Turkish pizzas', or Alsatian 'Flammkuchen'. In some parts of Scotland you can get a deep-fried pizza from Fish and Chip shops. A frozen pizza, whole or half, dipped in batter and deep fried. It is usually served with in the same manner as any other fried item from these shops.\n\nIceland has all of the typical pizza toppings you would expect like pepperoni and sausage but also have some unique ones. A pizza topping that is found in Iceland that may not be found elsewhere, except Sweden, would be bananas. Bananas are used as toppings across the country showing how they have created their own version of an Italian classic.\n\nMany Israeli and American pizza stores and chains, including Pizza Hut and Sbarro, have both kosher and non-kosher locations. Kosher locations either have no meat or use imitation meat because of the Jewish religious dietary prohibition against mixing meat with dairy products, such as cheese. Kosher pizza locations must also close during the holiday of Passover, when no leavened bread is allowed in kosher locations. Some Israeli pizza differs from pizza in other countries because of the very large portions of vegetable toppings such as mushrooms or onions, and some unusual toppings, like corn or labane, and middle-Eastern spices, such as za'atar. Like most foods in Israel, pizza choices reflect multiple cultures.\n\nPizza establishments in Turkey are a mixture of local restaurants, local chains (e.g. Pizza Max), and international chains like Pizza Hut, Domino's Pizza, Little Caesars, and Sbarro. While most combinations of toppings reflect common ingredients found in the US and Italy, there are additional ingredients available that cater to traditional tastes as well, such as minced beef, spicy Sucuk sausage, cured meats like Pastırma, cheeses like Kaşar and Beyaz, and local olives and herbs. With the exception of some restaurants, pork products like ham and bacon are not available, which are substituted with beef, chicken, or lamb equivalents.\n\nPizza has several equivalent or similar dishes in traditional Turkish cuisine, such as Black-Sea-style or Bafra-style Pide and Lahmacun, which adds to the popularity of the dish across Turkey.\n\nMexican pizza is a pizza made with ingredients typical of Mexican cuisine. The usual toppings that can be found throughout Mexico are chorizo, jalapeño pepper slices, grilled or fried onions, tomato, chile, shrimp, avocado, and sometimes beef, bell peppers, tripas or scallop. This pizza has the usual marinara sauce or white sauce and mozzarella cheese. Variations, substituting pepper jack cheese or Oaxaca cheese for mozzarella, are also popular.\n\nIn 1905, the first pizza establishment in the United States was opened in New York's Little Italy. Due to the influx of Italian immigrants, the U.S. has developed regional forms of pizza, some bearing only a casual resemblance to the Italian original. Chicago has its own style of a deep-dish pizza and New York City's style of pizza are well-known. New York-style pizza refers to the thin crust pizza popular in the states of New York, New Jersey, and Connecticut. Philadelphia provides sauce on top of the cheese; St. Louis and other Midwest pizzas use thin crusts and rectangular slices in its local pizzas. Detroit-style pizza is a square pizza that has a thick deep-dish crisp crust, and is generally served with the sauce on top of the cheese. The square shape is the result of an early tradition of using metal trays originally meant to hold small parts in factories. The jumbo slice is an oversized New York-style pizza sold by the slice to go, especially in the Adams Morgan neighborhood in Washington, D.C. The white clam pie is a pizza variety that originated at the Frank Pepe Pizzeria Napoletana restaurant in New Haven, Connecticut.\n\nCanada features many of the large pizza chains found in the United States, but with regional variations resulting from influences of local Canadian cuisine.\n\nThe \"Canadian pizza\" toppings typically include tomato sauce, mozzarella cheese, bacon, pepperoni, and mushrooms; variations exist.); this recipe is also known internationally by that name. The typical preparation of the same recipe is often referred to in Québécois as \"pizza québécoise\".\n\nPoutine pizza is one variety that can be found sporadically across the country, and adaptations of this item have even been featured in upscale restaurants.\n\nAtlantic Canada has several unique varieties, which have spread to other parts of the country as people migrate for work. Donair pizza is inspired by the Halifax fast food of the same name, and is topped with mozzarella cheese, spiced & roasted ground beef, tomatoes, onions, and a sweetened condensed milk-based donair sauce. Garlic fingers is an Atlantic Canadian pizza garnished with melted butter, garlic, cheese, and sometimes bacon, with the round sliced into fingers and served with donair sauce. Pictou County Pizza is a variant of pizza unique to Pictou County in Nova Scotia; this pizza has a \"brown sauce\" made from vegetables and spices instead of red tomato sauce. \n\nToronto-style pizza, is a medium-thick crust margarita pizza topped with garlic and basil oil topping, a fusion of an Italian-type pizza and the Vietnamese traditions of using herbed oil toppings.\n\nThe predominantly francophone Canadian province of Quebec has its specialties. One is the \"all dressed\": tomato sauce (a little spicy), pepperoni, onions, green pepper slices, and mushrooms.\nThe poutine pizza variety is topped with French fries, light gravy, and fresh mozarella curds.\n\nAccording to a number of news outlets, the Hawaiian-style (tomato sauce, ham and pineapple) is a Canadian invention, originating at the Satellite Restaurant in Chatham, Ontario. Sam Panopoulos, owner of Satellite, first concocted the Hawaiian pizza in 1962 . By that time Satellite had already started serving Chinese food and Panopoulos thought people would like a similar sweet and savoury flavours together so he took a can of pineapple and tossed the fruit onto a pizza.\n\nThe usual Italian varieties are available, though more common is the style popular in the U.S., with more and richer toppings than Italian style. A common unique type is the Aussie, Australian or Australiana, which has the usual tomato base or a seasoned base and mozzarella cheese with options of chicken, ham, bacon and egg (seen as quintessentially Australian breakfast fare). Pizzas with seafood such as prawns are also popular. In the 1980s some Australian pizza shops and restaurants began selling \"gourmet pizzas\", that is, pizzas with more expensive ingredients such as salmon, dill, bocconcini, tiger prawns, or unconventional toppings such as kangaroo meat, emu and crocodile. \"Wood-fired pizzas\", that is, those cooked in a ceramic oven heated by wood fuel, are well-regarded.\n\nFranchised chains coexists with independent pizzerias, Middle-Eastern bakeries and kebabs shops.\n\nNew Zealand's first dedicated pizza outlet was opened by Pizza Hut in New Lynn in 1974, with Dominos following. One notable indigenous chain is Hell Pizza established in 1996 - which now has outlets worldwide - distinguishing itself by often-controversial marketing and using only free-range ingredients. Independent restaurants are common.\n\nNew Zealand has no rules for pizza construction, leading to an eclectic and varied approach to toppings. Gourmet and \"wild\" ingredients are often used, and New Zealanders are apt to push the boundaries of what a pizza can be.\n\nStandard Argentine pizza has a thicker crust than traditional Italian style pizza and includes more cheese. Pizza is made with very thin, and sometimes thick, high-rising doughs, with or without cheese, cooked in the oven or \"a la piedra\" (on a stone oven), and stuffed with numerous ingredients -— is a dish which can be found in nearly every corner of the country. Buenos Aires, Rosario, and Córdoba also serve it with fainá, which is a chick pea-flour dough placed over the piece of pizza. People say that what makes the Argentine pizza unique is the blending of Italian and Spanish cultures. At the turn of the 19th century, immigrants from Naples and Genoa opened the first pizza bars, though Spanish residents subsequently owned most of the pizza businesses.\nAnother very popular kind is the fugazza, which consists in a regular pizza crust topped with onions, ground black pepper, olive oil and mozzarella cheese (in this case it is called fugazzeta).\n\nSão Paulo has 6,000 pizza establishments and 1.4 million pizzas are consumed daily. It is said that the first Brazilian pizzas were baked in the Brás district of São Paulo in the late part of the 19th century. Until the 1940s, almost only found in the Italian communities around the country. Since then, pizza became increasingly popular among the rest of the population. The most traditional pizzerias are still found in the Italian neighborhoods, such as Bexiga (official name: Bela Vista). Both Neapolitan (thick crust) and Roman (thin crust) varieties are common in Brazil, with traditional versions using tomato sauce and mozzarella as a base. Brazilian pizza in general, though, tends to have less tomato sauce than the authentic (Italian) pizza, or uses slices of tomato in place of sauce. Brazilian pizzerias offer also Brazilian variants such as \"pizza com catupiry\". July 10 is \"Pizza Day\" in São Paulo, marking the final day of an annual competition among \"pizzaiolos\". In Brazil, pizza quatro queijos (\"pizza quattro formaggi\") uses mozzarella, provolone, parmesan and gorgonzola, and there is also a variety with five cheeses, which adds catupiry.\n\n\"Hawaiian pizza\" is popular in Colombia. The pizza is topped with ham and pineapple.\n\n",
                "Cuisine of the United States\n\nThe cuisine of the United States reflects its history. The European colonization of the Americas yielded the introduction of a number of ingredients and cooking styles to the latter. The various styles continued expanding well into the 19th and 20th centuries, proportional to the influx of immigrants from many different nations; such influx developed a rich diversity in food preparation throughout the country.\n\nEarly Native Americans utilized a number of cooking methods in early American Cuisine that have been blended with early European cooking methods to form the basis of American Cuisine. When the colonists came to the colonies, they farmed animals for clothing and meat in a similar fashion to what they had done in Europe. They had cuisine similar to their previous British cuisine. The American colonial diet varied depending on the settled region in which someone lived. Commonly hunted game included deer, bear, buffalo, and wild turkey. A number of fats and oils made from animals served to cook much of the colonial foods. Prior to the Revolution, New Englanders consumed large quantities of rum and beer, as maritime trade provided them relatively easy access to the goods needed to produce these items: rum was the distilled spirit of choice, as the main ingredient, molasses, was readily available from trade with the West Indies. In comparison to the northern colonies, the southern colonies were quite diverse in their agricultural diet and did not have a central region of culture.\n\nDuring the 18th and 19th centuries, Americans developed many new foods. During the Progressive Era (1890s–1920s) food production and presentation became more industrialized. One characteristic of American cooking is the fusion of multiple ethnic or regional approaches into completely new cooking styles. A wave of celebrity chefs began with Julia Child and Graham Kerr in the 1970s, with many more following after the rise of cable channels such as Food Network.\n\nSeafood in the United States originated with the Native Americans, who often ate cod, lemon sole, flounder, herring, halibut, sturgeon, smelt, drum on the East Coast, and olachen and salmon on the West Coast. Whale was hunted by Native Americans off the Northwest coast, especially by the Makah, and used for their meat and oil. Seal and walrus were also eaten, in addition to eel from New York's Finger Lakes region. Catfish was also popular among native people, including the Modocs. Crustacean included shrimp, lobster, crayfish, and dungeness crabs in the Northwest and blue crabs in the East. Other shellfish include abalone and geoduck on the West Coast, while on the East Coast the surf clam, quahog, and the soft-shell clam. Oysters were eaten on both shores, as were mussels and periwinkles.\n\nEarly Native Americans used a number of cooking methods in early American Cuisine that have been blended with early European cooking methods to form the basis of American Cuisine. Grilling meats was common. Spit roasting over a pit fire was common as well. Vegetables, especially root vegetables were often cooked directly in the ashes of the fire. As early Native Americans lacked pottery that could be used directly over a fire, they developed a technique which has caused many anthropologists to call them \"Stone Boilers\". They would heat rocks directly in a fire and then add the rocks to a pot filled with water until it came to a boil so that it would cook the meat or vegetables in the boiling water. In what is now the Southwestern United States, they also created adobe ovens called hornos to bake products such as cornmeal bread. Other parts of America dug pit ovens; these pits were also used to steam foods by adding heated rocks or embers and then seaweed or corn husks placed on top to steam fish and shellfish as well as vegetables; potatoes would be added while still in skin and corn while in-husk, this would later be referred to as a clambake by the colonists.\n\nWhen the colonists came to Virginia, Pennsylvania, Massachusetts, or any of the other English colonies on the eastern seaboard of North America, their initial attempts at survival included planting crops familiar to them from back home in England. In the same way, they farmed animals for clothing and meat in a similar fashion. Through hardships and eventual establishment of trade with Britain, the West Indies and other regions, the colonists were able to establish themselves in the American colonies with a cuisine similar to their previous British cuisine. There were some exceptions to the diet, such as local vegetation and animals, but the colonists attempted to use these items in the same fashion as they had their equivalents or ignore them entirely if they could. The manner of cooking for the American colonists followed along the line of British cookery up until the Revolution. The British sentiment followed in the cookbooks brought to the New World as well.\n\nIn 1796, the first American cookbook was published.\n\nThere was a general disdain for French cookery, even with the French Huguenots in South Carolina and French-Canadians. One of the cookbooks that proliferated in the colonies was \"The Art of Cookery Made Plain and Easy\" (1747) by Hannah Glasse, who referred to \"the blind folly of this age that would rather be imposed on by a French booby, than give encouragement to a good English cook!\" Of the French recipes given in the text, she speaks out flagrantly against the dishes as she \"… think[s] it an odd jumble of trash.\" Reinforcing the anti-French sentiment was the French and Indian War from 1754 to 1764. This created a large anxiety against the French, which influenced the English to force many of the French to move, as in the expulsion of the Acadians from Nova Scotia to Louisiana. The Acadians left a French influence in the diet of those settled in Louisiana, and among the Acadian Francophones who settled eastern Maine and parts of what is now northern Vermont at the same time they colonized New Brunswick.\n\nThe American colonial diet varied depending on the settled region in which someone lived. Local cuisine patterns had established by the mid-18th century. The New England colonies were extremely similar in their dietary habits to those that many of them had brought from England. A striking difference for the colonists in New England compared to other regions was seasonality. While in the southern colonies, they could farm almost year-round, in the northern colonies, the growing seasons were very restricted. In addition, colonists' close proximity to the ocean gave them a bounty of fresh fish to add to their diet, especially in the northern colonies.\n\nWheat, however, the grain used to bake bread back in England was almost impossible to grow, and imports of wheat were far from cost productive. Substitutes in cases such as this included cornmeal. The Johnnycake was a poor substitute to some for wheaten bread, but acceptance by both the northern and southern colonies seems evident.\n\nAs many of the New Englanders were originally from England, game hunting was useful when they immigrated to the New World. Many of the northern colonists depended upon their ability to hunt, or upon others from whom they could purchase game. Hunting was the preferred method of protein consumption (as opposed to animal husbandry, which required much more work to defend the kept animals against Native Americans or the French).\n\nCommonly hunted game included deer, bear, buffalo, and wild turkey. The larger muscles of the animals were roasted and served with currant sauce, while the other smaller portions went into soups, stews, sausages, pies, and pastries. In addition to game, colonists' protein intake was supplemented by mutton. The Spanish in Florida originally introduced sheep to the New World, but this development never quite reached the North, and there they were introduced by the Dutch and English. The keeping of sheep was a result of the English non-practice of animal husbandry. The animals provided wool when young and mutton upon maturity after wool production was no longer desirable. The forage-based diet for sheep that prevailed in the Colonies produced a characteristically strong, gamy flavor and a tougher consistency, which required aging and slow cooking to tenderize.\n\nA number of fats and oils made from animals served to cook much of the colonial foods. Many homes had a sack made of deerskin filled with bear oil for cooking, while solidified bear fat resembled shortening. Rendered pork fat made the most popular cooking medium, especially from the cooking of bacon. Pork fat was used more often in the southern colonies than the northern colonies as the Spanish introduced pigs earlier to the South. The colonists enjoyed butter in cooking as well, but it was rare prior to the American Revolution, as cattle were not yet plentiful.\n\nPrior to the Revolution, New Englanders consumed large quantities of rum and beer, as maritime trade provided them relatively easy access to the goods needed to produce these items. Rum was the distilled spirit of choice, as the main ingredient, molasses, was readily available from trade with the West Indies. Further into the interior, however, one would often find colonists consuming whiskey, as they did not have similar access to sugar cane. They did have ready access to corn and rye, which they used to produce their whiskey. However, until the Revolution, many considered whiskey to be a coarse alcohol unfit for human consumption, as many believed that it caused the poor to become raucous and unkempt drunkards. In addition to these alcohol-based products produced in America, imports were seen on merchant shelves, including wine and brandy.\n\nIn comparison to the northern colonies, the Southern Colonies were quite diverse in their agricultural diet and did not have a central region of culture. The uplands and the lowlands made up the two main parts of the southern colonies. The slaves and poor whites of the south often ate a similar diet, which consisted of many of the indigenous New World crops. Salted or smoked pork often supplement the vegetable diet. Rural poor often ate squirrel, opossum, rabbit and other woodland animals. Those on the \"rice coast\" often ate ample amounts of rice, while the grain for the rest of the southern poor and slaves was cornmeal used in breads and porridges. Wheat was not an option for most of those who lived in the southern colonies.\n\nThe diet of the uplands often included cabbage, string beans, and white potatoes, while most avoided sweet potatoes and peanuts at the time. Those who could grow or afford wheat often had biscuits as part of their breakfast, along with healthy portions of pork. Salted pork was a staple of any meal, as it was used in the preparations of vegetables for flavor, in addition to being eaten directly as a protein.\n\nThe lowlands, which included much of the French regions of Louisiana and the surrounding area, included a varied diet heavily influenced by the French, Spanish, Acadians, Germans, Native Americans, Africans and Caribbeans. Rice played a large part of the diet in Louisiana. In addition, unlike the uplands, the lowlands subsistence of protein came mostly from coastal seafood and game meats. Much of the diet involved the use of peppers, as it still does to this day. Although the English had an inherent disdain for French foodways, as well as many of the native foodstuff of the colonies, the French had no such disdain for the indigenous foodstuffs, but rather a vast appreciation for the native ingredients and dishes.\n\nDuring the 18th and 19th centuries, Americans developed many new foods. Some, such as Rocky Mountain oysters, stayed regional; some spread throughout the nation but with little international appeal, such as peanut butter (a core ingredient of the famous peanut butter and jelly sandwich); and some spread throughout the world, such as popcorn, Coca-Cola and its competitors, fried chicken, cornbread, unleavened muffins such as the poppyseed muffin, and brownies.\n\nDuring the Progressive Era (1890s–1920s) food production and presentation became more industrialized. Major railroads featured upscale cuisine in their dining cars. Restaurant chains emerged with standardized decor and menus, most famously the Fred Harvey restaurants along the route of the Sante Fe Railroad in the Southwest.\n\nAt the universities, nutritionists and home economists taught a new scientific approach to food. During World War I the Progressives' moral advice about food conservation was emphasized in large-scale state and federal programs designed to educate housewives. Large-scale foreign aid during and after the war brought American standards to Europe.\n\nNewspapers and magazines ran recipe columns, aided by research from corporate kitchens, which were major food manufacturers like General Mills, Campbell's, and Kraft Foods. One characteristic of American cooking is the fusion of multiple ethnic or regional approaches into completely new cooking styles. For example, spaghetti is Italian, while hot dogs are German; a popular meal, especially among young children, is spaghetti containing slices of hot dogs. Since the 1960s Asian cooking has played a particularly large role in American fusion cuisine.\nNew York City is home to a diverse and cosmopolitan demographic, and since the nineteenth century, the city's world class chefs created complicated dishes with rich ingredients like Lobster Newberg, waldorf salad, vichyssoise, eggs benedict, and the New York strip steak out of a need to entertain and impress consumers in expensive bygone restaurants like Delmonico's and still standing establishments like the Waldorf-Astoria Hotel.\n\nSome dishes that are typically considered American have their origins in other countries. American cooks and chefs have substantially altered these dishes over the years, to the degree that the dishes now enjoyed around the world are considered to be American. Hot dogs and hamburgers are both based on traditional German dishes, but in their modern popular form they can be reasonably considered American dishes.\n\nPizza is based on the traditional Italian dish, brought by Italian immigrants to the United States, but varies highly in style based on the region of development since its arrival. For example, \"Chicago\" style has focus on a thicker, taller crust, whereas a \"New York Slice\" is known to have a much thinner crust which can be folded. These different types of pizza can be advertised throughout the country and are generally recognizable and well-known, with some restaurants going so far as to import New York City tap water from a thousand or more miles away to recreate the signature style in other regions.\nMany companies in the American food industry developed new products requiring minimal preparation, such as frozen entrees. Many of these recipes have become very popular. For example, the General Mills \"Betty Crocker's Cookbook\", first published in 1950, was a popular book in American homes.\n\nA wave of celebrity chefs began with Julia Child and Graham Kerr in the 1970s, with many more following after the rise of cable channels like Food Network. By the beginning of the 21st century regional variations in consumption of meat began to reduce, as more meat was consumed overall. Saying they eat too much protein, the \"2015–2020 Dietary Guidelines for Americans\" asked men and teenage boys to increase their consumption of underconsumed foods such as vegetables.\n\nDuring the 1980s, upscale restaurants introduced a mixing of cuisines that contain Americanized styles of cooking with foreign elements commonly referred as New American cuisine. New American cuisine refers to a type of fusion cuisine which assimilates flavors from the melting pot of traditional American cooking techniques mixed with flavors from other cultures and sometimes molecular gastronomy components.\n\nGenerally speaking, in the present day 21st century, the modern cuisine of the United States is very much regional in nature. Excluding Alaska and Hawaii the terrain spans 3,000 miles West to East and more than a thousand North to South.\n\nNew England is a Northeastern region of the United States bordering the Maritime Provinces of Canada and portions of Quebec in the north. It includes the six states of Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, and Vermont, with its cultural capital Boston, founded in 1630. The Native American cuisine became part of the cookery style that the early colonists brought with them. Tribes like the Nipmuck, Wampanoag, and other Algonquian cultures were noted for slashing and burning areas to create meadows and bogs that would attract animals like moose and deer, but also encourage the growth of plants like black raspberries, blueberries, and cranberries. In the forest they would have collected nuts of species like the shagbark hickory, American hazel, and American chestnuts and fruits like wild grapes and black cherries. All of these eventually showed up in the kitchens of colonial New England women and many were sent back to England and other portions of Europe to be catalogued by scientists, collectors, and horticulturalists.\n\nThe style of New England cookery originated from its colonial roots, that is to say practical, frugal and willing to eat anything other than what they were used to from their British roots . Most of the initial colonists came from East Anglia in England, with other groups following them over the ages like francophone regions of Canada (this was especially true of Northern New England, where there are still many speakers of a dialect of French), from Ireland, from Southern Italy, and most recently from Haiti, Brazil, the Dominican Republic, and Portugal. The oldest forms of the cuisine date to the early 17th century and in the case of Massachusetts, out of the entire country only the state of Virginia can claim recipes that are older. East Anglian cookery would have included recipes for dishes like suet puddings, wheaten breads, and a few shellfish delicacies, like winkles and would have been at the time of settlement simple Puritan fare quite in contrast to the fineries and excesses expected in London cavalier circles. Most of the cuisine started with one-pot cookery, which resulted in such dishes as succotash, chowder, baked beans, and others. Starches are fairly simple, and typically encompass just a handful of classics like potatoes and cornmeal, and a few native breads like Anadama bread, johnnycakes, bulkie rolls, Parker house rolls, popovers, and New England brown bread. This region is fairly conservative with its spices, but typical spices include nutmeg, ginger, cinnamon, cloves, and allspice, especially in desserts, and for savory foods, thyme, black pepper, sea salt, and sage. Typical condiments include maple syrup, grown from the native sugar maple, molasses, and the famous cranberry sauce.\n\nNew England is noted for having a heavy emphasis on seafood, a legacy inherited from coastal tribes like the Wampanoag and Narragansett, who equally used the rich fishing banks offshore for sustenance. Favorite fish include cod, salmon, winter flounder, haddock, striped bass, pollock, hake, bluefish, and, in southern New England, tautog. All of these are prepared numerous ways, such as frying cod for fish fingers, grilling bluefish over hot coals for summertime, smoking salmon or serving a whole poached one chilled for feasts with a dill sauce, or, on cold winter nights, serving haddock baked in casserole dish with a creamy sauce and crumbled breadcrumbs as a top so it forms a crust. Clam cakes, a savory fritter based on chopped clams, are a specialty of Rhode Island. Farther inland, brook trout, largemouth bass, and herring are sought after, especially in the rivers and icy finger lakes in upper New England.\n\nMeat is present though not as prominent, and typically is either stewed in dishes like Yankee pot roast and New England boiled dinner or braised, as in a picnic ham; these dishes suit the weather better as summers are humid and hot but winters are raw and cold, getting below 0 °C for most of the winter and only just above it by March. The roasting of whole turkeys began here as a centerpiece for large American banquets, and like all other East Coast tribes, the Native American tribes of New England prized wild turkeys as a source of sustenance and later Anglophone settlers were enamored of cooking them using methods they knew from Europe: often that meant trussing the bird and spinning it on a string or spit roasting. Today turkey meat is a key ingredient in soups, and also a favorite in several sandwiches like the Pilgrim (sandwich). For lunch, hot roast beef is sometimes chopped finely into small pieces and put on a roll with salami and American or provolone cheese to make a steak bomb. Bacon is often maple cured, and it is often the drippings from this bacon that are an ingredient in corn chowder. Veal consumption was prevalent in the North Atlantic States prior to World War II. A variety of linguiça is favored as a breakfast food, brought with Portuguese fisherman and Brazilian immigrants. In contrast with some parts of the United States, lamb (although less so mutton or goat) is a popular roasted or grilled meat across diverse groups in New England. Dairy farming and its resultant products figure strongly on the ingredient list, and homemade ice cream is a summertime staple of the region: it was a small seasonal roadside stand in Vermont that eventually became the world-famous Ben and Jerry's ice cream. Vermont in particular is famous for producing farmhouse style cheeses, especially a type of cheddar. The recipe goes all the way back to colonial times when English settlers brought the recipe with them from England and found the rocky landscape eminently suitable to making the cheese. Today Vermont has more artisanal cheese makers per capita than any other state, and diversity is such that interest in goat's milk cheeses has become prominent.\n\nCrustaceans and mollusks are also an essential ingredient in the regional cookery. Maine is noted for harvesting peekytoe crab and Jonah crab and making crab bisques, based on cream with 35% milkfat, and crabcakes out of them, and often they appear on the menu as far south as to be out of region in New York City, where they are sold to four star restaurants. Squid are heavily fished for and eaten as fried calamari, and often are an ingredient in Italian American cooking in this region. Whelks are eaten in salad, and most famous of all is the lobster, which is indigenous to the coastal waters of the region and are a feature of many dishes, baked, boiled, roasted, and steamed, or simply eaten as a sandwich, chilled with mayonnaise and chopped celery in Maine and Massachusetts, or slathered with melted butter on Long Island and in Connecticut.\n\nShellfish of all sorts are part of the diet, and shellfish of the coastal regions include little neck clams, sea scallops, blue mussels, oysters, soft shell clams and razor shell clams. Much of this shellfish contributes to New England tradition, the clambake. The clambake as known today is a colonial interpretation of an American Indian tradition. In summer, oysters and clams are dipped in batter and fried, often served in a basket with french fries, or commonly on a wheaten bun as a clam roll. Oysters are otherwise eaten chilled on a bed of crushed ice on the half shell with mignonette sauce, and are often branded on where they were harvested. Large quahogs are stuffed with breadcrumbs and seasoning and baked in their shells, and smaller ones often find their way into clam chowder. Other preparations include clams casino, clams on the half shell served stuffed with herbs like oregano and streaky bacon.\n\nThe fruits of the region include the \"Vitis labrusca\" grapes used in grape juice made by companies such as Welch's, along with jelly, Kosher wine by companies like Mogen David and Manischewitz along with other wineries that make higher quality wines. Apples from New England include the traditional varieties Baldwin, Lady, Mother, Pomme Grise, Porter, Roxbury Russet, Wright, Sops of Wine, Hightop Sweet, Peck's Pleasant, Titus Pippin, Westfield-Seek-No-Further, and Duchess of Oldenburg. Beach plums a small native species with fruits the size of a pinball, are sought after in summer to make into a jam. Cranberries are another fruit indigenous to the region, often collected in autumn in huge flooded bogs. Thereafter they are juiced so they can be drunk fresh for breakfast, or dried and incorporated into salads. Winter squashes like pumpkin and butternut squashes have been a staple for generations owing to their ability to keep for long periods over icy New England winters and being an excellent source of beta carotene; in summer, they are replaced with pattypan and zucchini, the latter brought to the region by immigrants from Southern Italy a century ago. Blueberries are a very common summertime treat owing to them being an important crop, and find their way into muffins, pies and pancakes. Typical favorite desserts are quite diverse, and encompass hasty pudding, blueberry pie, whoopie pies, Boston cream pie, pumpkin pie, Joe Frogger cookies, hand crafted ice cream, Hermit cookies, and most famous of all, the chocolate chip cookie, invented in Massachusetts in the 1930s.\n\nSouthern New England, particularly along the coast, shares many specialties with the Mid-Atlantic, including especially dishes from Jewish and Italian-American cuisine. Coastal Connecticut is known for distinctive kinds of pizza, locally called apizza, differing in texture (thin and slightly blackened) and toppings (such as clams) from pizza further south in the so-called pizza belt, which stretches from New Haven southward through New York, New Jersey, and into Maryland.\n\nThe mid-Atlantic states comprise the states of New York, New Jersey, Delaware, Pennsylvania, and Northern Maryland. The oldest major settlement in this area of the country is found in the most populous city in the nation, New York City, founded in 1653 by the Dutch. Today, it is a major cultural capital of the United States. The influences on cuisine in this region are extremely eclectic owing to the fact that it has been and continues to be a gateway for international culture as well as a gateway for new immigrants. Going back to colonial times, each new group has left their mark on homegrown cuisine and in turn the cities in this region disperse trends to the wider United States. In addition to importing and trading the finest specialty foods from all over the world, cities like New York and Philadelphia have had the past influence of Dutch, Italian, German, Irish, British, and Jewish cuisines, and that continues to this day. Baltimore has become the crossroads between North and South, a distinction it has held since the end of the Civil War.\n\nA global power city, New York City is internationally known for its extremely diverse and cosmopolitan dining scene and possesses the entire world spectrum of dining options within its city limits. Some of the most exclusive and prestigious restaurants and nightclubs in the world are headquartered in New York City and compete fiercely for good reviews in the Food and Dining section of The New York Times, online guides, and Zagat's, the last of which is widely considered the premier American dining guide, published yearly and headquartered in New York City.\nMany of the more complicated dishes with rich ingredients like Lobster Newberg, waldorf salad, vichyssoise, eggs benedict, and the New York strip steak were born out of a need to entertain and impress the well to do in expensive bygone restaurants like Delmonico's and still standing establishments like the Waldorf-Astoria Hotel, and today that tradition remains alive as some of the most expensive and exclusive restaurants in the country are found in this region. Modern commercial American cream cheese was developed in 1872, when William Lawrence, from Chester, New York, while looking for a way to recreate the soft, French cheese Neufchâtel, accidentally came up with a way of making an \"unripened cheese\" that is heavier and creamier; other dairymen came up with similar creations independently.\n\nSince the first reference to an alcoholic mixed drink called a cocktail comes from New York State in 1803, it is thus not a surprise that there have been many cocktails invented in New York and the surrounding environs. Even today New York City bars are noted for being highly influential in making national trends. Cosmopolitans, Long Island iced teas, Manhattans, Rob Roys, Tom Collins, Aviations, and Greyhounds were all invented in New York bars, and the gin martini was popularized in New York in speakeasies during the 1920s, as evidenced by its appearance in the works of New Yorker and American writer F. Scott Fitzgerald. Like its neighbor Philadelphia, many rare and unusual liquors and liqueurs often find their way into a mixologist's cupboard or restaurant wine list. New York State is the third most productive area in the country for wine grapes, just behind the more famous California and Washington. It has AVA's near the Finger Lakes, the Catskills, and Long Island, and in the Hudson Valley has the second most productive area in the country for growing apples, making it a center for hard cider production, just like New England. Pennsylvania has been growing rye since Germans began to emigrate to the area at the end of the 17th century and required a grain they knew from Germany. Therefore, overall it is not unusual to find New York grown Gewürtztraminer and Riesling, Pennsylvania rye whiskey, or marques of locally produced ciders like Original Sin on the same menu.\nSince their formative years, New York City, Philadelphia, and Baltimore have welcomed immigrants of every kind to their shores, and all three have been an important gateway through which new citizens to the general United States arrive. Traditionally natives have eaten cheek to jowl with newcomers for centuries as the newcomers would open new restaurants and small businesses and all the different groups would interact. Even in colonial days this region was a very diverse mosaic of peoples, as settlers from Switzerland, Wales, England, Ulster, Wallonia, Holland, Gelderland, the British Channel Islands, and Sweden sought their fortune in this region. This is very evident in many signature dishes and local foods, all of which have evolved to become American dishes in their own right. The original Dutch settlers of New York brought recipes they knew and understood from the Netherlands and their mark on local cuisine is still apparent today: in many quarters of New York their version of apple pie with a streusel top is still baked, while originating in the colony of New Amsterdam their predilection for waffles in time evolved into the American national recipe and forms part of a New York City brunch, and they also made coleslaw, originally a Dutch salad, but today accented with the later 18th century introduction of mayonnaise. The internationally famous American doughnut began its life originally as a New York pastry that arrived in the 18th century as the Dutch olykoek.\nCrab cakes were once a kind of English croquette, but over time as spices have been added they and the Maryland crab feast became two of Baltimore's signature dishes; fishing for the blue crab is a favorite summer pastime in the waters off Maryland, New Jersey, and Delaware where they may grace the table at summer picnics . Other mainstays of the region have been present since the early years of American history, like oysters from Cape May, the Chesapeake Bay, and Long Island, and lobster and tuna from the coastal waters found in New York and New Jersey, which are exported to the major cities as an expensive delicacy or a favorite locavore's quarry at the multitude of farmer's markets, very popular in this region. Philadelphia Pepper Pot, a tripe stew, was originally a British dish but today is a classic of home cooking in Pennsylvania alongside bookbinder soup, a type of turtle soup.\n\nIn the winter, New York City pushcarts sell roasted chestnuts, a delicacy dating back to English Christmas traditions, and it was in New York and Pennsylvania that the earliest Christmas cookies were introduced: Germans introduced crunchy molasses based gingerbread and sugar cookies in Pennsylvania, and the Dutch introduced cinnamon based cookies, all of which have become part of the traditional Christmas meal.Scrapple was originally a type of savory pudding that early Pennsylvania Germans made to preserve the offal of a pig slaughter. The Philadelphia soft pretzel was originally brought to Eastern Pennsylvania in the early 18th century, and later, 19th century immigrants sold them to the masses from pushcarts to make them the city's best-known bread product, having evolved into its own unique recipe.\nAfter the 1820s, new groups began to arrive and the character of the region began to change. There had been some Irish from Ulster prior to 1820, however largely they had been Protestants with somewhat different culture and (often) a different language than the explosion of emigrants that came to Castle Garden and Locust Point in Baltimore in their masses starting in the 1840s.\n\nThe Irish arrived in America in a rather woeful state, as Ireland at the time was often plagued by some of the worst poverty in Europe and often heavy disenfranchisement among the masses: many of them arrived barely alive having ridden coffin ships to the New World, sick with typhus and starvation. In addition, they were the first to face challenges other groups did not have: they were the first large wave of Catholics. They faced prejudice for their faith and the cities of Philadelphia, New York, and Baltimore were not always set up for their needs. For example, Catholic bishops in the U.S. mandated until the 1960s that all Catholics were forbidden from eating red meat on Fridays and during Lent, and attending Mass sometimes conflicted with work as produce and meat markets would be open on holy days; this was difficult for Irishmen supporting families since many worked as laborers. Unsurprisingly, many Irishmen also found their fortunes working as longshoremen, which would have given their families access to fish and shellfish whenever a fisherman made berth, which was frequent on the busy docks of Baltimore and New York. Though there had been some activity in Baltimore in founding a see earlier by the Carroltons, the Irish were the first major wave of Catholic worship in this region, and that meant bishops and cardinals sending away to Europe for wine. Part of the Catholic mass includes every parishioner taking a sip of wine from the chalice as part of the Eucharist. Taverns had existed prior to their emigration to America in the region, though the Irish brought their particular brand of pub culture and founded some of the first saloons and bars that served stout and red ale; they brought with them the knowledge of single malt style whiskey and sold it. The Irish were the first immigrant group to arrive in this region in massive millions, and these immigrants also founded some of the earliest saloons and bars in this region, of which McSorley's is an example.\nIt was also in this region that the Irish introduced something that today is a very important festival in American culture that involves a large amount of food, drink, and merry making: Halloween. In England and Wales, where prior immigrants had come from, the feast of All Hallows Eve had died out in the Reformation, dismissed as superstition and excess having nothing to do with the Bible and often replaced with the festival of Guy Fawkes Night. Other immigrant groups like the Germans preferred to celebrate October 31 as Reformation Day, and after the American Revolution all of the above were less and less eager to celebrate the legacy of an English festival when they had fought a very bloody war to leave the British Empire. The Catholicism of the Irish demanded attendance at church on November 1 and charity and deeds, not just faith, as a cornerstone of dogma, and many of their older traditions survived the Reformation and traveled with them. Naturally, they went door-to-door to collect victuals for masked parties as well as gave them out, like nuts to roast on the fire, whiskey, beer, or cider, and barmbracks; they also bobbed for apples and made dumb cakes. Later in the century they were joined by Scots going guising, children going door-to-door to ask for sweets and treats in costume. From the Mid Atlantic this trend spread to be nationwide and evolved into American children trick-or-treating on October 31 wearing costumes and their older counterparts having wild costume parties with lots of food and drink like caramel apples, candy apples, dirt cakes, punch, cocktails, cider (both alcoholic and non,) pumpkin pie, candy corn, chocolate turtles, peanut brittle, taffy, tipsy cake, and copious buckets full of candy; children carving jack-o-lanterns and eating squash derived foods derive from Halloween's heritage as a harvest festival and from Irish and Scottish traditions of carving turnips and eating root vegetables at this time of year. Their bobbing for apples has survived to the present day as a Halloween party classic game, as has a variation on the parlor game of trying to grab an apple hanging from the ceiling blindfolded: it has evolved into trying to catch a donut in one's teeth.\n\nImmigrants from Southern Europe, namely Sicily, Campania, Lazio, and Calabria, appeared between 1880 and 1960 in New York, New Jersey, Pennsylvania, and Eastern Maryland hoping to escape the extreme poverty and corruption endemic to Italy; typically none of them spoke English, but rather dialects of Italian and had a culture that was more closely tied to the village they were born in than the high culture only accessible to those who could afford it at this time; many could not read or write in any language. They were employed in manual labor or factory work but it is because of them that dishes like spaghetti with meatballs, New York–style pizza, calzones, and baked ziti exist, and Americans of today are very familiar with semolina based pasta noodles. Their native cuisine had less of an emphasis on meat, as evidenced by dishes they introduced like pasta e fagioli and minestrone, but the dishes they created in America often piled it on as a sign of wealth and newfound prosperity since for the first time even cheap cuts of it were affordable: the American recipe for lasagna is proof of this, as mostly it is derived from the Neapolitan version of the dish with large amounts of meat and cheese.\nNew York–style hot dogs came about with German speaking emigrants from Austria and Germany, particularly with the frankfurter sausage and the smaller wiener sausage. Today, the New York–style hot dog with sauerkraut, mustard, and the optional cucumber pickle relish is such a part of the local fabric, that it is one of the favorite comestibles of New York City. Hot dogs are a typical street food sold year round in all by the most inclement weather from thousands of pushcarts. As with all other stadiums in Major League Baseball they are an essential for New York Yankees and the New York Mets games though it is the local style of preparation that predominates without exception. Hot dogs are also the focus of a televised eating contest on the Fourth of July in Coney Island, at Nathan's Famous, one of the earliest hot dog stands opened in the United States in 1916.\n\nA summertime treat, Italian ice, began its life as a lemon flavored penny lick brought to Philadelphia by Italians; its Hispanic counterpart, piragua, is a common and evolving shaved ice treat brought to New York City by Puerto Ricans in the 1930s. Unlike the original dish which included flavors like tamarind, mango, coconut, piragua is evolving to include flavors like grape, a fruit not grown in Puerto Rico. Taylor ham, a meat delicacy of New Jersey, first appeared around the time of the Civil War and today is often served for breakfast with eggs and cheese on a kaiser roll, the bread upon which this is served was brought to the area by Austrians in the second half of the nineteenth century and is a very common roll for sandwiches at lunchtime, usually tipped with poppyseeds. This breakfast meat is generally known as pork roll in southern New Jersey and Philadelphia, and Taylor ham in northern New Jersey.\nOther dishes came about during the early 20th century and have much to do with delicatessen fare, set up largely by Jewish immigrants from Eastern Europe who came to America incredibly poor. Most often they were completely unable to partake in the outdoor food markets that the general population utilized as most of the food for sale was not kosher. The influence of European Jewry before their destruction in the Holocaust on modern mid Atlantic cooking remains extremely strong and reinforced by their many descendants in the region. American-style pickles were brought by Polish Jews, now a common addition to hamburgers and sandwiches, and Hungarian Jews brought a recipe for almond horns that now is a common regional cookie, diverting from the original recipe in dipping the ends in dark chocolate. New York–style cheesecake has copious amounts of cream and eggs because animal rennet is not kosher and thus could not be sold to a large number of the deli's clientele. New York inherited its bagels and bialys from Jews, as well as Challah bread. Pastrami first entered the country via Romanian Jews, and is a feature of many sandwiches, often eaten on marble rye, a bread that was born in the mid Atlantic. Whitefish salad, lox, and matzoh ball soup are now standard fare made to order at local diners and delicatessens, but started their life as foods that made up a strict dietary code.\n\nLike other groups before them, many of their dishes passed into the mainstream enough so that they became part of diner fare by the end of the 20th century, a type of restaurant that is now more numerous in this region than any other and formerly the subject matter of artist Edward Hopper. In the past this sort of establishment was the haven of the short order cook grilling or frying simple foods for the working man. Today typical service would include regional staples like beef on weck, manhattan clam chowder, the club sandwich, Buffalo wings, Philadelphia cheesesteak, the black and white cookie, shoofly pie, snapper soup, Smith Island cake, grape pie, milkshakes, and the egg cream, a vanilla or chocolate fountain drink with a frothy top and fizzy taste. As in Hopper's painting from 1942, many of these businesses are open 24 hours a day.\n\nMidwestern cuisine today covers everything from Kansas City-style barbecue to the Chicago-style hot dog, though many of its classics are very simple, hearty fare. Mostly this region was completely untouched by European and American settlers until after the American Civil War, and excepting Missouri and the heavily forested states near the Great Lakes was mainly populated by nomadic tribes like the Sioux, Osage, Arapaho, and Cheyenne. As with most other Native American tribes, these tribes consumed the Three Sisters of beans, maize, and squash, but also for thousands of years followed the herds of bison and hunted them first on foot and then, after the spread of mustangs from the Southwest due to the explorations of conquistadors, on horseback, typically using bow and arrow. There are buffalo jumps dating back nearly ten thousand years and several photographs and written accounts of trappers and homesteaders attesting to their dependence on the buffalo and to a lesser degree elk. After nearly wiping out the elk and bison to nothingness, this region has taken to raising bison alongside cattle for their meat and at an enormous profit, making them into burgers and steaks.\n\nThis region today comprises the states near the Great Lakes and also the Great Plains; much of it is prairie with a very flat terrain where the blue sky meets a neverending horizon. Winters are bitterly cold, windy, and wet. Often that means very harsh blizzards especially near the Great Lakes where Arctic winds blow off of Canada and where the ice on rivers and lakes freezes reliably thick enough for ice hockey to be a favorite pastime in the region and for ice fishing for pike and muskies to be ubiquitous in Minnesota, Wisconsin, and Michigan, where they often there after become part of the local tradition of the fish fry. Population density is extremely low away from the Great Lakes and very small towns dominated by enormous farms are the rule with larger cities being the exception. Detroit, Cleveland, St. Louis, Cincinnati, Indianapolis, Milwaukee, Minneapolis and her twin sister city across the river St. Paul dominate the landscape in wealth and size, owing to their ties with manufacturing, finance, transportation, and meatpacking. Smaller places like Omaha, Tulsa, and Kansas City make up local capitals, but the king of them all is Chicago, third largest city in the country.\n\nThe Upper Midwest includes the states of Illinois, Minnesota, Wisconsin, Ohio, Indiana, and Michigan. Non-Native American settlement began here earlier than anywhere else in the region, and thus the food available here ranges from the sublime to the bizarre. As with all of the Midwest, the primary meats here are beef and poultry, since the Midwest has been raising turkeys, chickens, and geese for over a hundred and fifty years; chickens have been so common for so long that the Midwest has several native breeds that are prized for both backyard farming and for farmer's markets, such as the Buckeye and Wyandotte; one, Billina, appears as a character in the second book of the Oz series by L. Frank Baum. Favorite fruits of the region include a few native plants inherited from Native American tribes like the pawpaw and the American persimmons are also highly favored. As with the American South, pawpaws are the region's largest native fruit, about the size of a mango, and are often found growing wild in the region come September, whereafter they are made into preserves and cakes and command quite a price at farmer's markets in Chicago. The American persimmon is often smaller than it is Japanese cousin, about the size of a small plum, but in the Midwest and portions of the East it is the main ingredient in a steamed pudding called persimmon pudding, topped with crème anglaise. Other crops inherited from the Native Americans include wild rice, which grows on the banks of lakes and is a local favorite for fancy meals and today often used in stuffing for Thanksgiving.\n\nTypical fruits of the region are cold weather crops. Once it was believed that the region had winters that were far too harsh for apple growing, but then a breeder in Minnesota came forth with the Wealthy apple and thence came forth the third most productive region for apple growing in the land, with local varieties comprising Wolf River, Enterprise, Melrose, Paula Red, Rome Beauty, Honeycrisp, and the world-famous Red Delicious. Cherries are important to Michigan and Wisconsin grows many cranberries, a legacy of early 19th century emigration of New England farmers. Crabapple jelly is a favorite condiment of the region.\n\nThe influence of German, Scandinavian, and Slavic peoples on the northern portion of the region is very strong; many of these emigrated to Wisconsin, Minnesota, Michigan, Ohio, and Illinois in the 19th century to take advantage of jobs in the meatpacking business as well as being homesteaders. Bratwurst is a very common sausage eaten at tailgate parties for the Green Bay Packers, Chicago Bears, or Detroit Lions football teams and is often served boiled in lager beer with sauerkraut, different than many of the recipes currently found in Germany. Polish sausage, in particular a locally invented type of kielbasa, is an essential for sporting events in Chicago: Chicago today has approximately 200,000 speakers of Polish and has had a population of that description for over a hundred years. When Poles came to Chicago and surrounding cities from the Old World, they brought with them long ropes of kielbasa, cabbage rolls, and pierogis. Poles that left Poland after the fall of the Berlin Wall and the descendants of earlier immigrants still make all of the above and such comestibles are common in local diners and delis as result. Today alongside the pierogi, the sausage is served on a long roll with mustard like a hot dog or as a Maxwell Street Polish, a sandwich that has caramelized onions as an essential ingredient. In Cleveland, the same sausage is served in the form of the Polish boy: this is a weird but tasty sandwich made of french fries, spicy barbecue sauce, and coleslaw; unlike cities in the East where the hot dog alone is traditional fans of the Cleveland Indians, Detroit Tigers, Chicago Cubs, and Milwaukee Brewers favor at least two or three different kinds of sausage sold in the little pushcarts outside the stadium; the hot dogs themselves tend to follow the Chicago style, which is loaded with mustard, and pickled vegetables. In Cincinnati, where the Cincinnati Reds play, the predilection for sausage has a competitor in Cincinnati chili, invented by Macedonian immigrants: this bizarre but tasty dish includes spaghetti as its base, chili with a Mediterranean-inspired spice mix, and cheddar cheese; the chili itself is often a topping for local hot dogs at games.\n\nIn the Midwest and especially Minnesota, the tradition of the church potluck has become a gathering in which local foods reign, and so it has been since the era of the frontier: pioneers would often need to pool resources together to have a celebration in the 19th century and that simply never changed. Nowhere is this more clear than with the ever famous hotdish: this is a type of casserole believed to have derived somehow from a Norwegian recipe, and it is usually topped with potatoes or tater tots. Next to the hotdish at the potlucks usually is where the glorified rice is found: this is a dish made of a kind of rice pudding mixed with crushed pineapple and maraschino cherries. Next to that is the booyah, a thick soup made of a number or combinations of meat, vegetables, and seasonings that is meant to simmer on the stove for up to two days. Lefse, traditionally a Scandinavian flatbread, has been handed down to descendants for over a hundred years and is common on the table. Behind that is the venison, a popular meat around the Great Lakes and often eaten in steaks, sandwiches, and crown roasts for special events. If in North or South Dakota, tiger meat, a dish similar to steak tartare, is present. Last on the table are the dessert bars and most especially the brownies: this confection was created originally in 1898 in Chicago and has gone on to become a global food.\nFurther South, barbecue has its own style in places in Kansas and St. Louis that are different to the South and the American West. Kansas City and St. Louis were and remain important hubs for the railroad that connected the plains with the Great Lakes and cities farther east, like Philadelphia. At the turn of the 19th century, the St. Louis area, Omaha, and Kansas City had huge stockyards, waystations for cattle and pigs on their way East to the cities of the coast and North to the Great Lakes. They all had large growing immigrant and migrant populations from Europe and the South respectively, so this region has developed unique styles of barbecue. St. Louis-style barbecue favors a heavy emphasis on a sticky sweet barbecue sauce. Its standbys include the pork steak, a cut taken from the shoulder of the pig, grilled, and then slowly stewed in a pan over charcoal, crispy snoots, a cut from the cheek and nose of the pig that is fried up like cracklin and eaten dipped in sauce, pork spare ribs, and a mix of either beer boiled bratwurst or grilled Italian derived sausage, flavored with fennel. Dessert is usually something like gooey butter cake, invented in the city in the 1930s. Kansas City-style barbecue uses several different kinds of meat, more than most styles of American barbecue- turkey, mutton, pork, and beef just to name a few- but is distinct from St. Louis in that the barbecue sauce adds molasses in with the typical tomato based recipe and typically has a more tart taste. Traditionally, Kansas City uses a low-and-slow method of smoking the meat in addition to just stewing it in the sauce. It also favors using hickory would for smoking and continual watering or layering of the sauce while cooking to form a glaze; with burnt ends this step is necessary to create the \"bark\" or charred outer layer of the brisket.\n\nWhen referring to the American South as a region, typically it should indicate Southern Maryland and the states that were once part of the Old Confederacy, with the dividing line between the East and West jackknifing about 100 miles west of Dallas, Texas, and mostly south of the old Mason-Dixon line. Cities found in this area include New Orleans, Miami, Atlanta, Washington, D.C., Memphis, Charleston, and Charlotte with Houston, Texas being the largest. These states are much more closely tied to each other and have been part of US territory for much longer than states much farther west than East Texas, and in the case of food, the influences and cooking styles are strictly separated as the terrain begins to change to prairie and desert from bayou and hardwood forest.\n\nThis section of the country has some of the oldest known foodways in the land, with some recipes almost 400 years old. Native American influences are still quite visible in the use of cornmeal as an essential staple and found in the Southern predilection for hunting wild game, in particular wild turkey, deer, woodcock, and various kinds of waterfowl; for example, coastal North Carolina is a place where hunters will seek tundra swan as a part of Christmas dinner; the original English and Scottish settlers would have rejoiced at this revelation owing to the fact that such was banned amongst the commoner class in what is now the United Kingdom, and naturally, their descendants have not forgotten. Native Americans also consumed turtles and catfish, specifically the snapping turtle and blue catfish, both important parts of the diet in the South today. Catfish are often caught with one's bare hands, gutted, breaded, and fried to make a Southern variation on English fish and chips and turtles are turned into stews and soups. Native American tribes of the region such as the Cherokee or Choctaw often cultivated or gathered local plants like pawpaw, maypop,spicebush,sassafras, and several sorts of squash and maize, and the aforementioned fruits still are cultivated as food in a Southerner's back garden. Maize is to this day found in dishes for breakfast, lunch and dinner in the form of grits, hoecakes, baked cornbread, and spoonbread, and nuts like the hickory, black walnut and pecan are commonly included in desserts and pastries as varied as mince pies, pecan pie, pecan rolls and honey buns (both are types of sticky bun), and quick breads, which were themselves invented in the South during the American Civil War.\n\nEuropean influence began soon after the settlement of Jamestown in 1607 and the earliest recipes emerging by the end of the 17th century. Specific influences from Europe were quite varied, and remain traditional and essential to the modern cookery overall. To the upper portion of the South, French Huguenots brought the concept of making rouxs to make sauces and soups, and later French settlers hunted for frogs in the swamps to make frog's legs. German speakers often settled in Appalachia on small farms or in the backcountry away from the coast, and invented an American breakfast delicacy that is now nationally beloved, apple butter, based on their recipe for apfelkraut, and later introduced red cabbage and rye. From the UK, an enormous amount of influence was bestowed upon the South, specifically foodways found in 17th and 18th century Ulster, the borderlands between England and Scotland, the Scottish Highlands, portions of Wales, the West Midlands and Black Country. Settlers bound for America fled the tumult of the Civil War and troubles in the plantation of Ireland and the Highland Clearances, and often ships manifests show their belongings nearly always included their wives' cookpots or bakestones and seed stock for plants like peaches, plums, and apples to grow orchards, which they planted in their hundreds: today, the biggest fruit crop of the region is the yellow peach, and noted apple varieties include Carolina Red June, Arkansas Black, Carter Blue, Magnum Bonum, and the infamous Golden Delicious. Each group brought foods and ideas from their region. Settlers from Ireland and Scotland were well known for creating peatreak and poitín, strong hard liquor based on fermenting potatoes or barley, but when they settled in the Appalachians and portions of the piedmont, they found sugar and maize were the only things available. In time they came up with a method where the brew is distilled once using a maize mash with added sugar and the charcoal of sugar maple, which created a whiskey with a high proof and a need for aging in barrels from local species of oak rather than English oak. In time this gave birth in time to American whiskey and Kentucky bourbon, and its infamous later cousins moonshine and Everclear.\n\nCloser to the coast, 18th century recipes for English trifle turned into tipsy cakes, replacing the sherry with whiskey and their recipe for pound cake, brought to the South around the same time, still works with American baking units: 1 pound sugar, one pound eggs, one pound butter, one pound flour. All of the above groups made the staple meat of the South pork, to this day the meat no Southerner can cook without.\n\nWith the exception of Kentucky, where mutton is a common choice, or Southern Maryland, where the custom is to take the carcass of an entire bull and roast it over coals for many hours, pork is the popular choice of Southern style barbecue and features in other preparations like sausages and sandwiches. Among both African-Americans and European-Americans in the antebellum period, corn and pork were a staple of the diet. For breakfast, it is a feature of country sausage, which in turn are an ingredient in the Southern breakfast dish of biscuits and gravy. Head cheese is a popular sliced meat of the region, taken from the pig's head, and pickled pig's feet have always been a cheap snack since they were introduced by Scotch-Irish settlers; today they are often served in bars.. Baby back ribs, hog maw, cracklins, and even whole pig roasts in specially constructed ovens are found in all parts of the South, as are its two best known condiments, barbecue sauce and hot sauce, with hundreds of local variations. In Virginia and the Appalachians, the mainstay for special occasions is the country ham, often served for Christmas and cured with salt or hickory, with the Virginia recipe often feeding the hogs peanuts for finishing and giving the ham a distinct taste, and red pepper flakes in ham cured in Tennessee. Accompanying many meals is the southern style fluffy biscuit, where the leavening agent is sodium bicarbonate and often includes buttermilk, and for breakfast they often accompany country ham, grits, and scrambled eggs.\n\nDesserts in the South tend to be quite rich and very much a legacy of entertaining to impress guests, since a Southern housewife was (and to a degree still is) expected to show her hospitality by laying out as impressive a banquet as she is able to manage. Desserts are vast and encompass Lane cake, sweet potato pie, peach cobbler, pecan pie, hummingbird cake, Jefferson Davis pie, peanut brittle, coconut cake, apple fritters, peanut cookies, Moravian spice cookies, chess pie, doberge cake, Lady Baltimore cake, bourbon balls, and caramel cake. American style sponge cakes tend to be the rule rather than the exception as is American style buttercream, a place where Southern baking intersects with the rest of the United States. Nuts like pecan and hickory tend to be revered as garnishes for these desserts, and make their way into local bakeries as fillings for chocolates.\n\nIn the parts of the South which face the Atlantic Ocean, French influences often were dictated by where French Huguenots settled, however it is Louisiana that got the lion's share of older French cooking methods from Poitou and Normandy via Nova Scotia, most of which are foodways that pre-date the codification of haute cuisine during the reign of Louis XIV and have more in common with rustic cuisines of the 17th and 18th century than anything ever found at the French court in Versailles or the bistros of 19th and 20th century Paris; this is especially true of Cajun cuisine. Louisiana is a state named for Louis XIV and to this day French is still a commonly spoken tongue in the areas west of New Orleans, in Acadiana. The Cajuns and their dialect have occupied Southern Louisiana since the 1700s owing to \"Le Grande Dérangement\", an event in which they were forcibly evicted by the English Crown from their lands in Canada and made to occupy more marginal lands on the bayou.\n\nCajun French is more closely related to dialects spoken in Northern Maine, New Brunswick, and to a lesser degree Haiti than anything spoken in modern France, and likewise their terminology, methodology, and culture concerning food is much more closely related to the styles of these former French colonies even today. Unlike other areas of the South, Cajuns were and still are largely Catholics and thus much of what they eat is seasonal; for example pork is an important component of the Cajun \"boucherie\" (a large community event where the hog is butchered, prepared with a fiery spice mix, and eaten snout to tail) but it is never consumed in the five weeks of Lent, when such would be forbidden. Cajun cuisine tends to focus on what is locally available, historically because Cajuns were often poor, illiterate, independent farmers and not plantation owners but today it is because such is deeply imbedded in local culture. Boudin is a type of sausage found only in this area of the country, and it is often by far more spicy than anything found in France or Belgium. Chaudin is unique to the area, and the method of cooking is comparable to the Scottish dish haggis: the stuffing includes onions, rice, bell peppers, spices, and pork sewn up in the stomach of a pig, and served in slices piping hot. Crayfish are a staple of the Cajun grandmother's cookpot, as they are abundant in the bayous of Southern Louisiana and a main source of livelihood, as are blue crabs, shrimp, corn on the cob, and red potatoes, since these are the basic ingredients of the Louisiana crawfish boil.\nSince the end of the Civil War, New Orleans has had a thriving fine dining scene that predates the much younger 20th century metropoli of Atlanta and Miami. It was here that cocktails like the sazerac and hurricane were invented as well as the liqueur Southern Comfort. New Orleans has been the capital of Creole culture since before Louisiana was a state; this culture is that of the colonial French and Spanish that evolved in the city of New Orleans, which was and still is quite distinct from the rural culture of Cajuns and dovetails with what would have been eaten in antebellum Louisiana plantation culture long ago. Cooking to impress and show one's wealth was a staple of Creole culture, which often mixed French, Spanish, Italian, German, African, Caribbean and Native American cooking methods, producing rich dishes like oysters bienville, pompano en papillote, and even the muffaletta sandwich. However, Louisiana Creole cuisine tends to diverge from the original ideas brought to the region in ingredients: profiteroles, for example, use a near identical choux pastry to that which is found in modern Paris but often use vanilla or chocolate ice cream rather than custard as the filling, pralines nearly always use pecan and not almonds, and bananas foster came about when New Orleans was a key port for the import of bananas from the Caribbean Sea. Gumbos tend to be thickened with okra, or the leaves of the sassafrass tree. Andouille is often used, but not the andouille currently known in France, since French andouille uses tripe whereas Louisiana andouille is made from a Boston butt, usually inflected with pepper flakes, and smoked for hours over pecan wood. Other ingredients that are native to Louisiana and not found in the cuisine of modern France would include rice, which has been a staple of both Creole and Cajun cooking for generations, and sugarcane, which has been grown in Louisiana since the early 1800s.\nGround cayenne pepper is a key spice of the region, as is the meat of the American alligator, something settlers learned from the Choctaws and Houma. The maypop plant has been a favorite of Southerners for 350 years; it gives its name to the Ocoee River in Tennessee, a legacy of the Cherokees, and in Southern Louisiana it is known as liane de grenade, indicating its consumption by Cajuns. It is a close relative of the commercial passionfruit, similar in size, and is a common plant growing in gardens all over the South as a source of fresh summertime fruit.\n\nAfrican influences came with slaves from Ghana, Benin, Mali, Ivory Coast, Angola, Sierra Leone, Nigeria, and other portions of West Africa, and the mark they and their descendants have made on Southern food is extremely strong today and an essential addition to the Southern table. Crops like okra, sorghum, sesame seeds, eggplant, chili peppers, and many different kinds of melons were brought with them from West Africa along with the incredibly important introduction of rice to the Carolinas and later to Texas and Louisiana, whence it became a staple grain of the region and still remains a staple today, found in dishes like Hoppin John, purloo, and Charleston red rice. Other crops, like sugar cane, kidney beans, and certain spices would have been familiar to slaves through contact with British colonies in the Caribbean; Southern plantation owners could and did buy slaves from slave ports and seasoning camps in Havana, San Juan, Port au Prince, Kingston, Bridgetown and Willemstad. Like the poorer indentured servants that came to the South, slaves often got the leftovers of what was slaughtered for the consumption of the master of the plantation and so many recipes had to be adapted for offal, like pig's ears and fatbacks though other methods encouraged low and slow methods of cooking to tenderize the tougher cuts of meat, like braising, smoking, and pit roasting, the last of which was a method known to West Africans in the preparation of roasting goat. African cooks, mostly the women, knew that braising ham hocks flavors collard greens, and this dish remains unchanged after almost 400 years of cooking it and also is often accompanied by black eyed peas, an African crop they would have known before slavery. Other recipes certainly brought by Africans involve peanuts, as evidenced by the local nickname for the legume in Southern dialects of American English: \"goober\", taken from the Kongo word for peanut, \"nguba\". The 300-year-old recipe for peanut soup is a classic of Southern cuisine that has never stopped being eaten, handed down to the descendants of Virginia slaves and adapted to be creamier and less spicy than the original African dish. Boiled peanuts are a common food served at bars as a snack and have been eaten in the South for as long as there have been pots to boil them.\n\nCertain portions of the South often have their own distinct subtypes of cuisine owing to local history and landscape: though Cajun cuisine is more famous, Floridian cuisine, for example, has a distinct way of cooking that includes ingredients her other Southern sisters do not use, especially points south of Tampa and Orlando. The Spanish Crown had control of the state until the early 19th century and used the southern tip as an outpost to guard the Spanish Main beginning in the 1500s, but Florida kept and still maintains ties with the Caribbean Sea, including the Bahamas Haiti, Cuba, Puerto Rico, the Dominican Republic, and Jamaica. South of Tampa, there are and have been for a long time many speakers of Caribbean Spanish, Haitian French, Jamaican Patois, and Haitian Creole and each Caribbean culture has a strong hold on cooking methods and spices in Florida. In turn, each mixes and matches with the foodways of the Seminole tribe and Anglophone settlers. Thus, for almost 200 years, Floridian cooking has had a more tropical flavor than any other Southern state. Allspice, a spice originally from Jamaica, is an ingredient found in spice mixes in summer barbecues along with ginger, garlic, scotch bonnet peppers, sea salt, and nutmeg; in Floridian cooking this is often a variant of Jamaican jerk spice. Coconuts are grown in the areas surrounding Miami and are shipped in daily through its port for consumption of the milk, meat, and water of the coconut. Bananas are not just the yellow Cavendish variety found in supermarkets across America: in Florida they are available as bananitos, colorados, plátanos, and maduros. The first of these is a tiny miniature banana only about 4-5 inches (10–13 cm) in length and it is sweet. The second has a red peel and an apple like after taste, and the third and fourth are used as a starch on nearly every Caribbean island as a side dish, baked or fried: all of the above are a staple of Florida outdoor markets when in season and all have been grown in the Caribbean for almost 400 years. Mangoes are grown as a backyard plant in Southern Florida and otherwise are a favorite treat coming in many different shapes in sizes from \"Nam Doc Mai\", brought to Florida after the Vietnam War, to \"Madame Francis\", a mango from Haiti. Sweetsop and soursop are popular around Miami, but nearly unheard of in other areas of the South.\n\nCitrus is a major crop of Florida, and features at every breakfast table and every market with the height of the season near the first week of January. Hamlin oranges are the main cultivar planted, and from this crop the rest of the United States and to a lesser extent Europe gets orange juice. Other plantings would include grapefruits, tangerines, clementine oranges, limes, and even a few more rare ones, like cara cara navels, tangelos, and the Jamaican Ugli fruit. Tomatoes, bell peppers, habañero peppers, and figs, especially taken from the Florida strangler fig, complete the produce menu. Blue crab, conch, Florida stone crab, red drum, dorado, and marlins tend to be local favorite ingredients. Dairy is available in this region, but it is less emphasized due to the year round warmth. Traditional key lime pie, a dessert from the islands off the coast of Miami, is made with condensed milk to form the custard with the eye wateringly tart limes native to the Florida Keys in part because milk would spoil in an age before refrigeration. Pork in this region tends to be roasted in methods similar to those found in Puerto Rico and Cuba, owing to mass emigration from those countries in the 20th century, especially in the counties surrounding Miami. Orange blossom honey is a specialty of the state, and is widely available in farmer's markets.\n\nPtarmigan, grouse, crow blackbirds, dove, ducks and other game fowl are consumed in the United States. In the American state of Arkansas, beaver tail stew is consumed in Cotton town. Squirrel, raccoon, possum, bear, muskrat, chipmunk, skunk, groundhog, pheasant, armadillo and rabbit are also consumed in the United States.\n\nCooking in the American West gets its influence from Native American and Hispanophone cultures, as well as later settlers that came in the 19th century: Texas, for example, has some influence from Germany in its choice of barbecue by using sausages. Another instance can be found in the Northwestern region, which encompasses Oregon, Washington, and Northern California. All of the aforementioned rely on local seafood and a few classics of their own. In New Mexico, Colorado, Nevada, Arizona, Utah, West Texas, and Southern California, Mexican flavors and influences are extremely common, especially from the Mexican states of Chihuahua, Baja California, and Sonora.\n\nThe Pacific Northwest as a region generally includes Alaska and the state of Washington near the Canada–US border and terminates near Sacramento, California. Here, the terrain is mostly temperate rainforest on the Coast mixed with pine forest as one approaches the Canada–US border inland. One of the core favorite foodstuffs is Pacific salmon, native to many of the larger rivers of the area and often smoked or grilled on cedar planks. In Alaska, wild game like ptarmigan and moose meat feature extensively since much of the state is wilderness. Fresh fish like steelhead trout, Pacific cod, Pacific halibut, and pollock are fished for extensively and feature on the menu of many restaurants, as do a plethora of fresh berries and vegetables, like Cameo apples from Washington state, the headquarters of the U.S. apple industry, cherries from Oregon, blackberries, and marionberries, a feature of many pies. Hazelnuts are grown extensively in this region and are a feature of baking, such as in chocolate hazelnut pie, an Oregon favorite, and Almond Roca is a local candy.\n\nThis region is also heavily dominated by some notable wineries producing a high quality product, with Sonoma found within this region as well as the newer vinicultural juggernauts of Washington State, like the Yakima Valley. The first plantings of vineyards in the United States began many miles to the South on the Pacific coast in what is now San Diego, because the Franciscan friars that settled Alta California required wines they could use for their table and for the Eucharist, and the variety they planted, the mission grape, is still available on a limited basis. Today, French, Spanish, and Italian varietals are sold by the hogshead, and much of the area directly north of San Francisco is under vine, in particular Pinot noir, Garnacha, and Ruffina and several Tuscan varietals.\n\nLike its counterpart on the opposite coast to the East, there is a grand variety of shellfish in this region. Geoducks are a native species of giant clam that have incredibly long necks, and they are eaten by the bucket full as well as shipped to Asia for millions of dollars as they are believed to be an aphrodisiac. Gaper clams are a favorite food, often grilled or steamed in a sauce, as is the native California abalone, which although protected as a food source is a traditional foodway predating settlement by whites and today features heavily in the cooking of fine restaurants as well as in home cooking, in mirin-flavored soups (the influence of Japanese cooking is strong in the region) noodle dishes and on the barbecue. Olympia oysters are served on the half shell as well as the Kumamoto oyster, introduced by Japanese immigrants and a staple at dinner as an appetizer. California mussels are a delicacy of the region, and have been a feature of the cooking for generations: there is evidence that Native American tribes consumed them up and down the California coast for centuries in their masses.\n\nCrabs are a delicacy, and included in this are Alaskan king crab, red crab, yellow crab, and the world-famous Dungeness crab. Californian and Oregonian sportsmen pursue the last three extensively using hoop nets, and prepare them in a multitude of ways. Alaska king crab, able to get up to 10 kg, is often served steamed for a whole table with lemon butter sauce or put in chunks of salad with avocado, and native crabs are the base of dishes like the California roll, cioppino, a tomato based fisherman's stew, and Crab Louie, another kind of salad native to San Francisco. Favorite grains are mainly wheat, and the region is famous for sourdough bread. Cheeses of the region include Humboldt Fog, Cougar Gold and Teleme.\n\nThe states of the Four Corners (Arizona, New Mexico, Colorado, and Utah) plus Nevada, Southern California, and West Texas make up a large chunk of the United States and there is a distinct Hispanic accent to the cookery here, with each having a cultural capital in Salt Lake City, Phoenix, Santa Fe, Las Vegas, Denver, and Los Angeles. This region was part of the Spanish Empire for more than two centuries before California's statehood in the 1830s, and today is the home of a large population of immigrants from Mexico and Central America; Spanish is a commonly spoken secondary language here and the state of New Mexico has its own distinct dialect. With the exception of Southern California, the signature meat is beef, since this is one of the two regions in which cowboys lived and modern cattle ranchers still eke out their living today. High quality beefstock is a feature that has been present in the region for more than 200 years and the many cuts of beef are unique to the United States. These cuts of meat are different from the related Mexican cuisine over the border in that certain kind of offal, like \"lengua\" (tongue) \"cabeza\" (head) and \"tripas\" (tripe) are considered less desirable and are thus less emphasized. Typical cuts would include the ribs, brisket, sirloin, flank steak, skirt steak, and t-bone.\nHistorically, Spanish settlers that came to the region found it completely unsuitable to the mining operations that much older settlements in Mexico had to offer as the technology of the age was not yet advanced enough to get at the silver that would later be found in the region. They had no knowledge of the gold to be discovered in California, something nobody would find until 1848, and knew even less about the silver in Nevada, something nobody would find until after the Civil War. Instead, in order to make the pueblos prosper, they adapted the old rancho system of places like Andalusia in Spain and brought the earliest beefstock, among these were breeds that would go feral and become the Texas longhorn, and Churro sheep, still used as breeding stock because they are easy to keep and well adapted to the extremely arid and hot climate, where temperatures easily exceed 38 °C. Later, cowboys learned from their management practices, many of which still stand today, like the practical management of stock on horseback using the Western saddle.\nLikewise, settlers learned the cooking methods of those who came before and local tribes as well: for example, portions of Arizona and New Mexico still use the aforementioned beehive shaped clay contraption called an \"horno\", an outdoor wood fired oven both Native American tribes like the Navajo and Spaniards used for roasting meat, maize, and baking bread. Other meats that see frequent use in this region are elk meat, a favorite in crown roasts and burgers, and nearer the Mexican border rattlesnake, often skinned and stewed. The taste for alcohol in this region tends toward light and clean flavors found in tequila, a staple of this region since the days of the Wild West and a staple in the bartender's arsenal for cocktails, especially in Las Vegas. In Utah, a state heavily populated by Mormons, alcohol is frowned upon by the Church of Jesus Christ of Latter Day Saints but still available in area bars in Salt Lake City, mainly consumed by the populations of Catholics and other Protestant denominations living there.\n\nIntroduction of agriculture was limited prior to the 20th century and the development of better irrigation techniques, but included the addition of peaches, a crop still celebrated by Native American tribes like the Havasupai, and oranges; today in Arizona, Texas, and New Mexico the favored orange today is the Moro blood orange, which often finds its way into the local cuisine, like cakes and marmalade. Pine nuts are a particular regional specialty and feature often in fine dining and cookies; indeed in Nevada the Native American tribes that live there are by treaty given rights to exclusive harvest. From Native Americans, Westerners learned the practice of eating cactus fruit from the myriad species of opuntia that occupy the Chihuahuan, Sonoran, and Mojave desert lands. In California, Spanish missionaries brought with them the mission fig: today this fruit is a delicacy.\nCuisine in this region tends to have certain key ingredients: tomatoes, onions, black beans, pinto beans, rice, bell peppers, chile peppers, and cheese, in particular Monterey Jack, invented in Southern California in the 19th century and itself often further altered into pepperjack where spicy jalapeño peppers are incorporated into the cheese to create a smoky taste. Chili peppers play an important role in the cuisine, with a few native to the region (Anaheim pepper, Hatch pepper); these still grown by Spanish speakers in New Mexico. In New Mexico, chile is eaten on a variety of foods, such as the green chile cheeseburger, made popular by fast food chains such as Blake's Lotaburger. Indeed, even national fast food chains operating in the state, such as McDonald's, offer locally grown chile on many of their menu items. In the 20th century a few more recent additions have arrived like the poblano pepper, rocoto pepper, ghost pepper, thai chili pepper, and Korean pepper, the last three especially when discussing Southern California and its large population from East and South Asia. Cornbread is consumed in this area, however the recipe differs from ones in the East in that the batter is cooked in a cast iron skillet. Outdoor cooking is popular and still utilizes an old method settlers brought from the East with them, in which a cast iron dutch oven is covered with the coals of the fire and stacked or hung from a tripod: this is different from the earthenware pots of Mexico. Tortillas are still made the traditional way in this area and form an important component of the spicy breakfast burrito, which contains ham, eggs, and salsa or pico de gallo. They also comprise the regular burrito, which contains any combination of marinated meats, vegetables, and piquant chilis; The smothered burrito, often both containing and topped with New Mexico chile sauces; the quesadilla, a much loved grilled dish where cheese and other ingredients are stuffed between two tortillas and served by the slice, and the steak fajita, where sliced skirt steak sizzles in a skillet with caramelized onions.\nUnlike Mexico, tortillas of this region also may incorporate vegetable matter like spinach into the flatbread dough to make wraps, which were invented in Southern California. Food here tends to use pungent spices and condiments, typically chili verde sauce, various kinds of hot sauce, sriracha sauce, chili powder, cayenne pepper, white pepper, cumin, paprika, onion powder, thyme and black pepper. Nowhere is this fiery mix of spice more evident than in the dishes chili con carne, a meaty stew, and cowboy beans, both of which are a feature of regional cookoffs. Southern California has several additions like five spice powder, rosemary, curry powder, kimchi, and lemongrass, with many of these brought by recent immigration to the region and often a feature of Southern California's fusion cuisine, popular in fine dining.\n\nIn Texas, the local barbecue is often entirely made up of beef brisket or large rib racks, where the meat is seasoned with a spice rub and cooked over coals of mesquite, and in other portions of the state they smoke their meat and peppery sausages over high heat using pecan, apple, and oak and served it with a side of pickled vegetables, a legacy of German and Czech settlers of the late 1800s. California is home to Santa Maria-style barbecue, where the spices involved generally are black pepper, paprika, and garlic salt, and grilled over the coals of coast live oak.\nNative American additions may include Navajo frybread and corn on the cob, often roasted on the grill in its husk. A typical accompaniment or appetizer of all these states is the tortilla chip, which sometimes includes cornmeal from cultivars of corn that are blue or red in addition to the standard yellow of sweetcorn, and is served with salsa of varying hotness. Tortilla chips also are an ingredient in the Tex Mex dish nachos, where these chips are loaded with any combination of ground beef, melted Monterey Jack, cheddar, or Colby cheese, guacamole, sour cream, and salsa, and Texas usually prefers a version of potato salad as a side dish. For alcohol, a key ingredient is tequila: this spirit has been made on both sides of the US-Mexican border for generations, and in modern cuisine it is a must have in a bartender's arsenal as well as an addition to dishes for sauteeing.\n\nSouthern California is located more towards the coast and has had more contact with immigration from the West Pacific and Baja California, in addition to having the international city of Los Angeles as its capital. Here, the prime mode of transportation is by car. Drive through fast food was invented in this area, but so was the concept of the gourmet burger movement, giving birth to chains like In and Out Burger, with many variations of burgers including chili, multiple patties, avocado, special sauces, and angus or wagyu beef; common accompaniments include thick milkshakes in various flavors like mint, chocolate, peanut butter, vanilla, strawberry, and mango. Smoothies are a common breakfast item made with fresh fruit juice, yogurt, and crushed ice. Agua fresca, a drink originated by Mexican immigrants, is a common hot weather beverage sold in many supermarkets and at mom and pop stands, available in citrus, watermelon, and strawberry flavors; the California version usually served chilled without grain in it.\nThe weather in Southern California is such that the temperature rarely drops below 12 °C in winter, thus, sun loving crops like pistachios, kiwifruit, avocadoes, strawberries, and tomatoes are staple crops of the region, the last often dried in the sun and a feature of salads and sandwiches. Olive oil is a staple cooking oil of the region and has been since the days of Junípero Serra; today the mission olive is a common tree growing in a Southern Californian's back garden; as a crop olives are increasingly a signature of the region along with Valencia oranges and Meyer lemons. Soybeans, bok choy, Japanese persimmon, thai basil, Napa cabbage, nori, mandarin oranges, water chestnuts, and mung beans are other crops brought to the region from East Asia and are common additions to salads as the emphasis on fresh produce in both Southern and Northern California is strong. Other vegetables and herbs have a distinct Mediterranean flavor which would include oregano, basil, summer squash, eggplant, and broccoli, with all of the above extensively available at farmers' markets all around Southern California. Naturally, salads native to Southern California tend to be hearty affairs, like Cobb salad and Chinese chicken salad, and dressings like green goddess and ranch are a staple. California-style pizza tends to have disparate ingredients with an emphasis on vegetables, with any combination of chili oil, prawns, eggs, chicken, shiitake mushrooms, olives, bell pepper, goat cheese, and feta cheese. Peanut noodles tend to include a sweet dressing with lo mein noodles and chopped peanuts.\n\nFresh fish and shellfish in Southern California tends to be expensive in restaurants, but by no means out of reach of the masses. Every year since the end of WWII, the Pismo clam festival has taken place where the local population takes a large species of clam and bakes, stuffs, and roasts it to their heart's content as it is a regional delicacy. Fishing for pacific species of octopus and the Humboldt squid are common, and both are a feature of East Asian and other L.A. fish markets.Lingcod is a coveted regional fish that is often caught in the autumn off the coast of San Diego and in the Channel Islands and often served baked. California sheephead are often grilled and are much sought after by spear fishermen and the immigrant Chinese population, in which case it is basket steamed. Most revered of all in recent years is the California spiny lobster, a beast that can grow to be 20 kg, and is a delicacy that now rivals the fishery for Dungeness crab in its importance.\n\nHawaii is often considered to be one of the most culturally diverse U.S. states, as well as being the only state with an Asian majority population and being one of the few places where United States territory extends into the tropics. As a result, Hawaiian cuisine borrows elements of a variety of cuisines, particularly those of Asian and Pacific-rim cultures, as well as traditional native Hawaiian and a few additions from the American mainland. American influence of the last 150 years has brought cattle, goats, and sheep to the islands, introducing cheese, butter, and yogurt products, as well as crops like red cabbage. Just to name a few, major Asian and Polynesian influences on modern Hawaiian cuisine are from Japan, Korea, Vietnam, China (especially near the Pearl River delta,) Samoa, and the Philippines. From Japan, the concept of serving raw fish as a meal with rice was introduced, as was soft tofu, setting the stage for the popular dish called poke. From Korea, immigrants to Hawaii brought a love of spicy garlic marinades for meat and kimchi. From China, their version of char siu baau became modern manapua, a type of steamed pork bun with a spicy filling. Filipinos brought vinegar, bagoong, and lumpia, and during the 20th century immigrants from American Samoa brought the open pit fire umu and the Vietnamese introduced lemongrass and fish sauce. Each East Asian culture brought several different kinds of noodles, including udon, ramen, mei fun, and pho, and today these are common lunchtime meals.\n\nMuch of this cuisine mixes and melts into traditions like the infamous lu'au, whose traditional elaborate fare was once the prerogative of kings and queens but today is the subject of parties for both tourists and also private parties for the \"‘ohana\" (meaning family and close friends.) Traditionally, women and men ate separately under the Hawaiian \"kapu\" system, a system of religious beliefs that honored the Hawaiian gods similar to the Maori tapu system, though in this case had some specific prohibitions towards females eating things like coconut, pork, turtle meat, and bananas as these were considered parts of the male gods. Punishment for violation could be severe, as a woman might endanger a man's mana, or soul, by eating with him or otherwise by eating the forbidden food because doing so dishonored all the male gods. As the system broke down after 1810, introductions of foods from laborers on plantations began to be included at feasts and much cross pollination occurred, where Asian foodstuffs mixed with Polynesian foodstuffs like breadfruit, kukui nuts, and purple sweet potatoes.\n\nSome notable Hawaiian fare includes seared ahi tuna, opakapaka (snapper) with passionfruit, Hawaiian island-raised lamb, beef and meat products, Hawaiian plate lunch, and Molokai shrimp. Seafood traditionally is caught fresh in Hawaiian waters, and particular delicacies are \"ula poni\", \"papaikualoa\", \"‘opihi\", and \"‘opihi malihini\", better known as Hawaiian spiny lobster, Kona crab, Hawaiian limpet, and abalone, the last brought over with Japanese immigrants. Some cuisine also incorporates a broad variety of produce and locally grown agricultural products, including tomatoes, sweet Maui onions, taro, and macadamia nuts. Tropical fruits equally play an important role in the cuisine as a flavoring in cocktails and in desserts, including local cultivars of bananas, sweetsop, mangoes, lychee, coconuts, papayas, and lilikoi (passionfruit). Pineapples have been an island staple since the 19th century and figure into many marinades and drinks.\n\nThe demand for ethnic foods in the United States reflects the nation's changing diversity as well as its development over time. According to the National Restaurant Association, \nRestaurant industry sales are expected to reach a record high of $476 billion in 2005, an increase of 4.9 percent over 2004... Driven by consumer demand, the ethnic food market reached record sales in 2002, and has emerged as the fastest growing category in the food and beverage product sector, according to USBX Advisory Services. Minorities in the U.S. spend a combined $142 billion on food and by 2010, America's ethnic population is expected to grow by 40 percent.\nA movement began during the 1980s among popular leading chefs to reclaim America's ethnic foods within its regional traditions, where these trends originated. One of the earliest was Paul Prudhomme, who in 1984 began the introduction of his influential cookbook, \"Paul Prodhomme's Louisiana Kitchen\", by describing the over 200-year history of Creole and Cajun cooking; he aims to \"preserve and expand the Louisiana tradition.\" Prodhomme's success quickly inspired other chefs. Norman Van Aken embraced a Floridian type cuisine fused with many ethnic and globalized elements in his \"Feast of Sunlight\" cookbook in 1988. The movement finally gained fame around the world when California became swept up in the movement, then seemingly started to lead the trend itself, in, for example, the popular restaurant Chez Panisse in Berkeley. Examples of the Chez Panisse phenomenon, chefs who embraced a new globalized cuisine, were celebrity chefs like Jeremiah Tower and Wolfgang Puck, both former colleagues at the restaurant. Puck went on to describe his belief in contemporary, new style American cuisine in the introduction to \"The Wolfgang Puck Cookbook\":\n\nAnother major breakthrough, whose originators were once thought to be crazy, is the mixing of ethnic cuisines. It is not at all uncommon to find raw fish listed next to tortillas on the same menu. Ethnic crossovers also occur when distinct elements meet in a single recipe. This country is, after all, a huge melting pot. Why should its cooking not illustrate the American transformation of diversity into unity?\nPuck's former colleague, Jeremiah Tower became synonymous with California Cuisine and the overall American culinary revolution. Meanwhile, the restaurant that inspired both Puck and Tower became a distinguished establishment, popularizing its so called \"mantra\" in its book by Paul Bertolli and owner Alice Waters, \"Chez Panisse Cooking\", in 1988. Published well after the restaurants' founding in 1971, this new cookbook from the restaurant seemed to perfect the idea and philosophy that had developed over the years. The book embraced America's natural bounty, specifically that of California, while containing recipes that reflected Bertoli and Waters' appreciation of both northern Italian and French style foods.\n\nWhile the earliest cuisine of the United States was influenced by Native Americans, the thirteen colonies, or the antebellum South; the overall culture of the nation, its gastronomy and the growing culinary arts became ever more influenced by its changing ethnic mix and immigrant patterns from the 18th and 19th centuries unto the present. Some of the ethnic groups that continued to influence the cuisine were here in prior years; while others arrived more numerously during \"The Great Transatlantic Migration\" (of 1870—1914) or other mass migrations.\n\nSome of the ethnic influences could be found across the nation after the American Civil War and into the continental expansion for most of the 19th century. Ethnic influences already in the nation at that time would include the following groups and their respective cuisines:\n\nMass migrations of immigrants to the United States came in several waves. Historians identify several waves of migration to the United States: one from 1815 to 1860, in which some five million English, Irish, Germanic, Scandinavian, and others from northwestern Europe came to the United States; one from 1865 to 1890, in which some 10 million immigrants, also mainly from northwestern Europe, settled, and a third from 1890 to 1914, in which 15 million immigrants, mainly from central, eastern, and southern Europe (many Austrian, Hungarian, Turkish, Lithuanian, Russian, Jewish, Greek, Italian, and Romanian) settled in the United States.\n\nTogether with earlier arrivals to the United States (including the indigenous Native Americans, Hispanic and Latino Americans, particularly in the West, Southwest, and Texas; African Americans who came to the United States in the Atlantic slave trade; and early colonial migrants from Britain, France, Germany, Spain, and elsewhere), these new waves of immigrants had a profound impact on national or regional cuisine. Some of these more prominent groups include the following:\nItalian, Mexican and Chinese (Cantonese) cuisines have indeed joined the mainstream. These three cuisines have become so ingrained in the American culture that they are no longer foreign to the American palate. According to the study, more than nine out of 10 consumers are familiar with and have tried these foods, and about half report eating them frequently. The research also indicates that Italian, Mexican and Chinese (Cantonese) have become so adapted to such an extent that \"authenticity\" is no longer a concern to customers.\nContributions from these ethnic foods have become as common as traditional \"American\" fares such as hot dogs, hamburgers, beef steak, which are derived from German cuisine, (chicken-fried steak, for example, is a variation on German schnitzel), cherry pie, Coca-Cola, milkshakes, fried chicken (Fried chicken is of Scottish and African influence) and so on. Nowadays, Americans also have a ubiquitous consumption of foods like pizza and pasta, tacos and burritos to \"General Tso's chicken\" and fortune cookies. Fascination with these and other ethnic foods may also vary with region.\n\nAmerican chefs have been influential both in the food industry and in popular culture. An important 19th Century American chef was Charles Ranhofer of Delmonico's Restaurant in New York City. American cooking has been exported around the world, both through the global expansion of restaurant chains such as T.G.I. Friday's and McDonald's and the efforts of individual restaurateurs such as Bob Payton, credited with bringing American-style pizza to the UK.\n\nThe first generation of television chefs such as Robert Carrier and Julia Child tended to concentrate on cooking based primarily on European, especially French and Italian, cuisines. Only during the 1970s and 1980s did television chefs such as James Beard and Jeff Smith shift the focus towards home-grown cooking styles, particularly those of the different ethnic groups within the nation. Notable American restaurant chefs include Thomas Keller, Charlie Trotter, Grant Achatz, Alfred Portale, Paul Prudhomme, Paul Bertolli, Frank Stitt, Alice Waters, Patrick O'Connell and celebrity chefs like Mario Batali, David Chang, Alton Brown, Emeril Lagasse, Cat Cora, Michael Symon, Bobby Flay, Ina Garten, Todd English, Anthony Bourdain, and Paula Deen.\n\nRegional chefs are emerging as localized celebrity chefs with growing broader appeal, such as Peter Merriman (Hawaii Regional Cuisine), Jerry Traunfeld, Alan Wong (Pacific Rim cuisine), Norman Van Aken (New World Cuisine – fusion Latin, Caribbean, Asian, African and American), and Mark Miller (American Southwest cuisine).\n\n\n\n\n",
                "Pizza box\n\nThe pizza box or pizza package is a folding box made of cardboard in which hot pizzas are stored in for takeaway. The \"pizza box\" also makes home delivery and takeaway substantially easier. The pizza box has to be highly resistant, cheap, stackable, thermally insulated to regulate humidity and suitable for food transportation. In addition, it provides space for advertising. The pizza packages differ from those of frozen pizzas, which contain the frozen product in heat-sealed plastic foils as is the case with much frozen food.\n\nContainers to deliver freshly baked pizzas have existed at least since the 19th century, when Neapolitan pizza bakers put their products in multi-layered metallic containers known as \"stufe\" (singular \"stufa\", \"oven\") and then sent them to the street sellers. The aerated container was round and made of tin or copper.\nDisposable packaging started to be developed in the United States, after the Second World War. At that time pizza was becoming increasingly popular and the first pizza delivery services were created. In the beginning they attempted to deliver pizzas in simple cardboard boxes, similar to those used in cake shops, but these often became wet, bent or even broke in two. Other pizza chefs tried to put pizzas on plates and transport them inside paper bags. This partly solved the problem. However, it was almost impossible to transport at the same time more pizzas inside one bag. In this way, the pizzas on the top would have ruined the surface of the others.\n\nThe first patent for a pizza box made of corrugated cardboard was applied in 1963 and it already displayed the characteristics of today's pizza packaging: plane blanks, foldability without need of adhesive, stackability and ventilation slots. The combination of such slots along with water vapour absorbing materials (absorption agent) prevented the humidity build-ups that characterized traditional transport packaging.\nIt is assumed that the pizza box was invented by Domino's Pizza, even if they did not file a patent application. Until 1988 this chain employed a type of packaging whose front side was not directly connected to the lateral sides, but rather the flaps fixed to the lateral sides were folded inward under the lid. This design is also known as \"Chicago folding\". Domino's was the first pizza producer which employed pizza boxes on a large scale and in this way expanded its delivery range beyond the area immediately close to the pizzeria. Towards the end of the 1960s, the delivery service was further developed thanks to the introduction of heat-insulating bags.\n\nMost packaging for the transport of pizzas is made of cardboard, because this material is cheap and has many useful properties. Both solid fibre board and single wall corrugated board are used. The corrugated cardboard in use is often of the E flute size (micro or fine waves with a flute pitch of 1.0 to 1.8 mm), but thicker B flute cardboard with a flute pitch of 2.2 to 3.0 mm are also used. Not only the geometric construction of the cardboard determines the stability of the box, but also especially the type of paper and its grammage. For the liner mostly containerboard is used on the inside of the box. This does not only make the box stable, it becomes more resistant against humidity and oil as well.\nTo reduce the space needed to store the packaging to a minimum, the pizza boxes are folded just before use out of flat blanks. The required storage space depends very much on the different thickness of the material.\n\nPizza boxes made from solid fibre board take up about half the space the boxes with E flute size cardboard need, and a quarter of the space of those made of B flute size cardboard. Apart from the material itself, the stability of the box is determined by the form it is folded into. The type of pizza box with flaps firmly attached to the side walls that are folded into the front wall has established itself as the standard. In this case, the walls of the box are connected at the edges and this increases its stability. The traditional way of folding the box is an example of this folding type. Its disadvantage is that the walls of the box rise vertically which makes cutting the pizza in the box with a pizza cutter more difficult.\n\nThe pizza box is supposed to allow the transport of a baked pizza with the minimum loss in quality possible. This means the box has two tasks to fulfill that are not easy to combine: On the one hand, the box should insulate as well as possible against the cold air outside, the occasional wind and heat radiation, in order to keep the pizza warm. To reduce heat flow the box has to close as firmly as possible to keep the warm air inside. Consumers consider a temperature between 70 and 85 °C to be ideal for pizza consumption. On the other hand, the box should keep the pizza from getting soggy, so that the crust and the covering are crisp on arrival. To ensure this, the condesation caused by the pizza must be let out (airing holes and some diffusion through the cardboard) or absorbed by the box. Pizza boxes made of single wall corrugated board that are not equipped with additional insulation cool the transported pizza down too far after just ten minutes.\n\nThe oil in the pizza dough can extract some of the essence in the cellulose when in contact with untreated corrugated cardboard. To prevent a change in the taste of the pizza through the material of the pizza box and simultaneously to stop the cardboard from getting soggy the pizza boxes have a thin coating of aluminium foil on the inside. Another possibility is to lay the pizza on aluminium foil, a mixture of corrugated cardboard and blotting paper, or waxed paper. However, this changes the thermodynamic properties of the pizza box considerably. The pizza crust cannot give up any humidity downwards, meaning the layer beneath does not only impede fat from trickling down but also steam.\n\nIn the US, many pizza boxes include a spacer made of heat-resistant plastic (usually (polypropylene) placed in the middle of the pizza. It is called the 'pizza saver' (also known as 'package saver', 'box tent', 'pizza table' or 'pizza lid support').\nThis stops the box lid from touching the pizza and prevents toppings (such as cheese) from sticking to the lid when it is being delivered.\nThe 'pizza saver's' origin goes back to a 1985 patent, taken out by the American Carmela Vitale. The little piece of plastic called the spacer is often criticized for being a waste of resources as it is only used once and then thrown away. For this reason, ideas for its reuse are being developed.\n\nThere are special padded transport bags and boxes for the delivery of pizzas in pizza boxes that are fitted to the typical sizes of pizza boxes. Some of these bags can be heated in order to keep the temperature at the desired level. Mostly, they can either be plugged to a socket or powered by the 12 volt car battery. This means that the insulation of pizza boxes themselves is less important.\nPizza boxes with pizzas inside should be held horizontally at all times and should be protected against high acceleration to the sides and impact. To ensure this, there are usually handles on either side of the carry bags for carrying. Combined, these handles allow the box to be carried in one hand by the side of the body. Carry bags insulate best when their lid is connected to the rest of the bag on one side and the remaining three sides can be attached to lid by a zip. A bag can contain about three to five pizza boxes, which is usually enough for one delivery address. If more pizzas have to be delivered a higher bag is used or two regular bags are placed on top of each other and carried with both hands in front of the body.\n\nThe pizza boxes by Pizza Hut in Morocco and other countries have a thermometer indicator on the outside which colour codes the temperature of the pizza inside, letting the outside world know about what is going on inside. When the pizza is hot the indicator shows the words 'HOT' in red letters on a white background. However, if the temperature of the pizza goes below a certain value the 'Hot Dot' turns black and the words are not longer legible. If the pizza is delivered to the receiver cold, then the next delivery is free of charge. The thermondicator is intended for one use only.\n\nDIPN is used as a solvent in certain kinds of paper. These can be part of scrap paper which is used in the manufacturing of packaging like the pizza box. Direct contact or evaporation can transfer DIPN from the packaging onto the packed food, contaminating it. Especially food containing fat like pizza with a layer of cheese can absorb DIPN. To date, there is no knowledge of danger to the health of the consumer. Still, paper contaminated with DIPN must not be used in the food sector in order to minimise danger of contamination.\n"
            ],
            "good_message": false,
            "loggedData": [],
            "offensive": null,
            "researchTask": true,
            "selectedData": [],
            "shown_passages": [
                [
                    "List of pizza varieties by country",
                    [
                        "During the latter half of the 20th century, pizza became a globally accessible dish, mainly due to Italian immigrants that had brought their dishes to new people with resounding success, often in racially and culturally resistive environments.",
                        "A survey from 2004 showed that Norwegians eat the most frozen pizza (5.4 kg/person*year), followed by Germans.",
                        "Pizza became a popular fast food in Bangladeshi urban areas.",
                        "Since the introduction of various pizza brands such as Domino's and Pizza Hut in the early to mid-2000s, it has reached almost all classes of urban peoples.",
                        "Pizza is an emerging fast food in Indian urban areas."
                    ]
                ],
                [
                    "Cuisine of the United States",
                    [
                        "The cuisine of the United States reflects its history.",
                        "The European colonization of the Americas yielded the introduction of a number of ingredients and cooking styles to the latter.",
                        "The various styles continued expanding well into the 19th and 20th centuries, proportional to the influx of immigrants from many different nations; such influx developed a rich diversity in food preparation throughout the country.",
                        "Early Native Americans utilized a number of cooking methods in early American Cuisine that have been blended with early European cooking methods to form the basis of American Cuisine.",
                        "When the colonists came to the colonies, they farmed animals for clothing and meat in a similar fashion to what they had done in Europe."
                    ]
                ],
                [
                    "Pizza box",
                    [
                        "The pizza box or pizza package is a folding box made of cardboard in which hot pizzas are stored in for takeaway.",
                        "The \"pizza box\" also makes home delivery and takeaway substantially easier.",
                        "The pizza box has to be highly resistant, cheap, stackable, thermally insulated to regulate humidity and suitable for food transportation.",
                        "In addition, it provides space for advertising.",
                        "The pizza packages differ from those of frozen pizzas, which contain the frozen product in heat-sealed plastic foils as is the case with much frozen food.",
                        "Containers to deliver freshly baked pizzas have existed at least since the 19th century, when Neapolitan pizza bakers put their products in multi-layered metallic containers known as \"stufe\" (singular \"stufa\", \"oven\") and then sent them to the street sellers."
                    ]
                ]
            ],
            "speaker": "0_Wizard",
            "suggestionsConfig": "show",
            "suggestionsNumber": 5,
            "suggestionsShown": true,
            "text": "Well pizza goes great with almost any meat or vegetable topping, what do you like",
            "textboxFilled": 0,
            "time": 1580233160.0667832,
            "timeTaken": 3.293,
            "totalTasks": 120,
            "turn": 3
        },
        {
            "checked_passage": {
                "no_passages_used": "no_passages_used"
            },
            "checked_sentence": {
                "no_passages_used": "no_passages_used"
            },
            "clicked_passages": {},
            "context": {
                "conversation": [
                    "I love the tasty pizza",
                    "Me too. Which topping do you like for it. additional vegetables, meats, or condiments?",
                    "Pizza is pizza! Any recipe is cool",
                    "If you are  a big fan of pizza, then you should have your favorite as we have global access to the dish which is dues to Italian immigrants that had brought their dishes to new people with resounding success",
                    "You mean it originates in Italy?",
                    "Of course yes Pizza is a traditional Italian dish  and of course with their topping like tomato sauce and cheese "
                ],
                "index": 2
            },
            "contextCount": 0,
            "full_passages": [
                "List of pizza varieties by country\n\nDuring the latter half of the 20th century, pizza became a globally accessible dish, mainly due to Italian immigrants that had brought their dishes to new people with resounding success, often in racially and culturally resistive environments.\n\nA survey from 2004 showed that Norwegians eat the most frozen pizza (5.4 kg/person*year), followed by Germans.\n\nPizza became a popular fast food in Bangladeshi urban areas. Since the introduction of various pizza brands such as Domino's and Pizza Hut in the early to mid-2000s, it has reached almost all classes of urban peoples.\n\nPizza is an emerging fast food in Indian urban areas. American pizza chains Domino's Pizza and Pizza Hut opened their first outlets in India in 1996. Domestic pizza brands include U.S.Pizza, Smokin' Joes and Pizza Corner. Branded pizza is available in most cities in India.\n\nPizzas served in India by foreign pizza brands feature greater \"recipe localization\" from pizza makers than many other markets such as Latin America and Europe, but similar to other Asian pizza markets. Indian pizzas are generally spicier and more vegetable-oriented than those in other countries. For instance, oregano spice packs are included with a typical pizza order in India instead of Parmesan cheese. In addition to spicier and more vegetable-oriented ingredients, Indian pizza also utilized unique toppings. For example, a pizza topping unique to India would be pickled ginger.\n\nPizza outlets serve pizzas with several Indian-style toppings, such as tandoori chicken and paneer. More conventional pizzas are also eaten. Pizzas available in India range from localized basic variants, available in neighborhood bakeries, to gourmet pizzas with exotic and imported ingredients available at specialty restaurants.\n\nIn Indonesia, Pizza Hut is the largest pizza chain restaurant who entered Indonesia in 1984, followed by Domino Pizza and Papa Ron's Pizza. Popular pizza recipes such as meat lover with pepperoni, tuna with melt cheese, and beef blackpepper exist in Indonesia. Those recipes are originated either from United States or Italy, thus derived from western counterpart. \n\nHowever, there are also Asian eastern pizza which includes Indonesian fusion pizza that combine Indonesian favourite as pizza toppings — such as satay, balado and rendang.\nOther than Indonesian fusion, other Asian fusion pizza are also known in Indonesia, including:\n\nAmerican pizza chains entered Japan in the 1970s (e.g. Shakey's Pizza and Pizza Hut 1973, Domino's pizza in 1985). The largest Japanese pizza chain is Pizza-La. Local types of pizza are popular, with many using mayonnaise sauces, and sometimes other ingredients such as corn, potatoes, avocado, eel, or even honey or chocolate (as in dessert). \"Side orders\" also often include items such as french fries, fried chicken, baked pasta, as well as vegetable soups, green salads, desserts, and soda or Japanese tea. There is also a strong tradition of using Tabasco sauce on cooked pizzas.\n\nPizza toppings in Japan also differ from that found in the United States. One of the unique pizza toppings found in Japan is squid. Seafood may be found on pizza everywhere, but having squid as the focal ingredient is unique to Japan.\n\nLocal crust variants also exist, for instance mochi pizza (crust made with Japanese mochi cakes). Traditional pizza served in Italian-style restaurants are also popular, and the most popular pizza chain promoting Italian style artisanal pizza is Salvatore Cuomo. The Italian association Associazione Verace Pizza Napoletana also has an independent branch in Japan.\n\nPizza is a popular snack food in South Korea, especially among younger people. Major American brands such as Domino's, Pizza Hut, and Papa John's Pizza compete against domestic brands such as Mr. Pizza and Pizza Etang, offering traditional as well as local varieties which may include toppings such as bulgogi and dak galbi. Korean-style pizza tends to be complicated, and often has nontraditional toppings such as corn, potato wedges, sweet potato, shrimp, or crab. Traditional Italian-style thin-crust pizza is served in the many Italian restaurants in Seoul and other major cities. \nNorth Korea's first pizzeria opened in its capital Pyongyang in 2009.\n\nPizza restaurants in Malaysia include Domino's, Pizza Hut, Papa John's, Jom Pizza, and Sure Pizza.\n\nPizza is becoming more popular as a fast food in the urban areas of Nepal, particularly in the capital city, Kathmandu. There are a number of restaurants that serve pizzas in Kathmandu. With the opening of number of international pizza restaurants, the popularity as well as consumption has markedly increased in recent times.They are many types of pizza are there. Some are listed below\n1.mushroom pizza\n2.chicken pizza\n3. pane-er pizza\n\nThe first pizzerias opened in Karachi and Islamabad in the late 1980s, with Pappasallis serving pizza in Islamabad since 1990. Pizza has gained a measure of popularity in the eastern regions of Pakistan—namely, the provinces of Sindh, Punjab, and P.O.K, as well as the autonomous territory of Gilgit-Baltistan. Pizza has not penetrated into western Pakistan; of the remaining provinces and territories of Pakistan, only one (Khyber Pakhtunkhwa) has seen much of the dish, in the form of a single Pizza Hut in Peshawar. Chicken Tikka and achari chicken pizzas are popular. In the regions where pizza is known, spicy chicken and sausage-based pizzas are also very popular, as they cater to the local palate.\n\nAuthentic Neapolitan pizzas (\"pizza napoletana\") are typically made with tomatoes and mozzarella cheese. They can be made with ingredients like San Marzano tomatoes, which grow on the volcanic plains to the south of Mount Vesuvius, and mozzarella di bufala Campana, made with the milk from water buffalo raised in the marshlands of Campania and Lazio in a semi-wild state (this mozzarella is protected with its own European protected designation of origin).\n\nAccording to the rules proposed by the \"Associazione Verace Pizza Napoletana\", the genuine Neapolitan pizza dough consists of wheat flour (type \"0\" or \"00\", or a mixture of both), natural Neapolitan yeast or brewer's yeast, salt and water. For proper results, strong flour with high protein content (as used for bread-making rather than cakes) must be used. The dough must be kneaded by hand or with a low-speed mixer. After the rising process, the dough must be formed by hand without the help of a rolling pin or other machine, and may be no more than thick. The pizza must be baked for 60–90 seconds in a stone oven with an oak-wood fire. When cooked, it should be crispy, tender and fragrant. There are three official variants: \"pizza marinara\", which is made with tomato, garlic, oregano and extra virgin olive oil, \"pizza Margherita\", made with tomato, sliced mozzarella, basil and extra-virgin olive oil, and \"pizza Margherita extra\" made with tomato, mozzarella from Campania in fillets, basil and extra virgin olive oil. The pizza napoletana is a Traditional Speciality Guaranteed (\"Specialità Tradizionale Garantita\", STG) product in Europe.\n\nPizza in Lazio (Rome), as well as in many other parts of Italy, is available in two different styles. Take-away shops sell \"pizza rustica\" or \"pizza al taglio\". This pizza is cooked in long, rectangular baking pans and relatively thick (1–2 cm). The pizza is often cooked in an electric oven. It is usually cut with scissors or a knife and sold by weight. In pizzerias, pizza is served in a dish in its traditional round shape. It has a thin, crisp base quite different from the thicker and softer Neapolitan style base. It is usually cooked in a wood-fired oven, giving the pizza its unique flavor and texture. In Rome, a \"pizza napoletana\" is topped with tomato, mozzarella, anchovies and oil (thus, what in Naples is called \"pizza romana\", in Rome is called \"pizza napoletana\"). Other types of Lazio-style pizza include\n\nPizza quattro stagioni is a popular style prepared with various ingredients in four sections, with each section representing a season of the year.\n\nPizza pugliese is prepared with tomato, mozzarella and onion.\n\nPizzetta a small pizza that can range in size from around three inches in diameter to the size of a small personal-sized pizza. It may be served as an hors d'oeuvre.\n\nSicilian pizza is prepared in a manner originating in Sicily, Italy. Just in the US, the phrase \"Sicilian pizza\" is often synonymous with thick-crust or deep-dish pizza derived from the Sicilian \"Sfincione\". In Sicily, there is a variety of pizza called \"Sfincione\". It is that believed Sicilian pizza, Sfincione, or focaccia with toppings, was popular on the western portion of the island as far back as the 1860s.\n\nThere was a bill before the Italian Parliament in 2002 to safeguard the \"traditional Italian pizza\", specifying permissible ingredients and methods of processing (e.g., excluding frozen pizzas). Only pizzas which followed these guidelines could be called \"traditional Italian pizzas\" in Italy. On 9 December 2009, the European Union, upon Italian request, granted Traditional Speciality Guaranteed (TSG) safeguard to traditional Neapolitan pizza, in particular to \"Margherita\" and \"Marinara\". The European Union enacted a protected designation of origin system in the 1990s.\n\nThe Maltese enjoy eating Italian style pizza and fast-food pizzas, as well as experimenting with various toppings, including local produce. One style of fast-food pizza is the typical \"pizza kwadra\" (square pizza), which is found in Pastizzi shops (\"pastizzeriji\"), a deep-pan pizza cut into squares, generally topped with either green olives (\"taż-żebbuġ\"), hard boiled egg and cocktail sausage (\"bajd u zalzett\"), or chicken and barbecue sauce (\"tat-tiġieġ\"). A typical \"Pizzerija\" restaurant will offer a vast number of different pizza recipes, mostly based on the Italian style ones. A typical menu would include:\n\n\nPizza has become a household dish. Nevertheless, the traditional Maltese pizza consists of a typical Maltese ftira covered in cheese (mainly local gbejna), onions and potatoes. In fact, it is most often known simply as \"ftira\" and is mainly sold on the island of Gozo. Different toppings can be added, including tuna, olives, anchovies, sundried tomatoes, and even traditional Maltese sausage.\n\nNorwegians eat the most pizza in the world according to a 2004 survey by ACNielsen 2004, 5,4 kg/year per capita. 50 million frozen pizzas were sold that year, with consumption being 22,000 tons of frozen pizza, 15,000 tons of home-baked and 13,000 tons of restaurant-made pizzas. By far the most popular is the frozen pizza Grandiosa, every other pizza sold, frozen or fresh is a Pizza Grandiosa. Since its start in 1980 the Grandiosa has been part of Norwegian modern culture and trends, going so far to be unofficial called \"The national dish of Norway\".\n\nNorway also has a traditional home-made pizza called \"lørdagspizza\" (literally translates to \"Saturday pizza\"). The dough is shaped to the pan (usually rectangular), then a mix of minced meat and tomato sauce follows. Finally it is gratinated with a generous amount of cheese.\n\nPizza arrived in Sweden with Italian guest workers and became popular around 1970. Swedish pizza is mainly of the Neapolitan type and most pizzerias in Sweden have Margherita, Capricciosa and Quattro Stagioni pizzas at the top of the menu, although with altered recipes. For example, a Swedish Margherita uses Swedish hard cheese instead of mozzarella and dried oregano instead of fresh basil. The Swedish pizza has been developed with lots of innovations and styles, creating a tradition distinct from the Italian one, although some names may overlap. Occasionally pizzerias offer \"Italian pizza\" imitating Italian recipes in addition to the Swedish ones.\n\nA typical Swedish pizzeria offers 40-50 different named varieties on the menu, even up to 100, and personal modifications are allowed. Also, many pizzerias also serve salads, lasagne, kebab and hamburgers, especially if there is a facility to sit and eat. Italian style restaurants often combine a restaurant menu with a pizza menu.\n\nSome popular varieties common in most of Sweden, mostly with the same name, all having tomato sauce and cheese to start with and additional toppings:\n\nPerhaps the most extreme pizza sort heard of in Sweden is the Calskrove or Calzskrove (a portmanteau of calzone and \"skrovmål\" meaning \"big meal\" but also Northern slang for \"hamburger meal\"), sold at some pizzerias in northern Sweden, a complete meal of a 150 or 250 grams hamburger with bread and all regular toppings, and chips (french fries), baked into a regular Calzone with ham as well.\n\nOne of the most popular types of pizza in Sweden since the 1990s is kebab-pizza, and a song in the Swedish Melodifestivalen 2008 was \"Kebabpizza slivovitza\". The invention is most likely the result of the common tendency of pizza bakers to create their own flagship compositions and novel flavors, using whatever might be available in their kitchen. In recent years one can find pizza with fresh lettuce or chips (French fries) put on top after baking. The amount of topping compared to the crust is rather high by international standards.\n\nThe typical side order with Swedish pizza is a free \"pizza salad\". 1969 Giuseppe \"Peppino\" Sperandio opened \"Pizzeria Piazza Opera\", one of the first restaurants only serving pizza in Stockholm, Sweden. Sperandio was born in northeast Italy where a cabbage salad called \"kupus salata\" was a very common dish, from bordering country Croatia. This salad from his childhood, was offered as a free side dish. Eaten, while waiting for the pizza to be baked. Sperandio became Stockholm's pizza king and had during his hey day more than 30 pizza restaurants. Today this Balkan salad (renamed to pizza salad), is as Swedish as the Dala horse.\nThe pizza salad is made with shredded cabbage, coarse pepper and sometimes red bell pepper, slightly pickled (fermented) in vinaigrette for a few days.\n\nIn general, Swedish pizzerias are private enterprises and not franchise, often owned as a family business by immigrants, but very seldom Italians. Of international restaurant chains only Pizza Hut is well established, although Vapiano has a few restaurants in Stockholm and Domino's have been trying to establish itself in southern Sweden since 2008. Many pizzerias offer affordable (about 1-2 € total, or free with large order) home delivery in less than 30 minutes and many are connected to an on-line ordering service. The take-away price of one standard size (30 cm) pizza is 5 to 8 € depending on topping, about the double for a \"family pizza\" of double size (weight), and about the half for a \"children's pizza\" (mostly served in restaurants). Pizza has become a staple food in Sweden (1,1 kg/year), although most people prepare their own food, as home cooking skills generally are good, and is largely considered as an acceptable occasional fast food alternative to a proper meal.\n\nSince the 1980s, a wide variety of pizzas ranging from fairly authentic Italian to American style to the mass-processed varieties are widely available and pizzas are also commonly made at home with children using local substitutions such as bacon for prosciutto and cheddar for mozzarella. Dough bases vary widely from homemade scone doughs to thin Roman-style and thick American stuffed-crust types. The typical British high-street now has a variety of international Italian- and American-style pizza chains, including homegrown chains PizzaExpress, Strada and Prezzo as well as Dominos, Pizza Hut, and Papa John's alongside much more authentic independent Italian-run restaurants with wood-fired ovens particularly in large cities such as London. Unique spicy varieties enjoy some popularity, including Chicken tikka masala or other curry toppings, chilli pizzas and a typical mid-range restaurant or takeaway will usually have versions of such standard \"Italian-American\" combinations as 'Hawaiian' (ham and pineapple); 'Peperroni' (spicy salami) and 'Meat Feast' (a mix of meats and salami) and a 'Vegeteriana' options. Non-Italian varieties are common too, for example, lahmacun called 'Turkish pizzas', or Alsatian 'Flammkuchen'. In some parts of Scotland you can get a deep-fried pizza from Fish and Chip shops. A frozen pizza, whole or half, dipped in batter and deep fried. It is usually served with in the same manner as any other fried item from these shops.\n\nIceland has all of the typical pizza toppings you would expect like pepperoni and sausage but also have some unique ones. A pizza topping that is found in Iceland that may not be found elsewhere, except Sweden, would be bananas. Bananas are used as toppings across the country showing how they have created their own version of an Italian classic.\n\nMany Israeli and American pizza stores and chains, including Pizza Hut and Sbarro, have both kosher and non-kosher locations. Kosher locations either have no meat or use imitation meat because of the Jewish religious dietary prohibition against mixing meat with dairy products, such as cheese. Kosher pizza locations must also close during the holiday of Passover, when no leavened bread is allowed in kosher locations. Some Israeli pizza differs from pizza in other countries because of the very large portions of vegetable toppings such as mushrooms or onions, and some unusual toppings, like corn or labane, and middle-Eastern spices, such as za'atar. Like most foods in Israel, pizza choices reflect multiple cultures.\n\nPizza establishments in Turkey are a mixture of local restaurants, local chains (e.g. Pizza Max), and international chains like Pizza Hut, Domino's Pizza, Little Caesars, and Sbarro. While most combinations of toppings reflect common ingredients found in the US and Italy, there are additional ingredients available that cater to traditional tastes as well, such as minced beef, spicy Sucuk sausage, cured meats like Pastırma, cheeses like Kaşar and Beyaz, and local olives and herbs. With the exception of some restaurants, pork products like ham and bacon are not available, which are substituted with beef, chicken, or lamb equivalents.\n\nPizza has several equivalent or similar dishes in traditional Turkish cuisine, such as Black-Sea-style or Bafra-style Pide and Lahmacun, which adds to the popularity of the dish across Turkey.\n\nMexican pizza is a pizza made with ingredients typical of Mexican cuisine. The usual toppings that can be found throughout Mexico are chorizo, jalapeño pepper slices, grilled or fried onions, tomato, chile, shrimp, avocado, and sometimes beef, bell peppers, tripas or scallop. This pizza has the usual marinara sauce or white sauce and mozzarella cheese. Variations, substituting pepper jack cheese or Oaxaca cheese for mozzarella, are also popular.\n\nIn 1905, the first pizza establishment in the United States was opened in New York's Little Italy. Due to the influx of Italian immigrants, the U.S. has developed regional forms of pizza, some bearing only a casual resemblance to the Italian original. Chicago has its own style of a deep-dish pizza and New York City's style of pizza are well-known. New York-style pizza refers to the thin crust pizza popular in the states of New York, New Jersey, and Connecticut. Philadelphia provides sauce on top of the cheese; St. Louis and other Midwest pizzas use thin crusts and rectangular slices in its local pizzas. Detroit-style pizza is a square pizza that has a thick deep-dish crisp crust, and is generally served with the sauce on top of the cheese. The square shape is the result of an early tradition of using metal trays originally meant to hold small parts in factories. The jumbo slice is an oversized New York-style pizza sold by the slice to go, especially in the Adams Morgan neighborhood in Washington, D.C. The white clam pie is a pizza variety that originated at the Frank Pepe Pizzeria Napoletana restaurant in New Haven, Connecticut.\n\nCanada features many of the large pizza chains found in the United States, but with regional variations resulting from influences of local Canadian cuisine.\n\nThe \"Canadian pizza\" toppings typically include tomato sauce, mozzarella cheese, bacon, pepperoni, and mushrooms; variations exist.); this recipe is also known internationally by that name. The typical preparation of the same recipe is often referred to in Québécois as \"pizza québécoise\".\n\nPoutine pizza is one variety that can be found sporadically across the country, and adaptations of this item have even been featured in upscale restaurants.\n\nAtlantic Canada has several unique varieties, which have spread to other parts of the country as people migrate for work. Donair pizza is inspired by the Halifax fast food of the same name, and is topped with mozzarella cheese, spiced & roasted ground beef, tomatoes, onions, and a sweetened condensed milk-based donair sauce. Garlic fingers is an Atlantic Canadian pizza garnished with melted butter, garlic, cheese, and sometimes bacon, with the round sliced into fingers and served with donair sauce. Pictou County Pizza is a variant of pizza unique to Pictou County in Nova Scotia; this pizza has a \"brown sauce\" made from vegetables and spices instead of red tomato sauce. \n\nToronto-style pizza, is a medium-thick crust margarita pizza topped with garlic and basil oil topping, a fusion of an Italian-type pizza and the Vietnamese traditions of using herbed oil toppings.\n\nThe predominantly francophone Canadian province of Quebec has its specialties. One is the \"all dressed\": tomato sauce (a little spicy), pepperoni, onions, green pepper slices, and mushrooms.\nThe poutine pizza variety is topped with French fries, light gravy, and fresh mozarella curds.\n\nAccording to a number of news outlets, the Hawaiian-style (tomato sauce, ham and pineapple) is a Canadian invention, originating at the Satellite Restaurant in Chatham, Ontario. Sam Panopoulos, owner of Satellite, first concocted the Hawaiian pizza in 1962 . By that time Satellite had already started serving Chinese food and Panopoulos thought people would like a similar sweet and savoury flavours together so he took a can of pineapple and tossed the fruit onto a pizza.\n\nThe usual Italian varieties are available, though more common is the style popular in the U.S., with more and richer toppings than Italian style. A common unique type is the Aussie, Australian or Australiana, which has the usual tomato base or a seasoned base and mozzarella cheese with options of chicken, ham, bacon and egg (seen as quintessentially Australian breakfast fare). Pizzas with seafood such as prawns are also popular. In the 1980s some Australian pizza shops and restaurants began selling \"gourmet pizzas\", that is, pizzas with more expensive ingredients such as salmon, dill, bocconcini, tiger prawns, or unconventional toppings such as kangaroo meat, emu and crocodile. \"Wood-fired pizzas\", that is, those cooked in a ceramic oven heated by wood fuel, are well-regarded.\n\nFranchised chains coexists with independent pizzerias, Middle-Eastern bakeries and kebabs shops.\n\nNew Zealand's first dedicated pizza outlet was opened by Pizza Hut in New Lynn in 1974, with Dominos following. One notable indigenous chain is Hell Pizza established in 1996 - which now has outlets worldwide - distinguishing itself by often-controversial marketing and using only free-range ingredients. Independent restaurants are common.\n\nNew Zealand has no rules for pizza construction, leading to an eclectic and varied approach to toppings. Gourmet and \"wild\" ingredients are often used, and New Zealanders are apt to push the boundaries of what a pizza can be.\n\nStandard Argentine pizza has a thicker crust than traditional Italian style pizza and includes more cheese. Pizza is made with very thin, and sometimes thick, high-rising doughs, with or without cheese, cooked in the oven or \"a la piedra\" (on a stone oven), and stuffed with numerous ingredients -— is a dish which can be found in nearly every corner of the country. Buenos Aires, Rosario, and Córdoba also serve it with fainá, which is a chick pea-flour dough placed over the piece of pizza. People say that what makes the Argentine pizza unique is the blending of Italian and Spanish cultures. At the turn of the 19th century, immigrants from Naples and Genoa opened the first pizza bars, though Spanish residents subsequently owned most of the pizza businesses.\nAnother very popular kind is the fugazza, which consists in a regular pizza crust topped with onions, ground black pepper, olive oil and mozzarella cheese (in this case it is called fugazzeta).\n\nSão Paulo has 6,000 pizza establishments and 1.4 million pizzas are consumed daily. It is said that the first Brazilian pizzas were baked in the Brás district of São Paulo in the late part of the 19th century. Until the 1940s, almost only found in the Italian communities around the country. Since then, pizza became increasingly popular among the rest of the population. The most traditional pizzerias are still found in the Italian neighborhoods, such as Bexiga (official name: Bela Vista). Both Neapolitan (thick crust) and Roman (thin crust) varieties are common in Brazil, with traditional versions using tomato sauce and mozzarella as a base. Brazilian pizza in general, though, tends to have less tomato sauce than the authentic (Italian) pizza, or uses slices of tomato in place of sauce. Brazilian pizzerias offer also Brazilian variants such as \"pizza com catupiry\". July 10 is \"Pizza Day\" in São Paulo, marking the final day of an annual competition among \"pizzaiolos\". In Brazil, pizza quatro queijos (\"pizza quattro formaggi\") uses mozzarella, provolone, parmesan and gorgonzola, and there is also a variety with five cheeses, which adds catupiry.\n\n\"Hawaiian pizza\" is popular in Colombia. The pizza is topped with ham and pineapple.\n\n",
                "Cuisine of the United States\n\nThe cuisine of the United States reflects its history. The European colonization of the Americas yielded the introduction of a number of ingredients and cooking styles to the latter. The various styles continued expanding well into the 19th and 20th centuries, proportional to the influx of immigrants from many different nations; such influx developed a rich diversity in food preparation throughout the country.\n\nEarly Native Americans utilized a number of cooking methods in early American Cuisine that have been blended with early European cooking methods to form the basis of American Cuisine. When the colonists came to the colonies, they farmed animals for clothing and meat in a similar fashion to what they had done in Europe. They had cuisine similar to their previous British cuisine. The American colonial diet varied depending on the settled region in which someone lived. Commonly hunted game included deer, bear, buffalo, and wild turkey. A number of fats and oils made from animals served to cook much of the colonial foods. Prior to the Revolution, New Englanders consumed large quantities of rum and beer, as maritime trade provided them relatively easy access to the goods needed to produce these items: rum was the distilled spirit of choice, as the main ingredient, molasses, was readily available from trade with the West Indies. In comparison to the northern colonies, the southern colonies were quite diverse in their agricultural diet and did not have a central region of culture.\n\nDuring the 18th and 19th centuries, Americans developed many new foods. During the Progressive Era (1890s–1920s) food production and presentation became more industrialized. One characteristic of American cooking is the fusion of multiple ethnic or regional approaches into completely new cooking styles. A wave of celebrity chefs began with Julia Child and Graham Kerr in the 1970s, with many more following after the rise of cable channels such as Food Network.\n\nSeafood in the United States originated with the Native Americans, who often ate cod, lemon sole, flounder, herring, halibut, sturgeon, smelt, drum on the East Coast, and olachen and salmon on the West Coast. Whale was hunted by Native Americans off the Northwest coast, especially by the Makah, and used for their meat and oil. Seal and walrus were also eaten, in addition to eel from New York's Finger Lakes region. Catfish was also popular among native people, including the Modocs. Crustacean included shrimp, lobster, crayfish, and dungeness crabs in the Northwest and blue crabs in the East. Other shellfish include abalone and geoduck on the West Coast, while on the East Coast the surf clam, quahog, and the soft-shell clam. Oysters were eaten on both shores, as were mussels and periwinkles.\n\nEarly Native Americans used a number of cooking methods in early American Cuisine that have been blended with early European cooking methods to form the basis of American Cuisine. Grilling meats was common. Spit roasting over a pit fire was common as well. Vegetables, especially root vegetables were often cooked directly in the ashes of the fire. As early Native Americans lacked pottery that could be used directly over a fire, they developed a technique which has caused many anthropologists to call them \"Stone Boilers\". They would heat rocks directly in a fire and then add the rocks to a pot filled with water until it came to a boil so that it would cook the meat or vegetables in the boiling water. In what is now the Southwestern United States, they also created adobe ovens called hornos to bake products such as cornmeal bread. Other parts of America dug pit ovens; these pits were also used to steam foods by adding heated rocks or embers and then seaweed or corn husks placed on top to steam fish and shellfish as well as vegetables; potatoes would be added while still in skin and corn while in-husk, this would later be referred to as a clambake by the colonists.\n\nWhen the colonists came to Virginia, Pennsylvania, Massachusetts, or any of the other English colonies on the eastern seaboard of North America, their initial attempts at survival included planting crops familiar to them from back home in England. In the same way, they farmed animals for clothing and meat in a similar fashion. Through hardships and eventual establishment of trade with Britain, the West Indies and other regions, the colonists were able to establish themselves in the American colonies with a cuisine similar to their previous British cuisine. There were some exceptions to the diet, such as local vegetation and animals, but the colonists attempted to use these items in the same fashion as they had their equivalents or ignore them entirely if they could. The manner of cooking for the American colonists followed along the line of British cookery up until the Revolution. The British sentiment followed in the cookbooks brought to the New World as well.\n\nIn 1796, the first American cookbook was published.\n\nThere was a general disdain for French cookery, even with the French Huguenots in South Carolina and French-Canadians. One of the cookbooks that proliferated in the colonies was \"The Art of Cookery Made Plain and Easy\" (1747) by Hannah Glasse, who referred to \"the blind folly of this age that would rather be imposed on by a French booby, than give encouragement to a good English cook!\" Of the French recipes given in the text, she speaks out flagrantly against the dishes as she \"… think[s] it an odd jumble of trash.\" Reinforcing the anti-French sentiment was the French and Indian War from 1754 to 1764. This created a large anxiety against the French, which influenced the English to force many of the French to move, as in the expulsion of the Acadians from Nova Scotia to Louisiana. The Acadians left a French influence in the diet of those settled in Louisiana, and among the Acadian Francophones who settled eastern Maine and parts of what is now northern Vermont at the same time they colonized New Brunswick.\n\nThe American colonial diet varied depending on the settled region in which someone lived. Local cuisine patterns had established by the mid-18th century. The New England colonies were extremely similar in their dietary habits to those that many of them had brought from England. A striking difference for the colonists in New England compared to other regions was seasonality. While in the southern colonies, they could farm almost year-round, in the northern colonies, the growing seasons were very restricted. In addition, colonists' close proximity to the ocean gave them a bounty of fresh fish to add to their diet, especially in the northern colonies.\n\nWheat, however, the grain used to bake bread back in England was almost impossible to grow, and imports of wheat were far from cost productive. Substitutes in cases such as this included cornmeal. The Johnnycake was a poor substitute to some for wheaten bread, but acceptance by both the northern and southern colonies seems evident.\n\nAs many of the New Englanders were originally from England, game hunting was useful when they immigrated to the New World. Many of the northern colonists depended upon their ability to hunt, or upon others from whom they could purchase game. Hunting was the preferred method of protein consumption (as opposed to animal husbandry, which required much more work to defend the kept animals against Native Americans or the French).\n\nCommonly hunted game included deer, bear, buffalo, and wild turkey. The larger muscles of the animals were roasted and served with currant sauce, while the other smaller portions went into soups, stews, sausages, pies, and pastries. In addition to game, colonists' protein intake was supplemented by mutton. The Spanish in Florida originally introduced sheep to the New World, but this development never quite reached the North, and there they were introduced by the Dutch and English. The keeping of sheep was a result of the English non-practice of animal husbandry. The animals provided wool when young and mutton upon maturity after wool production was no longer desirable. The forage-based diet for sheep that prevailed in the Colonies produced a characteristically strong, gamy flavor and a tougher consistency, which required aging and slow cooking to tenderize.\n\nA number of fats and oils made from animals served to cook much of the colonial foods. Many homes had a sack made of deerskin filled with bear oil for cooking, while solidified bear fat resembled shortening. Rendered pork fat made the most popular cooking medium, especially from the cooking of bacon. Pork fat was used more often in the southern colonies than the northern colonies as the Spanish introduced pigs earlier to the South. The colonists enjoyed butter in cooking as well, but it was rare prior to the American Revolution, as cattle were not yet plentiful.\n\nPrior to the Revolution, New Englanders consumed large quantities of rum and beer, as maritime trade provided them relatively easy access to the goods needed to produce these items. Rum was the distilled spirit of choice, as the main ingredient, molasses, was readily available from trade with the West Indies. Further into the interior, however, one would often find colonists consuming whiskey, as they did not have similar access to sugar cane. They did have ready access to corn and rye, which they used to produce their whiskey. However, until the Revolution, many considered whiskey to be a coarse alcohol unfit for human consumption, as many believed that it caused the poor to become raucous and unkempt drunkards. In addition to these alcohol-based products produced in America, imports were seen on merchant shelves, including wine and brandy.\n\nIn comparison to the northern colonies, the Southern Colonies were quite diverse in their agricultural diet and did not have a central region of culture. The uplands and the lowlands made up the two main parts of the southern colonies. The slaves and poor whites of the south often ate a similar diet, which consisted of many of the indigenous New World crops. Salted or smoked pork often supplement the vegetable diet. Rural poor often ate squirrel, opossum, rabbit and other woodland animals. Those on the \"rice coast\" often ate ample amounts of rice, while the grain for the rest of the southern poor and slaves was cornmeal used in breads and porridges. Wheat was not an option for most of those who lived in the southern colonies.\n\nThe diet of the uplands often included cabbage, string beans, and white potatoes, while most avoided sweet potatoes and peanuts at the time. Those who could grow or afford wheat often had biscuits as part of their breakfast, along with healthy portions of pork. Salted pork was a staple of any meal, as it was used in the preparations of vegetables for flavor, in addition to being eaten directly as a protein.\n\nThe lowlands, which included much of the French regions of Louisiana and the surrounding area, included a varied diet heavily influenced by the French, Spanish, Acadians, Germans, Native Americans, Africans and Caribbeans. Rice played a large part of the diet in Louisiana. In addition, unlike the uplands, the lowlands subsistence of protein came mostly from coastal seafood and game meats. Much of the diet involved the use of peppers, as it still does to this day. Although the English had an inherent disdain for French foodways, as well as many of the native foodstuff of the colonies, the French had no such disdain for the indigenous foodstuffs, but rather a vast appreciation for the native ingredients and dishes.\n\nDuring the 18th and 19th centuries, Americans developed many new foods. Some, such as Rocky Mountain oysters, stayed regional; some spread throughout the nation but with little international appeal, such as peanut butter (a core ingredient of the famous peanut butter and jelly sandwich); and some spread throughout the world, such as popcorn, Coca-Cola and its competitors, fried chicken, cornbread, unleavened muffins such as the poppyseed muffin, and brownies.\n\nDuring the Progressive Era (1890s–1920s) food production and presentation became more industrialized. Major railroads featured upscale cuisine in their dining cars. Restaurant chains emerged with standardized decor and menus, most famously the Fred Harvey restaurants along the route of the Sante Fe Railroad in the Southwest.\n\nAt the universities, nutritionists and home economists taught a new scientific approach to food. During World War I the Progressives' moral advice about food conservation was emphasized in large-scale state and federal programs designed to educate housewives. Large-scale foreign aid during and after the war brought American standards to Europe.\n\nNewspapers and magazines ran recipe columns, aided by research from corporate kitchens, which were major food manufacturers like General Mills, Campbell's, and Kraft Foods. One characteristic of American cooking is the fusion of multiple ethnic or regional approaches into completely new cooking styles. For example, spaghetti is Italian, while hot dogs are German; a popular meal, especially among young children, is spaghetti containing slices of hot dogs. Since the 1960s Asian cooking has played a particularly large role in American fusion cuisine.\nNew York City is home to a diverse and cosmopolitan demographic, and since the nineteenth century, the city's world class chefs created complicated dishes with rich ingredients like Lobster Newberg, waldorf salad, vichyssoise, eggs benedict, and the New York strip steak out of a need to entertain and impress consumers in expensive bygone restaurants like Delmonico's and still standing establishments like the Waldorf-Astoria Hotel.\n\nSome dishes that are typically considered American have their origins in other countries. American cooks and chefs have substantially altered these dishes over the years, to the degree that the dishes now enjoyed around the world are considered to be American. Hot dogs and hamburgers are both based on traditional German dishes, but in their modern popular form they can be reasonably considered American dishes.\n\nPizza is based on the traditional Italian dish, brought by Italian immigrants to the United States, but varies highly in style based on the region of development since its arrival. For example, \"Chicago\" style has focus on a thicker, taller crust, whereas a \"New York Slice\" is known to have a much thinner crust which can be folded. These different types of pizza can be advertised throughout the country and are generally recognizable and well-known, with some restaurants going so far as to import New York City tap water from a thousand or more miles away to recreate the signature style in other regions.\nMany companies in the American food industry developed new products requiring minimal preparation, such as frozen entrees. Many of these recipes have become very popular. For example, the General Mills \"Betty Crocker's Cookbook\", first published in 1950, was a popular book in American homes.\n\nA wave of celebrity chefs began with Julia Child and Graham Kerr in the 1970s, with many more following after the rise of cable channels like Food Network. By the beginning of the 21st century regional variations in consumption of meat began to reduce, as more meat was consumed overall. Saying they eat too much protein, the \"2015–2020 Dietary Guidelines for Americans\" asked men and teenage boys to increase their consumption of underconsumed foods such as vegetables.\n\nDuring the 1980s, upscale restaurants introduced a mixing of cuisines that contain Americanized styles of cooking with foreign elements commonly referred as New American cuisine. New American cuisine refers to a type of fusion cuisine which assimilates flavors from the melting pot of traditional American cooking techniques mixed with flavors from other cultures and sometimes molecular gastronomy components.\n\nGenerally speaking, in the present day 21st century, the modern cuisine of the United States is very much regional in nature. Excluding Alaska and Hawaii the terrain spans 3,000 miles West to East and more than a thousand North to South.\n\nNew England is a Northeastern region of the United States bordering the Maritime Provinces of Canada and portions of Quebec in the north. It includes the six states of Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, and Vermont, with its cultural capital Boston, founded in 1630. The Native American cuisine became part of the cookery style that the early colonists brought with them. Tribes like the Nipmuck, Wampanoag, and other Algonquian cultures were noted for slashing and burning areas to create meadows and bogs that would attract animals like moose and deer, but also encourage the growth of plants like black raspberries, blueberries, and cranberries. In the forest they would have collected nuts of species like the shagbark hickory, American hazel, and American chestnuts and fruits like wild grapes and black cherries. All of these eventually showed up in the kitchens of colonial New England women and many were sent back to England and other portions of Europe to be catalogued by scientists, collectors, and horticulturalists.\n\nThe style of New England cookery originated from its colonial roots, that is to say practical, frugal and willing to eat anything other than what they were used to from their British roots . Most of the initial colonists came from East Anglia in England, with other groups following them over the ages like francophone regions of Canada (this was especially true of Northern New England, where there are still many speakers of a dialect of French), from Ireland, from Southern Italy, and most recently from Haiti, Brazil, the Dominican Republic, and Portugal. The oldest forms of the cuisine date to the early 17th century and in the case of Massachusetts, out of the entire country only the state of Virginia can claim recipes that are older. East Anglian cookery would have included recipes for dishes like suet puddings, wheaten breads, and a few shellfish delicacies, like winkles and would have been at the time of settlement simple Puritan fare quite in contrast to the fineries and excesses expected in London cavalier circles. Most of the cuisine started with one-pot cookery, which resulted in such dishes as succotash, chowder, baked beans, and others. Starches are fairly simple, and typically encompass just a handful of classics like potatoes and cornmeal, and a few native breads like Anadama bread, johnnycakes, bulkie rolls, Parker house rolls, popovers, and New England brown bread. This region is fairly conservative with its spices, but typical spices include nutmeg, ginger, cinnamon, cloves, and allspice, especially in desserts, and for savory foods, thyme, black pepper, sea salt, and sage. Typical condiments include maple syrup, grown from the native sugar maple, molasses, and the famous cranberry sauce.\n\nNew England is noted for having a heavy emphasis on seafood, a legacy inherited from coastal tribes like the Wampanoag and Narragansett, who equally used the rich fishing banks offshore for sustenance. Favorite fish include cod, salmon, winter flounder, haddock, striped bass, pollock, hake, bluefish, and, in southern New England, tautog. All of these are prepared numerous ways, such as frying cod for fish fingers, grilling bluefish over hot coals for summertime, smoking salmon or serving a whole poached one chilled for feasts with a dill sauce, or, on cold winter nights, serving haddock baked in casserole dish with a creamy sauce and crumbled breadcrumbs as a top so it forms a crust. Clam cakes, a savory fritter based on chopped clams, are a specialty of Rhode Island. Farther inland, brook trout, largemouth bass, and herring are sought after, especially in the rivers and icy finger lakes in upper New England.\n\nMeat is present though not as prominent, and typically is either stewed in dishes like Yankee pot roast and New England boiled dinner or braised, as in a picnic ham; these dishes suit the weather better as summers are humid and hot but winters are raw and cold, getting below 0 °C for most of the winter and only just above it by March. The roasting of whole turkeys began here as a centerpiece for large American banquets, and like all other East Coast tribes, the Native American tribes of New England prized wild turkeys as a source of sustenance and later Anglophone settlers were enamored of cooking them using methods they knew from Europe: often that meant trussing the bird and spinning it on a string or spit roasting. Today turkey meat is a key ingredient in soups, and also a favorite in several sandwiches like the Pilgrim (sandwich). For lunch, hot roast beef is sometimes chopped finely into small pieces and put on a roll with salami and American or provolone cheese to make a steak bomb. Bacon is often maple cured, and it is often the drippings from this bacon that are an ingredient in corn chowder. Veal consumption was prevalent in the North Atlantic States prior to World War II. A variety of linguiça is favored as a breakfast food, brought with Portuguese fisherman and Brazilian immigrants. In contrast with some parts of the United States, lamb (although less so mutton or goat) is a popular roasted or grilled meat across diverse groups in New England. Dairy farming and its resultant products figure strongly on the ingredient list, and homemade ice cream is a summertime staple of the region: it was a small seasonal roadside stand in Vermont that eventually became the world-famous Ben and Jerry's ice cream. Vermont in particular is famous for producing farmhouse style cheeses, especially a type of cheddar. The recipe goes all the way back to colonial times when English settlers brought the recipe with them from England and found the rocky landscape eminently suitable to making the cheese. Today Vermont has more artisanal cheese makers per capita than any other state, and diversity is such that interest in goat's milk cheeses has become prominent.\n\nCrustaceans and mollusks are also an essential ingredient in the regional cookery. Maine is noted for harvesting peekytoe crab and Jonah crab and making crab bisques, based on cream with 35% milkfat, and crabcakes out of them, and often they appear on the menu as far south as to be out of region in New York City, where they are sold to four star restaurants. Squid are heavily fished for and eaten as fried calamari, and often are an ingredient in Italian American cooking in this region. Whelks are eaten in salad, and most famous of all is the lobster, which is indigenous to the coastal waters of the region and are a feature of many dishes, baked, boiled, roasted, and steamed, or simply eaten as a sandwich, chilled with mayonnaise and chopped celery in Maine and Massachusetts, or slathered with melted butter on Long Island and in Connecticut.\n\nShellfish of all sorts are part of the diet, and shellfish of the coastal regions include little neck clams, sea scallops, blue mussels, oysters, soft shell clams and razor shell clams. Much of this shellfish contributes to New England tradition, the clambake. The clambake as known today is a colonial interpretation of an American Indian tradition. In summer, oysters and clams are dipped in batter and fried, often served in a basket with french fries, or commonly on a wheaten bun as a clam roll. Oysters are otherwise eaten chilled on a bed of crushed ice on the half shell with mignonette sauce, and are often branded on where they were harvested. Large quahogs are stuffed with breadcrumbs and seasoning and baked in their shells, and smaller ones often find their way into clam chowder. Other preparations include clams casino, clams on the half shell served stuffed with herbs like oregano and streaky bacon.\n\nThe fruits of the region include the \"Vitis labrusca\" grapes used in grape juice made by companies such as Welch's, along with jelly, Kosher wine by companies like Mogen David and Manischewitz along with other wineries that make higher quality wines. Apples from New England include the traditional varieties Baldwin, Lady, Mother, Pomme Grise, Porter, Roxbury Russet, Wright, Sops of Wine, Hightop Sweet, Peck's Pleasant, Titus Pippin, Westfield-Seek-No-Further, and Duchess of Oldenburg. Beach plums a small native species with fruits the size of a pinball, are sought after in summer to make into a jam. Cranberries are another fruit indigenous to the region, often collected in autumn in huge flooded bogs. Thereafter they are juiced so they can be drunk fresh for breakfast, or dried and incorporated into salads. Winter squashes like pumpkin and butternut squashes have been a staple for generations owing to their ability to keep for long periods over icy New England winters and being an excellent source of beta carotene; in summer, they are replaced with pattypan and zucchini, the latter brought to the region by immigrants from Southern Italy a century ago. Blueberries are a very common summertime treat owing to them being an important crop, and find their way into muffins, pies and pancakes. Typical favorite desserts are quite diverse, and encompass hasty pudding, blueberry pie, whoopie pies, Boston cream pie, pumpkin pie, Joe Frogger cookies, hand crafted ice cream, Hermit cookies, and most famous of all, the chocolate chip cookie, invented in Massachusetts in the 1930s.\n\nSouthern New England, particularly along the coast, shares many specialties with the Mid-Atlantic, including especially dishes from Jewish and Italian-American cuisine. Coastal Connecticut is known for distinctive kinds of pizza, locally called apizza, differing in texture (thin and slightly blackened) and toppings (such as clams) from pizza further south in the so-called pizza belt, which stretches from New Haven southward through New York, New Jersey, and into Maryland.\n\nThe mid-Atlantic states comprise the states of New York, New Jersey, Delaware, Pennsylvania, and Northern Maryland. The oldest major settlement in this area of the country is found in the most populous city in the nation, New York City, founded in 1653 by the Dutch. Today, it is a major cultural capital of the United States. The influences on cuisine in this region are extremely eclectic owing to the fact that it has been and continues to be a gateway for international culture as well as a gateway for new immigrants. Going back to colonial times, each new group has left their mark on homegrown cuisine and in turn the cities in this region disperse trends to the wider United States. In addition to importing and trading the finest specialty foods from all over the world, cities like New York and Philadelphia have had the past influence of Dutch, Italian, German, Irish, British, and Jewish cuisines, and that continues to this day. Baltimore has become the crossroads between North and South, a distinction it has held since the end of the Civil War.\n\nA global power city, New York City is internationally known for its extremely diverse and cosmopolitan dining scene and possesses the entire world spectrum of dining options within its city limits. Some of the most exclusive and prestigious restaurants and nightclubs in the world are headquartered in New York City and compete fiercely for good reviews in the Food and Dining section of The New York Times, online guides, and Zagat's, the last of which is widely considered the premier American dining guide, published yearly and headquartered in New York City.\nMany of the more complicated dishes with rich ingredients like Lobster Newberg, waldorf salad, vichyssoise, eggs benedict, and the New York strip steak were born out of a need to entertain and impress the well to do in expensive bygone restaurants like Delmonico's and still standing establishments like the Waldorf-Astoria Hotel, and today that tradition remains alive as some of the most expensive and exclusive restaurants in the country are found in this region. Modern commercial American cream cheese was developed in 1872, when William Lawrence, from Chester, New York, while looking for a way to recreate the soft, French cheese Neufchâtel, accidentally came up with a way of making an \"unripened cheese\" that is heavier and creamier; other dairymen came up with similar creations independently.\n\nSince the first reference to an alcoholic mixed drink called a cocktail comes from New York State in 1803, it is thus not a surprise that there have been many cocktails invented in New York and the surrounding environs. Even today New York City bars are noted for being highly influential in making national trends. Cosmopolitans, Long Island iced teas, Manhattans, Rob Roys, Tom Collins, Aviations, and Greyhounds were all invented in New York bars, and the gin martini was popularized in New York in speakeasies during the 1920s, as evidenced by its appearance in the works of New Yorker and American writer F. Scott Fitzgerald. Like its neighbor Philadelphia, many rare and unusual liquors and liqueurs often find their way into a mixologist's cupboard or restaurant wine list. New York State is the third most productive area in the country for wine grapes, just behind the more famous California and Washington. It has AVA's near the Finger Lakes, the Catskills, and Long Island, and in the Hudson Valley has the second most productive area in the country for growing apples, making it a center for hard cider production, just like New England. Pennsylvania has been growing rye since Germans began to emigrate to the area at the end of the 17th century and required a grain they knew from Germany. Therefore, overall it is not unusual to find New York grown Gewürtztraminer and Riesling, Pennsylvania rye whiskey, or marques of locally produced ciders like Original Sin on the same menu.\nSince their formative years, New York City, Philadelphia, and Baltimore have welcomed immigrants of every kind to their shores, and all three have been an important gateway through which new citizens to the general United States arrive. Traditionally natives have eaten cheek to jowl with newcomers for centuries as the newcomers would open new restaurants and small businesses and all the different groups would interact. Even in colonial days this region was a very diverse mosaic of peoples, as settlers from Switzerland, Wales, England, Ulster, Wallonia, Holland, Gelderland, the British Channel Islands, and Sweden sought their fortune in this region. This is very evident in many signature dishes and local foods, all of which have evolved to become American dishes in their own right. The original Dutch settlers of New York brought recipes they knew and understood from the Netherlands and their mark on local cuisine is still apparent today: in many quarters of New York their version of apple pie with a streusel top is still baked, while originating in the colony of New Amsterdam their predilection for waffles in time evolved into the American national recipe and forms part of a New York City brunch, and they also made coleslaw, originally a Dutch salad, but today accented with the later 18th century introduction of mayonnaise. The internationally famous American doughnut began its life originally as a New York pastry that arrived in the 18th century as the Dutch olykoek.\nCrab cakes were once a kind of English croquette, but over time as spices have been added they and the Maryland crab feast became two of Baltimore's signature dishes; fishing for the blue crab is a favorite summer pastime in the waters off Maryland, New Jersey, and Delaware where they may grace the table at summer picnics . Other mainstays of the region have been present since the early years of American history, like oysters from Cape May, the Chesapeake Bay, and Long Island, and lobster and tuna from the coastal waters found in New York and New Jersey, which are exported to the major cities as an expensive delicacy or a favorite locavore's quarry at the multitude of farmer's markets, very popular in this region. Philadelphia Pepper Pot, a tripe stew, was originally a British dish but today is a classic of home cooking in Pennsylvania alongside bookbinder soup, a type of turtle soup.\n\nIn the winter, New York City pushcarts sell roasted chestnuts, a delicacy dating back to English Christmas traditions, and it was in New York and Pennsylvania that the earliest Christmas cookies were introduced: Germans introduced crunchy molasses based gingerbread and sugar cookies in Pennsylvania, and the Dutch introduced cinnamon based cookies, all of which have become part of the traditional Christmas meal.Scrapple was originally a type of savory pudding that early Pennsylvania Germans made to preserve the offal of a pig slaughter. The Philadelphia soft pretzel was originally brought to Eastern Pennsylvania in the early 18th century, and later, 19th century immigrants sold them to the masses from pushcarts to make them the city's best-known bread product, having evolved into its own unique recipe.\nAfter the 1820s, new groups began to arrive and the character of the region began to change. There had been some Irish from Ulster prior to 1820, however largely they had been Protestants with somewhat different culture and (often) a different language than the explosion of emigrants that came to Castle Garden and Locust Point in Baltimore in their masses starting in the 1840s.\n\nThe Irish arrived in America in a rather woeful state, as Ireland at the time was often plagued by some of the worst poverty in Europe and often heavy disenfranchisement among the masses: many of them arrived barely alive having ridden coffin ships to the New World, sick with typhus and starvation. In addition, they were the first to face challenges other groups did not have: they were the first large wave of Catholics. They faced prejudice for their faith and the cities of Philadelphia, New York, and Baltimore were not always set up for their needs. For example, Catholic bishops in the U.S. mandated until the 1960s that all Catholics were forbidden from eating red meat on Fridays and during Lent, and attending Mass sometimes conflicted with work as produce and meat markets would be open on holy days; this was difficult for Irishmen supporting families since many worked as laborers. Unsurprisingly, many Irishmen also found their fortunes working as longshoremen, which would have given their families access to fish and shellfish whenever a fisherman made berth, which was frequent on the busy docks of Baltimore and New York. Though there had been some activity in Baltimore in founding a see earlier by the Carroltons, the Irish were the first major wave of Catholic worship in this region, and that meant bishops and cardinals sending away to Europe for wine. Part of the Catholic mass includes every parishioner taking a sip of wine from the chalice as part of the Eucharist. Taverns had existed prior to their emigration to America in the region, though the Irish brought their particular brand of pub culture and founded some of the first saloons and bars that served stout and red ale; they brought with them the knowledge of single malt style whiskey and sold it. The Irish were the first immigrant group to arrive in this region in massive millions, and these immigrants also founded some of the earliest saloons and bars in this region, of which McSorley's is an example.\nIt was also in this region that the Irish introduced something that today is a very important festival in American culture that involves a large amount of food, drink, and merry making: Halloween. In England and Wales, where prior immigrants had come from, the feast of All Hallows Eve had died out in the Reformation, dismissed as superstition and excess having nothing to do with the Bible and often replaced with the festival of Guy Fawkes Night. Other immigrant groups like the Germans preferred to celebrate October 31 as Reformation Day, and after the American Revolution all of the above were less and less eager to celebrate the legacy of an English festival when they had fought a very bloody war to leave the British Empire. The Catholicism of the Irish demanded attendance at church on November 1 and charity and deeds, not just faith, as a cornerstone of dogma, and many of their older traditions survived the Reformation and traveled with them. Naturally, they went door-to-door to collect victuals for masked parties as well as gave them out, like nuts to roast on the fire, whiskey, beer, or cider, and barmbracks; they also bobbed for apples and made dumb cakes. Later in the century they were joined by Scots going guising, children going door-to-door to ask for sweets and treats in costume. From the Mid Atlantic this trend spread to be nationwide and evolved into American children trick-or-treating on October 31 wearing costumes and their older counterparts having wild costume parties with lots of food and drink like caramel apples, candy apples, dirt cakes, punch, cocktails, cider (both alcoholic and non,) pumpkin pie, candy corn, chocolate turtles, peanut brittle, taffy, tipsy cake, and copious buckets full of candy; children carving jack-o-lanterns and eating squash derived foods derive from Halloween's heritage as a harvest festival and from Irish and Scottish traditions of carving turnips and eating root vegetables at this time of year. Their bobbing for apples has survived to the present day as a Halloween party classic game, as has a variation on the parlor game of trying to grab an apple hanging from the ceiling blindfolded: it has evolved into trying to catch a donut in one's teeth.\n\nImmigrants from Southern Europe, namely Sicily, Campania, Lazio, and Calabria, appeared between 1880 and 1960 in New York, New Jersey, Pennsylvania, and Eastern Maryland hoping to escape the extreme poverty and corruption endemic to Italy; typically none of them spoke English, but rather dialects of Italian and had a culture that was more closely tied to the village they were born in than the high culture only accessible to those who could afford it at this time; many could not read or write in any language. They were employed in manual labor or factory work but it is because of them that dishes like spaghetti with meatballs, New York–style pizza, calzones, and baked ziti exist, and Americans of today are very familiar with semolina based pasta noodles. Their native cuisine had less of an emphasis on meat, as evidenced by dishes they introduced like pasta e fagioli and minestrone, but the dishes they created in America often piled it on as a sign of wealth and newfound prosperity since for the first time even cheap cuts of it were affordable: the American recipe for lasagna is proof of this, as mostly it is derived from the Neapolitan version of the dish with large amounts of meat and cheese.\nNew York–style hot dogs came about with German speaking emigrants from Austria and Germany, particularly with the frankfurter sausage and the smaller wiener sausage. Today, the New York–style hot dog with sauerkraut, mustard, and the optional cucumber pickle relish is such a part of the local fabric, that it is one of the favorite comestibles of New York City. Hot dogs are a typical street food sold year round in all by the most inclement weather from thousands of pushcarts. As with all other stadiums in Major League Baseball they are an essential for New York Yankees and the New York Mets games though it is the local style of preparation that predominates without exception. Hot dogs are also the focus of a televised eating contest on the Fourth of July in Coney Island, at Nathan's Famous, one of the earliest hot dog stands opened in the United States in 1916.\n\nA summertime treat, Italian ice, began its life as a lemon flavored penny lick brought to Philadelphia by Italians; its Hispanic counterpart, piragua, is a common and evolving shaved ice treat brought to New York City by Puerto Ricans in the 1930s. Unlike the original dish which included flavors like tamarind, mango, coconut, piragua is evolving to include flavors like grape, a fruit not grown in Puerto Rico. Taylor ham, a meat delicacy of New Jersey, first appeared around the time of the Civil War and today is often served for breakfast with eggs and cheese on a kaiser roll, the bread upon which this is served was brought to the area by Austrians in the second half of the nineteenth century and is a very common roll for sandwiches at lunchtime, usually tipped with poppyseeds. This breakfast meat is generally known as pork roll in southern New Jersey and Philadelphia, and Taylor ham in northern New Jersey.\nOther dishes came about during the early 20th century and have much to do with delicatessen fare, set up largely by Jewish immigrants from Eastern Europe who came to America incredibly poor. Most often they were completely unable to partake in the outdoor food markets that the general population utilized as most of the food for sale was not kosher. The influence of European Jewry before their destruction in the Holocaust on modern mid Atlantic cooking remains extremely strong and reinforced by their many descendants in the region. American-style pickles were brought by Polish Jews, now a common addition to hamburgers and sandwiches, and Hungarian Jews brought a recipe for almond horns that now is a common regional cookie, diverting from the original recipe in dipping the ends in dark chocolate. New York–style cheesecake has copious amounts of cream and eggs because animal rennet is not kosher and thus could not be sold to a large number of the deli's clientele. New York inherited its bagels and bialys from Jews, as well as Challah bread. Pastrami first entered the country via Romanian Jews, and is a feature of many sandwiches, often eaten on marble rye, a bread that was born in the mid Atlantic. Whitefish salad, lox, and matzoh ball soup are now standard fare made to order at local diners and delicatessens, but started their life as foods that made up a strict dietary code.\n\nLike other groups before them, many of their dishes passed into the mainstream enough so that they became part of diner fare by the end of the 20th century, a type of restaurant that is now more numerous in this region than any other and formerly the subject matter of artist Edward Hopper. In the past this sort of establishment was the haven of the short order cook grilling or frying simple foods for the working man. Today typical service would include regional staples like beef on weck, manhattan clam chowder, the club sandwich, Buffalo wings, Philadelphia cheesesteak, the black and white cookie, shoofly pie, snapper soup, Smith Island cake, grape pie, milkshakes, and the egg cream, a vanilla or chocolate fountain drink with a frothy top and fizzy taste. As in Hopper's painting from 1942, many of these businesses are open 24 hours a day.\n\nMidwestern cuisine today covers everything from Kansas City-style barbecue to the Chicago-style hot dog, though many of its classics are very simple, hearty fare. Mostly this region was completely untouched by European and American settlers until after the American Civil War, and excepting Missouri and the heavily forested states near the Great Lakes was mainly populated by nomadic tribes like the Sioux, Osage, Arapaho, and Cheyenne. As with most other Native American tribes, these tribes consumed the Three Sisters of beans, maize, and squash, but also for thousands of years followed the herds of bison and hunted them first on foot and then, after the spread of mustangs from the Southwest due to the explorations of conquistadors, on horseback, typically using bow and arrow. There are buffalo jumps dating back nearly ten thousand years and several photographs and written accounts of trappers and homesteaders attesting to their dependence on the buffalo and to a lesser degree elk. After nearly wiping out the elk and bison to nothingness, this region has taken to raising bison alongside cattle for their meat and at an enormous profit, making them into burgers and steaks.\n\nThis region today comprises the states near the Great Lakes and also the Great Plains; much of it is prairie with a very flat terrain where the blue sky meets a neverending horizon. Winters are bitterly cold, windy, and wet. Often that means very harsh blizzards especially near the Great Lakes where Arctic winds blow off of Canada and where the ice on rivers and lakes freezes reliably thick enough for ice hockey to be a favorite pastime in the region and for ice fishing for pike and muskies to be ubiquitous in Minnesota, Wisconsin, and Michigan, where they often there after become part of the local tradition of the fish fry. Population density is extremely low away from the Great Lakes and very small towns dominated by enormous farms are the rule with larger cities being the exception. Detroit, Cleveland, St. Louis, Cincinnati, Indianapolis, Milwaukee, Minneapolis and her twin sister city across the river St. Paul dominate the landscape in wealth and size, owing to their ties with manufacturing, finance, transportation, and meatpacking. Smaller places like Omaha, Tulsa, and Kansas City make up local capitals, but the king of them all is Chicago, third largest city in the country.\n\nThe Upper Midwest includes the states of Illinois, Minnesota, Wisconsin, Ohio, Indiana, and Michigan. Non-Native American settlement began here earlier than anywhere else in the region, and thus the food available here ranges from the sublime to the bizarre. As with all of the Midwest, the primary meats here are beef and poultry, since the Midwest has been raising turkeys, chickens, and geese for over a hundred and fifty years; chickens have been so common for so long that the Midwest has several native breeds that are prized for both backyard farming and for farmer's markets, such as the Buckeye and Wyandotte; one, Billina, appears as a character in the second book of the Oz series by L. Frank Baum. Favorite fruits of the region include a few native plants inherited from Native American tribes like the pawpaw and the American persimmons are also highly favored. As with the American South, pawpaws are the region's largest native fruit, about the size of a mango, and are often found growing wild in the region come September, whereafter they are made into preserves and cakes and command quite a price at farmer's markets in Chicago. The American persimmon is often smaller than it is Japanese cousin, about the size of a small plum, but in the Midwest and portions of the East it is the main ingredient in a steamed pudding called persimmon pudding, topped with crème anglaise. Other crops inherited from the Native Americans include wild rice, which grows on the banks of lakes and is a local favorite for fancy meals and today often used in stuffing for Thanksgiving.\n\nTypical fruits of the region are cold weather crops. Once it was believed that the region had winters that were far too harsh for apple growing, but then a breeder in Minnesota came forth with the Wealthy apple and thence came forth the third most productive region for apple growing in the land, with local varieties comprising Wolf River, Enterprise, Melrose, Paula Red, Rome Beauty, Honeycrisp, and the world-famous Red Delicious. Cherries are important to Michigan and Wisconsin grows many cranberries, a legacy of early 19th century emigration of New England farmers. Crabapple jelly is a favorite condiment of the region.\n\nThe influence of German, Scandinavian, and Slavic peoples on the northern portion of the region is very strong; many of these emigrated to Wisconsin, Minnesota, Michigan, Ohio, and Illinois in the 19th century to take advantage of jobs in the meatpacking business as well as being homesteaders. Bratwurst is a very common sausage eaten at tailgate parties for the Green Bay Packers, Chicago Bears, or Detroit Lions football teams and is often served boiled in lager beer with sauerkraut, different than many of the recipes currently found in Germany. Polish sausage, in particular a locally invented type of kielbasa, is an essential for sporting events in Chicago: Chicago today has approximately 200,000 speakers of Polish and has had a population of that description for over a hundred years. When Poles came to Chicago and surrounding cities from the Old World, they brought with them long ropes of kielbasa, cabbage rolls, and pierogis. Poles that left Poland after the fall of the Berlin Wall and the descendants of earlier immigrants still make all of the above and such comestibles are common in local diners and delis as result. Today alongside the pierogi, the sausage is served on a long roll with mustard like a hot dog or as a Maxwell Street Polish, a sandwich that has caramelized onions as an essential ingredient. In Cleveland, the same sausage is served in the form of the Polish boy: this is a weird but tasty sandwich made of french fries, spicy barbecue sauce, and coleslaw; unlike cities in the East where the hot dog alone is traditional fans of the Cleveland Indians, Detroit Tigers, Chicago Cubs, and Milwaukee Brewers favor at least two or three different kinds of sausage sold in the little pushcarts outside the stadium; the hot dogs themselves tend to follow the Chicago style, which is loaded with mustard, and pickled vegetables. In Cincinnati, where the Cincinnati Reds play, the predilection for sausage has a competitor in Cincinnati chili, invented by Macedonian immigrants: this bizarre but tasty dish includes spaghetti as its base, chili with a Mediterranean-inspired spice mix, and cheddar cheese; the chili itself is often a topping for local hot dogs at games.\n\nIn the Midwest and especially Minnesota, the tradition of the church potluck has become a gathering in which local foods reign, and so it has been since the era of the frontier: pioneers would often need to pool resources together to have a celebration in the 19th century and that simply never changed. Nowhere is this more clear than with the ever famous hotdish: this is a type of casserole believed to have derived somehow from a Norwegian recipe, and it is usually topped with potatoes or tater tots. Next to the hotdish at the potlucks usually is where the glorified rice is found: this is a dish made of a kind of rice pudding mixed with crushed pineapple and maraschino cherries. Next to that is the booyah, a thick soup made of a number or combinations of meat, vegetables, and seasonings that is meant to simmer on the stove for up to two days. Lefse, traditionally a Scandinavian flatbread, has been handed down to descendants for over a hundred years and is common on the table. Behind that is the venison, a popular meat around the Great Lakes and often eaten in steaks, sandwiches, and crown roasts for special events. If in North or South Dakota, tiger meat, a dish similar to steak tartare, is present. Last on the table are the dessert bars and most especially the brownies: this confection was created originally in 1898 in Chicago and has gone on to become a global food.\nFurther South, barbecue has its own style in places in Kansas and St. Louis that are different to the South and the American West. Kansas City and St. Louis were and remain important hubs for the railroad that connected the plains with the Great Lakes and cities farther east, like Philadelphia. At the turn of the 19th century, the St. Louis area, Omaha, and Kansas City had huge stockyards, waystations for cattle and pigs on their way East to the cities of the coast and North to the Great Lakes. They all had large growing immigrant and migrant populations from Europe and the South respectively, so this region has developed unique styles of barbecue. St. Louis-style barbecue favors a heavy emphasis on a sticky sweet barbecue sauce. Its standbys include the pork steak, a cut taken from the shoulder of the pig, grilled, and then slowly stewed in a pan over charcoal, crispy snoots, a cut from the cheek and nose of the pig that is fried up like cracklin and eaten dipped in sauce, pork spare ribs, and a mix of either beer boiled bratwurst or grilled Italian derived sausage, flavored with fennel. Dessert is usually something like gooey butter cake, invented in the city in the 1930s. Kansas City-style barbecue uses several different kinds of meat, more than most styles of American barbecue- turkey, mutton, pork, and beef just to name a few- but is distinct from St. Louis in that the barbecue sauce adds molasses in with the typical tomato based recipe and typically has a more tart taste. Traditionally, Kansas City uses a low-and-slow method of smoking the meat in addition to just stewing it in the sauce. It also favors using hickory would for smoking and continual watering or layering of the sauce while cooking to form a glaze; with burnt ends this step is necessary to create the \"bark\" or charred outer layer of the brisket.\n\nWhen referring to the American South as a region, typically it should indicate Southern Maryland and the states that were once part of the Old Confederacy, with the dividing line between the East and West jackknifing about 100 miles west of Dallas, Texas, and mostly south of the old Mason-Dixon line. Cities found in this area include New Orleans, Miami, Atlanta, Washington, D.C., Memphis, Charleston, and Charlotte with Houston, Texas being the largest. These states are much more closely tied to each other and have been part of US territory for much longer than states much farther west than East Texas, and in the case of food, the influences and cooking styles are strictly separated as the terrain begins to change to prairie and desert from bayou and hardwood forest.\n\nThis section of the country has some of the oldest known foodways in the land, with some recipes almost 400 years old. Native American influences are still quite visible in the use of cornmeal as an essential staple and found in the Southern predilection for hunting wild game, in particular wild turkey, deer, woodcock, and various kinds of waterfowl; for example, coastal North Carolina is a place where hunters will seek tundra swan as a part of Christmas dinner; the original English and Scottish settlers would have rejoiced at this revelation owing to the fact that such was banned amongst the commoner class in what is now the United Kingdom, and naturally, their descendants have not forgotten. Native Americans also consumed turtles and catfish, specifically the snapping turtle and blue catfish, both important parts of the diet in the South today. Catfish are often caught with one's bare hands, gutted, breaded, and fried to make a Southern variation on English fish and chips and turtles are turned into stews and soups. Native American tribes of the region such as the Cherokee or Choctaw often cultivated or gathered local plants like pawpaw, maypop,spicebush,sassafras, and several sorts of squash and maize, and the aforementioned fruits still are cultivated as food in a Southerner's back garden. Maize is to this day found in dishes for breakfast, lunch and dinner in the form of grits, hoecakes, baked cornbread, and spoonbread, and nuts like the hickory, black walnut and pecan are commonly included in desserts and pastries as varied as mince pies, pecan pie, pecan rolls and honey buns (both are types of sticky bun), and quick breads, which were themselves invented in the South during the American Civil War.\n\nEuropean influence began soon after the settlement of Jamestown in 1607 and the earliest recipes emerging by the end of the 17th century. Specific influences from Europe were quite varied, and remain traditional and essential to the modern cookery overall. To the upper portion of the South, French Huguenots brought the concept of making rouxs to make sauces and soups, and later French settlers hunted for frogs in the swamps to make frog's legs. German speakers often settled in Appalachia on small farms or in the backcountry away from the coast, and invented an American breakfast delicacy that is now nationally beloved, apple butter, based on their recipe for apfelkraut, and later introduced red cabbage and rye. From the UK, an enormous amount of influence was bestowed upon the South, specifically foodways found in 17th and 18th century Ulster, the borderlands between England and Scotland, the Scottish Highlands, portions of Wales, the West Midlands and Black Country. Settlers bound for America fled the tumult of the Civil War and troubles in the plantation of Ireland and the Highland Clearances, and often ships manifests show their belongings nearly always included their wives' cookpots or bakestones and seed stock for plants like peaches, plums, and apples to grow orchards, which they planted in their hundreds: today, the biggest fruit crop of the region is the yellow peach, and noted apple varieties include Carolina Red June, Arkansas Black, Carter Blue, Magnum Bonum, and the infamous Golden Delicious. Each group brought foods and ideas from their region. Settlers from Ireland and Scotland were well known for creating peatreak and poitín, strong hard liquor based on fermenting potatoes or barley, but when they settled in the Appalachians and portions of the piedmont, they found sugar and maize were the only things available. In time they came up with a method where the brew is distilled once using a maize mash with added sugar and the charcoal of sugar maple, which created a whiskey with a high proof and a need for aging in barrels from local species of oak rather than English oak. In time this gave birth in time to American whiskey and Kentucky bourbon, and its infamous later cousins moonshine and Everclear.\n\nCloser to the coast, 18th century recipes for English trifle turned into tipsy cakes, replacing the sherry with whiskey and their recipe for pound cake, brought to the South around the same time, still works with American baking units: 1 pound sugar, one pound eggs, one pound butter, one pound flour. All of the above groups made the staple meat of the South pork, to this day the meat no Southerner can cook without.\n\nWith the exception of Kentucky, where mutton is a common choice, or Southern Maryland, where the custom is to take the carcass of an entire bull and roast it over coals for many hours, pork is the popular choice of Southern style barbecue and features in other preparations like sausages and sandwiches. Among both African-Americans and European-Americans in the antebellum period, corn and pork were a staple of the diet. For breakfast, it is a feature of country sausage, which in turn are an ingredient in the Southern breakfast dish of biscuits and gravy. Head cheese is a popular sliced meat of the region, taken from the pig's head, and pickled pig's feet have always been a cheap snack since they were introduced by Scotch-Irish settlers; today they are often served in bars.. Baby back ribs, hog maw, cracklins, and even whole pig roasts in specially constructed ovens are found in all parts of the South, as are its two best known condiments, barbecue sauce and hot sauce, with hundreds of local variations. In Virginia and the Appalachians, the mainstay for special occasions is the country ham, often served for Christmas and cured with salt or hickory, with the Virginia recipe often feeding the hogs peanuts for finishing and giving the ham a distinct taste, and red pepper flakes in ham cured in Tennessee. Accompanying many meals is the southern style fluffy biscuit, where the leavening agent is sodium bicarbonate and often includes buttermilk, and for breakfast they often accompany country ham, grits, and scrambled eggs.\n\nDesserts in the South tend to be quite rich and very much a legacy of entertaining to impress guests, since a Southern housewife was (and to a degree still is) expected to show her hospitality by laying out as impressive a banquet as she is able to manage. Desserts are vast and encompass Lane cake, sweet potato pie, peach cobbler, pecan pie, hummingbird cake, Jefferson Davis pie, peanut brittle, coconut cake, apple fritters, peanut cookies, Moravian spice cookies, chess pie, doberge cake, Lady Baltimore cake, bourbon balls, and caramel cake. American style sponge cakes tend to be the rule rather than the exception as is American style buttercream, a place where Southern baking intersects with the rest of the United States. Nuts like pecan and hickory tend to be revered as garnishes for these desserts, and make their way into local bakeries as fillings for chocolates.\n\nIn the parts of the South which face the Atlantic Ocean, French influences often were dictated by where French Huguenots settled, however it is Louisiana that got the lion's share of older French cooking methods from Poitou and Normandy via Nova Scotia, most of which are foodways that pre-date the codification of haute cuisine during the reign of Louis XIV and have more in common with rustic cuisines of the 17th and 18th century than anything ever found at the French court in Versailles or the bistros of 19th and 20th century Paris; this is especially true of Cajun cuisine. Louisiana is a state named for Louis XIV and to this day French is still a commonly spoken tongue in the areas west of New Orleans, in Acadiana. The Cajuns and their dialect have occupied Southern Louisiana since the 1700s owing to \"Le Grande Dérangement\", an event in which they were forcibly evicted by the English Crown from their lands in Canada and made to occupy more marginal lands on the bayou.\n\nCajun French is more closely related to dialects spoken in Northern Maine, New Brunswick, and to a lesser degree Haiti than anything spoken in modern France, and likewise their terminology, methodology, and culture concerning food is much more closely related to the styles of these former French colonies even today. Unlike other areas of the South, Cajuns were and still are largely Catholics and thus much of what they eat is seasonal; for example pork is an important component of the Cajun \"boucherie\" (a large community event where the hog is butchered, prepared with a fiery spice mix, and eaten snout to tail) but it is never consumed in the five weeks of Lent, when such would be forbidden. Cajun cuisine tends to focus on what is locally available, historically because Cajuns were often poor, illiterate, independent farmers and not plantation owners but today it is because such is deeply imbedded in local culture. Boudin is a type of sausage found only in this area of the country, and it is often by far more spicy than anything found in France or Belgium. Chaudin is unique to the area, and the method of cooking is comparable to the Scottish dish haggis: the stuffing includes onions, rice, bell peppers, spices, and pork sewn up in the stomach of a pig, and served in slices piping hot. Crayfish are a staple of the Cajun grandmother's cookpot, as they are abundant in the bayous of Southern Louisiana and a main source of livelihood, as are blue crabs, shrimp, corn on the cob, and red potatoes, since these are the basic ingredients of the Louisiana crawfish boil.\nSince the end of the Civil War, New Orleans has had a thriving fine dining scene that predates the much younger 20th century metropoli of Atlanta and Miami. It was here that cocktails like the sazerac and hurricane were invented as well as the liqueur Southern Comfort. New Orleans has been the capital of Creole culture since before Louisiana was a state; this culture is that of the colonial French and Spanish that evolved in the city of New Orleans, which was and still is quite distinct from the rural culture of Cajuns and dovetails with what would have been eaten in antebellum Louisiana plantation culture long ago. Cooking to impress and show one's wealth was a staple of Creole culture, which often mixed French, Spanish, Italian, German, African, Caribbean and Native American cooking methods, producing rich dishes like oysters bienville, pompano en papillote, and even the muffaletta sandwich. However, Louisiana Creole cuisine tends to diverge from the original ideas brought to the region in ingredients: profiteroles, for example, use a near identical choux pastry to that which is found in modern Paris but often use vanilla or chocolate ice cream rather than custard as the filling, pralines nearly always use pecan and not almonds, and bananas foster came about when New Orleans was a key port for the import of bananas from the Caribbean Sea. Gumbos tend to be thickened with okra, or the leaves of the sassafrass tree. Andouille is often used, but not the andouille currently known in France, since French andouille uses tripe whereas Louisiana andouille is made from a Boston butt, usually inflected with pepper flakes, and smoked for hours over pecan wood. Other ingredients that are native to Louisiana and not found in the cuisine of modern France would include rice, which has been a staple of both Creole and Cajun cooking for generations, and sugarcane, which has been grown in Louisiana since the early 1800s.\nGround cayenne pepper is a key spice of the region, as is the meat of the American alligator, something settlers learned from the Choctaws and Houma. The maypop plant has been a favorite of Southerners for 350 years; it gives its name to the Ocoee River in Tennessee, a legacy of the Cherokees, and in Southern Louisiana it is known as liane de grenade, indicating its consumption by Cajuns. It is a close relative of the commercial passionfruit, similar in size, and is a common plant growing in gardens all over the South as a source of fresh summertime fruit.\n\nAfrican influences came with slaves from Ghana, Benin, Mali, Ivory Coast, Angola, Sierra Leone, Nigeria, and other portions of West Africa, and the mark they and their descendants have made on Southern food is extremely strong today and an essential addition to the Southern table. Crops like okra, sorghum, sesame seeds, eggplant, chili peppers, and many different kinds of melons were brought with them from West Africa along with the incredibly important introduction of rice to the Carolinas and later to Texas and Louisiana, whence it became a staple grain of the region and still remains a staple today, found in dishes like Hoppin John, purloo, and Charleston red rice. Other crops, like sugar cane, kidney beans, and certain spices would have been familiar to slaves through contact with British colonies in the Caribbean; Southern plantation owners could and did buy slaves from slave ports and seasoning camps in Havana, San Juan, Port au Prince, Kingston, Bridgetown and Willemstad. Like the poorer indentured servants that came to the South, slaves often got the leftovers of what was slaughtered for the consumption of the master of the plantation and so many recipes had to be adapted for offal, like pig's ears and fatbacks though other methods encouraged low and slow methods of cooking to tenderize the tougher cuts of meat, like braising, smoking, and pit roasting, the last of which was a method known to West Africans in the preparation of roasting goat. African cooks, mostly the women, knew that braising ham hocks flavors collard greens, and this dish remains unchanged after almost 400 years of cooking it and also is often accompanied by black eyed peas, an African crop they would have known before slavery. Other recipes certainly brought by Africans involve peanuts, as evidenced by the local nickname for the legume in Southern dialects of American English: \"goober\", taken from the Kongo word for peanut, \"nguba\". The 300-year-old recipe for peanut soup is a classic of Southern cuisine that has never stopped being eaten, handed down to the descendants of Virginia slaves and adapted to be creamier and less spicy than the original African dish. Boiled peanuts are a common food served at bars as a snack and have been eaten in the South for as long as there have been pots to boil them.\n\nCertain portions of the South often have their own distinct subtypes of cuisine owing to local history and landscape: though Cajun cuisine is more famous, Floridian cuisine, for example, has a distinct way of cooking that includes ingredients her other Southern sisters do not use, especially points south of Tampa and Orlando. The Spanish Crown had control of the state until the early 19th century and used the southern tip as an outpost to guard the Spanish Main beginning in the 1500s, but Florida kept and still maintains ties with the Caribbean Sea, including the Bahamas Haiti, Cuba, Puerto Rico, the Dominican Republic, and Jamaica. South of Tampa, there are and have been for a long time many speakers of Caribbean Spanish, Haitian French, Jamaican Patois, and Haitian Creole and each Caribbean culture has a strong hold on cooking methods and spices in Florida. In turn, each mixes and matches with the foodways of the Seminole tribe and Anglophone settlers. Thus, for almost 200 years, Floridian cooking has had a more tropical flavor than any other Southern state. Allspice, a spice originally from Jamaica, is an ingredient found in spice mixes in summer barbecues along with ginger, garlic, scotch bonnet peppers, sea salt, and nutmeg; in Floridian cooking this is often a variant of Jamaican jerk spice. Coconuts are grown in the areas surrounding Miami and are shipped in daily through its port for consumption of the milk, meat, and water of the coconut. Bananas are not just the yellow Cavendish variety found in supermarkets across America: in Florida they are available as bananitos, colorados, plátanos, and maduros. The first of these is a tiny miniature banana only about 4-5 inches (10–13 cm) in length and it is sweet. The second has a red peel and an apple like after taste, and the third and fourth are used as a starch on nearly every Caribbean island as a side dish, baked or fried: all of the above are a staple of Florida outdoor markets when in season and all have been grown in the Caribbean for almost 400 years. Mangoes are grown as a backyard plant in Southern Florida and otherwise are a favorite treat coming in many different shapes in sizes from \"Nam Doc Mai\", brought to Florida after the Vietnam War, to \"Madame Francis\", a mango from Haiti. Sweetsop and soursop are popular around Miami, but nearly unheard of in other areas of the South.\n\nCitrus is a major crop of Florida, and features at every breakfast table and every market with the height of the season near the first week of January. Hamlin oranges are the main cultivar planted, and from this crop the rest of the United States and to a lesser extent Europe gets orange juice. Other plantings would include grapefruits, tangerines, clementine oranges, limes, and even a few more rare ones, like cara cara navels, tangelos, and the Jamaican Ugli fruit. Tomatoes, bell peppers, habañero peppers, and figs, especially taken from the Florida strangler fig, complete the produce menu. Blue crab, conch, Florida stone crab, red drum, dorado, and marlins tend to be local favorite ingredients. Dairy is available in this region, but it is less emphasized due to the year round warmth. Traditional key lime pie, a dessert from the islands off the coast of Miami, is made with condensed milk to form the custard with the eye wateringly tart limes native to the Florida Keys in part because milk would spoil in an age before refrigeration. Pork in this region tends to be roasted in methods similar to those found in Puerto Rico and Cuba, owing to mass emigration from those countries in the 20th century, especially in the counties surrounding Miami. Orange blossom honey is a specialty of the state, and is widely available in farmer's markets.\n\nPtarmigan, grouse, crow blackbirds, dove, ducks and other game fowl are consumed in the United States. In the American state of Arkansas, beaver tail stew is consumed in Cotton town. Squirrel, raccoon, possum, bear, muskrat, chipmunk, skunk, groundhog, pheasant, armadillo and rabbit are also consumed in the United States.\n\nCooking in the American West gets its influence from Native American and Hispanophone cultures, as well as later settlers that came in the 19th century: Texas, for example, has some influence from Germany in its choice of barbecue by using sausages. Another instance can be found in the Northwestern region, which encompasses Oregon, Washington, and Northern California. All of the aforementioned rely on local seafood and a few classics of their own. In New Mexico, Colorado, Nevada, Arizona, Utah, West Texas, and Southern California, Mexican flavors and influences are extremely common, especially from the Mexican states of Chihuahua, Baja California, and Sonora.\n\nThe Pacific Northwest as a region generally includes Alaska and the state of Washington near the Canada–US border and terminates near Sacramento, California. Here, the terrain is mostly temperate rainforest on the Coast mixed with pine forest as one approaches the Canada–US border inland. One of the core favorite foodstuffs is Pacific salmon, native to many of the larger rivers of the area and often smoked or grilled on cedar planks. In Alaska, wild game like ptarmigan and moose meat feature extensively since much of the state is wilderness. Fresh fish like steelhead trout, Pacific cod, Pacific halibut, and pollock are fished for extensively and feature on the menu of many restaurants, as do a plethora of fresh berries and vegetables, like Cameo apples from Washington state, the headquarters of the U.S. apple industry, cherries from Oregon, blackberries, and marionberries, a feature of many pies. Hazelnuts are grown extensively in this region and are a feature of baking, such as in chocolate hazelnut pie, an Oregon favorite, and Almond Roca is a local candy.\n\nThis region is also heavily dominated by some notable wineries producing a high quality product, with Sonoma found within this region as well as the newer vinicultural juggernauts of Washington State, like the Yakima Valley. The first plantings of vineyards in the United States began many miles to the South on the Pacific coast in what is now San Diego, because the Franciscan friars that settled Alta California required wines they could use for their table and for the Eucharist, and the variety they planted, the mission grape, is still available on a limited basis. Today, French, Spanish, and Italian varietals are sold by the hogshead, and much of the area directly north of San Francisco is under vine, in particular Pinot noir, Garnacha, and Ruffina and several Tuscan varietals.\n\nLike its counterpart on the opposite coast to the East, there is a grand variety of shellfish in this region. Geoducks are a native species of giant clam that have incredibly long necks, and they are eaten by the bucket full as well as shipped to Asia for millions of dollars as they are believed to be an aphrodisiac. Gaper clams are a favorite food, often grilled or steamed in a sauce, as is the native California abalone, which although protected as a food source is a traditional foodway predating settlement by whites and today features heavily in the cooking of fine restaurants as well as in home cooking, in mirin-flavored soups (the influence of Japanese cooking is strong in the region) noodle dishes and on the barbecue. Olympia oysters are served on the half shell as well as the Kumamoto oyster, introduced by Japanese immigrants and a staple at dinner as an appetizer. California mussels are a delicacy of the region, and have been a feature of the cooking for generations: there is evidence that Native American tribes consumed them up and down the California coast for centuries in their masses.\n\nCrabs are a delicacy, and included in this are Alaskan king crab, red crab, yellow crab, and the world-famous Dungeness crab. Californian and Oregonian sportsmen pursue the last three extensively using hoop nets, and prepare them in a multitude of ways. Alaska king crab, able to get up to 10 kg, is often served steamed for a whole table with lemon butter sauce or put in chunks of salad with avocado, and native crabs are the base of dishes like the California roll, cioppino, a tomato based fisherman's stew, and Crab Louie, another kind of salad native to San Francisco. Favorite grains are mainly wheat, and the region is famous for sourdough bread. Cheeses of the region include Humboldt Fog, Cougar Gold and Teleme.\n\nThe states of the Four Corners (Arizona, New Mexico, Colorado, and Utah) plus Nevada, Southern California, and West Texas make up a large chunk of the United States and there is a distinct Hispanic accent to the cookery here, with each having a cultural capital in Salt Lake City, Phoenix, Santa Fe, Las Vegas, Denver, and Los Angeles. This region was part of the Spanish Empire for more than two centuries before California's statehood in the 1830s, and today is the home of a large population of immigrants from Mexico and Central America; Spanish is a commonly spoken secondary language here and the state of New Mexico has its own distinct dialect. With the exception of Southern California, the signature meat is beef, since this is one of the two regions in which cowboys lived and modern cattle ranchers still eke out their living today. High quality beefstock is a feature that has been present in the region for more than 200 years and the many cuts of beef are unique to the United States. These cuts of meat are different from the related Mexican cuisine over the border in that certain kind of offal, like \"lengua\" (tongue) \"cabeza\" (head) and \"tripas\" (tripe) are considered less desirable and are thus less emphasized. Typical cuts would include the ribs, brisket, sirloin, flank steak, skirt steak, and t-bone.\nHistorically, Spanish settlers that came to the region found it completely unsuitable to the mining operations that much older settlements in Mexico had to offer as the technology of the age was not yet advanced enough to get at the silver that would later be found in the region. They had no knowledge of the gold to be discovered in California, something nobody would find until 1848, and knew even less about the silver in Nevada, something nobody would find until after the Civil War. Instead, in order to make the pueblos prosper, they adapted the old rancho system of places like Andalusia in Spain and brought the earliest beefstock, among these were breeds that would go feral and become the Texas longhorn, and Churro sheep, still used as breeding stock because they are easy to keep and well adapted to the extremely arid and hot climate, where temperatures easily exceed 38 °C. Later, cowboys learned from their management practices, many of which still stand today, like the practical management of stock on horseback using the Western saddle.\nLikewise, settlers learned the cooking methods of those who came before and local tribes as well: for example, portions of Arizona and New Mexico still use the aforementioned beehive shaped clay contraption called an \"horno\", an outdoor wood fired oven both Native American tribes like the Navajo and Spaniards used for roasting meat, maize, and baking bread. Other meats that see frequent use in this region are elk meat, a favorite in crown roasts and burgers, and nearer the Mexican border rattlesnake, often skinned and stewed. The taste for alcohol in this region tends toward light and clean flavors found in tequila, a staple of this region since the days of the Wild West and a staple in the bartender's arsenal for cocktails, especially in Las Vegas. In Utah, a state heavily populated by Mormons, alcohol is frowned upon by the Church of Jesus Christ of Latter Day Saints but still available in area bars in Salt Lake City, mainly consumed by the populations of Catholics and other Protestant denominations living there.\n\nIntroduction of agriculture was limited prior to the 20th century and the development of better irrigation techniques, but included the addition of peaches, a crop still celebrated by Native American tribes like the Havasupai, and oranges; today in Arizona, Texas, and New Mexico the favored orange today is the Moro blood orange, which often finds its way into the local cuisine, like cakes and marmalade. Pine nuts are a particular regional specialty and feature often in fine dining and cookies; indeed in Nevada the Native American tribes that live there are by treaty given rights to exclusive harvest. From Native Americans, Westerners learned the practice of eating cactus fruit from the myriad species of opuntia that occupy the Chihuahuan, Sonoran, and Mojave desert lands. In California, Spanish missionaries brought with them the mission fig: today this fruit is a delicacy.\nCuisine in this region tends to have certain key ingredients: tomatoes, onions, black beans, pinto beans, rice, bell peppers, chile peppers, and cheese, in particular Monterey Jack, invented in Southern California in the 19th century and itself often further altered into pepperjack where spicy jalapeño peppers are incorporated into the cheese to create a smoky taste. Chili peppers play an important role in the cuisine, with a few native to the region (Anaheim pepper, Hatch pepper); these still grown by Spanish speakers in New Mexico. In New Mexico, chile is eaten on a variety of foods, such as the green chile cheeseburger, made popular by fast food chains such as Blake's Lotaburger. Indeed, even national fast food chains operating in the state, such as McDonald's, offer locally grown chile on many of their menu items. In the 20th century a few more recent additions have arrived like the poblano pepper, rocoto pepper, ghost pepper, thai chili pepper, and Korean pepper, the last three especially when discussing Southern California and its large population from East and South Asia. Cornbread is consumed in this area, however the recipe differs from ones in the East in that the batter is cooked in a cast iron skillet. Outdoor cooking is popular and still utilizes an old method settlers brought from the East with them, in which a cast iron dutch oven is covered with the coals of the fire and stacked or hung from a tripod: this is different from the earthenware pots of Mexico. Tortillas are still made the traditional way in this area and form an important component of the spicy breakfast burrito, which contains ham, eggs, and salsa or pico de gallo. They also comprise the regular burrito, which contains any combination of marinated meats, vegetables, and piquant chilis; The smothered burrito, often both containing and topped with New Mexico chile sauces; the quesadilla, a much loved grilled dish where cheese and other ingredients are stuffed between two tortillas and served by the slice, and the steak fajita, where sliced skirt steak sizzles in a skillet with caramelized onions.\nUnlike Mexico, tortillas of this region also may incorporate vegetable matter like spinach into the flatbread dough to make wraps, which were invented in Southern California. Food here tends to use pungent spices and condiments, typically chili verde sauce, various kinds of hot sauce, sriracha sauce, chili powder, cayenne pepper, white pepper, cumin, paprika, onion powder, thyme and black pepper. Nowhere is this fiery mix of spice more evident than in the dishes chili con carne, a meaty stew, and cowboy beans, both of which are a feature of regional cookoffs. Southern California has several additions like five spice powder, rosemary, curry powder, kimchi, and lemongrass, with many of these brought by recent immigration to the region and often a feature of Southern California's fusion cuisine, popular in fine dining.\n\nIn Texas, the local barbecue is often entirely made up of beef brisket or large rib racks, where the meat is seasoned with a spice rub and cooked over coals of mesquite, and in other portions of the state they smoke their meat and peppery sausages over high heat using pecan, apple, and oak and served it with a side of pickled vegetables, a legacy of German and Czech settlers of the late 1800s. California is home to Santa Maria-style barbecue, where the spices involved generally are black pepper, paprika, and garlic salt, and grilled over the coals of coast live oak.\nNative American additions may include Navajo frybread and corn on the cob, often roasted on the grill in its husk. A typical accompaniment or appetizer of all these states is the tortilla chip, which sometimes includes cornmeal from cultivars of corn that are blue or red in addition to the standard yellow of sweetcorn, and is served with salsa of varying hotness. Tortilla chips also are an ingredient in the Tex Mex dish nachos, where these chips are loaded with any combination of ground beef, melted Monterey Jack, cheddar, or Colby cheese, guacamole, sour cream, and salsa, and Texas usually prefers a version of potato salad as a side dish. For alcohol, a key ingredient is tequila: this spirit has been made on both sides of the US-Mexican border for generations, and in modern cuisine it is a must have in a bartender's arsenal as well as an addition to dishes for sauteeing.\n\nSouthern California is located more towards the coast and has had more contact with immigration from the West Pacific and Baja California, in addition to having the international city of Los Angeles as its capital. Here, the prime mode of transportation is by car. Drive through fast food was invented in this area, but so was the concept of the gourmet burger movement, giving birth to chains like In and Out Burger, with many variations of burgers including chili, multiple patties, avocado, special sauces, and angus or wagyu beef; common accompaniments include thick milkshakes in various flavors like mint, chocolate, peanut butter, vanilla, strawberry, and mango. Smoothies are a common breakfast item made with fresh fruit juice, yogurt, and crushed ice. Agua fresca, a drink originated by Mexican immigrants, is a common hot weather beverage sold in many supermarkets and at mom and pop stands, available in citrus, watermelon, and strawberry flavors; the California version usually served chilled without grain in it.\nThe weather in Southern California is such that the temperature rarely drops below 12 °C in winter, thus, sun loving crops like pistachios, kiwifruit, avocadoes, strawberries, and tomatoes are staple crops of the region, the last often dried in the sun and a feature of salads and sandwiches. Olive oil is a staple cooking oil of the region and has been since the days of Junípero Serra; today the mission olive is a common tree growing in a Southern Californian's back garden; as a crop olives are increasingly a signature of the region along with Valencia oranges and Meyer lemons. Soybeans, bok choy, Japanese persimmon, thai basil, Napa cabbage, nori, mandarin oranges, water chestnuts, and mung beans are other crops brought to the region from East Asia and are common additions to salads as the emphasis on fresh produce in both Southern and Northern California is strong. Other vegetables and herbs have a distinct Mediterranean flavor which would include oregano, basil, summer squash, eggplant, and broccoli, with all of the above extensively available at farmers' markets all around Southern California. Naturally, salads native to Southern California tend to be hearty affairs, like Cobb salad and Chinese chicken salad, and dressings like green goddess and ranch are a staple. California-style pizza tends to have disparate ingredients with an emphasis on vegetables, with any combination of chili oil, prawns, eggs, chicken, shiitake mushrooms, olives, bell pepper, goat cheese, and feta cheese. Peanut noodles tend to include a sweet dressing with lo mein noodles and chopped peanuts.\n\nFresh fish and shellfish in Southern California tends to be expensive in restaurants, but by no means out of reach of the masses. Every year since the end of WWII, the Pismo clam festival has taken place where the local population takes a large species of clam and bakes, stuffs, and roasts it to their heart's content as it is a regional delicacy. Fishing for pacific species of octopus and the Humboldt squid are common, and both are a feature of East Asian and other L.A. fish markets.Lingcod is a coveted regional fish that is often caught in the autumn off the coast of San Diego and in the Channel Islands and often served baked. California sheephead are often grilled and are much sought after by spear fishermen and the immigrant Chinese population, in which case it is basket steamed. Most revered of all in recent years is the California spiny lobster, a beast that can grow to be 20 kg, and is a delicacy that now rivals the fishery for Dungeness crab in its importance.\n\nHawaii is often considered to be one of the most culturally diverse U.S. states, as well as being the only state with an Asian majority population and being one of the few places where United States territory extends into the tropics. As a result, Hawaiian cuisine borrows elements of a variety of cuisines, particularly those of Asian and Pacific-rim cultures, as well as traditional native Hawaiian and a few additions from the American mainland. American influence of the last 150 years has brought cattle, goats, and sheep to the islands, introducing cheese, butter, and yogurt products, as well as crops like red cabbage. Just to name a few, major Asian and Polynesian influences on modern Hawaiian cuisine are from Japan, Korea, Vietnam, China (especially near the Pearl River delta,) Samoa, and the Philippines. From Japan, the concept of serving raw fish as a meal with rice was introduced, as was soft tofu, setting the stage for the popular dish called poke. From Korea, immigrants to Hawaii brought a love of spicy garlic marinades for meat and kimchi. From China, their version of char siu baau became modern manapua, a type of steamed pork bun with a spicy filling. Filipinos brought vinegar, bagoong, and lumpia, and during the 20th century immigrants from American Samoa brought the open pit fire umu and the Vietnamese introduced lemongrass and fish sauce. Each East Asian culture brought several different kinds of noodles, including udon, ramen, mei fun, and pho, and today these are common lunchtime meals.\n\nMuch of this cuisine mixes and melts into traditions like the infamous lu'au, whose traditional elaborate fare was once the prerogative of kings and queens but today is the subject of parties for both tourists and also private parties for the \"‘ohana\" (meaning family and close friends.) Traditionally, women and men ate separately under the Hawaiian \"kapu\" system, a system of religious beliefs that honored the Hawaiian gods similar to the Maori tapu system, though in this case had some specific prohibitions towards females eating things like coconut, pork, turtle meat, and bananas as these were considered parts of the male gods. Punishment for violation could be severe, as a woman might endanger a man's mana, or soul, by eating with him or otherwise by eating the forbidden food because doing so dishonored all the male gods. As the system broke down after 1810, introductions of foods from laborers on plantations began to be included at feasts and much cross pollination occurred, where Asian foodstuffs mixed with Polynesian foodstuffs like breadfruit, kukui nuts, and purple sweet potatoes.\n\nSome notable Hawaiian fare includes seared ahi tuna, opakapaka (snapper) with passionfruit, Hawaiian island-raised lamb, beef and meat products, Hawaiian plate lunch, and Molokai shrimp. Seafood traditionally is caught fresh in Hawaiian waters, and particular delicacies are \"ula poni\", \"papaikualoa\", \"‘opihi\", and \"‘opihi malihini\", better known as Hawaiian spiny lobster, Kona crab, Hawaiian limpet, and abalone, the last brought over with Japanese immigrants. Some cuisine also incorporates a broad variety of produce and locally grown agricultural products, including tomatoes, sweet Maui onions, taro, and macadamia nuts. Tropical fruits equally play an important role in the cuisine as a flavoring in cocktails and in desserts, including local cultivars of bananas, sweetsop, mangoes, lychee, coconuts, papayas, and lilikoi (passionfruit). Pineapples have been an island staple since the 19th century and figure into many marinades and drinks.\n\nThe demand for ethnic foods in the United States reflects the nation's changing diversity as well as its development over time. According to the National Restaurant Association, \nRestaurant industry sales are expected to reach a record high of $476 billion in 2005, an increase of 4.9 percent over 2004... Driven by consumer demand, the ethnic food market reached record sales in 2002, and has emerged as the fastest growing category in the food and beverage product sector, according to USBX Advisory Services. Minorities in the U.S. spend a combined $142 billion on food and by 2010, America's ethnic population is expected to grow by 40 percent.\nA movement began during the 1980s among popular leading chefs to reclaim America's ethnic foods within its regional traditions, where these trends originated. One of the earliest was Paul Prudhomme, who in 1984 began the introduction of his influential cookbook, \"Paul Prodhomme's Louisiana Kitchen\", by describing the over 200-year history of Creole and Cajun cooking; he aims to \"preserve and expand the Louisiana tradition.\" Prodhomme's success quickly inspired other chefs. Norman Van Aken embraced a Floridian type cuisine fused with many ethnic and globalized elements in his \"Feast of Sunlight\" cookbook in 1988. The movement finally gained fame around the world when California became swept up in the movement, then seemingly started to lead the trend itself, in, for example, the popular restaurant Chez Panisse in Berkeley. Examples of the Chez Panisse phenomenon, chefs who embraced a new globalized cuisine, were celebrity chefs like Jeremiah Tower and Wolfgang Puck, both former colleagues at the restaurant. Puck went on to describe his belief in contemporary, new style American cuisine in the introduction to \"The Wolfgang Puck Cookbook\":\n\nAnother major breakthrough, whose originators were once thought to be crazy, is the mixing of ethnic cuisines. It is not at all uncommon to find raw fish listed next to tortillas on the same menu. Ethnic crossovers also occur when distinct elements meet in a single recipe. This country is, after all, a huge melting pot. Why should its cooking not illustrate the American transformation of diversity into unity?\nPuck's former colleague, Jeremiah Tower became synonymous with California Cuisine and the overall American culinary revolution. Meanwhile, the restaurant that inspired both Puck and Tower became a distinguished establishment, popularizing its so called \"mantra\" in its book by Paul Bertolli and owner Alice Waters, \"Chez Panisse Cooking\", in 1988. Published well after the restaurants' founding in 1971, this new cookbook from the restaurant seemed to perfect the idea and philosophy that had developed over the years. The book embraced America's natural bounty, specifically that of California, while containing recipes that reflected Bertoli and Waters' appreciation of both northern Italian and French style foods.\n\nWhile the earliest cuisine of the United States was influenced by Native Americans, the thirteen colonies, or the antebellum South; the overall culture of the nation, its gastronomy and the growing culinary arts became ever more influenced by its changing ethnic mix and immigrant patterns from the 18th and 19th centuries unto the present. Some of the ethnic groups that continued to influence the cuisine were here in prior years; while others arrived more numerously during \"The Great Transatlantic Migration\" (of 1870—1914) or other mass migrations.\n\nSome of the ethnic influences could be found across the nation after the American Civil War and into the continental expansion for most of the 19th century. Ethnic influences already in the nation at that time would include the following groups and their respective cuisines:\n\nMass migrations of immigrants to the United States came in several waves. Historians identify several waves of migration to the United States: one from 1815 to 1860, in which some five million English, Irish, Germanic, Scandinavian, and others from northwestern Europe came to the United States; one from 1865 to 1890, in which some 10 million immigrants, also mainly from northwestern Europe, settled, and a third from 1890 to 1914, in which 15 million immigrants, mainly from central, eastern, and southern Europe (many Austrian, Hungarian, Turkish, Lithuanian, Russian, Jewish, Greek, Italian, and Romanian) settled in the United States.\n\nTogether with earlier arrivals to the United States (including the indigenous Native Americans, Hispanic and Latino Americans, particularly in the West, Southwest, and Texas; African Americans who came to the United States in the Atlantic slave trade; and early colonial migrants from Britain, France, Germany, Spain, and elsewhere), these new waves of immigrants had a profound impact on national or regional cuisine. Some of these more prominent groups include the following:\nItalian, Mexican and Chinese (Cantonese) cuisines have indeed joined the mainstream. These three cuisines have become so ingrained in the American culture that they are no longer foreign to the American palate. According to the study, more than nine out of 10 consumers are familiar with and have tried these foods, and about half report eating them frequently. The research also indicates that Italian, Mexican and Chinese (Cantonese) have become so adapted to such an extent that \"authenticity\" is no longer a concern to customers.\nContributions from these ethnic foods have become as common as traditional \"American\" fares such as hot dogs, hamburgers, beef steak, which are derived from German cuisine, (chicken-fried steak, for example, is a variation on German schnitzel), cherry pie, Coca-Cola, milkshakes, fried chicken (Fried chicken is of Scottish and African influence) and so on. Nowadays, Americans also have a ubiquitous consumption of foods like pizza and pasta, tacos and burritos to \"General Tso's chicken\" and fortune cookies. Fascination with these and other ethnic foods may also vary with region.\n\nAmerican chefs have been influential both in the food industry and in popular culture. An important 19th Century American chef was Charles Ranhofer of Delmonico's Restaurant in New York City. American cooking has been exported around the world, both through the global expansion of restaurant chains such as T.G.I. Friday's and McDonald's and the efforts of individual restaurateurs such as Bob Payton, credited with bringing American-style pizza to the UK.\n\nThe first generation of television chefs such as Robert Carrier and Julia Child tended to concentrate on cooking based primarily on European, especially French and Italian, cuisines. Only during the 1970s and 1980s did television chefs such as James Beard and Jeff Smith shift the focus towards home-grown cooking styles, particularly those of the different ethnic groups within the nation. Notable American restaurant chefs include Thomas Keller, Charlie Trotter, Grant Achatz, Alfred Portale, Paul Prudhomme, Paul Bertolli, Frank Stitt, Alice Waters, Patrick O'Connell and celebrity chefs like Mario Batali, David Chang, Alton Brown, Emeril Lagasse, Cat Cora, Michael Symon, Bobby Flay, Ina Garten, Todd English, Anthony Bourdain, and Paula Deen.\n\nRegional chefs are emerging as localized celebrity chefs with growing broader appeal, such as Peter Merriman (Hawaii Regional Cuisine), Jerry Traunfeld, Alan Wong (Pacific Rim cuisine), Norman Van Aken (New World Cuisine – fusion Latin, Caribbean, Asian, African and American), and Mark Miller (American Southwest cuisine).\n\n\n\n\n",
                "Italian Brazilians\n\nItalian Brazilians (, ) are Brazilian citizens of full or partial Italian descent.\n\nThere are no official numbers about how many Brazilians have Italian ancestry, as the national census conducted by IBGE does not ask the ancestry of the Brazilian people. In the last census to ask about ancestry, from 1940, 1,260,931 Brazilians said to be the child of an Italian father, while 1,069,862 said to be the child of an Italian mother. Italians were 285,000 and naturalized Brazilians, 40,000. Therefore, Italians and their children were just over 3.8% of Brazil's population in 1940.\n\nA 1999 survey, conducted by the sociologist, former president of the Brazilian Institute of Geography and Statistics (IBGE), Simon Schwartzman, indicated that 10.5% of Brazilian respondents claimed to have Italian ancestry; hence they would make up around 20 million descendants in a national population of 200 million. An Italian source, from 1996, cites the number of 22,753,000 descendants. The Embassy of Italy in Brazil, in 2013, reported the number of 31 million descendants of Italian immigrants in Brazil (about 15% of the population), half of them in the state of São Paulo.\n\nAccording to the Italian government, there are 31 million Brazilians of Italian descent, All those figures relate to Brazilians of any Italian descent, not necessarily linked to Italian culture in any significant way. According to García, the number of Brazilians with actual links to Italian identity and culture would be around 3.5 to 4.5 million people. Scholar Luigi Favero, in a book on Italian emigration between 1876 and 1976, pinpointed that Italians were present in Brasil since the Renaissance: Genoese sailors and merchants were between the first to settle in colonial Brazil since the first half of the 16th century, and so—because of the many descendants of Italians emigrated there from Columbus times until 1860—the number of Brazilians with Italian roots should be increased to 35 million.\n\nAlthough victims of some prejudice in the first decades and in spite of the persecution during World War II, Brazilians of Italian descent managed to mingle and to incorporate seamlessly into the Brazilian society.\n\nMany Brazilian politicians, artists, footballers, models and personalities are or were of Italian descent. Amongst Italian-Brazilian one finds several State Governors, Congressmen, mayors and ambassadors. Three Presidents of Brazil were of Italian descent (though none of them were directly elected to such a position): Pascoal Ranieri Mazzilli (Senate president who served as interim president), Itamar Franco (elected vice-President under Fernando Collor, whom he eventually replaced as the latter was impeached), and Emílio Garrastazu Médici (third of the series of generals who presided over Brazil during the military regime). Médici was also of Basque descent.\n\nAccording to the Brazilian Constitution, anyone born in the country is a Brazilian citizen by birthright. In addition, many who were born in Italy have become naturalized citizens after settling in Brazil. The Brazilian government used to prohibit multiple citizenships. However, this changed in 1994 with a new constitutional amendment. After the changes in 1994 over half a million Italian-Brazilians have requested the recognition of their Italian citizenship.\n\nAccording to the Italian legislation an individual with an Italian parent is automatically recognized as an Italian citizen, the jus sanguinis principle being applied. In order to exercise the rights and obligations of citizenship an individual needs to have all documents registered in Italy, which normally involves the local consulate or embassy. Some limitations are applied to this process of recognition such as the renouncement of the Italian citizenship by the individual or the parent (if before the child's birth), a second limitation is that women only transferred citizenship to their children after 1948. After constitutional reform in Italy, Italian citizens abroad can also elect representatives to the Italian Chamber of Deputies and the Italian Senate. Italian citizens residing in Brazil elect representatives together with Argentina, Uruguay and other countries in South America. According to Italian senator Edoardo Pollastri, currently there are over half a million Brazilians in line to have their Italian citizenship recognized.\n\nItaly only united as a sovereign national state in 1861. Before that Italy was politically divided in several kingdoms, ducates and other small states. This fact influenced deeply the character of the Italian migrant. \"Before 1914, the typical Italian migrant was a man without a clear national identity but with strong attachments to his town or village or region of birth, to which half of all migrants returned.\"\n\nDuring the 19th century, many Italians fled the political persecutions in Italy led by the Imperial Austrian government after the failure of unification movements in 1848 and 1861. Although very small, these well educated and revolutionary group of emigrants left a deep mark where they settled. In Brazil, the most famous Italian in this period was Líbero Badaró. Despite that, the mass Italian immigration that contributed to shape Brazilian culture after the Portuguese and the German migration movements started only after Italian unification.\n\nDuring the last quarter of the 19th century, the newly united Italy suffered an economic crisis. In the Northern regions, there was unemployment due to the introduction of new techniques in agriculture, while Southern Italy remained underdeveloped and untouched by modernization in agrarian structure. Even in the North, industrialization was still in its initial stages, and illiteracy was still common in Italy (though more in the south and islands than in the north). Thus, poverty and lack of jobs and income stimulated Northern (and also Southern) Italians to emigrate. Most Italian immigrants were very poor rural workers (\"braccianti\").\n\nIn 1850, under British pressure, Brazil finally passed a law that effectively banned transatlantic slave trade. The increased pressure of the abolitionist movement, on the other hand, made clear that the days of slavery in Brazil were coming to an end. Slave trade was in fact effectively suppressed, but the slave system still endured for almost four decades. So the discussion about European immigration to Brazil became a priority for Brazilian landowners. The latter claimed that such migrants were or would soon become indispensable for Brazilian agriculture. They would soon win the argument and mass migration would begin in earnest.\n\nAn Agriculture Congress in 1878 in Rio de Janeiro discussed the lack of labour and proposed to the government the stimulation of European immigration to Brazil. Immigrants from Italy, Portugal and Spain were considered the best ones, because they were white and, mainly, Catholics. Therefore, the Brazilian government started to attract more Italian immigrants to the coffee plantations.\n\nAt the end of the 19th century, the Brazilian government was influenced by eugenics theories. According to some Brazilian scholars, it was necessary to bring immigrants from Europe to enhance the Brazilian population.\n\nThe Brazilian government (with or following the Emperor's support) had created the first colonies of immigrants (\"colônias de imigrantes\") in the early 19th century. These colonies were established in rural areas of the country, being settled by European families, mainly German immigrants that settled in many areas of Southern Brazil. \n\nThe first groups of Italians arrived in 1875, but the boom of Italian immigration in Brazil happened in the late 19th century, between 1880 and 1900, when almost one million Italians arrived.\n\nA great number of Italians was naturalized Brazilian at the end of the 19th century, when the 'Great Naturalization' conceded automatically citizenship to all the immigrants residing in Brazil prior to November 15, 1889 \"unless they declared a desire to keep their original nationality within six months.\"\n\nDuring the last years of the 19th century, the denouncements of bad conditions in Brazil increased in the press. Reacting to the public clamor and many proved cases of mistreatments of Italian immigrants, the government of Italy issued, in 1902, the Prinetti decree forbidding subsidized immigration to Brazil. In consequence, the number of Italian immigrants in Brazil fell drastically in the beginning of the 20th century, but the wave of Italian immigration continued until 1920.\n\nOver half of the Italian immigrants came from Northern Italian regions of Veneto, Lombardy, Tuscany and Emilia-Romagna. About 30% emigrated from Veneto. On the other hand, during the 20th century, Central and Southern Italians predominated in Brazil, coming from the regions of Campania, Abruzzo, Molise, Basilicata and Sicily.\n\nIn 1924 Umberto, Prince of Piedmont (the future King Umberto II of Italy) came to Brazil as part of a state visit to various South American country. This was part of the political plan of the newly installed Fascist government to link Italian people living outside of Italy with their mother country and with the interests of the regime. The visit was considerably disrupted by the then ongoing Tenente revolts, making it impossible for the visiting Prince to reach Rio de Janeiro and São Paulo. Nevertheless, he was hosted at Bahia where members of the Italian colony in the city were very happy and proud about his visit, thus partially achieving the visit's purpose.\n\nThe Brazilian Census of 1940 asked Brazilians where their fathers came from. The Census revealed that at that time there were 3,275,732 Brazilians who were born to an immigrant father. Of those, 1,260,931 Brazilians were born to an Italian father. Italian was the main reported paternal immigrant origin, followed by Portuguese with 735,929 children, Spanish with 340,479 and German with 159,809 children.\n\nThe Census also revealed that the 458,281 foreign mothers of 12 or more years who lived in Brazil had 2,852,427 children, of whom 2,657,974 were born alive. The Italian women had more children than any other female immigrant community in Brazil: 1,069,862 Brazilians were born to an Italian mother, followed by 524,940 who were born to a Portuguese mother, 436,305 to a Spanish mother and 171,790 to a Japanese mother. The 6,809,772 Brazilian-born mothers of 12 or more years had 38,716,508 children, of whom 35,777,402 were born alive.\n\nOn the other hand, in 1998, the IBGE, within its preparation for the 2000 Census, experimentally introduced a question about \"origem\" (ancestry) in its \"Pesquisa Mensal de Emprego\" (Monthly Employment Research), in order to test the viability of introducing that variable in the Census (the IBGE ended by deciding against the inclusion of questions about it in the Census). This research interviewed about 90,000 people in six metropolitan regions (São Paulo, Rio de Janeiro, Porto Alegre, Belo Horizonte, Salvador, and Recife).\n\nThe results of this survey stand in contradiction with the claims of the Italian embassy to Brazil. While the latter point to \"Italian Brazilians\" making up to 18% of the Brazilian population, with absolute figures varying between 28 and 31 million, and figures for the city of São Paulo as high as 60% or 6 million, the IBGE found actually a figure of 10%, which would correspond, at the time, to a total population of about 3.5 million people of Italian origin in the whole set of metropolitan regions it researched, including São Paulo (and Porto Alegre, another metropolitan region with a high concentration of \"oriundi\").\n\n.* Comissariato Generale dell'Emigrazione\n\n.** Consulates\n\nThe 1920 Census was the first one to show a more specific figure about the size of the Italian population in Brazil (558,405). However, since the 20th century the arrival of new Italian immigrants to Brazil was in full decline. The previous censuses of 1890 and 1900 had limited information (in fact, the 1900 Census never existed). In consequence, there are no official figures about the size of the Italian population in Brazil during the mass immigration period (1880–1900). There are estimates available, and the most reliable is the one done by Giorgio Mortara, even though the figures he found may have underestimated the real size of the Italian population. On the other hand, Angelo Trento believes that the Italian estimates are \"certainly exaggerated\", and \"lacking of any foundation\", since they found the figure of 1,837,887 Italians in Brazil as of 1927. Another evaluation conducted by Bruno Zuculin found the presence of 997,887 Italians in Brazil as of 1927. Notice that all these figures only include people born in Italy, and not their Brazilian born descendants.\n\nAmong all Italians who immigrated to Brazil, 70% went to the State of São Paulo. In consequence, São Paulo has more people with Italian ancestry than any region of Italy itself. The rest went mostly to the states of Rio Grande do Sul and Minas Gerais.\n\nDue to the internal migration, many Italians, second and third generations descendants, moved to other areas. In the early 20th century, many rural Italian workers from Rio Grande do Sul migrated to the west of Santa Catarina and then further north to Paraná.\n\nMore recently, third and fourth generations have been migrating to other areas; thus it is possible to find people of Italian descent in Brazilian regions where the immigrants had never settled, such as in the Cerrado region of Central-West, in the Northeast and in the Amazon rainforest area, in the extreme North of Brazil.\n\nThe main areas of Italian settlement in Brazil were the Southern and Southeastern regions, namely the states of São Paulo, Rio Grande do Sul, Santa Catarina, Paraná, Espírito Santo and Minas Gerais.\n\nThe first colonies to be populated by Italians were created in the highlands of Rio Grande do Sul (Serra Gaúcha). These were Garibaldi and Bento Gonçalves. These immigrants were predominantly from Veneto, in northern Italy. After five years, in 1880, the great numbers of Italian immigrants arriving caused the Brazilian government to create another Italian colony, Caxias do Sul. After initially settling in the government-promoted colonies, many of the Italian immigrants spread into other areas of Rio Grande do Sul seeking better opportunities. They created many other Italian colonies on their own, mainly in highlands, because the lowlands were already populated by German immigrants and native \"gaúchos\". The Italian established many vineyards in the region. Nowadays, the wine produced in these areas of Italian colonization in southern Brazil is much appreciated within the country, though little is available for export. In 1875, the first Italian colonies were established in Santa Catarina, which lies immediately to the north of Rio Grande do Sul. The colonies gave rise to towns such as Criciúma, and later also spread further north, to Paraná.\n\nIn the colonies of southern Brazil, Italian immigrants at first confined themselves within themselves, where they could speak their native Italian dialects and keep their culture and traditions. With time, however, they would become thoroughly integrated economically and culturally into the larger society. In any case, Italian immigration to southern Brazil was very important to the economic development, as well to the culture of the region.\n\nA part of the immigrants settled in the colonies in Southern Brazil. However, the majority of them settled in Southeastern Brazil (mainly in the State of São Paulo). In the beginning, the government was responsible for bringing the immigrants (in most cases, paying for their transportation by ship), but later the farmers were responsible for making contracts with immigrants or specialized companies in recruiting Italian workers. Many posters were spread in Italy, with pictures of Brazil, selling the idea that everybody could become rich there by working with coffee, which was called by the Italian immigrants the \"green gold\". Most coffee plantations were in the States of São Paulo and Minas Gerais, and in a smaller proportion also in the States of Espírito Santo and Rio de Janeiro. Rio de Janeiro was declining in the 19th century as a farming producer and São Paulo had already taken the lead as a coffee producer/exporter around the start of the 20th century, as well as big producer of sugar and other important crops. Thus, migrants were naturally more attracted to the State of São Paulo and the southern states.\n\nItalians used to migrate to Brazil in families. The \"colono\", as rural immigrants were called, had to sign a contract with the farmer and was obliged to work in the coffee plantation during a minimum period of time. However, the situation was not easy. Many Brazilian farmers were used to command slaves and treated the immigrants as indentured servants.\n\nWhile, in Southern Brazil, the Italian immigrants were living in relatively well developed colonies, in Southeastern Brazil they were living in semi-slavery conditions in the coffee plantations. Many rebellions against Brazilian farmers occurred and the public denouncements caused great commotion in Italy, forcing the Italian government to issue the Prinetti decree that established barriers to immigration to Brazil.\n\nIn 1901, 90% of industrial workers and 80% of construction workers at São Paulo city were Italians.\n\nSão Carlos and Ribeirão Preto were two of the main coffee plantation centers. Located both, respectively, in the North-Central and Northeastern regions of São Paulo state, a zone known by its hot temperature and a fertile soil in which some of the richest coffee farms were in, it attracted most of the immigrants arriving in São Paulo, including the Italians, between 1901 and 1940.\n\nAlthough the majority of Brazilians of Italian descent live in the South and Southeast part of the country, in recent decades (1960s-present), people from southern Brazil, mainly of Italian descent, have played a vital role in settling and developing the vast \"cerrado\" grasslands of Central-West, North and the west part of Northeastern Brazil.\n\nThese areas, once economically neglected, are fast becoming one of the world's most important agricultural regions. The cerrado (Portuguese for thick and dense, meaning thick grassland) is a vast area of savanna-like grasslands in Brazil. In the State of Mato Grosso do Sul, Italian descendants are 5% of the population.\n\nIn 1902, the Italian immigration to Brazil started to decline. From 1903 to 1920, only 306,652 Italians immigrated to Brazil, compared to 953,453 to Argentina and 3,581,322 to the United States. This was mainly due to the Prinetti Decree in Italy, that banned the subsidized immigration to Brazil (the Brazilian Government or landowners could not pay the passage of the immigrants anymore). Prinetti Decree was created because of the commotion in the Italian press due to the penury faced by most Italians in Brazil. The immigrants who went to Southern Brazil became small landowners and, despite the problems faced by them (dense forest, epidemics of yellow fever, lack of consumer market) the easy access to lands increased their opportunities. However, only a minority of the Italians were taken to Southern Brazil. Most of the country's economy was based on coffee plantations, and Brazil was already the main coffee exporter in the world (since the 1850s). As a consequence of the end of slavery and that most former slaves left the plantations, there was a labour shortage on coffee plantations. Moreover, “natural inequality of human beings”, “hierarchy of races”, Social Darwinism, Positivism and other theories were used to explain that the European workers were superior to the native workers. In consequence, passages were offered to Europeans (the so-called \"subsidized immigration\"), mostly to Italians, so that they could come to Brazil and work on the plantations.\n\nThose immigrants were employed in enormous latifundia (large-scale farms), formerly employing slaves. In Brazil, there were no labour laws (the first concrete labour laws only appeared in the 1930s, under Getúlio Vargas's government) and, therefore, workers had almost no legal protection. Contracts signed by the immigrants could easily be violated by the Brazilian landowners. Accustomed to dealing with African slaves, the remnants of slavery influenced on how Brazilian landowners dealt with Italian workers: immigrants were often monitored, with extensive hours of work. In some cases, they were obliged to buy the products they needed from the landowner. Moreover, the coffee farms were located in rather isolated regions. If the immigrants became sick, they would take hours to reach the nearest hospital. The structure of labor used on farms included the labor of Italian women and children. Keeping their Italian culture was also made more difficult: the Catholic churches and Italian cultural centers were far from the farms. The immigrants who did not accept the standards imposed by the landowner were replaced by other immigrants. This forced them to accept the impositions of the landowner or they would have to leave his lands. Even though Italians were considered to be \"superior\" to blacks by Brazilian landowners, the situation faced by Italians in Brazil was so similar to that of the slaves that farmers called them \"escravos brancos\" (white slaves in Portuguese).\n\nThe destitution faced by Italians and other immigrants in Brazil caused great commotion in the Italian press, which culminated in the Prinetti Decree in 1902. Many immigrants left Brazil after their experience on São Paulo's coffee farms. Between 1882 and 1914, 1.5 million immigrants of different nationalities came to São Paulo, while 695,000 left the state, or 45% of the total. The high numbers of Italians asking the Italian Consulate a passage to leave Brazil was so significant that in 1907 most of the Italian funds for repatriation were used in Brazil. It is estimated that, between 1890 and 1904, 223,031 (14,869 annually) Italians left Brazil, mainly after failed experiences on coffee farms. The majority of the Italians who left the country were unable to add the money they wanted. Most of these people returned to Italy, while others re-migrated to Argentina, Uruguay or to the United States. The output of immigrants concerned Brazilian landowners, who constantly complained about the lack of workers. Spanish immigrants began arriving in greater numbers, but soon Spain also started to create barriers for further immigration of Spaniards to coffee farms in Brazil. The continuing problem of lack of labor in the farms was, then, temporarily resolved with the arrival of Japanese immigrants, from 1908.\n\nDespite the high numbers of immigrants leaving the country, the majority of the Italians remained in Brazil. Most of the immigrants only remained one year working on coffee farms and then they left the plantations. A small number of them earned enough money to buy their own lands, and became farmers themselves. However, the majority migrated to Brazilian urban centers. Many Italians worked in factories (in 1901, 81% of the São Paulo's factory workers were Italians). In Rio de Janeiro, a considerable number of the factory workers was also composed of Italians. In São Paulo, those workers established themselves in the center of the city, living in cortiços (degraded multifamily row houses). Those agglomerations of Italians in urban centers gave birth to typically Italian neighborhoods, such as Mooca, which is until today linked to its Italian past. Other Italians became traders, mostly itinerant traders, selling their products in different regions. A common presence on the streets of São Paulo were the Italian boys working as newspaper-boys, as an Italian traveler observed: \"In the crowd, we can see many Italian boys, shabby and barefoot, selling the newspapers from the city and from Rio de Janeiro, bothering the passersby with their offerings and their shouting of street roguish\".\n\nDespite the poverty and even semi-slavery conditions faced by many Italians in Brazil, over time most of this population achieved some personal success and changed their low class economic situation. Even though most of the first generation of immigrants still lived in poverty, the children of Italians, born in Brazil, often changed their social status as they diversified their field of work, leaving the poor conditions of their parents and not rarely becoming part of the local elite.\n\nWith the exception of some isolated cases of violence between Brazilians and Italians, especially between 1892 and 1896, the integration of immigrants in Brazil happened quick and peacefully. For the Italians in São Paulo, scholars suggest that this process of assimilation occurred in up to two generations. There is research that suggests that even first-generation immigrants, born in Italy, soon became assimilated in the new country. Even in Southern Brazil, where most of the Italians were living in isolated rural communities, without much contact with Brazilians, and where they kept the Italian patriarchal family structure (and therefore the father chose the wive or husband for their children, giving preference to the Italians) the assimilation process was also quick.\n\nAccording to the 1940 Census in Rio Grande do Sul, 393,934 people reported to speak German as their first language (11.86% of the state's population). In comparison, 295,995 reported to speak Italian, mostly dialects (8.91% of the state's population). Even though the Italian immigration was larger and more recent than the German one, the Italian group tended to be more easily assimilated. In the 1950 Census, the number of people in Rio Grande do Sul who reported to speak Italian dropped to 190,376. In São Paulo, where a larger number of Italians settled, in the 1940 census 28,910 Italian born people reported to speak Italian at home (only 13.6% of the state's Italian population). In comparison, 49.1% of the immigrants of other nationalities reported to keep speaking their native languages at home (with the exception of the Portuguese, of course). Then, the prohibition of speaking Italian, German and Japanese during World War II was not so great to the Italian community as it was to the other two groups.\n\nA major measure of the government occurred in 1889, when the Brazilian citizenship was granted to all immigrants, although this act had little influence on their identity or assimilation process. The Italian newspapers in Brazil and also the Italian government, in turn, were uncomfortable with the assimilation of Italians in the country. This occurred mostly after the Great Naturalization period. The Italian institutions encouraged the entry of Italians in Brazilian politics, although the presence of immigrants was, initially, small. The Italian dialects came to dominate the streets of São Paulo and in some Southern localities. Over time, these languages based on Italian dialects tended to disappear and nowadays their presence is small.\n\nIn the beginning, specially in rural Southern Brazil, Italians tended to marry only other Italians. On the other hand, Italians in São Paulo and, mainly, those living in urban centers tended to marry Brazilians. Over time and with the decrease of more immigrants arriving, even in Southern Brazil they started to integrate themselves with Brazilians. About the Italians in Santa Catarina, the Italian Consul asserted:\n\nThere is little information about this trend, but it was noticed a large process of integration since World War I: between 1917 and 1923, in Rio Grande do Sul: weddings between an Italian man and a Brazilian woman (997, 66.1%); Italian woman and Brazilian man (135, 9%) and Italian man and Italian woman (375, 24.9%).\n\nThese marriages between Italians and Brazilians were extremely common, mostly in the low classes, and were largely accepted for both people. However, some more closed members of the Italian community saw this integration process as negative. The German Brazilian population was also treated by some Italians as repulsive, even though many Germans and Italians lived together in many areas of Southern Brazil. The Brazilian Indians were often treated as wild people, and cases of conflicts between Italians and Indians for the occupation of lands in Southern Brazil were not uncommon.\n\nHistorically, Italians have been divided into two groups in Brazil. Those in Southern Brazil lived in rural colonies, in contact mostly with other people of Italian descent. Italians living in Southeast Brazil on the other hand, the most populated region of the country, integrated into Brazilian society quite quickly.\n\nAfter some years working in coffee plantations, some immigrants earned enough money to buy their own land and become farmers themselves. Others left the rural areas and moved to urban centres, mainly São Paulo, Campinas, São Carlos and Ribeirão Preto. A small minority became very rich in the process and attracted more Italian immigrants. In the early 20th century, São Paulo became known as the \"City of the Italians\", because 31% of its inhabitants were of Italian nationality in 1900. The city of São Paulo had the second highest population of people with Italian ancestry in the world at this time, second only to Rome. In Campinas, street signs in Italian were common, a large commercial and services sector owned by Italian Brazilians developed, and more than 60% of the population had Italian surnames. Today, nearly 30% of the population of Belo Horizonte remains of Italian ancestry.\n\nItalian immigrants were very important to the development of many of the big cities in Brazil, such as São Paulo, Porto Alegre, Curitiba and Belo Horizonte. Bad conditions in rural areas made thousands of Italians move to these big cities. Most of them became laborers and participated actively in the industrialization of Brazil in the early 20th century. Others became investors, bankers and industrialists, such as Count Matarazzo, whose family became the richest industrialists in São Paulo, with a holding of more than 200 industries and businesses. In Rio Grande do Sul, 42% of industrial companies have Italians roots.\n\nItalians and their descendants were also quick to organize themselves and establish mutual aid societies (such as the Circolo Italiano), hospitals, schools (such as the Istituto Colégio Dante Alighieri, in São Paulo), labor unions, newspapers as Il Piccolo from Mooca and Fanfulla (for the whole city to São Paulo), magazines, radio stations and association football teams such as: Clube Atlético Votorantim, the old Sport Club Savóia from Sorocaba, Clube Atlético Juventus of Italians Brazilians from Mooca (old worker quarter from city of São Paulo) and the great clubs (which had the same name) Palestra Italia, later renamed to: Sociedade Esportiva Palmeiras in São Paulo and Cruzeiro Esporte Clube in Belo Horizonte.\n\nA greatest number of the Italian immigrants to Brazil came from Northern Italy; however, they were not distributed homogeneously among the extensive Brazilian regions. In the state of São Paulo, the Italian community was more diverse including a large number of people from the South and from the center of Italy. Even today, 42% of the Italians in Brazil came from the Northern regions, 36% from central regions and only 22% from the south of Italy. Brazil is the only American country with a large Italian community where the Southern Italian immigrants are minority.\n\nIn the first decades, the vast majority of the immigrants came from the North. Since Southern Brazil received most of the early settlers, the vast majority of the immigrants in this region came from the extreme North of Italy, mainly from Veneto and particularly from the provinces of Vicenza (32%), Belluno (30%) and Treviso (24%). In Rio Grande do Sul, many came from Cremona, Mantua, from parts of Brescia, and also from Bergamo, in the region of Lombardy, close to Veneto. The regions of Trentino and of Friuli-Venezia Giulia also sent many immigrants to the South of Brazil. Of the immigrants in Rio Grande do Sul, 54% came from the Veneto, 33% from Lombardy, 7% from Trentino, 4.5% from Friuli-Venezia Giulia and only 1.5% from other parts of Italy.\n\nStarting in the early 20th century, the agrarian crisis also started to affect Southern Italy and many of them immigrated to Brazil. The Southerners went mostly to the state of São Paulo, since it was in need of workers to embrace the coffee plantations. Among the Italian immigrants in São Paulo, most came from Calabria, Campania and Veneto.\n\nItalian regional origin of Brazil, Argentina, Uruguay, Paraguay immigrants\n\nNowadays, most Brazilians with Italian ancestry speak Portuguese as their native language. During the Second World War, the public use of Italian, German and Japanese was forbidden.\n\nThe Italian dialects have influenced the Portuguese spoken in some areas of Brazil. The Italian Language was so spread in São Paulo that the Portuguese traveller, Sousa Pinto, said that he could not speak with cart drivers in Portuguese because they all spoke Italians dialects and gesticulating as Neapolitans.\n\nCurrently, the Italian influence on Portuguese spoken in São Paulo is not as great as in the past, although the accent of the city's inhabitants still has some traces of the Italian accents common in the beginning of the 20th century, like the intonation and also such expressions as \"Belo\", \"Ma vá!\", \"Orra meu!\" and \"Tá entendendo?\". Other characteristic is the difficulty to speak Portuguese in plural, saying plural words as they were singulars. The lexical influence of Italian on Brazilian Portuguese, however, has remained quite small.\n\nA similar phenomenon occurred in the countryside of Rio Grande do Sul, but encompassing almost exclusively those of Italian origin. On the other hand, there exists a different phenomenon; Talian, a language which emerged mostly in the northeastern part of the state (Serra Gaúcha). Talian is a variant of the Venetian language, with influences from other Italian dialects and Portuguese. In southern Brazilian rural areas marked by bilingualism, even among the monolingual Portuguese-speaking population, the Italian-influenced accent is fairly typical.\n\nThe Italian influence in Brazil reached also the music, not only with traditional Italian songs but also with the merging with other Brazilians music styles. One of the main results of the fusion is the Samba Paulista, a Samba with strong Italians influence.\n\nThe Samba Paulista was created by Adoniran Barbosa(born João Rubinato), son of Italians immigrants. His songs translated the life of the Italian neighborhoods in São Paulo, merging the São Paulo's dialect with Samba, what latter celebrate him as the \"People's Poet\".\n\nOne of the main example is the Samba Italiano, a song that has Brazilian rhythm and theme, but (mostly) Italian lyrics. Below, the lyrics of this song, with the parts in (mangled) Portuguese in bold and the parts in Italian in normal font:\n\nSt. Vito Festival is one of the most important Italian festivals in São Paulo. It is a celebration in honor of Saint Vito, the patron saint of Polignano a Mare, a city in the Puglia region, in Italy. Many Italian immigrants in Brás, a São Paulo district, came from Puglia. Festa de São Vito is also a time when the Italian community in São Paulo gathers to party and eat traditional food. Other important Italian celebrations in São Paulo are Our Lady of Casaluce, also in Brás (May), Our Lady of Achiropita, in Bela Vista (August), and St. Gennaro, in Mooca (September). Italian immigrants from the Puglia region who moved in great numbers to the Brás neighborhood in São Paulo at the end of the 19th century brought along a devotion to Saint Vito, a Christian martyr who was killed in June 303 a.D.\n\nJust like Polignano a Mare, eventually Brás had a church devoted to St. Vito. An association was formed and hosted the first festival in June 1919. As São Paulo grew, so did the Italian community and St. Vito Festival. Today, about 6 million of São Paulo's 10,886,518 inhabitants are Italians and descendants (known as \"oriundi\"), according to statistics provided by Conscre, a São Paulo state council for foreign communities. An estimated 140,000 people were expected to attend the festival in 2008.\n\nItalians brought new recipes and types of food to Brazil but also helped in the development of Brazil's cuisine.\n\nAside from the typical Italian cuisine like pizza, pasta, risotto, panettone, milanesa, polenta, calzone, ossobuco and others, Italians helped to create new dishes that today are typically Brazilians. \"Galeto\"(from the Italian \"Galletto\", little rooster), \"Frango com Polenta\" (Chicken with fried Polenta),\"Bife à parmegiana\"(a steak prepared with Parmigiano-Reggiano), Catupiry cheese, new types of sausage like \"Linguiça Calabresa\" and \"Linguiça Toscana\" (literally Calabrian and Tuscan Sausage), \"Chocotone\" (Panettone with chocolate chips) and many others recipes were created or influenced by the Italian community.\n\n\nItalian international schools in Brazil:\n\n\n"
            ],
            "good_message": false,
            "loggedData": [
                {
                    "curr_message": " During the latter half of the 20th century, pizza became a globally accessible dish, mainly due to Italian immigrants that had brought their dishes to new people with resounding success, often in racially and culturally resistive environments.",
                    "last_key_pressed": "<<selected>>"
                }
            ],
            "offensive": null,
            "researchTask": true,
            "selectedData": [
                "During the latter half of the 20th century, pizza became a globally accessible dish, mainly due to Italian immigrants that had brought their dishes to new people with resounding success, often in racially and culturally resistive environments."
            ],
            "shown_passages": [
                [
                    "List of pizza varieties by country",
                    [
                        "During the latter half of the 20th century, pizza became a globally accessible dish, mainly due to Italian immigrants that had brought their dishes to new people with resounding success, often in racially and culturally resistive environments.",
                        "A survey from 2004 showed that Norwegians eat the most frozen pizza (5.4 kg/person*year), followed by Germans.",
                        "Pizza became a popular fast food in Bangladeshi urban areas.",
                        "Since the introduction of various pizza brands such as Domino's and Pizza Hut in the early to mid-2000s, it has reached almost all classes of urban peoples.",
                        "Pizza is an emerging fast food in Indian urban areas."
                    ]
                ],
                [
                    "Cuisine of the United States",
                    [
                        "The cuisine of the United States reflects its history.",
                        "The European colonization of the Americas yielded the introduction of a number of ingredients and cooking styles to the latter.",
                        "The various styles continued expanding well into the 19th and 20th centuries, proportional to the influx of immigrants from many different nations; such influx developed a rich diversity in food preparation throughout the country.",
                        "Early Native Americans utilized a number of cooking methods in early American Cuisine that have been blended with early European cooking methods to form the basis of American Cuisine.",
                        "When the colonists came to the colonies, they farmed animals for clothing and meat in a similar fashion to what they had done in Europe."
                    ]
                ],
                [
                    "Italian Brazilians",
                    [
                        "Italian Brazilians (, ) are Brazilian citizens of full or partial Italian descent.",
                        "There are no official numbers about how many Brazilians have Italian ancestry, as the national census conducted by IBGE does not ask the ancestry of the Brazilian people.",
                        "In the last census to ask about ancestry, from 1940, 1,260,931 Brazilians said to be the child of an Italian father, while 1,069,862 said to be the child of an Italian mother.",
                        "Italians were 285,000 and naturalized Brazilians, 40,000.",
                        "Therefore, Italians and their children were just over 3.8% of Brazil's population in 1940.",
                        "A 1999 survey, conducted by the sociologist, former president of the Brazilian Institute of Geography and Statistics (IBGE), Simon Schwartzman, indicated that 10.5% of Brazilian respondents claimed to have Italian ancestry; hence they would make up around 20 million descendants in a national population of 200 million."
                    ]
                ]
            ],
            "speaker": "0_Wizard",
            "suggestionsConfig": "show",
            "suggestionsNumber": 5,
            "suggestionsShown": true,
            "text": " During the latter half of the 20th century, pizza became a globally accessible dish, mainly due to Italian immigrants that had brought their dishes to new people with resounding success, often in racially and culturally resistive environments.",
            "textboxFilled": 0,
            "time": 1580233193.4824991,
            "timeTaken": 33.56,
            "totalTasks": 120,
            "turn": 4
        },
        {
            "checked_passage": {
                "no_passages_used": "no_passages_used"
            },
            "checked_sentence": {
                "no_passages_used": "no_passages_used"
            },
            "clicked_passages": {},
            "context": {
                "conversation": [
                    "I have been thinking of getting a tattoo . Can you tell me more about this so I can make a decision?",
                    "Body modifications designed on bodies are tattoos "
                ],
                "index": 25
            },
            "contextCount": 0,
            "full_passages": [
                "Tattoo\n\nA tattoo is a form of body modification where a design is made by inserting ink, dyes and pigments, either indelible or temporary, into the dermis layer of the skin to change the pigment. Tattoos fall into three broad categories: purely decorative (with no specific meaning); symbolic (with a specific meaning pertinent to the wearer); pictorial (a depiction of a specific person or item). Tattoos have historically been regarded in the West as 'uncivilised', and over the last 100 years the fashion has been associated mainly with sailors, working men and criminals. By the end of the 20th Century many Western stigmas of the tattoo culture had been dismissed and the practice has become more acceptable and accessible for people of all trades and levels of society.\n\nThe word \"tattoo\", or \"tattow\" in the 18th century, is a loanword from the Polynesian word \"tatau\", meaning \"to write\". The \"Oxford English Dictionary\" gives the etymology of tattoo as \"In 18th c. tattaow, tattow. From Polynesian (Samoan, Tahitian, Tongan, etc.) tatau. In Marquesan, tatu.\" Before the importation of the Polynesian word, the practice of tattooing had been described in the West as painting, scarring, or staining.\n\nThis is not to be confused with the origins of the word for the military drumbeat or performance — see \"military tattoo\". In this case, the English word \"tattoo\" is derived from the Dutch word \"taptoe\".\n\nThe first written reference to the word \"tattoo\" (or \"tatau\") appears in the journal of Joseph Banks (24 February 1743 – 19 June 1820), the naturalist aboard explorer Captain Cook's ship the \"HMS Endeavour\": \"I shall now mention the way they mark themselves indelibly, each of them is so marked by their humour or disposition\".\nThe word \"tattoo\" was brought to Europe by Cook, when he returned in 1769 from his first voyage to Tahiti and New Zealand. In his narrative of the voyage, he refers to an operation called \"tattaw\".\n\nTattoo enthusiasts may refer to tattoos as \"ink\", \"pieces\", \"skin art\", \"tattoo art\", \"tats\", or \"work\"; to the creators as \"tattoo artists\", \"tattooers\", or \"tattooists\"; and to places where they work as \"tattoo shops\", \"tattoo studios\", or \"tattoo parlors\".\nMainstream art galleries hold exhibitions of both conventional and custom tattoo designs such as \"Beyond Skin\", at the Museum of Croydon. Copyrighted tattoo designs that are mass-produced and sent to tattoo artists are known as \"flash\", a notable instance of industrial design. Flash sheets are prominently displayed in many tattoo parlors for the purpose of providing both inspiration and ready-made tattoo images to customers.\n\nThe Japanese word \"irezumi\" means \"insertion of ink\" and can mean tattoos using \"tebori\", the traditional Japanese hand method, a Western-style machine, or any method of tattooing using insertion of ink. The most common word used for traditional Japanese tattoo designs is \"horimono\". Japanese may use the word \"tattoo\" to mean non-Japanese styles of tattooing.\n\nAnthropologist Ling Roth in 1900 described four methods of skin marking and suggested they be differentiated under the names \"tatu\", \"moko\", \"cicatrix\", and \"keloid\".\n\nThe American Academy of Dermatology distinguishes five types of tattoos: traumatic tattoos, also called \"natural tattoos\", that result from injuries, especially asphalt from road injuries or pencil lead; amateur tattoos; professional tattoos, both via traditional methods and modern tattoo machines; cosmetic tattoos, also known as \"permanent makeup\"; and medical tattoos.\n\nAccording to George Orwell, coal miners could develop characteristic tattoos owing to coal dust getting into wounds. This can also occur with substances like gunpowder. Similarly, a traumatic tattoo occurs when a substance such as asphalt is rubbed into a wound as the result of some kind of accident or trauma. These are particularly difficult to remove as they tend to be spread across several layers of skin, and scarring or permanent discoloration is almost unavoidable depending on the location. An amalgam tattoo is when amalgam particles are implanted in to the soft tissues of the mouth, usually the gums, during dental filling placement or removal; another example of such accidental tattoos is the result of a deliberate or accidental stabbing with a pencil or pen, leaving graphite or ink beneath the skin.\n\nMany tattoos serve as rites of passage, marks of status and rank, symbols of religious and spiritual devotion, decorations for bravery, sexual lures and marks of fertility, pledges of love, amulets and talismans, protection, and as punishment, like the marks of outcasts, slaves and convicts. The symbolism and impact of tattoos varies in different places and cultures. Tattoos may show how a person feels about a relative (commonly mother/father or daughter/son) or about an unrelated person. Today, people choose to be tattooed for artistic, cosmetic, sentimental/memorial, religious, and magical reasons, and to symbolize their belonging to or identification with particular groups, including criminal gangs (see criminal tattoos) or a particular ethnic group or law-abiding subculture. Some Māori still choose to wear intricate moko on their faces. In Cambodia, Laos, and Thailand, the yantra tattoo is used for protection against evil and to increase luck. Text-based tattoos including quotes, lyrics, personal mottos or scripture are popular in western culture. As an example some Christians might have a Psalm or verse from the Bible tattooed on their body. Popular verses include John 3:16, Philippians 4:13, and Psalms 23.\n\nExtensive decorative tattooing is common among members of traditional freak shows and by performance artists who follow in their tradition.\n\nPeople have also been forcibly tattooed.\n\nA well-known example is the Nazi practice of forcibly tattooing Nazi concentration camp inmates with identification numbers during The Holocaust as part of the Nazis' identification system, beginning in fall 1941. The Nazis' SS introduced the practice at Auschwitz concentration camp in order to identify the bodies of registered prisoners in the concentration camps. During registration, the Nazis would pierce the outlines of the serial-number digits onto the prisoners' arms. Of the Nazi concentration camps, only Auschwitz put tattoos on inmates. The tattoo was the prisoner's camp number, sometimes with a special symbol added: some Jews had a triangle, and Romani had the letter \"Z\" (from German \"Zigeuner\" for \"Gypsy\"). In May 1944, the Jewish men received the letters \"A\" or \"B\" to indicate particular series of numbers. For unknown reasons, this number series for women never began again with the \"B\" series after they had reached the number limit of 20,000 for the \"A\" series. The practice continued until the last days of Auschwitz.\n\nTattoos have also been used for identification in other ways. As early as the Zhou, Chinese authorities would employ facial tattoos as a punishment for certain crimes or to mark prisoners or slaves. During the Roman Empire, Roman soldiers were required by law to have identifying tattoos on their hands in order to make desertion difficult. Gladiators and slaves were likewise tattooed: exported slaves were tattooed with the words \"tax paid\", and it was a common practice to tattoo \"Stop me, I'm a runaway\" on their foreheads. Owing to the Biblical strictures against the practice, Emperor Constantine I banned tattooing the face around AD 330, and the Second Council of Nicaea banned all body markings as a pagan practice in AD 787.\n\nIn the period of early contact between the Māori and Europeans, the Maori people hunted and decapitated each other for their moko tattoos, which they traded for European items including axes and firearms. Moko tattoos were facial designs worn to indicate lineage, social position, and status within the tribe. The tattoo art was a sacred marker of identity among the Maori and also referred to as a vehicle for storing one's tapu, or spiritual being, in the afterlife.\nTattoos are sometimes used by forensic pathologists to help them identify burned, putrefied, or mutilated bodies. As tattoo pigment lies encapsulated deep in the skin, tattoos are not easily destroyed even when the skin is burned.\n\nTattoos are also placed on animals, though rarely for decorative reasons. Pets, show animals, thoroughbred horses, and livestock are sometimes tattooed with identification and other marks. Tattooing with a 'slap mark' on the shoulder or on the ear is the standard identification method in commercial pig farming. Branding is used for similar reasons and is often performed without anesthesia, but is different from tattooing as no ink or dye is inserted during the process, the mark instead being caused by permanent scarring of the skin. Pet dogs and cats are sometimes tattooed with a serial number (usually in the ear, or on the inner thigh) via which their owners can be identified. However, the use of a microchip has become an increasingly popular choice and since 2016 is a legal requirement for all 8.5 million pet dogs in the UK.\n\nThe cosmetic surgery industry continues to see a trend of increased popularity for both surgical and noninvasive procedures. When used as a form of cosmetics, tattooing includes permanent makeup and hiding or neutralizing skin discolorations. Permanent makeup is the use of tattoos to enhance eyebrows, lips (liner and/or lipstick), eyes (liner), and even moles, usually with natural colors, as the designs are intended to resemble makeup.\n\nA growing trend in the US and UK is to place artistic titoos over the surgical scars of a mastectomy. \"More women are choosing not to reconstruct after a mastectomy and tattoo over the scar tissue instead... The mastectomy tattoo will become just another option for post cancer patients and a truly personal way of regaining control over post cancer bodies and proving once and for all that breast cancer is not just a pink ribbon.\" The tattooing of nipples on reconstructed breasts remains in high demand, however.\n\nFunctional tattoos are used primarily for a purpose other than aesthetics. One such use is to tattoo Alzheimer patients with their name, so they may be easily identified if they go missing.\n\nMedical tattoos are used to ensure instruments are properly located for repeated application of radiotherapy and for the areola in some forms of breast reconstruction. Tattooing has also been used to convey medical information about the wearer (e.g., blood group, medical condition, etc.). Additionally, tattoos are used in skin tones to cover vitiligo, a skin pigmentation disorder.\n\nSS blood group tattoos (German: Blutgruppentätowierung) were worn by members of the Waffen-SS in Nazi Germany during World War II to identify the individual's blood type. After the war, the tattoo was taken to be prima facie, if not perfect, evidence of being part of the Waffen-SS, leading to potential arrest and prosecution. This led a number of Ex-Waffen-SS to shoot themselves through the arm with a gun, removing the tattoo and leaving scars like the ones resulting from pox inoculation, making the removal less obvious.\n\nTattoos were probably also used in ancient medicine as part of the treatment of the patient. In 1898, Daniel Fouquet, a medical doctor, wrote an article on “medical tattooing” practices in Ancient Egypt, in which he describes the tattooed markings on the female mummies found at the Deir el-Bahari site. He speculated that the tattoos and other scarifications observed on the bodies may have served a medicinal or therapeutic purpose: \"The examination of these scars, some white, others blue, leaves in no doubt that they are not, in essence, ornament, but an established treatment for a condition of the pelvis, very probably chronic pelvic peritonitis.\"\n\nPreserved tattoos on ancient mummified human remains reveal that tattooing has been practiced throughout the world for many centuries. The Ainu, an indigenous people of Japan, traditionally had facial tattoos, as did the Austroasians. Today, one can find Atayal, Seediq, Truku, and Saisiyat of Taiwan, Berbers of Tamazgha (North Africa), Yoruba, Fulani and Hausa people of Nigeria, and Māori of New Zealand with facial tattoos. Tattooing was popular among certain ethnic groups in southern China, Polynesia, Africa, Borneo, Cambodia, Europe, Japan, the Mentawai Islands, MesoAmerica, New Zealand, North America and South America, the Philippines, Iron Age Britain, and Taiwan. In 2015, scientific re-assessment of the age of the two oldest known tattooed mummies, identified Ötzi as the oldest currently known example. This body, with 61 tattoos, was found embedded in glacial ice in the Alps, and was dated to 3,250 BC.\nIt is a myth that the modern revival of tattooing stems from Captain James Cook's three voyages to the South Pacific in the late 18th century. Certainly, Cook's voyages and the dissemination of the texts and images from them brought more awareness about tattooing (and, as noted above, imported the word \"tattow\" into Western languages), but Europeans have been tattooed throughout history. On Cook's first voyage in 1768, his science officer and expedition botanist, Sir Joseph Banks, as well as artist Sydney Parkinson and many others of the crew, returned to England with tattoos, although many of these men would have had pre-existing tattoos. Banks was a highly regarded member of the English aristocracy that had acquired his position with Cook by co-financing the expedition with ten thousand pounds, a very large sum at the time. In turn, Cook brought back with him a tattooed Raiatean man, Omai, whom he presented to King George and the English Court. On subsequent voyages other crew members, from officers, such as American John Ledyard, to ordinary seamen, were tattooed. \n\nThe first documented professional tattooist in Britain was established in the port of Liverpool in the 1870s. In Britain tattooing was still largely associated with sailors and the lower or even criminal class, but by the 1870s had become fashionable among some members of the upper classes, including royalty, and in its upmarket form it could be an expensive and sometimes painful process. A marked class division on the acceptability of the practice continued for some time in Britain. Recently a trend has arisen marketed as 'Stick and Poke' tattooing; primitive figures are permanently inscribed by the user himself after he obtains a 'DIY' kit containing needles, ink and a collection of suggestions.\n\nAs most tattoos in the U.S. were done by Polynesian and Japanese amateurs, tattoo artists were in great demand in port cities all over the world, especially by European and American sailors. The first recorded professional tattoo artist in the United States was a German immigrant, Martin Hildebrandt. He opened a shop in New York City in 1846 and quickly became popular during the American Civil War among soldiers and sailors of both Union and Confederate militaries.\n\nHildebrandt began traveling from camp to camp to tattoo soldiers, making his popularity increase, and also giving birth to the tradition of getting tattoos while being an American serviceman. Soon after the Civil War, tattoos became fashionable among upper-class young adults. This trend lasted until the beginning of World War I. The invention of the electric tattoo machine caused popularity of tattoos among the wealthy to drop off. The machine made the tattooing procedure both much easier and cheaper, thus, eliminating the status symbol tattoos previously held, as they were now affordable for all socioeconomic classes. The status symbol of a tattoo shifted from a representation of wealth, to a mark typically seen on rebels and criminals. Despite this change, tattoos remained popular among military servicemen, and the tradition continues today.\n\nMany studies have been done of the tattooed population and society's view of tattoos. In June 2006, the \"Journal of the American Academy of Dermatology\" published the results of a telephone survey of 2004. It found that 36% of Americans ages 18–29, 24% of those 30–40, and 15% of those 41–51 had a tattoo. In September 2006, the Pew Research Center conducted a telephone survey that found that 36% of Americans ages 18–25, 40% of those 26–40 and 10% of those 41–64 had a tattoo. They concluded that Generation X and Generation Y are not afraid to express themselves through their appearance, and tattoos are the most popular form of self-expression. In January 2008, a survey conducted online by Harris Interactive estimated that 14% of all adults in the United States have a tattoo, slightly down from 2003, when 16% had a tattoo. Among age groups, 9% of those ages 18–24, 32% of those 25–29, 25% of those 30–39 and 12% of those 40–49 have tattoos, as do 8% of those 50–64. Men are slightly more likely to have a tattoo than women.\n\nRichmond, Virginia, has been cited as one of the most tattooed cities in the United States. That distinction led the Valentine Richmond History Center to create an online exhibit titled \"History, Ink: The Tattoo Archive Project.\" The introduction to the exhibit notes, \"In the past, western culture associated tattoos with those individuals who lived on the edge of society; however, today they are recognized as a legitimate art form and widely accepted in mainstream culture.\"\n\nSince the 1970s, tattoos have become a mainstream part of Western fashion, common among all genders, to all economic classes, and to age groups from the later teen years to middle age. For many young Americans, the tattoo has taken on a decidedly different meaning than for previous generations. The tattoo has \"undergone dramatic redefinition\" and has shifted from a form of deviance to an acceptable form of expression.\n\nProtection papers were used by American sailors to prevent themselves from being taken off American ships and impressed into the Royal Navy. These were simple documents that described the sailor as being an American sailor. Many of the protection certificates were so general, and it was so easy to abuse the system, that many impressment officers of the Royal Navy paid no attention to them. \"In applying for a duplicate Seaman's Protection Certificate in 1817, James Francis stated that he 'had a protection granted him by the Collector of this Port on or about 12 March 1806 which was torn up and destroyed by a British Captain when at sea.'\" One way of making them more specific was to describe a tattoo, which is highly personal, and thus use that description to identify the seaman. As a result, many of the later certificates carried information about tattoos and scars, as well as other specific information. This also perhaps led to an increase and proliferation of tattoos among American seamen. \"Frequently their 'protection papers' made reference to tattoos, clear evidence that individual was a seafaring man; rarely did members of the general public adorn themselves with tattoos.\"\n\n\"In the late eighteenth and early nineteenth centuries, tattoos were as much about self-expression as they were about having a unique way to identify a sailor's body should he be lost at sea or impressed by the British navy. The best source for early American tattoos is the protection papers issued following a 1796 congressional act to safeguard American seamen from impressment. These proto-passports catalogued tattoos alongside birthmarks, scars, race, and height. Using simple techniques and tools, tattoo artists in the early republic typically worked on board ships using anything available as pigments, even gunpowder and urine. Men marked their arms and hands with initials of themselves and loved ones, significant dates, symbols of the seafaring life, liberty poles, crucifixes, and other symbols.\"\n\nBecause these protection papers were used to define freemen and citizenship, many black sailors and other men also used them to show that they were freemen if they were stopped by officials or slave catchers. They also called them \"free papers\" because they certified their non-slave status. Many of the freed blacks used descriptions of tattoos for identification purposes on their freedom papers.\n\nTattooing involves the placement of pigment into the skin's dermis, the layer of dermal tissue underlying the epidermis. After initial injection, pigment is dispersed throughout a homogenized damaged layer down through the epidermis and upper dermis, in both of which the presence of foreign material activates the immune system's phagocytes to engulf the pigment particles. As healing proceeds, the damaged epidermis flakes away (eliminating surface pigment) while deeper in the skin granulation tissue forms, which is later converted to connective tissue by collagen growth. This mends the upper dermis, where pigment remains trapped within fibroblasts, ultimately concentrating in a layer just below the dermis/epidermis boundary. Its presence there is stable, but in the long term (decades) the pigment tends to migrate deeper into the dermis, accounting for the degraded detail of old tattoos.\nSome tribal cultures traditionally created tattoos by cutting designs into the skin and rubbing the resulting wound with ink, ashes or other agents; some cultures continue this practice, which may be an adjunct to scarification. Some cultures create tattooed marks by hand-tapping the ink into the skin using sharpened sticks or animal bones (made like needles) with clay formed disks or, in modern times, needles.\n\nThe most common method of tattooing in modern times is the electric tattoo machine, which inserts ink into the skin via a single needle or a group of needles that are soldered onto a bar, which is attached to an oscillating unit. The unit rapidly and repeatedly drives the needles in and out of the skin, usually 80 to 150 times a second. The needles are single-use needles that come packaged individually.\n\nTattooing is regulated in many countries because of the associated health risks to client and practitioner, specifically local infections and virus transmission. Disposable plastic aprons and eye protection can be worn depending on the risk of blood or other secretions splashing into the eyes or clothing of the tattoist. Hand hygiene, assessment of risks and appropriate disposal of all sharp objects and materials contaminated with blood are crucial areas. The tattoo artist must wash his or her hands and must also wash the area that will be tattooed. Gloves must be worn at all times and the wound must be wiped frequently with a wet disposable towel of some kind. All equipment must be sterilized in a certified autoclave before and after every use. It is good practice to provide clients with a printed consent form that outlines risks and complications as well as instructions for after care.\n\nThe Government of Meiji Japan had outlawed tattoos in the 19th century, a prohibition that stood for 70 years before being repealed in 1948. As of 6 June 2012 all new tattoos are forbidden for employees of the city of Osaka. Existing tattoos are required to be covered with proper clothing. The regulations were added to Osaka's ethical codes, and employees with tattoos were encouraged to have them removed. This was done because of the strong connection of tattoos with the yakuza, or Japanese organized crime, after an Osaka official in February 2012 threatened a schoolchild by showing his tattoo.\n\nTattoos had negative connotations in historical China, where criminals often had been marked by tattooing. The association of tattoos with criminals was transmitted from China to influence Japan. Today, tattoos have remained a taboo in Chinese society.\n\nThe Romans tattooed criminals and slaves, and in the 19th century released US convicts, Australian convicts, and British army deserters were identified by tattoos. Prisoners in Nazi concentration camps were tattooed with an identification number. Today, many prison inmates still tattoo themselves as an indication of time spent in prison.\n\nNative Americans also used tattoos to represent their tribe. Catholic Croats of Bosnia used religious Christian tattooing, especially of children and women, for protection against conversion to Islam during the Ottoman rule in the Balkans.\n\nTattoos are strongly empirically associated with deviance, personality disorders, and criminality. Although the general acceptance of tattoos is on the rise in Western society, they still carry a heavy stigma among certain social groups. Tattoos are generally considered an important part of the culture of the Russian mafia.\n\nCurrent cultural understandings of tattoos in Europe and North America have been greatly influenced by long-standing stereotypes based on deviant social groups in the 19th and 20th centuries. Particularly in North America tattoos have been associated with stereotypes, folklore, and racism. Not until the 1960s and 1970s did people associate tattoos with such societal outcasts as bikers and prisoners. Today, in the United States many prisoners and criminal gangs use distinctive tattoos to indicate facts about their criminal behavior, prison sentences, and organizational affiliation. A teardrop tattoo, for example, can be symbolic of murder, or each tear represents the death of a friend. At the same time, members of the U.S. military have an equally well-established and longstanding history of tattooing to indicate military units, battles, kills, etc., an association that remains widespread among older Americans. In Japan tattoos are associated with yakuza criminal groups but there are non-yakuza groups such as Fukushi Masaichi's tattoo association that sought to preserve the skins of dead Japanese who have extensive tattoos. Tattooing is also common in the British Armed Forces. Depending on vocation tattoos are accepted in a number of professions in America. Companies across many fields are increasingly focused on diversity and inclusion.\n\nIn Britain, there is evidence of women with tattoos, concealed by their clothing, throughout the 20th century, and records of women tattoists such as Jessie Knight from the 1920s. A study of \"at-risk\" (as defined by school absenteeism and truancy) adolescent girls showed a positive correlation between body modification and negative feelings towards the body and low self-esteem; however, the study also demonstrated that a strong motive for body modification is the search for \"self and attempts to attain mastery and control over the body in an age of increasing alienation\". The prevalence of women in the tattoo industry in the 21st century, along with larger numbers of women bearing tattoos, appears to be changing negative perceptions.\n\nFormer sailor Rowland Hussey Macy, who formed Macy's department stores, used a red star tattoo that he had on his hand for the store's logo.\n\nTattoos have also been used in marketing and advertising with companies paying people to have logos of brands like HBO, Red Bull, ASOS.com, and Sailor Jerry's rum tattooed in their bodies. This practice is known as \"skinvertising\".\n\nB.T.'s Smokehouse, a barbecue restaurant located in Massachusetts, offered customers free meals for life if they had the logo of the establishment tattooed on a visible part of their bodies. Nine people took the business up on the offer.\n\nBecause it requires breaking the skin barrier, tattooing carries health risks including infection and allergic reactions. Tattooing can be uncomfortable to excruciating depending on the area and can result in the person fainting. Modern tattooists reduce risks by following universal precautions working with single-use items and sterilizing their equipment after each use. Many jurisdictions require that tattooists have blood-borne pathogen training such as that provided through the Red Cross and OSHA. As of 2009 (in the United States) there have been no reported cases of HIV contracted from tattoos.\n\nIn amateur tattooing, such as that practiced in prisons, however, there is an elevated risk of infection. Infections that can theoretically be transmitted by the use of unsterilized tattoo equipment or contaminated ink include surface infections of the skin, fungal infections, some forms of hepatitis, herpes simplex virus, HIV, staph, tetanus, and tuberculosis.\n\nTattoo inks have been described as \"remarkably nonreactive histologically\". However, cases of allergic reactions to tattoo inks, particularly certain colors, have been medically documented. This is sometimes due to the presence of nickel in an ink pigment, which triggers a common metal allergy. Occasionally, when a blood vessel is punctured during the tattooing procedure, a bruise/hematoma may appear.\n\nCertain colours - red or similar colours such as purple, pink, and orange - tend to cause more problems and damage compared to other colours. Red ink has even caused skin and flesh damages so severe that the amputation of a leg or an arm has been necessary. If part of a tattoo (especially if red) begins to cause even minor troubles, like becoming itchy or worse, lumpy, then Danish experts strongly suggest to remove the red parts.\n\nIn 2017, researchers from the European Synchrotron Radiation Facility in France say the chemicals in tattoo ink can travel in the bloodstream and accumulate in the lymph nodes, obstructing their ability to fight infections. However, the authors noted in their paper that most tattooed individuals including the donors analyzed do not suffer from chronic inflammation.\n\nWhile tattoos are considered permanent, it is sometimes possible to remove them, fully or partially, with laser treatments. Typically, black and some colored inks can be removed more completely than inks of other colors. The expense and pain associated with removing tattoos are typically greater than the expense and pain associated with applying them. Pre-laser tattoo removal methods include dermabrasion, salabrasion (scrubbing the skin with salt), cryosurgery, and excision—which is sometimes still used along with skin grafts for larger tattoos. These older methods, however, have been nearly completely replaced by laser removal treatment options.\n\nA temporary tattoo is a non-permanent image on the skin resembling a permanent tattoo. Temporary tattoos can be drawn, painted, airbrushed or needled as a permanent tattoo with an ink which can be dissolved in blood within 6 months of art as a form of body painting.\n\nDecal (press-on) temporary tattoos are used to decorate any part of the body. They may last for a day or for more than a week.\n\nFoil temporary tattoos are a variation of decal-style temporary tattoos, printed using a foil stamping technique instead of using ink. The foil design is printed as a mirror image in order to be viewed in the right direction once it is applied to the skin. Each metallic tattoo is protected by a transparent protective film.\n\nAlthough they have become more popular and usually require a greater investment, airbrush temporary tattoos are less likely to achieve the look of a permanent tattoo, and may not last as long as press-on temporary tattoos. An artist sprays on airbrush tattoos using a stencil with alcohol-based, FDA-approved cosmetic inks. Like decal tattoos, airbrush temporary tattoos also are easily removed with rubbing alcohol or baby oil.\n\nAnother tattoo alternative is henna-based tattoos, which generally contain no additives. Henna is a plant-derived substance which is painted on the skin, staining it a reddish-orange-to-brown color. Because of the semi-permanent nature of henna, they lack the realistic colors typical of decal temporary tattoos. Due to the time-consuming application process, it is a relatively poor option for children. If you do choose henna temporary tattoos, ensure that they are pure henna. Dermatological publications report that allergic reactions to natural henna are very rare and the product is generally considered safe for skin application. Serious problems can occur, however, from the use of henna with certain additives. The FDA and medical journals report that painted black henna temporary tattoos are especially dangerous. Black Henna or Pre-Mixed Henna Temporary Tattoos May Be Harmful - see below for safety information.\n\nDecal temporary tattoos, when legally sold in the United States, have had their color additives approved by the U.S. Food and Drug Administration (FDA) as cosmetics --- the FDA has determined these colorants are safe for “direct dermal contact.” While the FDA has received some accounts of minor skin irritation, including redness and swelling, from this type of temporary tattoo, the agency has found these symptoms to be “child specific” and not significant enough to support warnings to the public. Unapproved pigments, however, which are sometimes used by non-US manufacturers, can provoke allergic reactions in anyone. Understanding the types of temporary tattoos available to consumers, knowing where they are manufactured, and ensuring they come from a reliable source are keys to determining whether temporary tattoos are safe.\n\nThe types of airbrush paints manufactured for crafting, creating art or decorating clothing should never be used for tattooing. These paints are not approved for direct contact with skin, and can be allergenic or toxic. Always ask the airbrush tattoo artist what kind of ink he or she is using and whether it meets FDA approval.\n\nThe FDA regularly issues warnings to consumers about avoiding any temporary tattoos labeled as black henna or pre-mixed henna as these may contain potentially harmful ingredients including silver nitrate, carmine, pyrogallol, disperse orange dye and chromium. Black henna gets its color from paraphenylenediamine (PPD), a textile dye approved by the FDA for human use only in hair coloring. In Canada, the use of PPD on the skin, including hair dye, is banned. Research has linked these and other ingredients to a range of health problems including allergic reactions, chronic inflammatory reactions, and late-onset allergic reactions to related clothing and hairdressing dyes. They can cause these reactions long after application. Neither black henna nor pre-mixed henna are approved for cosmetic use by the FDA.\n\nJudaism generally prohibits tattoos among its adherents based on the commandments in Leviticus 19. Jews tend to believe this commandment only applies to Jews and not to gentiles.\nThere is no specific rule in the New Testament prohibiting tattoos and most Christian denominations believe the laws in Leviticus are outdated as well as believing the commandment only applied to the Israelites, not to the gentiles. While most Christian groups tolerate tattoos, some Evangelical and fundamentalist Protestant denominations do believe the commandment does apply today for Christians and believe it is a sin to get one.\n\nMany Coptic Christians in Egypt take a cross tattoo in their right wrist to differ from the Muslims.\n\nTattoos are considered to be haram in Sunni Islam, based on rulings from scholars and passages in the Hadith. Shia Islam does not entirely prohibit tattooing, although it may be looked down upon in Shia communities.\n\nAnthropological\n\nPopular and artistic\n\nMedical\n\n",
                "History of tattooing\n\nTattooing has been practiced across the globe since at least Neolithic times, as evidenced by mummified preserved skin, ancient art, and the archaeological record. Both ancient art and archaeological finds of possible tattoo tools suggest tattooing was practiced by the Upper Paleolithic period in Europe. However, direct evidence for tattooing on mummified human skin extends only to the 4th millennium BC. The oldest discovery of tattooed human skin to date is found on the body of Ötzi the Iceman, dating to between 3370 and 3100 BC. Other tattooed mummies have been recovered from at least 49 archaeological sites including locations in Greenland, Alaska, Siberia, Mongolia, western China, Egypt, Sudan, the Philippines, and the Andes. These include Amunet, Priestess of the Goddess Hathor from ancient Egypt (c. 2134–1991 BC), multiple mummies from Siberia including the Pazyryk culture of Russia, and from several cultures throughout pre-Columbian South America.\n\nCemeteries throughout the Tarim Basin (Xinjiang of western China) including the sites of Qäwrighul, Yanghai, Shengjindian, Zaghunluq, and Qizilchoqa have revealed several tattooed mummies with Western Asian/Indo-European physical traits and cultural materials. These date from between 2100 and 550 BC.\n\nIn ancient China, tattoos were considered a barbaric practice, and were often referred to in literature depicting bandits and folk heroes. As late as the Qing Dynasty, it was common practice to tattoo characters such as (\"Prisoner\") on convicted criminals' faces. Although relatively rare during most periods of Chinese history, slaves were also sometimes marked to display ownership.\n\nHowever, tattoos seem to have remained a part of southern culture. Marco Polo wrote of Quanzhou, \"Many come hither from Upper India to have their bodies painted with the needle in the way we have elsewhere described, there being many adepts at this craft in the city\". At least three of the main characters Lu Zhishen, Shi Jin (史進), and Yan Ching (燕青) in the classic novel \"Water Margin\" are described as having tattoos covering nearly all of their bodies. Wu Song was sentenced to a facial tattoo describing his crime after killing Xi Menqing (西门庆) to avenge his brother. In addition, Chinese legend claimed the mother of Yue Fei (a famous Song general) tattooed the words \"Repay the Country with Pure Loyalty\" (, \"jing zhong bao guo\") down her son's back before he left to join the army.\n\nGreek written records of tattooing date back to at least the 5th-century BCE. The ancient Greeks and Romans used tattooing to penalize slaves, criminals, and prisoners of war. While known, decorative tattooing was looked down upon and religious tattooing was mainly practiced in Egypt and Syria. According to Robert Graves in his book \"The Greek Myths\", tattooing was common amongst certain religious groups in the ancient Mediterranean world, which may have contributed to the prohibition of tattooing in Leviticus. The Romans of Late Antiquity also tattooed soldiers and arms manufacturers, a practice that continued into the ninth century.\n\nThe Greek verb \"stizein\" (στίζειν), meaning \"to prick,\" was used for tattooing. Its derivative \"stigma\" (στίγμα) was the common term for tattoo marks in both Greek and Latin. During the Byzantine period, the verb \"kentein\" (κεντεῖν) replaced \"stizein\", and a variety of new Latin terms replaced \"stigmata\" including \"signa\" \"signs,\" \"characteres\" \"stamps,\" and \"cicatrices\" \"scars.\"\n\nIn Southern India, permanent tattoos are called \"pachakutharathu\". It was very common in South India, especially Tamil Nadu, before 1980. In northern India, permanent tattoos are called \"godna\". Tattoos have been used as cultural symbols among many tribal populations, as well as the general Hindu population of India.\n\nIn India, tattoos have many names, including \"tarazwa\", \"gondan\", and \"ungkala\".\n\nSeveral Indonesian tribes have in their tattooing culture. One notable example is the Dayak people of Kalimantan in Borneo (Bornean traditional tattooing). Another ethnic group that practices tattooing are the Mentawai people, as well as Moi and Meyakh people in West Papua.\n\nTattooing for spiritual and decorative purposes in Japan is thought to extend back to at least the Jōmon or Paleolithic period and was widespread during various periods for both the Japanese and the native Ainu. Chinese texts from before 300 AD described social differences among Japanese people as being indicated through tattooing and other bodily marking. Chinese texts from the time also described Japanese men of all ages as decorating their faces and bodies with tattoos.\n\nBetween 1603 and 1868, Japanese tattooing was only practiced by the \"ukiyo\" (floating world) subculture. Generally firemen, manual workers and prostitutes wore tattoos to communicate their status. By the early 17th century, criminals were widely being tattooed as a visible mark of punishment. Criminals were marked with symbols typically including crosses, lines, double lines and circles on certain parts of the body, mostly the face and arms. These symbols sometimes designated the places where the crimes were committed. In one area, the character for \"dog\" was tattooed on the criminal's forehead.\n\nThe Government of Meiji Japan, formed in 1868, banned the art of tattooing altogether, viewing it as barbaric and lacking respectability. This subsequently created a subculture of criminals and outcasts. These people had no place in \"decent society\" and were frowned upon. They could not simply integrate into mainstream society because of their obvious visible tattoos, forcing many of them into criminal activities which ultimately formed the roots for the modern Japanese mafia, the Yakuza, with which tattoos have become almost synonymous in Japan.\n\nDespite a lack of direct textual references, tattooed human remains and iconographic evidence indicate that ancient Egyptians practiced tattooing from at least 2000 BCE. It is theorized that tattooing entered Egypt through Nubia, but this claim is complicated by the high mobility between Lower Nubia and Upper Egypt as well as Egypt's annexation of Lower Nubia during the Middle Kingdom. Archeologist Geoffrey J. Tassie argues that it may be more appropriate to classify tattoo in ancient Egypt and Nubia as part of a larger Nile Valley tradition.\n\nThe most famous tattooed mummies from this region are Amunet, a priestess of Hathor, and two Hathoric dancers from Dynasty XI that were found at Deir el-Bahari. In 1898, Daniel Fouquet, a medical doctor from Cairo, wrote an article on medical tattooing practices in ancient Egypt in which he describes the tattoos on these three mummies and speculates that they may have served a medicinal or therapeutic purpose: \"The examination of these scars, some white, others blue, leaves in no doubt that they are not, in essence, ornament, but an established treatment for a condition of the pelvis, very probably chronic pelvic peritonitis.\"\n\nAncient Egyptian tattooing appears to have been practiced on women exclusively; with the possible exception of one extremely worn Dynasty XII stele, there is no artistic or physical evidence that men were tattooed. However, by the Meroitic Period (300 BCE – 400 CE) it was practiced on Nubian men as well.\n\nAccounts of early travelers to ancient Egypt describe the tool used as an uneven number of metal needles attached to a wooden handle.\n\nCoptic tattoos often consist of three lines, three dots and two elements, reflecting the Trinity. The tools used had an odd the number of needles to bring luck and good fortune. Many Copts have the Coptic cross tattooed on the inside of their right arm. This may have been influenced by a similar practice tattooing religious symbols on the wrists and arms during the Ptolemaic Period.\n\nHerodotus' writings suggest that slaves and prisoners of war were tattooed in Persia during the classical era. This practice spread from Persia to Greece and then to Rome.\n\nThe most famous depiction of tattooing in Persian literature goes back 800 years to a tale by Rumi about a man who is proud to want a lion tattoo but changes his mind once he experiences the pain of the needle.\n\nIn the \"hamam\" (the baths), there were \"dallaks\" whose job was to help people wash themselves. This was a notable occupation because apart from helping the customers with washing, they were massage-therapists, dentists, barbers, and tattoo artists.\n\nTattooing has been a part of Filipino life since pre-Hispanic colonization of the Philippine Islands, tattooing in the Philippines to some were a form of rank and accomplishments, and some believed that tattoos had magical qualities. The more famous tattooed indigenous peoples of the Philippines resided in north Luzon, especially among the Bontoc Igorot, Kalinga, and Ifugao peoples. The Visayans of the southern islands were also heavily tattooed.\n\nFilipino tattooing was first documented by the European Spanish explorers as they landed among the Islands in the late 16th century, and called the natives \"Los Pintados\" (The Painted Ones) as they mistook the tattoos for paint. Before European exploration, tattooing was widespread, but conversion to Christianity greatly diminished the practice as heathen or low-class.\n\nAs Lane Wilcken's \"Filipino Tattoos Ancient to Modern\" denotes, there are many similarities between the tattooing traditions of the Philippines and indigenous Polynesian designs--not only with their societal function and similar designs, but in the tools used to hand-tap them on (generally a needle or thorn on a stick, with a hammer to pound it into the skin). While the most common modern term for indigenous tattoos is \"batok,\" an ancient Tagalog word for tattoos was \"tatak,\" extremely similar to the Samoan word \"tatau\".\n\nThe Māori people of New Zealand practised a form of tattooing known as Tā moko, traditionally created with chisels. In the colonial period Tā moko fell out of use, partly because of the European practice of collecting Mokomokai, or tattooed heads.\n\nHowever, from the late 20th century onward, there has been a resurgence of tā moko amongst Maori. There is also a related tattoo art, kirituhi, which has a similar aesthetic to tā moko but is worn by non-Maori.\n\nThe traditional male tattoo in Samoa is called the pe'a. The traditional female tattoo is called the malu. The word \"tattoo\" is believed to have originated from the Samoan word \"tatau.\" \n\nWhen the Samoan Islands were first seen by Europeans in 1722 three Dutch ships commanded by Jacob Roggeveen visited the eastern island known as Manua. A crew member of one of the ships described the natives in these words, \"They are friendly in their speech and courteous in their behavior, with no apparent trace of wildness or savagery. They do not paint themselves, as do the natives of some other islands, but on the lower part of the body they wear artfully woven silk tights or knee breeches. They are altogether the most charming and polite natives we have seen in all of the South Seas...\"\n\nThe ships lay at anchor off the islands for several days, but the crews did not venture ashore and did not even get close enough to the natives to realize that they were not wearing silk leggings, but their legs were completely covered in tattoos.\n\nIn Samoa, the tradition of applying tattoo, or tatau, by hand has been unbroken for over two thousand years. Tools and techniques have changed little. The skill is often passed from father to son, each tattoo artist, or tufuga, learning the craft over many years of serving as his father's apprentice. A young artist-in-training often spent hours, and sometimes days, tapping designs into sand or tree bark using a special tattooing comb, or au. Honoring their tradition, Samoan tattoo artists made this tool from sharpened boar's teeth fastened together with a portion of the turtle shell and to a wooden handle. \n\nTraditional Samoan tattooing of the \"pe'a\", body tattoo, is an ordeal that is not lightly undergone. It takes many weeks to complete. The process is very painful and used to be a necessary prerequisite to receiving a matai title; this however is no longer the case. Tattooing was also a very costly procedure.\n\nIt was not uncommon for half a dozen boys to be tattooed at the same time, requiring the services of four or more artists. It was not just the men who received tattoos, but the women too; their designs are of a much lighter nature rather than having the large areas of solid dye which are frequently seen in men’s tattoos. The tattooing of women was not nearly as ritualized like men’s were.\n\nSamoan society has long been defined by rank and title, with chiefs (ali'i) and their assistants, known as talking chiefs (tulafale). The tattooing ceremonies for young chiefs, typically conducted at the time of puberty, were part of their ascendance to a leadership role. The permanent marks left by the tattoo artists would forever celebrate their endurance and dedication to cultural traditions. The pain was extreme and the risk of death by infection was a concern; to back down from tattooing was to risk being labeled a \"pala'ai\" or coward. Those who could not endure the pain and abandoned their tattooing were left incomplete, would be forced to wear their mark of shame throughout their life. This would forever bring shame upon their family so it was avoided at all cost. \n\nThe Samoan tattooing process used a number of tools which remained almost unchanged since their first use. \"Autapulu\" is a wide tattooing comb used to fill in the large dark areas of the tattoo. \"Ausogi'aso tele\" is a comb used for making thick lines. \"Ausogi'aso laititi\" is a comb used for making thin lines. \"Aumogo\" small comb is used for making small marks. \"Sausau\" is the mallet is used for striking the combs. It is almost two feet in length and made from the central rib of a coconut palm leaf. \"Tuluma\" is the pot used for holding the tattooing combs. Ipulama is the cup used for holding the dye. The dye is made from the soot collected from burnt lama nuts. \"Tu'I\" used to grind up the dye. These tools were primarily made out of animal bones to ensure sharpness.\n\nThe tattooing process itself would be 5 sessions, in theory. These 5 sessions would be spread out over 10 days in order for the inflammation to subside.\n\nChristian missionaries from the west attempted to purge tattooing among the Samoans, thinking it barbaric and inhumane. Many young Samoans resisted mission schools since they forbade them to wear tattoos. But over time attitudes relaxed toward this cultural tradition and tattooing began to reemerge in Samoan culture.\n\nTattooed mummies dating to c. 500 BC were extracted from burial mounds on the Ukok plateau during the 1990s. Their tattooing involved animal designs carried out in a curvilinear style. The Man of Pazyryk, a Scythian chieftain, is tattooed with an extensive and detailed range of fish, monsters and a series of dots that lined up along the spinal column (lumbar region) and around the right ankle.\n\nSome artifacts dating back 3,000 years from the Solomon Islands may have been used for tattooing human skin. Obsidian pieces have been duplicated, then used to conduct tattoos on pig skin, then compared to the original artifacts. \"They conducted these experiments to observe the wear, such as chipping and scratches, and residues on the stones caused by tattooing, and then compared that use-wear with 3,000 year old artifacts. They found that the obsidian pieces, old and new, show similar patterns, suggesting that they hadn't been used for working hides, but were for adorning human skin.\"\n\nIn Taiwan, facial tattoos of the Atayal tribe are called \"ptasan\"; they are used to demonstrate that an adult man can protect his homeland, and that an adult woman is qualified to weave cloth and perform housekeeping.\nTaiwan is believed to be the homeland of all the Austronesian peoples which includes Filipinos, Indonesians, Polynesians and Malagasy peoples, all with strong tattoo traditions. This along with the striking correlation between Austronesian languages and the use of the so-called hand-tapping method suggests that Austronesian peoples inherited their tattooing traditions from their ancestors established in Taiwan or along the southern coast of the Chinese mainland.\n\nThai tattoos also known as Yantra tattooing was common since ancient times. Just as other native southeast Asian cultures, animistic tattooing was common in Tai tribes that were is southern China. Over time this animistic practice of tattooing for luck and protection assimilated Hindu and Buddhist ideas. The Sak Yant traditional tattoo is practiced today by many and are usually given either by a Buddhist monk or a Brahmin priest. The tattoos usually depict Hindu gods and use the Mon script or ancient Khmer script, which were the scripts of the classical civilizations of mainland southeast Asia.\n\nThe earliest possible evidence for tattooing in Europe appears on ancient art from the Upper Paleolithic period as incised designs on the bodies of humanoid figurines. The Löwenmensch figurine from the Aurignacian culture dates to approximately 40,000 years ago and features a series of parallel lines on its left shoulder. The ivory Venus of Hohle Fels, which dates to between 35,000 and 40,000 years ago also exhibits incised lines down both arms, as well as across the torso and chest.\n\nThe oldest and most famous direct proof of ancient European tattooing appears on the body of Ötzi the Iceman, who was found in the Ötz valley in the Alps and dates from the late 4th millennium BC. Studies have revealed that Ötzi had 61 carbon-ink tattoos consisting of 19 groups of lines simple dots and lines on his lower spine, left wrist, behind his right knee, and on his ankles. It has been argued that these tattoos were a form of healing because of their placement, though other explanations are plausible.\n\nPre-Christian Germanic, Celtic and other central and northern European tribes were often heavily tattooed, according to surviving accounts, but it may also have been normal paint. The Picts may have been tattooed (or scarified) with elaborate, war-inspired black or dark blue woad (or possibly copper for the blue tone) designs. Julius Caesar described these tattoos in Book V of his \"Gallic Wars\" (54 BC). Nevertheless, these may have been painted markings rather than tattoos.\n\nAhmad ibn Fadlan wrote of his encounter with the Scandinavian Rus' tribe in the early 10th century, describing them as tattooed from \"fingernails to neck\" with dark blue \"tree patterns\" and other \"figures.\" However, this may also have been paint, since the word used can mean both tattoo and painting. During the gradual process of Christianization in Europe, tattoos were often considered remaining elements of paganism and generally legally prohibited.\n\nThe significance of tattooing was long open to Eurocentric interpretations. In the mid-19th century, Baron Haussmann, while arguing against painting the interior of Parisian churches, said the practice \"reminds me of the tattoos used in place of clothes by barbarous peoples to conceal their nakedness\".\n\nBritish and other pilgrims to the Holy Lands throughout the 17th century were tattooed to commemorate their voyages, including William Lithgow in 1612. \n\nIn 1691 William Dampier brought to London a native of the western part of New Guinea (now part of Indonesia) who had a tattooed body and became known as the \"Painted Prince\".\n\nBetween 1766 and 1779, Captain James Cook made three voyages to the South Pacific, the last trip ending with Cook's death in Hawaii in February 1779. When Cook and his men returned home to Europe from their voyages to Polynesia, they told tales of the 'tattooed savages' they had seen. The word \"tattoo\" itself comes from the Tahitian \"tatau\", and was introduced into the English language by Cook's expedition (though the word 'tattoo' or 'tap-too', referring to a drumbeat, had existed in English since at least 1644)\n\nIt was in Tahiti aboard the Endeavour, in July 1769, that Cook first noted his observations about the indigenous body modification and is the first recorded use of the word tattoo to refer to the permanent marking of the skin. In the Ship's log book recorded this entry: \"Both sexes paint their Bodys, Tattow, as it is called in their Language. This is done by inlaying the Colour of Black under their skins, in such a manner as to be indelible.\" Cook went on to write, \"This method of Tattowing I shall now describe...As this is a painful operation, especially the Tattowing of their Buttocks, it is performed but once in their Lifetimes.\"\n\nCook's Science Officer and Expedition Botanist, Sir Joseph Banks, returned to England with a tattoo. Banks was a highly regarded member of the English aristocracy and had acquired his position with Cook by putting up what was at the time the princely sum of some ten thousand pounds in the expedition. In turn, Cook brought back with him a tattooed Raiatean man, Omai, whom he presented to King George and the English Court. Many of Cook's men, ordinary seamen and sailors, came back with tattoos, a tradition that would soon become associated with men of the sea in the public's mind and the press of the day. In the process sailors and seamen re-introduced the practice of tattooing in Europe and it spread rapidly to seaports around the globe.\n\nBy the 19th century tattooing had spread to British society but was still largely associated with sailors and the lower or even criminal class. Tattooing had however been practised in an amateur way by public schoolboys from at least the 1840s and by the 1870s had become fashionable among some members of the upper classes, including royalty. In its upmarket form it could be a lengthy, expensive and sometimes painful process.\n\nTattooing spread among the upper classes all over Europe in the 19th century, but particularly in Britain where it was estimated in Harmsworth Magazine in 1898 that as many as one in five members of the gentry were tattooed. Taking their lead from the British Court, where George V followed Edward VII's lead in getting tattooed; King Frederick IX of Denmark, the King of Romania, Kaiser Wilhelm II, King Alexander of Yugoslavia and even Tsar Nicholas II of Russia, all sported tattoos, many of them elaborate and ornate renditions of the Royal Coat of Arms or the Royal Family Crest. King Alfonso XIII of modern Spain also had a tattoo.\n\nThe perception that there is a marked class division on the acceptability of the practice has been a popular media theme in Britain, as successive generations of journalists described the practice as newly fashionable and no longer for a marginalised class. Examples of this cliché can be found in every decade since the 1870s. Despite this evidence, a myth persists that the upper and lower classes find tattooing attractive and the broader middle classes rejecting it. In 1969, the House of Lords debated a bill to ban the tattooing of minors, on grounds it had become \"trendy\" with the young in recent years but was associated with crime. It was noted that 40 per cent of young criminals had tattoos and that marking the skin in this way tended to encourage self-identification with criminal groups. Two peers, Lord Teynham and the Marquess of Aberdeen and Temair however rose to object that they had been tattooed as youngsters, with no ill effects. Since the 1970s tattoos have become more socially acceptable, and fashionable among celebrities. Tattoos are less prominent on figures of authority, and the practice of tattooing by the elderly is still considered remarkable.\n\nThe Jesuit Relations of 1652 describes tattooing among the Petun and the Neutrals:\nIn the period shortly after the American Revolution, to avoid impressment by British Navy ships, sailors used government issued protection papers to establish their American citizenship. However, many of the descriptions of the individual described in the seamen's protection certificates were so general, and it was so easy to abuse the system, that many impressment officers of the Royal Navy simply paid no attention to them. \"In applying for a duplicate Seaman's Protection Certificate in 1817, James Francis stated that he 'had a protection granted him by the Collector of this Port on or about 12 March 1806 which was torn up and destroyed by a British Captain when at sea.'\"\n\nOne way of making them more specific and more effective was to describe a tattoo, which is highly personal as to subject and location, and thus use that description to precisely identify the seaman. As a result, many of the official certificates also carried information about tattoos and scars, as well as any other specific identifying information. This also perhaps led to an increase and proliferation of tattoos among American seamen who wanted to avoid impressment. During this period, tattoos were not popular with the rest of the country. \"Frequently the \"protection papers\" made reference to tattoos, clear evidence that individual was a seafaring man; rarely did members of the general public adorn themselves with tattoos.\"\n\n\"In the late eighteenth and early nineteenth centuries, tattoos were as much about self-expression as they were about having a unique way to identify a sailor's body should he be lost at sea or impressed by the British navy. The best source for early American tattoos is the protection papers issued following a 1796 congressional act to safeguard American seamen from impressment. These proto-passports catalogued tattoos alongside birthmarks, scars, race, and height. Using simple techniques and tools, tattoo artists in the early republic typically worked on board ships using anything available as pigments, even gunpowder and urine. Men marked their arms and hands with initials of themselves and loved ones, significant dates, symbols of the seafaring life, liberty poles, crucifixes, and other symbols.\"\n\nSometimes, to protect themselves, the sailors requested not only that the tattoos be described, but that they would also be sketched out on the protection certificate as well. As one researched said, \"Clerks writing the documents often sketched the tattoos as well as describing them.\"\n\nSince the 1950s, a false belief has persisted that modern Western tattooing originated exclusively from Captain James Cook’s voyages to the South Pacific in the 1770s. Tattooing has been consistently present in Western society from the modern period stretching back to Ancient Greece. A long history of European tattoo predated these voyages, including among sailors and tradesmen, pilgrims visiting the Holy Land, and on Europeans living among Native Americans.\n\nTattoo historian Anna Felicity Friedman suggests a couple reasons for the \"Cook Myth.\" First, modern European words for the practice (e.g., “tattoo,” “tatuaje,” “tatouage,” “Tätowierung,” and “tatuagem”) derive from the Tahitian word “tatau,” which was introduced to European languages through Cook’s travels. However, prior European texts show that a variety of metaphorical terms were used for the practice, including “pricked,” “marked”, “engraved,” “decorated,” “punctured,” “stained,” and “embroidered.” Friedman also points out that the growing print culture at the time of Cook’s voyages may have increased the visibility of tattooing despite its prior existence in the West.\n\nThe first documented professional tattooer in the United States was Martin Hildebrandt, a German immigrant who arrived in Boston, Massachusetts in 1846. Between 1861 and 1865, he tattooed soldiers on both sides in the American Civil War. The first documented professional tattooist (with a permanent studio, working on members of the paying public) in Britain was Sutherland Macdonald in the early 1880s. Tattooing was an expensive and painful process, and by the late 1880s had become a mark of wealth for the crowned heads of Europe.\n\nIn 1891, New York tattooer Samuel O'Reilly patented the first electric tattoo machine, a modification of Thomas Edison's electric pen.\nThe earliest appearance of tattoos on women during this period were in the circus in the late 19th century. These \"Tattooed Ladies\" were covered — with the exception of their faces, hands, necks, and other readily visible areas — with various images inked into their skin. In order to lure the crowd, the earliest ladies, like Betty Broadbent and Nora Hildebrandt told tales of captivity; they usually claimed to have been taken hostage by Native Americans that tattooed them as a form of torture. However, by the late 1920s the sideshow industry was slowing and by the late 1990s the last tattooed lady was out of business.\n\nAlthough tattooing has steadily increased in popularity since the invention of the electric tattoo machine, it was not until the 1960s that the place of tattooing in popular culture radically shifted. The Tattoo Renaissance began in the late 1950s, and was greatly influenced by several artists in particular Lyle Tuttle, Cliff Raven, Don Nolan, Zeke Owens, Spider Webb, and Don Ed Hardy. A second generation of artists, trained by the first, continued these traditions into the 1970s, and included artists such as Bob Roberts, Jamie Summers, and Jack Rudy.\n\nSince the 1970s, tattoos have become a mainstream part of global and Western fashion, common among both sexes, to all economic classes, and to age groups from the later teen years to middle age. The decoration of blues singer Janis Joplin with a wrist let and a small heart on her left breast, by the San Francisco tattoo artist Lyle Tuttle, has been called a seminal moment in the popular acceptance of tattoos as art. Formal interest in the art of the tattoo became prominent in the 1970s through the beginning of the 21st century. For many young Americans, the tattoo has taken on a decidedly different meaning than for previous generations. The tattoo has \"undergone dramatic redefinition\" and has shifted from a form of deviance to an acceptable form of expression.\n\nIn 1988, scholar Arnold Rubin created a collection of works regarding the history of tattoo cultures, publishing them as the \"Marks of Civilization\". In this, the term \"Tattoo Renaissance\" was coined, referring to a period marked by technological, artistic, and social change. Wearers of tattoos, as members of the counterculture began to display their body art as signs of resistance to the values of the white, heterosexual, middle-class. The clientele changed from sailors, bikers, and gang members to the middle and upper class. There was also a shift in iconography from the badge-like images based on repetitive pre-made designs known as flash to customized full-body tattoo influenced by Polynesian and Japanese tattoo art, known as sleeves, which are categorized under the relatively new and popular Avant-garde genre. Tattooers transformed into \"Tattoo Artists\": men and women with fine art backgrounds began to enter the profession alongside the older, traditional tattooists.\n\nTattoos have experienced a resurgence in popularity in many parts of the world, particularly in Europe, Japan, and North and South America. The growth in tattoo culture has seen an influx of new artists into the industry, many of whom have technical and fine arts training. Coupled with advancements in tattoo pigments and the ongoing refinement of the equipment used for tattooing, this has led to an improvement in the quality of tattoos being produced.\n\nJennifer LeRoy (Miss February 1993) was the first Playboy Playmate with a visible tattoo on her centerfold.\n\nDuring the 2000s, the presence of tattoos became evident within pop culture, inspiring television shows such as A&E's \"Inked\" and TLC's \"Miami Ink\" and \"LA Ink\". In addition, many celebrities have made tattoos more acceptable in recent years.\n\nContemporary art exhibitions and visual art institutions have featured tattoos as art through such means as displaying tattoo flash, examining the works of tattoo artists, or otherwise incorporating examples of body art into mainstream exhibits. One such 2009 Chicago exhibition, \"Freaks & Flash\", featured both examples of historic body art as well as the tattoo artists who produced it.\n\nIn 2010, 25% of Australians under age 30 had tattoos. Mattel released a tattooed Barbie doll in 2011, which was widely accepted, although it did attract some controversy.\n\nOver the past three decades Western tattooing has become a practice that has crossed social boundaries from \"low\" to \"high\" class along with reshaping the power dynamics regarding gender. It has its roots in \"exotic\" tribal practices of the Native Americans and Japanese, which are still seen in present times.\n\nAs various kinds of social movements progressed bodily inscription crossed class boundaries, and became common among the general public. Specifically, the tattoo is one access point for revolutionary aesthetics of women. Feminist theory has much to say on the subject. \"Bodies of Subversion: A Secret History of Women and Tattoo\", by Margot Mifflin, became the first history of women's tattoo art when it was released in 1997. In it, she documents women's involvement in tattooing coinciding to feminist successes, with surges in the 1880s, 1920s, and the 1970s. Today, women sometimes use tattoos as forms of bodily reclamation after traumatic experiences like abuse or breast cancer. In 2012, tattooed women outnumbered men for the first time in American history - according to a Harris poll, 23% of women in America had tattoos in that year, compared to 19% of men. In 2013, Miss Kansas, Theresa Vail, became the first Miss America contestant to show off tattoos during the swimsuit competition — the insignia of the U.S. Army Dental Corps on her left shoulder and one of the \"Serenity Prayer\" along the right side of her torso.\n\nIn August 2013, William Mullane of London was tattooed with a portrait of his late father which used ink mixed with a small portion of his father’s ashes.\n\nThe legal status of tattoos is still developing. In recent years, various lawsuits have arisen in the United States regarding the status of tattoos as a copyrightable art form. However, these cases have either been settled out of court or are currently being disputed, and therefore no legal precedent exists directly on point. The process of tattooing was held to be a purely expressive activity protected by the First Amendment by the Ninth Circuit in 2010.\n\nThroughout the world's different military branches, tattoos are either regulated under policies or strictly prohibited to fit dress code rules.\n\nThe United States Air Force regulates all kinds of body modification. Any tattoos which are deemed to be \"prejudicial to good order and discipline\", or \"of a nature that may bring discredit upon the Air Force\" are prohibited. Specifically, any tattoo which may be construed as \"obscene or advocate sexual, racial, ethnic or religious discrimination\" is disallowed. Tattoo removal may not be enough to qualify; resultant \"excessive scarring\" may be disqualifying. Further, Air Force members may not have tattoos on their neck, face, head, tongue, lips or scalp.\n\nTattoos in the Army are regulated under the new Sergeant Major of the Army Daniel A. Dailey. Under its new rules tattoos are strictly prohibited on the arms and legs and anywhere above the collar bone line, tattoos that are visible on other parts of the body cannot be any longer than two inches wide, and only one band tattoo is permitted in the body. Soldiers who are currently in the army, and are affected under the new policy are grandfathered to keep the current tattoos under the validation of their commanders.\n\nThe United States Coast Guard policy has changes over the years.Tattoos should not be visible over the collarbone or when wearing a V-neck shirt. Tattoos or military brands on the arms should not surpass the wrist. But only one hand tattoos of a form of ring are permitted when not exceeding ¼ inch width. Face tattoos are also permitted as permanent eyeliners for females as long as they are appropriately worn and not brightly colored to fit uniform dressing code. Disrespectful derogatory tattoos and sexually explicit are prohibited on the body.\n\nThe United States Marine Corps has disclosed a new policy meeting their new standards of professionalism in the military appearance, on the Marine Corps Bulletin 1020 released on 6 February 2016, substituting any previous policy from the past\".\"\n\nThe new policy in the Marine Corps unauthorized tattoo’s in different parts of the body such as the wrist, knee, elbow and above the collar bone. Wrist tattoos have to be two inches above the wrist, elbow tattoos two inches above and one inch below, and the knee two inches above and two below.\n\n[[File:Tattooed sailor aboard the USS New Jersey.jpg|thumb|Tattooed sailor aboard the \n[[Category:Art history by medium|Tattoo]]\n[[Category:Tattooing]]",
                "Maya society\n\nMaya society concerns the social organization of the Pre-Hispanic Mayas, its political structures and social classes.\n\nA Classic period Maya polity was a small kingdom (\"ajawil\", \"ajawlel\", \"ajawlil\") headed by a hereditary ruler – \"ajaw\", later \"k’uhul ajaw\". Both terms appear in early Colonial texts including \"Papeles de Paxbolón\" where they are used as synonyms for Aztec and Spanish terms for rulers and their domains. These are \"tlatoani\" and \"tlahtocayotl\" in Nahuatl, and the Spanish words \"rey\", \"majestad\", and \"reino\" and \"señor\" for ruler/leader/lord and \"señorío\" or \"dominio\" of realm. Such kingdoms were usually no more than a capital city with its neighborhood and several dependent towns (similar to a city-state). There were also larger polities that controlled larger territories and subjugated smaller polities; the extensive systems controlled by Tikal and Caracol serve as examples of these.\n\nEach kingdom had its name that did not necessarily correspond to any locality within its territory. Its identity was that of a political unit associated with a particular ruling dynasty. For instance, the archaeological site of Naranjo was the capital of the kingdom of Saal. The land (\"chan ch’e’n\") of the kingdom and its capital were called \"Wakab’nal\" or \"Maxam\" and were part of a larger geographical entity known as \"Huk Tsuk\". Despite constant warfare and eventual shifts in regional power, most kingdoms never disappeared from the political landscape until the collapse of the whole system in the 9th century. In this respect, Classic Maya kingdoms were similar to late Postclassic polities encountered by the Spanish in Yucatán and Central Mexico: some polities were subordinate to hegemonic centers or rulers through conquest and/or dynastic unions and yet even then they persisted as distinct entities.\nMayanists have been increasingly accepting the \"court paradigm\" of Classic Maya societies that puts the emphasis on the centrality of the royal household and especially the person of the king. This approach focuses on the totality of Maya monumental spaces as the embodiment of the diverse activities of the royal household. It considers the role of places and spaces (including dwellings of royalty and nobles, throne rooms, temples, halls and plazas for public ceremonies) in establishing and negotiating power and social hierarchy, but also in producing and projecting aesthetic and moral values that define the order of a wider social realm.\n\nSpanish sources invariably describe even the largest Maya settlements of Yucatán and Guatemala as dispersed agglomerations of dwellings grouped around the temples and palaces of the ruling dynasty and lesser nobles. Though there was economic specialization among Classic period Maya centers (see Chunchucmil, for example), it was not conducted at a scale similar to that of the Aztec capital of Tenochtitlan. Some argue that Maya cities were not urban centers but were, instead, structured according to and conceptualized as enormous royal households, the locales of the administrative and ritual activities of the royal court. Within the theoretical framework of this model, they were the places where privileged nobles could approach the holy ruler, where aesthetical values of the high culture were formulated and disseminated, and where aesthetic items were consumed. They were the self-proclaimed centers and the sources of social, moral, and cosmic, order. The fall of a royal court as in the well-documented cases of Piedras Negras or Copán would cause the inevitable ‘death’ of the associated settlement.\n\nScribes held a prominent position in Maya courts and had their own patron deities (see Howler monkey gods and Maya maize god). They are likely to have come from aristocratic families. It appears that some scribes were attached to the royal house, while others were serving at temples and were, perhaps, counted among the priests. It seems likely that they were organized hierarchically. Maya art often depicts rulers with trappings indicating they were scribes or at least able to write, such as having pen bundles in their headdresses. Additionally, many rulers have been found in conjunction with writing tools such as shell or clay ink pots.\n\nAncient Maya kinship and descent have alternatively been described as patrilineal, matrilineal, and bilateral. Maya political organization has been characterized as both segmentary (involving well-defined lineages and clan-like structures) and centralized.\n\nTo the Maya, body modification is a reflection of a cultural, and individual identity. Through different modifications, the body can be experienced individually, used as a symbol, or as a political statement. Beauty was also used to outwardly show and perform social and moral values.\n\nPhysical remains of the Maya help piece together the motivation and significance for enduring vast amounts of pain, and using huge amounts of wealth to make themselves beautiful. Ancient Maya placed a high value on certain extreme body modifications, often undergoing tedious and painful procedures as a rite of passage, an homage to their gods, and as a permanently visible status symbol of their place in society that would last a lifetime, and into their afterlife. Therefore, there was aesthetic, religious, and social reasoning behind the modification. \n\nThe origin of cranial modification for the Maya is unknown, however it was possibly inherited from the Olmecs, predecessors of the Maya, located near the Tuxtla Mountains. Cranial modification was one of the Olmecs most important practices. Individuals enduring cranial modification could be of any status, but many more elite individuals were depicted with cranial modifications. Intentional deformation practices were used as a way to differentiate between members of the society. All members of an elite family were expected to go through cranial modification shortly after birth. The procedure occurred while the skull of the child was not fully grown and had some plasticity making it easily malleable. \n\nThere are as many as 14 different cranial shapes caused by several different types of purposeful modification or deformation techniques used by members of the Maya society. Neonatal deformation was performed in two main ways: compression of the head with pads and adjusted bindings, or restraining the child on specially designed cradles. Often, a binding device was attached to the forehead so instead of growing naturally into a round or circular form, the child’s cranium grew into a long, and tapered form which indented above the brow line. These different modifications resulted in an abundant amount of stress on the new child’s body, and often led to death.\n\n The two main head shapes of cranial modification for the Maya were erect deformation and oblique deformation. Erect deformation was modification through the use of cradle boards, which often left the occipital flattening asymmetrical, and affected a child’s mobility. Oblique deformation was modification through the use of a paddle applied to the head and was sometimes in use with a frontal board and bandaging. This type of deformation did not cause problems with mobility. Practiced by all members of society, there were distinct differences of temporal and regional preferences. \n\nDuring the Preclassic period, 2000 B.C. – A.D. 250, Maya skull modification imitated the head form of Olmec gods depicted throughout Olmec artwork. With the use of new and different techniques emerging in the Classic period, A.D. 250-900, new cranial modification styles were endured, possibly as an indicator of membership within a kin group, or as a sign of a specific status. Additionally, in the Classic period, the general population used the erect deformation style of modification, while children expected to have high-status positions were given oblique deformation. Around A.D. 900, the modification style standardized, and most human remains were found with tall skulls and flattened foreheads, the same modifications documented by the Spaniards when they arrived in Mesoamerica. \n\nRegional differences in cranial modification styles were also evident. Within the Western Maya lowlands, the popular style of modification imitated the shape of the Maize God’s head, and therefore, more people were discovered to have slanted skulls. High-status Maya mothers would artificially induce cross-eyedness (strabismus) and would strap on boards to flatten the foreheads of high-born infants as a lifelong sign of noble status. The eye condition was used to honor Kinich Ahau, the cross-eyed sun god of the Maya. Across the Guatemala highlands, erect deformation shapes were more likely to be used, and sometimes a band was placed vertically down the head to separate the head into two distinct sections. Cranial modification was able to draw lines between different ethnic groups, as well as represent social status/hierarchy within an individual culture.\nMost evidence of cranial deformation is found through osteological remains discovered through archaeological excavation of Maya sites. Through analysis of the different forms of the skulls, osteologists are able to differentiate between little difference in deformation styles to help understand minute differences between styles, and what these differences may mean. Interpretation of these different cranium types has been debated, but it is clear the shapes are different dependent based on time and region. As stated previously, Spanish and other European records provided detailed descriptions of cranium modification within their historical records. Some of the information the Spaniards recorded included the different types of methods, and the popular materials used for cranium modification. Lastly, other archaeological remains including art depictions, or figurines exhibiting modified skull shape help to illuminate the importance of distinguishing one’s self through the various types of modification. \n\nDue to a lack of written records on the reasons or motivation for cranial modification, the reason the head was the center of this modification is still not clear. One reason is possibly the need for children to be protected when they are born. The Maya believed when children were born, they were vulnerable and thus needed to be protected from soul loss and evil winds. The soul was encased within the head, and therefore these newly souled infants needed to be guarded; cranium modification was one of the ways to protect the soul from being snatched from the newborn. The head was understood to be a portal into a person’s true essence or essential entity, which could be harmed, stolen, or manipulated. Performing cranial modification as soon as the child was born ensured the soul, or essence, of the child was fully protected.\n\nAdditionally, hair was seen as a way to preserve the essence of the soul, \"tonalli,\" from leaving the head of the body. Cutting the hair of a boy too soon was thought to diminish that individual’s knowledge and reason. Lastly, \"Tzompantli\", or skull racks, were associated with passage to the celestial world, and the heads on the rack were believed to contain the essence of the individual spirits. Therefore, by killing an enemy and removing their skull’s from their bodies, the essence of the individual was violated.\n\nMembers of the community were expected to go through cranium modification as a part of a child’s integration into the society. Maya men aspired to look like their ruler, Pakal, who was meant to represent incarnations of the deities. Pakal’s body was shaped to resemble motifs and images of what the Sun and Maize gods were expected to look like. Evidence of the social hierarchy of the Maya was shown in pottery, figurines, drawings, monuments, and architecture picturing high-status elites with the oblique cranium modification. The oblique style cranuml modification, the style endured by Pakal, may have also meant to shape the head like a jaguar, a figure extremely important to Maya religion, sacred to their culture, and a status of power. Additionally, Maya women standards of beauty were also based on the Maize God. Overall, cranial modifications are significant because of their relation with deities and power symbols of the Maya and the outwardly performative aspect displaying specific characteristics of a member within the Maya culture and society. \n\nThe modification of teeth was dependent on social status, as well as location. Dental modification may have been a way to identify with a lineage, polity, ruler, or region. The Maya practiced primarily two different types: filing and inlay. Filing was altering the tooth shape to create notches, grooves, or points. This type of dental modification appeared during the Early Preclassic period (1400-1000 B.C.) and was completed with stone abraders and water. Inlay was drilling holes to insert different materials, and was popular throughout the Middle Preclassic period (900-600 B.C.). In the remainder of the time periods the two were used simultaneously, but filing was much more common overall. \n\nMost likely teeth were modified as a part of ritual or for aesthetic purposes, and younger children usually did not have modified teeth. Once their permanent teeth had arrived, adolescent warriors had their teeth filed to sharp points to give them a fierce appearance, and as a further mark of status. Maya women filed their teeth, or had holes drilled into them where precious stones or luxury materials, such as jade, pyrite, hematite, or turquoise could be inlaid into the teeth. High-status women often had their teeth filed, in different patterns, and would have jadeite, hematite, pyrite, turquoise, or other decorations inset into holes drilled in their teeth. Depending on the material, the meaning of the inlay varied. For example, jade symbolized pure breath or the ability to express elegant speech.\n\nOverall, little evidence for the relationship between socioeconomic status and types of dental modification exists. Most evidence comes from documentation of Europeans in the 16 century who viewed the processes of dental modification. However, these accounts can be problematic as they are filled with bias, and much of the process may be recorded incorrectly, or emphasized in a way to villainize the Maya. More concrete evidence is found through archaeological of ceramics or iconography, and osteological remains of Maya people themselves. Iconographic or images of dental modification, including filling and inlay, are pictured on ceramics or within paintings found at Maya or other Mesoamerican sites. Additionally, teeth of Maya individuals have been excavated from Maya sites and analyzed by dendrologists and other dental specialists recognized dental disease associated with excess filling or drilling of the teeth. This means dental modification was occurring on living subjects. Dental diseases found on the remains of the teeth of Maya individuals shows evidence for excessive dental modifications. Additionally, some of the dental remains were inlaid with various stones, and were filed in a variety of ways. \n\nModification of the teeth was important as different teeth styles exhibited certain characteristics and motifs important to Maya religion, and social status. “Modified dentition conveyed ideas about wealth, threat, and the nature of speech.” Incisors were filed in “T” shape to represent “wind” motifs, as wind was especially important to the Maya embodying the “life force” and a way to honor the maize god. Pakal, the Maya King, had his teeth filed in the shape of a “T” as a way to change the structure of his facial features to make it look as though he were squinting, a direct reflection of the maize god. Teeth were an advertisement of status and as it was an abundant amount of pain, it was likely a rite of passage into adulthood, signifying the ability to tolerate pain. Overall, dental modification was meant to show a specific kind of status, despite being endured by both men and women of various classes. Enduring this type of pain exhibited traits about the overall character of an individual, and congratulated members for reaching a milestone of life. \n\nBody paint, tattoos, and scarification were all used in different ways by the Mayan to signify important events in ones life, as well as to symbolize differing class distinctions. As evidence of skin modification from human remains can not be studied, the evidence for tattooing, scarification, and body paint among the Maya comes from iconographic images such as pottery and murals, artifacts such as tools and vessels used for storing pigment, as well as ethnohistoric accounts.\n\nBody paint patterns were incredibly localized and color and design varied according to location. Two of the most widely used colors among the Maya were red, which was made of cinnabar or vermilion, with hematite and iron ore added. Another popular culture, and one that was possibly the most valued among the Mayans was a blue or green pigment made with indigo and a mineral called Palygorskite. This blue/green color was highly valued because it was associated not only with jade, but with sacrifice to the gods as well. Spanish explorer Diego de Landa states in one of his accounts:\n\n\"...they had the custom of painting their faces and bodies red... they thought it very pleasing...the victim...having smeared him with blue... they brought him up to the round alter...\" The evidence for body painting among the Mayan largely comes from various murals. One mural found in Bonampak, Chiapa shows a man being painted red from the neck down by a servant while a woman’s face is painted red. Another mural found at Calakmul depicts merchants and non-elites wearing face paint of various colors and designs, suggesting that paint may have been used to differentiate class and gender.\n\nWhile there is a physical difference between scarification and tattoos, the Mayans may not have differentiated between the two practices. Tattoos and scarification were used to mark significant events in a Mayan’s life. Diego de Landa says: \"A thief from the highest class is punished by having his face tattooed on both sides from the beard to the forehead. … the young men do not tattoo except to a slight degree until marriage.”Scholar Cara Tremain argues that some tattoos and scars may have been associated with the elite, as “killing” and “rebirth” of the skin through cutting creates an association with death and the rebirth of deities. Tremain also argues that some types of tattooing and scarification symbolized valor and bravery. This theory is supported by the accounts of Diego de Landa who said: “They tattooed their bodies, and the more they do this, the more brave and valiant are they considered, as tattooing is accompanied with great suffering, and is done in this way. Those who do the work first painted the part which they wish with color and afterwards they delicately cut in the paintings, and so with the blood and coloring matter the marks remained in the body. This work is done a little at a time on account of the extreme pain, an afterwards also they were quite sick with it, since the designs festered and matter formed. In spite of all this they made fun of those who were not tattooed.”\n\nThe practice of piercing one’s ears, lips, nose, or cheeks was shared by all Mayans, but it was the type of jewelry worn that was used to differentiate social status. Children would be pierced at a young age as well, with ear flares and spools getting increasingly bigger as the child aged, stretching the ear. The majority of evidence for Mayan piercings comes from archaeological remains of jewelry found in tombs, such as labrets and ear spools. Ethnohistorical accounts also provide us evidence for the amount and high quality of the piercings the Mayans wore. In an account of his travels the Spanish Bishop Diego Lopez de Cogolludiois stated:“The holes in the noses and ears [were filled with] nose and ear pieces of cuzas and other stones of varied colors.”\n\nThe Mayas employed warfare in each period of their development for the purposes of obtaining sacrificial victims, settling competitive rivalries, acquiring critical resources and gaining control of trade routes. Warfare was important to the Maya religion, because raids on surrounding areas provided the victims required for human sacrifice, as well as slaves for the construction of temples. Large-scale battles were also fought to determine and defend territories as well as secure economic power. The Mayas defended their cities with defensive structures such as palisades, gateways, and earthworks. Some cities had a wall within the outer wall, so advancing enemies would be trapped in a killing alley, where they could be slaughtered in great number. During the post-Classic period, the amount of internal warfare increased greatly as the region became more politically fragmented. Armies were enlarged, and in some cases mercenaries were hired. The resulting destruction of many urban centers contributed to the decline of the Maya.\n\nThe ruler of a Maya city was the supreme war captain. Some only dictated military activity, while others participated in the battle. There was a core of warriors that served year-round as guards and obtained sacrificial victims, but most large Maya cities and religious centers had militias. These men were paid to fight for the duration of the battle. Then they would return to their fields or crafts. The militia units were headed by nacoms, hereditary war chiefs, that employed ritual as well as strategic methods in warfare. Some nacoms were only chief strategists, and the troops were led into battle by batabs, or officers. In a large war, even commoners who did not have weapons would fight using hunting tools and by hurling rocks. “In the highlands, women occasionally fought in battles according to native chronicles” (Foster, 144).\n\nThe jungle terrain of Mesoamerica made it difficult for large armies to reach their destination. The warriors who were familiar with the battle landscape could strategically retreat into familiar wilderness. Other war tactics included the siege of cities and the formation of alliances with lesser enemies to defeat more prominent ones. There is evidence that canoes were used to attack cities, located on lakes and rivers. In the late Classic period, destructive warfare methods, such as burning, became more prevalent.\n\nWarfare was a ritual process, which was believed to be sanctioned by the gods. Military leaders, in many instances, also had religious authority. Before going into battle, the armies would call upon the gods with dances and music of drums, whistles, conch shell horns and singing. The drumming and war cries would signify the start of the battle. The armies also carried religious idols into battle to inspire the warriors. They fought fiercely because they believed that death on the battle field secured them eternal bliss, whereas capture by the enemy was regarded as worse than any death. When an enemy was defeated, the victorious army exploited the religious icons and sometimes humiliated the defeated leader with prolonged captivity. The treatment of prisoners by the victorious was brutal and often ended in decapitation. The Maya also had a ritual of giving blood. The reason that they gave blood was to show respect to their gods. They gave blood from their genitals and tongue. Afterwards, they would drip their blood onto a piece of paper and burn it into the sky to show respect to their gods.\n\nWeapons used by the Maya included spear-throwers known as atlatls, blowguns, obsidian spiked clubs, spears, axes, lances and knives tipped with flint or obsidian blades. Bow and arrows were also used, but not as extensively. Though there were few helmets, they used decorated shields made from woven mats, wood and animal skins for protection. The Maya war leaders dressed to inspire their warriors and terrify their enemies. They usually wore padded cotton armor, a mantle with religious insignia, and elaborate wooden and cloth headdresses, that represented the animal spirit or “way” of the warrior. Metal was not used in battle because of the limited supply.\n\n\n"
            ],
            "good_message": false,
            "loggedData": [],
            "offensive": null,
            "researchTask": true,
            "selectedData": [],
            "shown_passages": [
                [
                    "Tattoo",
                    [
                        "A tattoo is a form of body modification where a design is made by inserting ink, dyes and pigments, either indelible or temporary, into the dermis layer of the skin to change the pigment.",
                        "Tattoos fall into three broad categories: purely decorative (with no specific meaning); symbolic (with a specific meaning pertinent to the wearer); pictorial (a depiction of a specific person or item).",
                        "Tattoos have historically been regarded in the West as 'uncivilised', and over the last 100 years the fashion has been associated mainly with sailors, working men and criminals.",
                        "By the end of the 20th Century many Western stigmas of the tattoo culture had been dismissed and the practice has become more acceptable and accessible for people of all trades and levels of society."
                    ]
                ],
                [
                    "History of tattooing",
                    [
                        "Tattooing has been practiced across the globe since at least Neolithic times, as evidenced by mummified preserved skin, ancient art, and the archaeological record.",
                        "Both ancient art and archaeological finds of possible tattoo tools suggest tattooing was practiced by the Upper Paleolithic period in Europe.",
                        "However, direct evidence for tattooing on mummified human skin extends only to the 4th millennium BC.",
                        "The oldest discovery of tattooed human skin to date is found on the body of Ötzi the Iceman, dating to between 3370 and 3100 BC.",
                        "Other tattooed mummies have been recovered from at least 49 archaeological sites including locations in Greenland, Alaska, Siberia, Mongolia, western China, Egypt, Sudan, the Philippines, and the Andes."
                    ]
                ],
                [
                    "Maya society",
                    [
                        "Maya society concerns the social organization of the Pre-Hispanic Mayas, its political structures and social classes.",
                        "A Classic period Maya polity was a small kingdom (\"ajawil\", \"ajawlel\", \"ajawlil\") headed by a hereditary ruler – \"ajaw\", later \"k’uhul ajaw\".",
                        "Both terms appear in early Colonial texts including \"Papeles de Paxbolón\" where they are used as synonyms for Aztec and Spanish terms for rulers and their domains.",
                        "These are \"tlatoani\" and \"tlahtocayotl\" in Nahuatl, and the Spanish words \"rey\", \"majestad\", and \"reino\" and \"señor\" for ruler/leader/lord and \"señorío\" or \"dominio\" of realm.",
                        "Such kingdoms were usually no more than a capital city with its neighborhood and several dependent towns (similar to a city-state)."
                    ]
                ]
            ],
            "speaker": "0_Wizard",
            "suggestionsConfig": "show",
            "suggestionsNumber": 5,
            "suggestionsShown": true,
            "text": "Body modifications designed on bodies are tattoos ",
            "textboxFilled": 0,
            "time": 1580233196.9899802,
            "timeTaken": 3.416,
            "totalTasks": 120,
            "turn": 5
        },
        {
            "checked_passage": {
                "no_passages_used": "no_passages_used"
            },
            "checked_sentence": {
                "no_passages_used": "no_passages_used"
            },
            "clicked_passages": {},
            "context": {
                "conversation": [
                    "I have been thinking of getting a tattoo . Can you tell me more about this so I can make a decision?",
                    "Body modifications designed on bodies are tattoos "
                ],
                "index": 25
            },
            "contextCount": 0,
            "full_passages": [
                "Tattoo\n\nA tattoo is a form of body modification where a design is made by inserting ink, dyes and pigments, either indelible or temporary, into the dermis layer of the skin to change the pigment. Tattoos fall into three broad categories: purely decorative (with no specific meaning); symbolic (with a specific meaning pertinent to the wearer); pictorial (a depiction of a specific person or item). Tattoos have historically been regarded in the West as 'uncivilised', and over the last 100 years the fashion has been associated mainly with sailors, working men and criminals. By the end of the 20th Century many Western stigmas of the tattoo culture had been dismissed and the practice has become more acceptable and accessible for people of all trades and levels of society.\n\nThe word \"tattoo\", or \"tattow\" in the 18th century, is a loanword from the Polynesian word \"tatau\", meaning \"to write\". The \"Oxford English Dictionary\" gives the etymology of tattoo as \"In 18th c. tattaow, tattow. From Polynesian (Samoan, Tahitian, Tongan, etc.) tatau. In Marquesan, tatu.\" Before the importation of the Polynesian word, the practice of tattooing had been described in the West as painting, scarring, or staining.\n\nThis is not to be confused with the origins of the word for the military drumbeat or performance — see \"military tattoo\". In this case, the English word \"tattoo\" is derived from the Dutch word \"taptoe\".\n\nThe first written reference to the word \"tattoo\" (or \"tatau\") appears in the journal of Joseph Banks (24 February 1743 – 19 June 1820), the naturalist aboard explorer Captain Cook's ship the \"HMS Endeavour\": \"I shall now mention the way they mark themselves indelibly, each of them is so marked by their humour or disposition\".\nThe word \"tattoo\" was brought to Europe by Cook, when he returned in 1769 from his first voyage to Tahiti and New Zealand. In his narrative of the voyage, he refers to an operation called \"tattaw\".\n\nTattoo enthusiasts may refer to tattoos as \"ink\", \"pieces\", \"skin art\", \"tattoo art\", \"tats\", or \"work\"; to the creators as \"tattoo artists\", \"tattooers\", or \"tattooists\"; and to places where they work as \"tattoo shops\", \"tattoo studios\", or \"tattoo parlors\".\nMainstream art galleries hold exhibitions of both conventional and custom tattoo designs such as \"Beyond Skin\", at the Museum of Croydon. Copyrighted tattoo designs that are mass-produced and sent to tattoo artists are known as \"flash\", a notable instance of industrial design. Flash sheets are prominently displayed in many tattoo parlors for the purpose of providing both inspiration and ready-made tattoo images to customers.\n\nThe Japanese word \"irezumi\" means \"insertion of ink\" and can mean tattoos using \"tebori\", the traditional Japanese hand method, a Western-style machine, or any method of tattooing using insertion of ink. The most common word used for traditional Japanese tattoo designs is \"horimono\". Japanese may use the word \"tattoo\" to mean non-Japanese styles of tattooing.\n\nAnthropologist Ling Roth in 1900 described four methods of skin marking and suggested they be differentiated under the names \"tatu\", \"moko\", \"cicatrix\", and \"keloid\".\n\nThe American Academy of Dermatology distinguishes five types of tattoos: traumatic tattoos, also called \"natural tattoos\", that result from injuries, especially asphalt from road injuries or pencil lead; amateur tattoos; professional tattoos, both via traditional methods and modern tattoo machines; cosmetic tattoos, also known as \"permanent makeup\"; and medical tattoos.\n\nAccording to George Orwell, coal miners could develop characteristic tattoos owing to coal dust getting into wounds. This can also occur with substances like gunpowder. Similarly, a traumatic tattoo occurs when a substance such as asphalt is rubbed into a wound as the result of some kind of accident or trauma. These are particularly difficult to remove as they tend to be spread across several layers of skin, and scarring or permanent discoloration is almost unavoidable depending on the location. An amalgam tattoo is when amalgam particles are implanted in to the soft tissues of the mouth, usually the gums, during dental filling placement or removal; another example of such accidental tattoos is the result of a deliberate or accidental stabbing with a pencil or pen, leaving graphite or ink beneath the skin.\n\nMany tattoos serve as rites of passage, marks of status and rank, symbols of religious and spiritual devotion, decorations for bravery, sexual lures and marks of fertility, pledges of love, amulets and talismans, protection, and as punishment, like the marks of outcasts, slaves and convicts. The symbolism and impact of tattoos varies in different places and cultures. Tattoos may show how a person feels about a relative (commonly mother/father or daughter/son) or about an unrelated person. Today, people choose to be tattooed for artistic, cosmetic, sentimental/memorial, religious, and magical reasons, and to symbolize their belonging to or identification with particular groups, including criminal gangs (see criminal tattoos) or a particular ethnic group or law-abiding subculture. Some Māori still choose to wear intricate moko on their faces. In Cambodia, Laos, and Thailand, the yantra tattoo is used for protection against evil and to increase luck. Text-based tattoos including quotes, lyrics, personal mottos or scripture are popular in western culture. As an example some Christians might have a Psalm or verse from the Bible tattooed on their body. Popular verses include John 3:16, Philippians 4:13, and Psalms 23.\n\nExtensive decorative tattooing is common among members of traditional freak shows and by performance artists who follow in their tradition.\n\nPeople have also been forcibly tattooed.\n\nA well-known example is the Nazi practice of forcibly tattooing Nazi concentration camp inmates with identification numbers during The Holocaust as part of the Nazis' identification system, beginning in fall 1941. The Nazis' SS introduced the practice at Auschwitz concentration camp in order to identify the bodies of registered prisoners in the concentration camps. During registration, the Nazis would pierce the outlines of the serial-number digits onto the prisoners' arms. Of the Nazi concentration camps, only Auschwitz put tattoos on inmates. The tattoo was the prisoner's camp number, sometimes with a special symbol added: some Jews had a triangle, and Romani had the letter \"Z\" (from German \"Zigeuner\" for \"Gypsy\"). In May 1944, the Jewish men received the letters \"A\" or \"B\" to indicate particular series of numbers. For unknown reasons, this number series for women never began again with the \"B\" series after they had reached the number limit of 20,000 for the \"A\" series. The practice continued until the last days of Auschwitz.\n\nTattoos have also been used for identification in other ways. As early as the Zhou, Chinese authorities would employ facial tattoos as a punishment for certain crimes or to mark prisoners or slaves. During the Roman Empire, Roman soldiers were required by law to have identifying tattoos on their hands in order to make desertion difficult. Gladiators and slaves were likewise tattooed: exported slaves were tattooed with the words \"tax paid\", and it was a common practice to tattoo \"Stop me, I'm a runaway\" on their foreheads. Owing to the Biblical strictures against the practice, Emperor Constantine I banned tattooing the face around AD 330, and the Second Council of Nicaea banned all body markings as a pagan practice in AD 787.\n\nIn the period of early contact between the Māori and Europeans, the Maori people hunted and decapitated each other for their moko tattoos, which they traded for European items including axes and firearms. Moko tattoos were facial designs worn to indicate lineage, social position, and status within the tribe. The tattoo art was a sacred marker of identity among the Maori and also referred to as a vehicle for storing one's tapu, or spiritual being, in the afterlife.\nTattoos are sometimes used by forensic pathologists to help them identify burned, putrefied, or mutilated bodies. As tattoo pigment lies encapsulated deep in the skin, tattoos are not easily destroyed even when the skin is burned.\n\nTattoos are also placed on animals, though rarely for decorative reasons. Pets, show animals, thoroughbred horses, and livestock are sometimes tattooed with identification and other marks. Tattooing with a 'slap mark' on the shoulder or on the ear is the standard identification method in commercial pig farming. Branding is used for similar reasons and is often performed without anesthesia, but is different from tattooing as no ink or dye is inserted during the process, the mark instead being caused by permanent scarring of the skin. Pet dogs and cats are sometimes tattooed with a serial number (usually in the ear, or on the inner thigh) via which their owners can be identified. However, the use of a microchip has become an increasingly popular choice and since 2016 is a legal requirement for all 8.5 million pet dogs in the UK.\n\nThe cosmetic surgery industry continues to see a trend of increased popularity for both surgical and noninvasive procedures. When used as a form of cosmetics, tattooing includes permanent makeup and hiding or neutralizing skin discolorations. Permanent makeup is the use of tattoos to enhance eyebrows, lips (liner and/or lipstick), eyes (liner), and even moles, usually with natural colors, as the designs are intended to resemble makeup.\n\nA growing trend in the US and UK is to place artistic titoos over the surgical scars of a mastectomy. \"More women are choosing not to reconstruct after a mastectomy and tattoo over the scar tissue instead... The mastectomy tattoo will become just another option for post cancer patients and a truly personal way of regaining control over post cancer bodies and proving once and for all that breast cancer is not just a pink ribbon.\" The tattooing of nipples on reconstructed breasts remains in high demand, however.\n\nFunctional tattoos are used primarily for a purpose other than aesthetics. One such use is to tattoo Alzheimer patients with their name, so they may be easily identified if they go missing.\n\nMedical tattoos are used to ensure instruments are properly located for repeated application of radiotherapy and for the areola in some forms of breast reconstruction. Tattooing has also been used to convey medical information about the wearer (e.g., blood group, medical condition, etc.). Additionally, tattoos are used in skin tones to cover vitiligo, a skin pigmentation disorder.\n\nSS blood group tattoos (German: Blutgruppentätowierung) were worn by members of the Waffen-SS in Nazi Germany during World War II to identify the individual's blood type. After the war, the tattoo was taken to be prima facie, if not perfect, evidence of being part of the Waffen-SS, leading to potential arrest and prosecution. This led a number of Ex-Waffen-SS to shoot themselves through the arm with a gun, removing the tattoo and leaving scars like the ones resulting from pox inoculation, making the removal less obvious.\n\nTattoos were probably also used in ancient medicine as part of the treatment of the patient. In 1898, Daniel Fouquet, a medical doctor, wrote an article on “medical tattooing” practices in Ancient Egypt, in which he describes the tattooed markings on the female mummies found at the Deir el-Bahari site. He speculated that the tattoos and other scarifications observed on the bodies may have served a medicinal or therapeutic purpose: \"The examination of these scars, some white, others blue, leaves in no doubt that they are not, in essence, ornament, but an established treatment for a condition of the pelvis, very probably chronic pelvic peritonitis.\"\n\nPreserved tattoos on ancient mummified human remains reveal that tattooing has been practiced throughout the world for many centuries. The Ainu, an indigenous people of Japan, traditionally had facial tattoos, as did the Austroasians. Today, one can find Atayal, Seediq, Truku, and Saisiyat of Taiwan, Berbers of Tamazgha (North Africa), Yoruba, Fulani and Hausa people of Nigeria, and Māori of New Zealand with facial tattoos. Tattooing was popular among certain ethnic groups in southern China, Polynesia, Africa, Borneo, Cambodia, Europe, Japan, the Mentawai Islands, MesoAmerica, New Zealand, North America and South America, the Philippines, Iron Age Britain, and Taiwan. In 2015, scientific re-assessment of the age of the two oldest known tattooed mummies, identified Ötzi as the oldest currently known example. This body, with 61 tattoos, was found embedded in glacial ice in the Alps, and was dated to 3,250 BC.\nIt is a myth that the modern revival of tattooing stems from Captain James Cook's three voyages to the South Pacific in the late 18th century. Certainly, Cook's voyages and the dissemination of the texts and images from them brought more awareness about tattooing (and, as noted above, imported the word \"tattow\" into Western languages), but Europeans have been tattooed throughout history. On Cook's first voyage in 1768, his science officer and expedition botanist, Sir Joseph Banks, as well as artist Sydney Parkinson and many others of the crew, returned to England with tattoos, although many of these men would have had pre-existing tattoos. Banks was a highly regarded member of the English aristocracy that had acquired his position with Cook by co-financing the expedition with ten thousand pounds, a very large sum at the time. In turn, Cook brought back with him a tattooed Raiatean man, Omai, whom he presented to King George and the English Court. On subsequent voyages other crew members, from officers, such as American John Ledyard, to ordinary seamen, were tattooed. \n\nThe first documented professional tattooist in Britain was established in the port of Liverpool in the 1870s. In Britain tattooing was still largely associated with sailors and the lower or even criminal class, but by the 1870s had become fashionable among some members of the upper classes, including royalty, and in its upmarket form it could be an expensive and sometimes painful process. A marked class division on the acceptability of the practice continued for some time in Britain. Recently a trend has arisen marketed as 'Stick and Poke' tattooing; primitive figures are permanently inscribed by the user himself after he obtains a 'DIY' kit containing needles, ink and a collection of suggestions.\n\nAs most tattoos in the U.S. were done by Polynesian and Japanese amateurs, tattoo artists were in great demand in port cities all over the world, especially by European and American sailors. The first recorded professional tattoo artist in the United States was a German immigrant, Martin Hildebrandt. He opened a shop in New York City in 1846 and quickly became popular during the American Civil War among soldiers and sailors of both Union and Confederate militaries.\n\nHildebrandt began traveling from camp to camp to tattoo soldiers, making his popularity increase, and also giving birth to the tradition of getting tattoos while being an American serviceman. Soon after the Civil War, tattoos became fashionable among upper-class young adults. This trend lasted until the beginning of World War I. The invention of the electric tattoo machine caused popularity of tattoos among the wealthy to drop off. The machine made the tattooing procedure both much easier and cheaper, thus, eliminating the status symbol tattoos previously held, as they were now affordable for all socioeconomic classes. The status symbol of a tattoo shifted from a representation of wealth, to a mark typically seen on rebels and criminals. Despite this change, tattoos remained popular among military servicemen, and the tradition continues today.\n\nMany studies have been done of the tattooed population and society's view of tattoos. In June 2006, the \"Journal of the American Academy of Dermatology\" published the results of a telephone survey of 2004. It found that 36% of Americans ages 18–29, 24% of those 30–40, and 15% of those 41–51 had a tattoo. In September 2006, the Pew Research Center conducted a telephone survey that found that 36% of Americans ages 18–25, 40% of those 26–40 and 10% of those 41–64 had a tattoo. They concluded that Generation X and Generation Y are not afraid to express themselves through their appearance, and tattoos are the most popular form of self-expression. In January 2008, a survey conducted online by Harris Interactive estimated that 14% of all adults in the United States have a tattoo, slightly down from 2003, when 16% had a tattoo. Among age groups, 9% of those ages 18–24, 32% of those 25–29, 25% of those 30–39 and 12% of those 40–49 have tattoos, as do 8% of those 50–64. Men are slightly more likely to have a tattoo than women.\n\nRichmond, Virginia, has been cited as one of the most tattooed cities in the United States. That distinction led the Valentine Richmond History Center to create an online exhibit titled \"History, Ink: The Tattoo Archive Project.\" The introduction to the exhibit notes, \"In the past, western culture associated tattoos with those individuals who lived on the edge of society; however, today they are recognized as a legitimate art form and widely accepted in mainstream culture.\"\n\nSince the 1970s, tattoos have become a mainstream part of Western fashion, common among all genders, to all economic classes, and to age groups from the later teen years to middle age. For many young Americans, the tattoo has taken on a decidedly different meaning than for previous generations. The tattoo has \"undergone dramatic redefinition\" and has shifted from a form of deviance to an acceptable form of expression.\n\nProtection papers were used by American sailors to prevent themselves from being taken off American ships and impressed into the Royal Navy. These were simple documents that described the sailor as being an American sailor. Many of the protection certificates were so general, and it was so easy to abuse the system, that many impressment officers of the Royal Navy paid no attention to them. \"In applying for a duplicate Seaman's Protection Certificate in 1817, James Francis stated that he 'had a protection granted him by the Collector of this Port on or about 12 March 1806 which was torn up and destroyed by a British Captain when at sea.'\" One way of making them more specific was to describe a tattoo, which is highly personal, and thus use that description to identify the seaman. As a result, many of the later certificates carried information about tattoos and scars, as well as other specific information. This also perhaps led to an increase and proliferation of tattoos among American seamen. \"Frequently their 'protection papers' made reference to tattoos, clear evidence that individual was a seafaring man; rarely did members of the general public adorn themselves with tattoos.\"\n\n\"In the late eighteenth and early nineteenth centuries, tattoos were as much about self-expression as they were about having a unique way to identify a sailor's body should he be lost at sea or impressed by the British navy. The best source for early American tattoos is the protection papers issued following a 1796 congressional act to safeguard American seamen from impressment. These proto-passports catalogued tattoos alongside birthmarks, scars, race, and height. Using simple techniques and tools, tattoo artists in the early republic typically worked on board ships using anything available as pigments, even gunpowder and urine. Men marked their arms and hands with initials of themselves and loved ones, significant dates, symbols of the seafaring life, liberty poles, crucifixes, and other symbols.\"\n\nBecause these protection papers were used to define freemen and citizenship, many black sailors and other men also used them to show that they were freemen if they were stopped by officials or slave catchers. They also called them \"free papers\" because they certified their non-slave status. Many of the freed blacks used descriptions of tattoos for identification purposes on their freedom papers.\n\nTattooing involves the placement of pigment into the skin's dermis, the layer of dermal tissue underlying the epidermis. After initial injection, pigment is dispersed throughout a homogenized damaged layer down through the epidermis and upper dermis, in both of which the presence of foreign material activates the immune system's phagocytes to engulf the pigment particles. As healing proceeds, the damaged epidermis flakes away (eliminating surface pigment) while deeper in the skin granulation tissue forms, which is later converted to connective tissue by collagen growth. This mends the upper dermis, where pigment remains trapped within fibroblasts, ultimately concentrating in a layer just below the dermis/epidermis boundary. Its presence there is stable, but in the long term (decades) the pigment tends to migrate deeper into the dermis, accounting for the degraded detail of old tattoos.\nSome tribal cultures traditionally created tattoos by cutting designs into the skin and rubbing the resulting wound with ink, ashes or other agents; some cultures continue this practice, which may be an adjunct to scarification. Some cultures create tattooed marks by hand-tapping the ink into the skin using sharpened sticks or animal bones (made like needles) with clay formed disks or, in modern times, needles.\n\nThe most common method of tattooing in modern times is the electric tattoo machine, which inserts ink into the skin via a single needle or a group of needles that are soldered onto a bar, which is attached to an oscillating unit. The unit rapidly and repeatedly drives the needles in and out of the skin, usually 80 to 150 times a second. The needles are single-use needles that come packaged individually.\n\nTattooing is regulated in many countries because of the associated health risks to client and practitioner, specifically local infections and virus transmission. Disposable plastic aprons and eye protection can be worn depending on the risk of blood or other secretions splashing into the eyes or clothing of the tattoist. Hand hygiene, assessment of risks and appropriate disposal of all sharp objects and materials contaminated with blood are crucial areas. The tattoo artist must wash his or her hands and must also wash the area that will be tattooed. Gloves must be worn at all times and the wound must be wiped frequently with a wet disposable towel of some kind. All equipment must be sterilized in a certified autoclave before and after every use. It is good practice to provide clients with a printed consent form that outlines risks and complications as well as instructions for after care.\n\nThe Government of Meiji Japan had outlawed tattoos in the 19th century, a prohibition that stood for 70 years before being repealed in 1948. As of 6 June 2012 all new tattoos are forbidden for employees of the city of Osaka. Existing tattoos are required to be covered with proper clothing. The regulations were added to Osaka's ethical codes, and employees with tattoos were encouraged to have them removed. This was done because of the strong connection of tattoos with the yakuza, or Japanese organized crime, after an Osaka official in February 2012 threatened a schoolchild by showing his tattoo.\n\nTattoos had negative connotations in historical China, where criminals often had been marked by tattooing. The association of tattoos with criminals was transmitted from China to influence Japan. Today, tattoos have remained a taboo in Chinese society.\n\nThe Romans tattooed criminals and slaves, and in the 19th century released US convicts, Australian convicts, and British army deserters were identified by tattoos. Prisoners in Nazi concentration camps were tattooed with an identification number. Today, many prison inmates still tattoo themselves as an indication of time spent in prison.\n\nNative Americans also used tattoos to represent their tribe. Catholic Croats of Bosnia used religious Christian tattooing, especially of children and women, for protection against conversion to Islam during the Ottoman rule in the Balkans.\n\nTattoos are strongly empirically associated with deviance, personality disorders, and criminality. Although the general acceptance of tattoos is on the rise in Western society, they still carry a heavy stigma among certain social groups. Tattoos are generally considered an important part of the culture of the Russian mafia.\n\nCurrent cultural understandings of tattoos in Europe and North America have been greatly influenced by long-standing stereotypes based on deviant social groups in the 19th and 20th centuries. Particularly in North America tattoos have been associated with stereotypes, folklore, and racism. Not until the 1960s and 1970s did people associate tattoos with such societal outcasts as bikers and prisoners. Today, in the United States many prisoners and criminal gangs use distinctive tattoos to indicate facts about their criminal behavior, prison sentences, and organizational affiliation. A teardrop tattoo, for example, can be symbolic of murder, or each tear represents the death of a friend. At the same time, members of the U.S. military have an equally well-established and longstanding history of tattooing to indicate military units, battles, kills, etc., an association that remains widespread among older Americans. In Japan tattoos are associated with yakuza criminal groups but there are non-yakuza groups such as Fukushi Masaichi's tattoo association that sought to preserve the skins of dead Japanese who have extensive tattoos. Tattooing is also common in the British Armed Forces. Depending on vocation tattoos are accepted in a number of professions in America. Companies across many fields are increasingly focused on diversity and inclusion.\n\nIn Britain, there is evidence of women with tattoos, concealed by their clothing, throughout the 20th century, and records of women tattoists such as Jessie Knight from the 1920s. A study of \"at-risk\" (as defined by school absenteeism and truancy) adolescent girls showed a positive correlation between body modification and negative feelings towards the body and low self-esteem; however, the study also demonstrated that a strong motive for body modification is the search for \"self and attempts to attain mastery and control over the body in an age of increasing alienation\". The prevalence of women in the tattoo industry in the 21st century, along with larger numbers of women bearing tattoos, appears to be changing negative perceptions.\n\nFormer sailor Rowland Hussey Macy, who formed Macy's department stores, used a red star tattoo that he had on his hand for the store's logo.\n\nTattoos have also been used in marketing and advertising with companies paying people to have logos of brands like HBO, Red Bull, ASOS.com, and Sailor Jerry's rum tattooed in their bodies. This practice is known as \"skinvertising\".\n\nB.T.'s Smokehouse, a barbecue restaurant located in Massachusetts, offered customers free meals for life if they had the logo of the establishment tattooed on a visible part of their bodies. Nine people took the business up on the offer.\n\nBecause it requires breaking the skin barrier, tattooing carries health risks including infection and allergic reactions. Tattooing can be uncomfortable to excruciating depending on the area and can result in the person fainting. Modern tattooists reduce risks by following universal precautions working with single-use items and sterilizing their equipment after each use. Many jurisdictions require that tattooists have blood-borne pathogen training such as that provided through the Red Cross and OSHA. As of 2009 (in the United States) there have been no reported cases of HIV contracted from tattoos.\n\nIn amateur tattooing, such as that practiced in prisons, however, there is an elevated risk of infection. Infections that can theoretically be transmitted by the use of unsterilized tattoo equipment or contaminated ink include surface infections of the skin, fungal infections, some forms of hepatitis, herpes simplex virus, HIV, staph, tetanus, and tuberculosis.\n\nTattoo inks have been described as \"remarkably nonreactive histologically\". However, cases of allergic reactions to tattoo inks, particularly certain colors, have been medically documented. This is sometimes due to the presence of nickel in an ink pigment, which triggers a common metal allergy. Occasionally, when a blood vessel is punctured during the tattooing procedure, a bruise/hematoma may appear.\n\nCertain colours - red or similar colours such as purple, pink, and orange - tend to cause more problems and damage compared to other colours. Red ink has even caused skin and flesh damages so severe that the amputation of a leg or an arm has been necessary. If part of a tattoo (especially if red) begins to cause even minor troubles, like becoming itchy or worse, lumpy, then Danish experts strongly suggest to remove the red parts.\n\nIn 2017, researchers from the European Synchrotron Radiation Facility in France say the chemicals in tattoo ink can travel in the bloodstream and accumulate in the lymph nodes, obstructing their ability to fight infections. However, the authors noted in their paper that most tattooed individuals including the donors analyzed do not suffer from chronic inflammation.\n\nWhile tattoos are considered permanent, it is sometimes possible to remove them, fully or partially, with laser treatments. Typically, black and some colored inks can be removed more completely than inks of other colors. The expense and pain associated with removing tattoos are typically greater than the expense and pain associated with applying them. Pre-laser tattoo removal methods include dermabrasion, salabrasion (scrubbing the skin with salt), cryosurgery, and excision—which is sometimes still used along with skin grafts for larger tattoos. These older methods, however, have been nearly completely replaced by laser removal treatment options.\n\nA temporary tattoo is a non-permanent image on the skin resembling a permanent tattoo. Temporary tattoos can be drawn, painted, airbrushed or needled as a permanent tattoo with an ink which can be dissolved in blood within 6 months of art as a form of body painting.\n\nDecal (press-on) temporary tattoos are used to decorate any part of the body. They may last for a day or for more than a week.\n\nFoil temporary tattoos are a variation of decal-style temporary tattoos, printed using a foil stamping technique instead of using ink. The foil design is printed as a mirror image in order to be viewed in the right direction once it is applied to the skin. Each metallic tattoo is protected by a transparent protective film.\n\nAlthough they have become more popular and usually require a greater investment, airbrush temporary tattoos are less likely to achieve the look of a permanent tattoo, and may not last as long as press-on temporary tattoos. An artist sprays on airbrush tattoos using a stencil with alcohol-based, FDA-approved cosmetic inks. Like decal tattoos, airbrush temporary tattoos also are easily removed with rubbing alcohol or baby oil.\n\nAnother tattoo alternative is henna-based tattoos, which generally contain no additives. Henna is a plant-derived substance which is painted on the skin, staining it a reddish-orange-to-brown color. Because of the semi-permanent nature of henna, they lack the realistic colors typical of decal temporary tattoos. Due to the time-consuming application process, it is a relatively poor option for children. If you do choose henna temporary tattoos, ensure that they are pure henna. Dermatological publications report that allergic reactions to natural henna are very rare and the product is generally considered safe for skin application. Serious problems can occur, however, from the use of henna with certain additives. The FDA and medical journals report that painted black henna temporary tattoos are especially dangerous. Black Henna or Pre-Mixed Henna Temporary Tattoos May Be Harmful - see below for safety information.\n\nDecal temporary tattoos, when legally sold in the United States, have had their color additives approved by the U.S. Food and Drug Administration (FDA) as cosmetics --- the FDA has determined these colorants are safe for “direct dermal contact.” While the FDA has received some accounts of minor skin irritation, including redness and swelling, from this type of temporary tattoo, the agency has found these symptoms to be “child specific” and not significant enough to support warnings to the public. Unapproved pigments, however, which are sometimes used by non-US manufacturers, can provoke allergic reactions in anyone. Understanding the types of temporary tattoos available to consumers, knowing where they are manufactured, and ensuring they come from a reliable source are keys to determining whether temporary tattoos are safe.\n\nThe types of airbrush paints manufactured for crafting, creating art or decorating clothing should never be used for tattooing. These paints are not approved for direct contact with skin, and can be allergenic or toxic. Always ask the airbrush tattoo artist what kind of ink he or she is using and whether it meets FDA approval.\n\nThe FDA regularly issues warnings to consumers about avoiding any temporary tattoos labeled as black henna or pre-mixed henna as these may contain potentially harmful ingredients including silver nitrate, carmine, pyrogallol, disperse orange dye and chromium. Black henna gets its color from paraphenylenediamine (PPD), a textile dye approved by the FDA for human use only in hair coloring. In Canada, the use of PPD on the skin, including hair dye, is banned. Research has linked these and other ingredients to a range of health problems including allergic reactions, chronic inflammatory reactions, and late-onset allergic reactions to related clothing and hairdressing dyes. They can cause these reactions long after application. Neither black henna nor pre-mixed henna are approved for cosmetic use by the FDA.\n\nJudaism generally prohibits tattoos among its adherents based on the commandments in Leviticus 19. Jews tend to believe this commandment only applies to Jews and not to gentiles.\nThere is no specific rule in the New Testament prohibiting tattoos and most Christian denominations believe the laws in Leviticus are outdated as well as believing the commandment only applied to the Israelites, not to the gentiles. While most Christian groups tolerate tattoos, some Evangelical and fundamentalist Protestant denominations do believe the commandment does apply today for Christians and believe it is a sin to get one.\n\nMany Coptic Christians in Egypt take a cross tattoo in their right wrist to differ from the Muslims.\n\nTattoos are considered to be haram in Sunni Islam, based on rulings from scholars and passages in the Hadith. Shia Islam does not entirely prohibit tattooing, although it may be looked down upon in Shia communities.\n\nAnthropological\n\nPopular and artistic\n\nMedical\n\n",
                "Process of tattooing\n\nThe process of tattooing involves the insertion of pigment into the skin's dermis. Traditionally, tattooing often involved rubbing pigment into cuts. Modern tattooing almost always requires the use of a tattoo machine and often procedures and accessories to reduce the risk to human health.\n\nTattooing involves the placement of pigment into the skin's dermis, the layer of dermal tissue underlying the epidermis. After initial injection, pigment is dispersed throughout a homogenized damaged layer down through the epidermis and upper dermis, in both of which the presence of foreign material activates the immune system's phagocytes to engulf the pigment particles. As healing proceeds, the damaged epidermis flakes away (eliminating surface pigment) while deeper in the skin granulation tissue forms, which is later converted to connective tissue by collagen growth. This mends the upper dermis, where pigment remains trapped within fibroblasts, ultimately concentrating in a layer just below the dermis/epidermis boundary. Its presence there is stable, but in the long term (decades) the pigment tends to migrate deeper into the dermis, accounting for the degraded detail of old tattoos.\n\nSome tribal cultures traditionally created tattoos by cutting designs into the skin and rubbing the resulting wound with ink, ashes or other agents; some cultures continue this practice, which may be an adjunct to scarification. Some cultures create tattooed marks by hand-tapping the ink into the skin using sharpened sticks or animal bones (made like needles) with clay formed disks or, in modern times, needles. Traditional Japanese tattoos (\"irezumi\") are still \"hand-poked,\" that is, the ink is inserted beneath the skin using non-electrical, hand-made and hand held tools with needles of sharpened bamboo or steel. This method is known as \"tebori\".\n\nTraditional Hawaiian hand-tapped tattoos are experiencing a renaissance, after the practice was nearly extinguished in the years following Western contact. The process involves lengthy protocols and prayers and is considered a sacred rite more than an application of artwork. The tattooist chooses the design, rather than the wearer, based on genealogical information. Each design is symbolic of the wearer's personal responsibility and role in the community. Tools are hand-carved from bone or tusk without the use of metal.\n\nThe most common method of tattooing in modern times is the electric tattoo machine, which inserts ink into the skin via a single needle or a group of needles that are soldered onto a bar, which is attached to an oscillating unit. The unit rapidly and repeatedly drives the needles in and out of the skin, usually 80 to 150 times a second. This modern procedure is ordinarily sanitary. The needles are single-use needles that come packaged individually. The tattoo artist must wash his or her hands and must also wash the area that will be tattooed. Gloves must be worn at all times and the wound must be wiped frequently with a wet disposable towel of some kind. The equipment must be sterilized in a certified autoclave before and after every use.\n\nPrices for this service vary widely globally and locally, depending on the complexity of the tattoo, the skill and expertise of the artist, the attitude of the customer, the costs of running a business, the economics of supply and demand, etc. The time it takes to get a tattoo is in proportion with its size and complexity. A small one of simple design might take 15 minutes, whereas an elaborate sleeve tattoo or back piece requires multiple sessions that may consist of several hours at a time ranging over months or even years.\n\nIn 1891 the first electric tattoo needle was invented in New York City by modifying Thomas Edison's electric engraving pen. This made the process cheaper and faster; it was taken up by the poor and abandoned by the rich. O'Reilly's machine was based on the rotary technology of the electric engraving device invented by Thomas Edison. Modern tattoo machines use electromagnetic coils. The first coil machine was patented by Thomas Riley in London in 1891, using a single coil. The first twin-coil machine, the predecessor of the modern configuration, was invented by another Englishman, Alfred Charles South of London, in 1899.\n\nAnother tattoo machine was developed 1970-1978 by the German tattoo artists Horst Heinrich Streckenbach (1929–2001) and Manfred Kohrs.\n\nEarly tattoo inks were obtained directly from nature and were extremely limited in pigment variety. In ancient Hawaii, for example, kukui nut ash was blended with coconut oil to produce an ebony ink. Today, an almost unlimited number of colors and shades of tattoo ink are mass-produced and sold to parlors worldwide. Tattoo artists commonly mix these inks to create their own unique pigments.\n\nA wide range of dyes and pigments can be used in tattoos, from inorganic materials like titanium dioxide and iron oxides to carbon black, azo dyes, and acridine, quinoline, phthalocyanine and naphthol derivatives, dyes made from ash, and other mixtures. Iron oxide pigments are used in greater extent in cosmetic tattooing.\n\nModern tattooing inks are carbon-based pigments that have uses outside of commercial tattoo applications.\n\nConcern has been expressed over the interaction between magnetic resonance imaging (MRI) procedures and tattoo pigments, some of which contain trace metals. The magnetic fields produced by MRI machines interact with these metals, including nonferrous metal particles, and while rare, are capable of causing first-degree or second-degree burns or distortions in the image. The type and density of the ink as well as shape of the tattoo may increase the risk, particularly if the shape approximates an RF pick-up loop. The television show \"MythBusters\" tested the hypothesis, and found a slight interaction between commonly used tattoo inks and MRI. The interaction was stronger with inks containing high levels of iron oxide.\n\nThe properly equipped tattoo studio will use biohazard containers for objects that have come into contact with blood or bodily fluids, sharps containers for old needles, and an autoclave for sterilizing tools. Certain jurisdictions also require studios by law to have a sink in the work area supplied with both hot and cold water.\n\nProper hygiene requires a body modification artist to wash his or her hands before starting to prepare a client for the stencil, between clients, and at any other time when cross contamination can occur. The use of single use gloves is also mandatory and disposed after each stage of tattooing. The same gloves should not be used to clean the tattoo station, tattoo the client, and clean the tattoo.\n\nIn some states and countries it is illegal to tattoo a minor even with parental consent, and (except in the case of medical tattoos) it is forbidden to tattoo impaired persons, people with contraindicated skin conditions, those who are pregnant or nursing, those incapable of consent due to mental incapacity, or those under the influence of alcohol or other drugs.\n\nBefore the tattooing begins the client is asked to approve the final position of the applied stencil. After approval is given the artist will open new, sterile needle packages in front of the client, and always use new, sterile, or sterile disposable instruments and supplies, and fresh ink for each session (loaded into disposable ink caps which are discarded after each client). Also, all areas which may be touched with contaminated gloves will be wrapped in clear plastic to prevent cross-contamination. Equipment that cannot be autoclaved (such as counter tops, machines, and furniture) will be wiped with an approved disinfectant.\n\nMembership in professional organizations or certificates of appreciation/achievement generally help artists to be aware of the latest trends. Most tattooists do not belong to any association.\n\nWhile specific requirements to become a tattooist vary between jurisdictions, many mandate only formal training in blood borne pathogens and cross contamination. The local department of health regulates tattoo studios in many jurisdictions. For example, according to the health departments in Oregon and Hawaii, tattoo artists in these states are required to take and pass a test ascertaining their knowledge of health and safety precautions as well as the current state regulations. Performing a tattoo in Oregon without a proper and current license or in an unlicensed facility is a felony offense. Tattooing was legalized in New York City in 1997, in Massachusetts in 2000 and Oklahoma between 2002 and 2006.\n\nTattoo artists and people with tattoos vary widely in their preferred methods of caring for new tattoos. Some artists recommend keeping a new tattoo wrapped for the first 24 hours while others suggest removing temporary bandaging after two hours or less to allow the skin to breathe. Many tattooists advise against allowing too much contact with hot tub, pool water or soaking in a tub for the first two weeks to prevent the tattoo ink from washing out. In contrast other artists suggest that a new tattoo be bathed in very hot water early.\n\nGeneral consensus for care advises against removing the flakes or scab that may form on a new tattoo, and avoiding exposing one's tattoo to the sun for extended periods for at least 3 weeks; both of these can contribute to fading of the image. It is agreed that a new tattoo needs to be kept clean. Various products may be recommended for application to the skin, ranging from those intended for the treatment of cuts, burns and scrapes, to panthenol, cocoa butter, A&D, hemp, lanolin, or salves. Oil based ointments are almost always recommended for use on very thin layers due to their inability to evaporate and therefore over-hydrate the already perforated skin. Recent scientific studies have demonstrated that wounds that are kept moist heal faster than wounds healing under dry conditions. In recent years specific commercial products have been developed for tattoo aftercare. Although opinions about these products vary, soap and warm water work well to keep a tattoo clean and free from infection.\n\nThe amount of ink that remains in the skin throughout the healing process determines how the final tattoo will look. If a tattoo becomes infected or the flakes fall off too soon (e.g., if it absorbs too much water and sloughs off early or is picked or scraped off) then the ink will not be properly fixed in the skin and the final image will be negatively affected.\n",
                "Tattoo removal\n\nTattoo removal has been performed with various tools since the start of tattooing. While tattoos were once considered permanent, it is now possible to remove them with treatments, fully or partially. Before the development of laser tattoo removal methods, common techniques included dermabrasion, TCA (Trichloroacetic acid, an acid that removes the top layers of skin, reaching as deep as the layer in which the tattoo ink resides), salabrasion (scrubbing the skin with salt), cryosurgery and excision which is sometimes still used along with skin grafts for larger tattoos. Some early forms of tattoo removal included the injection or application of wine, lime, garlic or pigeon excrement. Tattoo removal by laser was performed with continuous-wave lasers initially, and later with Q-switched lasers, which became commercially available in the early 1990s. Today, \"laser tattoo removal\" usually refers to the non-invasive removal of tattoo pigments using Q-switched lasers. Typically, black and other darker-colored inks can be removed completely.\n\nA poll conducted in January 2012 by Harris Interactive reported that 1 in 7 (14%) of the 21% of American adults who have a tattoo regret getting one. The poll didn't report the reasons for these regrets, but a poll that was done 4 years prior reported that the most common reasons were \"too young when I got the tattoo\" (20%), \"it's permanent\" and \"I'm marked for life\" (19%), and \"I just don't like it\" (18%). An earlier poll showed that 19% of Britons with tattoos suffered regret, as did 11% of Italians with tattoos. Surveys of tattoo removal patients were done in 1996 and 2006 and provided more insight. Of those polled, the patients who regretted their tattoos typically obtained their tattoos in their late teens or early twenties, and were evenly distributed by gender. Among those seeking removals, more than half reported that they \"suffered embarrassment\". A new job, problems with clothes, and a significant life event were also commonly cited as motivations. Extravagant motives include tattooing your ex-husband's or wife's name. Angelina Jolie, Eva Longoria, Marc Anthony and Denise Richards are some of the celebrities that got this kind of tattoo removed.\n\nThe choice to get a tattoo that is later regretted is related to the end-of-history illusion, in which teenagers and adults of all ages know that their tastes have changed regularly over the years before the current moment, but believe that their tastes will somehow not continue to grow and mature in the future. As a result, they wrongly believe that any tattoo that appeals to them today will always appeal to them in the future.\n\nSome wearers decide to cover an unwanted tattoo with a new tattoo. This is commonly known as a cover-up. An artfully done cover-up may render the old tattoo completely invisible, though this will depend largely on the size, style, colors and techniques used on the old tattoo and the skill of the tattoo artist. Covering up a previous tattoo necessitates darker tones in the new tattoo to effectively hide the older, unwanted piece. Many tattoos are too dark to cover up and in those cases patients may receive laser tattoo removal to lighten the existing ink to make themselves better candidates for a cover up tattoo.\n\nTattoo removal is most commonly performed using lasers that break down the ink particles in the tattoo. The broken-down ink is then fought off by the immune system, mimicking the natural fading that time or sun exposure would create. All tattoo pigments have specific light absorption spectra. A tattoo laser must be capable of emitting adequate energy within the given absorption spectrum of the pigment to provide an effective treatment. Certain tattoo pigments, such as yellows and fluorescent inks are more challenging to treat than darker blacks and blues, because they have absorption spectra that fall outside or on the edge of the emission spectra available in the tattoo removal laser. Recent pastel coloured inks contain high concentrations of titanium dioxide which is highly reflective. Consequently, such inks are difficult to remove since they reflect a significant amount of the incident light energy out of the skin.\n\nWidely considered the gold standard treatment modality to remove a tattoo, laser tattoo removal requires repeat visits. The newer Q-switched lasers are said by the National Institutes of Health to result in scarring only rarely and are usually used only after a topical anesthetic has been applied. Areas with thin skin will be more likely to scar than thicker-skinned areas. There are several types of Q-switched lasers, and each is effective at removing a different range of the color spectrum. Lasers developed after 2006 provide multiple wavelengths and can successfully treat a much broader range of tattoo pigments than previous individual Q-switched lasers. Unfortunately the dye systems used to change the wavelength result in significant power reduction such that the use of multiple separate specific wavelength lasers remains the gold standard. \n\nThe energy density (fluence), expressed as joules/cm, is determined prior to each treatment as well as the spot size and repetition rate (hertz). To mitigate pain the preferred method is simply to cool the area before and during treatment with a medical-grade chiller/cooler and to use a topical anesthetic. During the treatment process, the laser beam passes harmlessly through the skin, targeting only the ink resting in a liquid state within. While it is possible to see immediate results, in most cases the fading occurs gradually over the 7–8 week healing period between treatments.\n\nExperimental observations of the effects of short-pulsed lasers on tattoos were first reported in the late 1960s. In 1979 an argon laser was used for tattoo removal in 28 patients, with limited success. In 1978 a carbon dioxide laser was also used, but generally caused scarring after treatments.\n\nIn the early 1980s, a new clinical study began in Canniesburn Hospital's Burns and Plastic Surgery Unit, in Glasgow, Scotland, into the effects of Q-switched ruby laser energy on blue/black tattoos. Further studies into other tattoo colours were then carried out with various degrees of success. Research at the University of Strathclyde, Glasgow also showed that there was no detectable mutagenicity in tissues following irradiation with the Q-switched ruby laser. This essentially shows that the treatment is safe, from a biological viewpoint, with no detectable risk of the development of cancerous cells.\n\nIt was not until the late 1980s that Q-switched lasers became commercially practical with the first marketed laser coming from Dermalase Limited, Glasgow. One of the first American published articles describing laser tattoo removal was authored by a group at Massachusetts General Hospital in 1990.\n\nTattoos consist of thousands of particles of tattoo pigment suspended in the skin. While normal human growth and healing processes will remove small foreign particles from the skin, tattoo pigment particles are permanent because they are too big to be removed. Laser treatment causes tattoo pigment particles to heat up and fragment into smaller pieces. These smaller pieces are then removed by normal body processes.\n\nLaser tattoo removal is a successful application of the theory of selective photothermolysis (SPTL). However, unlike treatments for blood vessels or hair the mechanism required to shatter tattoo particles uses the photomechanical effect. In this situation the energy is absorbed by the ink particles in a very short time, typically nanoseconds. The surface temperature of the ink particles can rise to thousands of degrees but this energy profile rapidly collapses into a shock wave. This shock wave then propagates throughout the local tissue (the dermis) causing brittle structures to fragment. Hence tissues are largely unaffected since they simply vibrate as the shock wave passes. For laser tattoo removal the selective destruction of tattoo pigments depends on four factors:\n\nQ-switched lasers are the only commercially available devices that can meet these requirements.\n\nAlthough they occur infrequently, mucosal tattoos can be successfully treated with Q-switched lasers as well.\n\nA novel method for laser tattoo removal using a fractionated CO2 or Erbium:YAG laser, alone or in combination with Q-switched lasers, was reported by Ibrahimi and coworkers from the Wellman Center of Photomedicine at the Massachusetts General Hospital. This new approach to laser tattoo removal may afford the ability to remove colors such as yellow and white, which have proven to be resistant to traditional Q-switched laser therapy.\n\nSeveral colors of laser light (quantified by the laser wavelength) are used for tattoo removal, from visible light to near-infrared radiation. Different lasers are better for different tattoo colors. Consequently, multi-color tattoo removal almost always requires the use of two or more laser wavelengths. Tattoo removal lasers are usually identified by the lasing medium used to create the wavelength (measured in nanometers (nm)):\n\nPulsewidth or pulse duration is a critical laser parameter. All Q-switched lasers have appropriate pulse durations for tattoo removal.\n\nSpot size, or the width of the laser beam, affects treatment. Light is optically scattered in the skin, like automobile headlights in fog. Larger spot sizes slightly increase the effective penetration depth of the laser light, thus enabling more effective targeting of deeper tattoo pigments. Larger spot sizes also help make treatments faster.\n\nFluence or energy density is another important consideration. Fluence is measured in joules per square centimeter (J/cm²). It is important to be treated at high enough settings to fragment tattoo particles.\n\nRepetition rate helps make treatments faster but is not associated with any treatment effect. Faster treatments are usually preferred because the pain ends sooner.\n\nComplete laser tattoo removal requires numerous treatment sessions, typically spaced at least seven weeks apart. Treating more frequently than seven weeks increases the risk of adverse effects and does not necessarily increase the rate of ink absorption. Anecdotal reports of treatments sessions at four weeks leads to more scarring and dischromia and can be a source of liability for clinicians. At each session, some but not all of the tattoo pigment particles are effectively fragmented, and the body removes the smallest fragments over the course of several weeks or months. The result is that the tattoo is lightened over time. Remaining large particles of tattoo pigment are then targeted at subsequent treatment sessions, causing further lightening. The number of sessions and spacing between treatments depends on various parameters, including the area of the body treated, skin color and effectiveness of the immune system. Tattoos located on the extremities, such as the ankle, generally take longest. As tattoos fade clinicians may recommend that patients wait many months between treatments to facilitate ink resolution and minimize unwanted side effects.\n\nThe amount of time required for the removal of a tattoo and the success of the removal varies with each individual and their immune system function. Factors influencing this include: skin type, location, color, amount of ink, scarring or tissue change, layers of ink, immune system function and circulation. Factors under the individuals control are more time between treatments, nutrition, stress, sleep, exercise and fluid levels. In the past health care providers would simply guess on the number of treatments a patient needed which was rather frustrating to patients. A predictive scale, the \"Kirby-Desai Scale\", was developed by Dr. Will Kirby and Dr. Alpesh Desai, dermatologists with specialization in tattoo removal techniques, to assess the potential success and number of treatments necessary for laser tattoo removal, provided the medical practitioner is using a Q-switched Nd:YAG (neodymium-doped yttrium aluminium garnet) laser incorporating selective photothermolysis with six weeks between treatments.\n\nThe Kirby-Desai Scale assigns numerical values to six parameters: skin type, location, color, amount of ink, scarring or tissue change, and layering. Parameter scores are then added to yield a combined score that will show the estimated number of treatments needed for successful tattoo removal. Experts recommend that the Kirby-Desai scale be used by all laser practitioners prior to starting tattoo removal treatment to help determine the number of treatments required for tattoo removal and as a predictor of the success of the laser tattoo removal treatments. Prior to 2009, clinicians had no scientific basis by which to estimate the number of treatments needed to remove a tattoo and the use of this scale is now standard practice in laser tattoo removal.\n\nCertain colors have proved more difficult to remove than others. In particular, this occurs when treated with the wrong wavelength of laser light is used. Some have postulated that the reason for slow resolution of green ink in particular is due to its significantly smaller molecular size relative to the other colours. Consequently, green ink tattoos may require treatment with 755 nm light but may also respond to 694 nm, 650 nm and 1064 nm. Multiple wavelengths of light may be needed to remove colored inks.\n\nOne small Greek study showed that the R20 method--four passes with the laser, twenty minutes apart--caused more breaking up of the ink than the conventional method without more scarring or adverse effects. However, this study was performed on a very small patient population (12 patients total), using the weakest of the QS lasers, the 755 nm Alexandrite laser. One of the other main problems with this study was the fact that more than half of the 18 tattoos removed were not professional and amateur tattoos are always easier to remove. Proof of concept studies are underway, but many laser experts advise against the R20 method using the more modern and powerful tattoo removal lasers available at most offices as an increase in adverse side effects including scarring and dischromia is likely. Patients should inquire about the laser being used if the R20 treatment method is offered by a facility as it is usually only offered by clinics that are using the weak 755 nm Alexandrite as opposed to the more powerful and versatile devices that are more commonly used. Moreover, dermatologists offering the R20 method should inform patients that it just one alternative to proven protocols and is not a gold standard treatment method to remove tattoos.\n\nThere are a number of factors that determine how many treatments will be needed and the level of success one might experience. Age of tattoo, ink density, color and even where the tattoo is located on the body, all play an important role in how many treatments will be needed for complete removal. However, a rarely recognized factor of tattoo removal is the role of the client’s immune response. The normal process of tattoo removal is fragmentation followed by phagocytosis which is then drained away via the lymphatics. Consequently, it’s the inflammation resulting from the actual laser treatment and the natural stimulation of the hosts’s immune response that ultimately results in removal of tattoo ink; thus variations in results are enormous.\n\nLaser tattoo removal is uncomfortable - many patients say it is worse than getting the tattoo. The pain is often described to be similar to that of hot oil on the skin, or a \"snap\" from an elastic band. Depending on the patient's pain threshold, and while some patients may forgo anesthesia altogether, most patients will require some form of local anesthesia. Pre-treatment might include the application of an anesthetic cream under occlusion for 45 to 90 minutes or cooling by ice or cold air prior to the laser treatment session. A better method is complete anesthesia which can be administered locally by injections of 1% to 2% lidocaine with epinephrine.\n\nA simple, new technique (published in March 2014) which helps to reduce the pain sensation felt by patients has been described by MJ Murphy He used a standard microscope glass slide pressed against the tattooed skin and fired the laser through the glass. Results on 31 volunteers showed a significant reduction of up to 50% in pain alongside a reduction in blistering and punctate bleeding. This technique represents the simplest and most effective method to reduce the pain sensation using a non-invasive procedure.\n\nImmediately after laser treatment, a slightly elevated, white discoloration with or without the presence of punctuate bleeding is often observed. This white color change is thought to be the result of rapid, heat-formed steam or gas, causing dermal and epidermal vacuolization. Pinpoint bleeding represents vascular injury from photoacoustic waves created by the laser's interaction with tattoo pigment. Minimal edema and erythema of adjacent normal skin usually resolve within 24 hours. Subsequently, a crust appears over the entire tattoo, which sloughs off at approximately two weeks post-treatment. As noted above, some tattoo pigment may be found within this crust. Post-operative wound care consists of simple wound care and a non-occlusive dressing. Since the application of laser light is sterile there is no need for topical antibiotics. Moreover, topical antibiotic ointments can cause allergic reactions and should be avoided. Fading of the tattoo will be noted over the next eight weeks and re-treatment energy levels can be tailored depending on the clinical response observed.\n\nAbout half of the patients treated with Q-switched lasers for tattoo removal will show some transient changes in the normal skin pigmentation. These changes usually resolve in 6 to 12 months but may rarely be permanent.\n\nHyperpigmentation is related to the patient's skin tone, with skin types IV, V and VI more prone regardless of the wavelength used. Twice daily treatment with hydroquinones and broad-spectrum sunscreens usually resolves the hyperpigmentation within a few months, although, in some patients, resolution can be prolonged.\n\nHypopigmentation is more commonly observed in darker skin tones. It is more likely to occur with higher fluence and more frequent treatments. Sometimes lighter skin exhibits hypopigmentation after a series of treatments. Allowing more time between treatments reduces chances of hypopigmentation. Since it is more likely to see hypopigmentation after multiple treatments, some practitioners suggest waiting a few additional weeks, after a few sessions. Usually treatment stops until hypopigmentation resolves in a matter of months. \n\nTransient textural changes are occasionally noted but often resolve within a few months; however, permanent textural changes and scarring very rarely occur. If a patient is prone to pigmentary or textural changes, longer treatment intervals are recommended. Additionally, if a blister or crust forms following treatment, it is imperative that the patient does not manipulate this secondary skin change. Early removal of a blister of crust increases the chances of developing a scar. Additionally, patients with a history of hypertrophic or keloidal scarring need to be warned of their increased risk of scarring.\n\nLocal allergic responses to many tattoo pigments have been reported, and allergic reactions to tattoo pigment after Q-switched laser treatment are also possible. Rarely, when yellow cadmium sulfide is used to \"brighten\" the red or yellow portion of a tattoo, a photoallergic reaction may occur. The reaction is also common with red ink, which may contain cinnabar (mercuric sulphide). Erythema, pruritus, and even inflamed nodules, verrucose papules, or granulomas may present. The reaction will be confined to the site of the red/yellow ink. Treatment consists of strict sunlight avoidance, sunscreen, interlesional steroid injections, or in some cases, surgical removal. Unlike the destructive modalities described, Q-switched lasers mobilize the ink and may generate a systemic allergic response. Oral antihistamines and anti-inflammatory steroids have been used to treat allergic reactions to tattoo ink.\n\nStudies of various tattoo pigments have shown that a number of pigments (most containing iron oxide or titanium dioxide) change color when irradiated with Q-switched laser energy. Some tattoo colors including flesh tones, light red, white, peach and light brown containing pigments as well as some green and blue tattoo pigments, changed to black when irradiated with Q-switched laser pulses. The resulting gray-black color may require more treatments to remove. If tattoo darkening does occur, after 8 weeks the newly darkened tattoo can be treated as if it were black pigment.\n\nVery rarely, non Q-switched laser treatments, like CO2 or Argon lasers, which are very rarely offered these days, can rupture blood vessels and aerosolize tissue requiring a plastic shield or a cone device to protect the laser operator from tissue and blood contact. Protective eyewear may be worn if the laser operator chooses to do so.\n\nWith the mechanical or salabrasion method of tattoo removal, the incidence of scarring, pigmentary alteration (hyper- and hypopigmentation), and ink retention are extremely high.\n\nThe use of Q-switched lasers could very rarely produce the development of large bulla. However, if patients follow post care directions to elevate, rest, and apply intermittent icing, it should minimize the chances of bulla and other adverse effects. In addition, health care practitioners should contemplate the use of a cooling device during the tattoo removal procedure. While the infrequent bulla development is a possible side effect of Q-switched laser tattoo removal, if treated appropriately and quickly by the health care practitioner, it is unlikely that long term consequences would ensue.\n\nAlthough laser treatment is well known and often used to remove tattoos, unwanted side effects of laser tattoo removal include the possibility of discoloration of the skin such as hypopigmentation (white spots, more common in darker skin) and hyperpigmentation (dark spots) as well as textural changes - these changes are usually not permanent when the Nd:YAG is used but it is much more likely with the use of the 755 nm Alexandrite, the 694 nm Ruby and the R20 method. Very rarely, burns may result in scarring but this usually only occurs when patients don't care for the treated area properly. Occasionally, \"paradoxical darkening\" of a tattoo may occur, when a treated tattoo becomes darker instead of lighter. This occurs most often with white ink, flesh tones, pink, and cosmetic make-up tattoos.\n\nSome tattoo pigments contain metals that could theoretically break down into toxic chemicals in the body when exposed to light. This has not yet been reported in vivo but has been shown in laboratory tests. Laser removal of traumatic tattoos may similarly be complicated depending on the substance of the pigmenting material. In one reported instance, the use of a laser resulted in the ignition of embedded particles of firework debris.\n\nNotes\n\nFurther reading\n\n"
            ],
            "good_message": false,
            "loggedData": [
                {
                    "curr_message": " A tattoo is a form of body modification where a design is made by inserting ink, dyes and pigments, either indelible or temporary, into the dermis layer of the skin to change the pigment.",
                    "last_key_pressed": "<<selected>>"
                }
            ],
            "offensive": null,
            "researchTask": true,
            "selectedData": [
                "A tattoo is a form of body modification where a design is made by inserting ink, dyes and pigments, either indelible or temporary, into the dermis layer of the skin to change the pigment."
            ],
            "shown_passages": [
                [
                    "Tattoo",
                    [
                        "A tattoo is a form of body modification where a design is made by inserting ink, dyes and pigments, either indelible or temporary, into the dermis layer of the skin to change the pigment.",
                        "Tattoos fall into three broad categories: purely decorative (with no specific meaning); symbolic (with a specific meaning pertinent to the wearer); pictorial (a depiction of a specific person or item).",
                        "Tattoos have historically been regarded in the West as 'uncivilised', and over the last 100 years the fashion has been associated mainly with sailors, working men and criminals.",
                        "By the end of the 20th Century many Western stigmas of the tattoo culture had been dismissed and the practice has become more acceptable and accessible for people of all trades and levels of society."
                    ]
                ],
                [
                    "Process of tattooing",
                    [
                        "The process of tattooing involves the insertion of pigment into the skin's dermis.",
                        "Traditionally, tattooing often involved rubbing pigment into cuts.",
                        "Modern tattooing almost always requires the use of a tattoo machine and often procedures and accessories to reduce the risk to human health.",
                        "Tattooing involves the placement of pigment into the skin's dermis, the layer of dermal tissue underlying the epidermis.",
                        "After initial injection, pigment is dispersed throughout a homogenized damaged layer down through the epidermis and upper dermis, in both of which the presence of foreign material activates the immune system's phagocytes to engulf the pigment particles."
                    ]
                ],
                [
                    "Tattoo removal",
                    [
                        "Tattoo removal has been performed with various tools since the start of tattooing.",
                        "While tattoos were once considered permanent, it is now possible to remove them with treatments, fully or partially.",
                        "Before the development of laser tattoo removal methods, common techniques included dermabrasion, TCA (Trichloroacetic acid, an acid that removes the top layers of skin, reaching as deep as the layer in which the tattoo ink resides), salabrasion (scrubbing the skin with salt), cryosurgery and excision which is sometimes still used along with skin grafts for larger tattoos.",
                        "Some early forms of tattoo removal included the injection or application of wine, lime, garlic or pigeon excrement."
                    ]
                ]
            ],
            "speaker": "0_Wizard",
            "suggestionsConfig": "show",
            "suggestionsNumber": 5,
            "suggestionsShown": true,
            "text": " A tattoo is a form of body modification where a design is made by inserting ink, dyes and pigments, either indelible or temporary, into the dermis layer of the skin to change the pigment.",
            "textboxFilled": 0,
            "time": 1580233231.5870075,
            "timeTaken": 34.635,
            "totalTasks": 120,
            "turn": 6
        },
        {
            "checked_passage": {
                "no_passages_used": "no_passages_used"
            },
            "checked_sentence": {
                "no_passages_used": "no_passages_used"
            },
            "clicked_passages": {},
            "context": {
                "conversation": [
                    "I enjoy working for the New York Times.",
                    "That is the newspaper based in New York City.",
                    "Do you know when the paper was first founded?",
                    "Yes it was founded in 1851.",
                    "Wow thats a long time. Who actually owns the paper?",
                    "It is owned by The New York Times Company."
                ],
                "index": 29
            },
            "contextCount": 0,
            "full_passages": [
                "The New York Times\n\nThe New York Times (sometimes abbreviated as The\" \"NYT or The Times) is an American newspaper based in New York City with worldwide influence and readership. Founded in 1851, the paper has won 122 Pulitzer Prizes, more than any other newspaper.\n\nAs of September 2016, it had the largest combined print-and-digital circulation of any daily newspaper in the United States. \"The New York Times\" is ranked 18th in the world by circulation.\n\nThe paper is owned by The New York Times Company, which is publicly traded but primarily controlled by the Ochs-Sulzberger family through a dual-class share structure. It has been owned by the family since 1896; A.G. Sulzberger the paper's publisher and, his father, Arthur Ochs Sulzberger Jr. the company's chairman, is the fourth and fifth generation of the family to helm the paper.\n\nNicknamed \"The Gray Lady\", \"The New York Times\" has long been regarded within the industry as a national \"newspaper of record\". The paper's motto, \"All the News That's Fit to Print\", appears in the upper left-hand corner of the front page.\n\nSince the mid-1970s, \"The New York Times\" has greatly expanded its layout and organization, adding special weekly sections on various topics supplementing the regular news, editorials, sports, and features. Since 2008, \"The New York Times\" has been organized into the following sections: News, Editorials/Opinions-Columns/Op-Ed, New York (metropolitan), Business, Sports of The Times, Arts, Science, Styles, Home, Travel, and other features. On Sunday, \"The New York Times\" is supplemented by the \"Sunday Review\" (formerly the \"Week in Review\"), \"The New York Times Book Review\", \"The New York Times Magazine\" and \"\" (T is published 13 times a year). \"The New York Times\" stayed with the broadsheet full page set-up (as some others have changed into a tabloid lay-out) and an eight-column format for several years, after most papers switched to six, and was one of the last newspapers to adopt color photography, especially on the front page.\n\n\"The New York Times\" was founded as the New-York Daily Times on September 18, 1851. Founded by journalist and politician Henry Jarvis Raymond (1820–1869), and former banker George Jones, the \"Times\" was published by Raymond, Jones & Company (which raised about $70,000 initially). Early investors in the company were Edwin B. Morgan, Christopher Morgan, and Edward B. Wesley. Sold for a penny (equivalent to cents today), the inaugural edition attempted to address various speculations on its purpose and positions that preceded its release:\nIn 1852, the newspaper started a western division, \"The Times of California\", which arrived whenever a mail boat got to California. However, when local California newspapers came into prominence, the effort failed.\n\nThe newspaper shortened its name to The New-York Times on September 14, 1857. It dropped the hyphen in the city name on December 1, 1896. On April 21, 1861, \"The New York Times\" began publishing a Sunday edition to offer daily coverage of the Civil War. One of the earliest public controversies it was involved with was the Mortara Affair, the subject of twenty editorials it published alone.\n\nThe main office of \"The New York Times\" was attacked during the New York Draft Riots. The riots, sparked by the beginning of drafting for the Union Army, beginning on July 13, 1863. On \"Newspaper Row\", across from City Hall, Henry Raymond stopped the rioters with Gatling guns (early machine guns), one of which he manned himself. The mob diverted, and attacked the headquarters of abolitionist publisher Horace Greeley's \"New York Tribune\" until forced to flee by the Brooklyn City Police, who had crossed the East River to help the Manhattan authorities.\n\nIn 1869, Raymond died, and George Jones took over as publisher.\n\nThe newspaper's influence grew during 1870–1871 when it published a series of exposés on William Tweed, leader of the city's Democratic Party—popularly known as \"Tammany Hall\" (from its early 19th century meeting headquarters)—that led to the end of the Tweed Ring's domination of New York's City Hall. Tweed offered \"The New York Times\" five million dollars (equivalent to more than 100 million dollars today) to not publish the story. \n\nIn the 1880s, \"The New York Times\" transitioned gradually from editorially supporting Republican Party candidates to becoming more politically independent and analytical. In 1884, the paper supported Democrat Grover Cleveland (former Mayor of Buffalo and Governor of New York State) in his first presidential campaign. While this move cost \"The New York Times\" a portion of its readership among its more progressive and Republican readers (the revenue declined from $188,000 to $56,000 from 1883-1884\")\", the paper eventually regained most of its lost ground within a few years.\n\nAfter George Jones died in 1891, Charles Ransom Miller and other \"New York Times\" editors raised $1million dollars to buy the Times printing it under the New York Times Publishing Company. However, the newspaper was financially crippled by the Panic of 1893, and by 1896, the newspaper had a circulation of less than 9,000, and was losing $1,000 a day. That year, controlling interest in it was gained by Adolph Ochs, publisher of the \"Chattanooga Times\" for $75,000.\n\nShortly after assuming control of the paper, Ochs coined the paper's slogan, \"All The News That's Fit To Print\". The slogan has appeared in the paper since September 1896, and has been printed in a box in the upper left hand corner of the front page since early 1897. It was a jab at competing papers such as Joseph Pulitzer's \"New York World\" and William Randolph Hearst's \"New York Journal\" which were now being known for a lurid, sensationalist and often inaccurate reporting of facts and opinions known by the end of the century as \"yellow journalism\". Under Ochs' guidance, \"The New York Times\" achieved international scope, circulation, and reputation (the Sunday circulation went from under 9,000 in 1896 to 780,000 in 1934). In 1904, \"The New York Times\", along with \"The Times\" received the first on-the-spot wireless telegraph transmission from a naval battle, a report of the destruction of the Imperial Russian Navy's Baltic Fleet at the Battle of Port Arthur in the Straits of Tsushima off the eastern coast of Korea in the Yellow Sea in the western Pacific Ocean after just sailing across the globe from Europe from the press-boat \"Haimun\" during the Russo-Japanese War. In 1910, the first air delivery of \"The New York Times\" to Philadelphia began. \"The New York Times\" first trans-Atlantic delivery by air to London occurred in 1919 by dirigible. In 1920, a \"4 A.M. Airplane Edition\" was sent by plane to Chicago so it could be in the hands of Republican convention delegates by evening.\n\nIn the 1940s, the paper extended its breadth and reach. The crossword began appearing regularly in 1942, and the fashion section in 1946. \"The New York Times\" began an international edition in 1946. The international edition stopped publishing in 1967, when \"The New York Times\" joined the owners of the \"New York Herald Tribune\" and \"The Washington Post\" to publish the \"International Herald Tribune\" in Paris.\n\nThe paper's involvement in a 1964 libel case helped bring one of the key United States Supreme Court decisions supporting freedom of the press, \"New York Times Co. v. Sullivan\". In it, the United States Supreme Court established the \"actual malice\" standard for press reports about public officials or public figures to be considered defamatory or libelous. The malice standard requires the plaintiff in a defamation or libel case prove the publisher of the statement knew the statement was false or acted in reckless disregard of its truth or falsity. Because of the high burden of proof on the plaintiff, and difficulty in proving malicious intent, such cases by public figures rarely succeed.\n\nIn 1971, the \"Pentagon Papers\", a secret United States Department of Defense history of the United States' political and military involvement in the Vietnam War from 1945 to 1967, were given (\"leaked\") to Neil Sheehan of \"The New York Times\" by former State Department official Daniel Ellsberg, with his friend Anthony Russo assisting in copying them. \"The New York Times\" began publishing excerpts as a series of articles on June 13. Controversy and lawsuits followed. The papers revealed, among other things, that the government had deliberately expanded its role in the war by conducting air strikes over Laos, raids along the coast of North Vietnam, and offensive actions taken by U.S. Marines well before the public was told about the actions, all while President Lyndon B. Johnson had been promising not to expand the war. The document increased the credibility gap for the U.S. government, and hurt efforts by the Nixon administration to fight the ongoing war.\n\nWhen \"The New York Times\" began publishing its series, President Richard Nixon became incensed. His words to National Security Advisor Henry Kissinger included \"People have gotta be put to the torch for this sort of thing...\" and \"Let's get the son-of-a-bitch in jail.\" After failing to get \"The New York Times\" to stop publishing, Attorney General John Mitchell and President Nixon obtained a federal court injunction that \"The New York Times\" cease publication of excerpts. The newspaper appealed and the case began working through the court system. On June 18, 1971, \"The Washington Post\" began publishing its own series. Ben Bagdikian, a \"Post\" editor, had obtained portions of the papers from Ellsberg. That day the \"Post\" received a call from the Assistant Attorney General, William Rehnquist, asking them to stop publishing. When the \"Post\" refused, the U.S. Justice Department sought another injunction. The U.S. District court judge refused, and the government appealed. On June 26, 1971, the U.S. Supreme Court agreed to take both cases, merging them into \"New York Times Co. v. United States\" 403 US 713. On June 30, 1971, the Supreme Court held in a 6–3 decision that the injunctions were unconstitutional prior restraints and that the government had not met the burden of proof required. The justices wrote nine separate opinions, disagreeing on significant substantive issues. While it was generally seen as a victory for those who claim the First Amendment enshrines an absolute right to free speech, many felt it a lukewarm victory, offering little protection for future publishers when claims of national security were at stake.\n\nIn the 1970s, the paper introduced a number of new lifestyle sections including Weekend and Home, with the aim of attracting more advertisers and readers. Many criticized the move for betraying the paper's mission.\n\nOn September 7, 1976, the paper switched from an eight-column format to a six-column format. The overall page width stayed the same, with each column becoming wider. On September 14, 1987, the \"Times\" printed the heaviest ever newspaper, at over and 1,612 pages.\n\nThe \"Times\" was one of the last newspapers to adopt color photography, with the first color photograph on the front page appearing on October 16, 1997.\n\n\"The New York Times\" switched to a digital production process sometime before 1980, but only began preserving the resulting digital text that year.\n\nIn September 2008, \"The New York Times\" announced that it would be combining certain sections effective October 6, 2008, in editions printed in the New York metropolitan area. The changes folded the Metro Section into the main International / National news section and combined Sports and Business (except Saturday through Monday, when Sports is still printed as a standalone section). This change also included having the name of the Metro section be called New York outside of the Tri-State Area. The presses used by \"The New York Times\" allow four sections to be printed simultaneously; as the paper had included more than four sections all days except Saturday, the sections had to be printed separately in an early press run and collated together. The changes will allow \"The New York Times\" to print in four sections Monday through Wednesday, in addition to Saturday. \"The New York Times\" announcement stated that the number of news pages and employee positions will remain unchanged, with the paper realizing cost savings by cutting overtime expenses.\n\nIn 2009, the newspaper began production of local inserts in regions outside of the New York area. Beginning October 16, 2009, a two-page \"Bay Area\" insert was added to copies of the Northern California edition on Fridays and Sundays. The newspaper commenced production of a similar Friday and Sunday insert to the Chicago edition on November 20, 2009. The inserts consist of local news, policy, sports, and culture pieces, usually supported by local advertisements.\n\nFollowing industry trends, its weekday circulation had fallen in 2009 to fewer than one million.\n\nIn August 2007, the paper reduced the physical size of its print edition, cutting the page width from to a . This followed similar moves by a roster of other newspapers in the previous ten years, including \"USA Today\", \"The Wall Street Journal\", and \"The Washington Post\". The move resulted in a 5% reduction in news space, but (in an era of dwindling circulation and significant advertising revenue losses) also saved about $12million a year.\n\nBecause of its steadily declining sales attributed to the rise of online alternative media and social media, the newspaper has been going through a downsizing for several years, offering buyouts to workers and cutting expenses, in common with a general trend among print news media.\n\nIn December 2012, the \"Times\" published \"Snow Fall\", a six-part article about the 2012 Tunnel Creek avalanche which integrated videos, photos, and interactive graphics and was hailed as a watershed moment for online journalism.\n\nIn 2016, reporters for the newspaper were reportedly the target of cyber security breaches. The Federal Bureau of Investigation was reportedly investigating the attacks. The cyber security breaches have been described as possibly being related to cyberattacks that targeted other institutions, such as the Democratic National Committee.\n\nThe newspaper's first building was located at 113 Nassau Street in New York City. In 1854, it moved to 138 Nassau Street, and in 1858 to 41 Park Row, making it the first newspaper in New York City housed in a building built specifically for its use.\n\nThe newspaper moved its headquarters to the Times Tower, located at 1475 Broadway in 1904, in an area called Longacre Square, that was later renamed Times Square in honor of the newspaper. The top of the building now known as One Times Square is the site of the New Year's Eve tradition of lowering a lighted ball, which was started by the paper. The building is also notable for its electronic news ticker popularly known as \"The Zipper\" where headlines crawl around the outside of the building. It is still in use, but has been operated by Dow Jones & Company since 1995. After nine years in its Times Square tower the newspaper had an annex built at 229 West 43rd Street. After several expansions, the 43rd Street building became the newspaper's main headquarters in 1960 and the Times Tower on Broadway was sold the following year. It served as the newspaper's main printing plant until 1997, when the newspaper opened a state-of-the-art printing plant in the College Point section of the borough of Queens.\n\nA decade later, \"The New York Times\" moved its newsroom and businesses headquarters from West 43rd Street to a new tower at 620 Eighth Avenue between West 40th and 41st Streets, in Manhattan directly across Eighth Avenue from the Port Authority Bus Terminal. The new headquarters for the newspaper, known officially as The New York Times Building but unofficially called the new \"Times Tower\" by many New Yorkers, is a skyscraper designed by Renzo Piano.\n\nDiscriminatory practices restricting women in editorial positions were previously employed by the paper. The newspaper's first general woman reporter was Jane Grant, who described her experience afterwards. She wrote, \"In the beginning I was charged not to reveal the fact that a female had been hired\". Other reporters nicknamed her Fluff and she was subjected to considerable hazing. Because of her gender, promotions were out of the question, according to the then-managing editor. She was there for fifteen years, interrupted by World War I.\n\nIn 1935, Anne McCormick wrote to Arthur Hays Sulzberger, \"I hope you won't expect me to revert to 'woman's-point-of-view' stuff.\" Later, she interviewed major political leaders and appears to have had easier access than her colleagues did. Even those who witnessed her in action were unable to explain how she got the interviews she did. Clifton Daniel said, \"[After World War II,] I'm sure Adenauer called her up and invited her to lunch. She never had to grovel for an appointment.\" Covering world leaders' speeches after World War II at the National Press Club was limited to men by a Club rule. When women were eventually allowed in to hear the speeches, they still were not allowed to ask the speakers questions, although men were allowed and did ask, even though some of the women had won Pulitzer Prizes for prior work. \"Times\" reporter Maggie Hunter refused to return to the Club after covering one speech on assignment. Nan Robertson's article on the Union Stock Yards, Chicago, was read aloud as anonymous by a professor, who then said, \"'It will come as a surprise to you, perhaps, that the reporter is a \"girl,\" he began... [G]asps; amazement in the ranks. 'She had used all her senses, not just her eyes, to convey the smell and feel of the stockyards. She chose a difficult subject, an offensive subject. Her imagery was strong enough to revolt you.'\" \"The New York Times\" hired Kathleen McLaughlin after ten years at the \"Chicago Tribune\", where \"[s]he did a series on maids, going out herself to apply for housekeeping jobs.\"\n\n\"The New York Times\" has had one slogan. Since 1896, the newspaper's slogan has been \"All the News That's Fit to Print.\" In 1896, Adolph Ochs held a competition to attempt to find a replacement slogan, offering a $100 prize for the best one. Entries included \"News, Not Nausea\"; \"In One Word: Adequate\"; \"News Without Noise\"; \"Out Heralds The Herald, Informs The World, and Extinguishes The Sun\"; \"The Public Press is a Public Trust\"; and the winner of the competition, \"All the world's news, but not a school for scandal.\" On May 10, 1960, Wright Patman asked the FTC to investigate whether \"The New York Times's\" slogan was misleading or false advertising. Within 10 days, the FTC responded that it was not.\n\nAgain in 1996, a competition was held to find a new slogan, this time for NYTimes.com. Over 8,000 entries were submitted. Again however, \"All the News That's Fit to Print,\" was found to be the best.\n\nIn addition to its New York City headquarters, the paper has newsrooms in London and Hong Kong. Its Paris newsroom, which had been the headquarters of the paper's international edition, was closed in 2016, although the city remains home to a news bureau and an advertising office. The paper also has an editing and wire service center in Gainesville, Florida.\n\nAs of 2013, the newspaper had 6 news bureaus in the New York region, 14 elsewhere in the United States, and 24 in other countries.\n\nIn 2009, Russ Stanton, editor of the \"Los Angeles Times\", a competitor, stated that the newsroom of \"The New York Times\" was twice the size of the \"Los Angeles Times\", which had a newsroom of 600 at the time.\n\nIn 1896, Adolph Ochs bought \"The New York Times\", a money-losing newspaper, and formed the New York Times Company. The Ochs-Sulzberger family, one of the United States' newspaper dynasties, has owned \"The New York Times\" ever since. The publisher went public on January 14, 1969, trading at $42 a share on the American Stock Exchange. After this, the family continued to exert control through its ownership of the vast majority of Class B voting shares. Class A shareholders are permitted restrictive voting rights while Class B shareholders are allowed open voting rights.\n\nThe Ochs-Sulzberger family trust controls roughly 88 percent of the company's class B shares. Any alteration to the dual-class structure must be ratified by six of eight directors who sit on the board of the Ochs-Sulzberger family trust. The Trust board members are Daniel H. Cohen, James M. Cohen, Lynn G. Dolnick, Susan W. Dryfoos, Michael Golden, Eric M. A. Lax, Arthur O. Sulzberger, Jr. and Cathy J. Sulzberger.\n\nTurner Catledge, the top editor at \"The New York Times\" from 1952 to 1968, wanted to hide the ownership influence. Arthur Sulzberger routinely wrote memos to his editor, each containing suggestions, instructions, complaints, and orders. When Catledge would receive these memos he would erase the publisher's identity before passing them to his subordinates. Catledge thought that if he removed the publisher's name from the memos it would protect reporters from feeling pressured by the owner.\n\nThe position of public editor was established in 2003 to \"investigate matters of journalistic integrity\"; each public editor was to serve a two-year term. The post \"was established to receive reader complaints and question Times journalists on how they make decisions.\" The impetus for the creation of the public editor position was the Jayson Blair affair. Public editors were: Daniel Okrent (2003–2005), Byron Calame (2005–2007), Clark Hoyt (2007–2010) (served an extra year), Arthur S. Brisbane (2010–2012), Margaret Sullivan (2012–2016) (served a four-year term), and Elizabeth Spayd (2016–2017). In 2017, the Times eliminated the position of public editor.\n\nWhen referring to people, \"The New York Times\" generally uses honorifics, rather than unadorned last names (except in the sports pages, Book Review and Magazine).\n\n\"The New York Times\" printed a display advertisement on its first page on January 6, 2009, breaking tradition at the paper. The advertisement, for CBS, was in color and ran the entire width of the page. The newspaper promised it would place first-page advertisements on only the lower half of the page.\n\nIn August 2014, \"The Times\" decided to use the word \"torture\" to describe incidents in which interrogators \"inflicted pain on a prisoner in an effort to get information.\" This was a shift from the paper's previous practice of describe such practices as \"harsh\" or \"brutal\" interrogations.\n\nThe paper maintains a strict profanity policy. A 2007 review of a concert by punk band Fucked Up, for example, completely avoided mention of the group's name. However, the \"Times\" has on occasion published unfiltered video content that includes profanity and slurs where it has determined that such video has news value. During the 2016 U.S. presidential election campaign, the \"Times\" did print the words \"fuck\" and \"pussy,\" among others, when reporting on the vulgar statements made by Donald Trump in a 2005 recording. \"Times\" politics editor Carolyn Ryan said: \"It's a rare thing for us to use this language in our stories, even in quotes, and we discussed it at length,\" ultimately deciding to publish it because of its news value and because \"[t]o leave it out or simply describe it seemed awkward and less than forthright to us, especially given that we would be running a video that showed our readers exactly what was said.\"\n\nIn the absence of a major headline, the day's most important story generally appears in the top-right column, on the main page. The typefaces used for the headlines are custom variations of Cheltenham. The running text is set at 8.7 point Imperial.\n\nThe newspaper is organized in three sections, including the magazine.\n\nSome sections, such as Metro, are only found in the editions of the paper distributed in the New York–New Jersey–Connecticut Tri-state area and not in the national or Washington, D.C. editions. Aside from a weekly roundup of reprints of editorial cartoons from other newspapers, \"The New York Times\" does not have its own staff editorial cartoonist, nor does it feature a comics page or Sunday comics section.\n\nFrom 1851 to 2017, \"The New York Times\" published around 60,000 print issues containing about 3.5 million pages and 15 million articles.\n\nLike most other American newspapers, \"The New York Times\" has experienced a decline in circulation. Its printed weekday circulation dropped by percent to 571,500 copies from 2005 to 2016.\n\n\"The New York Times International Edition\" is a print version of the paper tailored for readers outside the United States. Formerly a joint venture with \"The Washington Post\" named \"The International Herald Tribune\", \"The New York Times\" took full ownership of the paper in 2002 and has gradually integrated it more closely into its domestic operations.\n\n\"The New York Times\" began publishing daily on the World Wide Web on January 22, 1996, \"offering readers around the world immediate access to most of the daily newspaper's contents.\" The website had 555million pageviews in March 2005. The domain \"nytimes.com\" attracted at least 146million visitors annually by 2008 according to a Compete.com study. In March 2009, \"The New York Times\" Web site ranked 59th by number of unique visitors, with over 20million unique visitors, making it the most visited newspaper site with more than twice the number of unique visitors as the next most popular site. , nytimes.com produced 22 of the 50 most popular newspaper blogs. NYTimes.com was ranked 118 in the world, and 32 in the U.S. by Alexa on June 4, 2017.\n\nIn September 2005, the paper decided to begin subscription-based service for daily columns in a program known as \"TimesSelect\", which encompassed many previously free columns. Until being discontinued two years later, \"TimesSelect\" cost $7.95 per month or $49.95 per year, though it was free for print copy subscribers and university students and faculty. To avoid this charge, bloggers often reposted TimesSelect material, and at least one site once compiled links of reprinted material. On September 17, 2007, \"The New York Times\" announced that it would stop charging for access to parts of its Web site, effective at midnight the following day, reflecting a growing view in the industry that subscription fees cannot outweigh the potential ad revenue from increased traffic on a free site. In addition to opening almost the entire site to all readers, \"The New York Times\" news archives from 1987 to the present are available at no charge, as well as those from 1851 to 1922, which are in the public domain. Access to the \"Premium Crosswords\" section continues to require either home delivery or a subscription for $6.95 per month or $39.95 per year. \"Times\" columnists including Nicholas Kristof and Thomas Friedman had criticized \"TimesSelect\", with Friedman going so far as to say \"I hate it. It pains me enormously because it's cut me off from a lot, a lot of people, especially because I have a lot of people reading me overseas, like in India ... I feel totally cut off from my audience.\"\n\n\"The New York Times\" was made available on the iPhone and iPod Touch in 2008, and on the iPad mobile devices in 2010. It was also the first newspaper to offer a video game as part of its editorial content, \"Food Import Folly\" by Persuasive Games. In 2010, \"The New York Times\" editors collaborated with students and faculty from New York University's Studio 20 Journalism Masters program to launch and produce \"The Local East Village\", a hyperlocal blog designed to offer news \"by, for and about the residents of the East Village\". That same year, reCAPTCHA helped to digitize old editions of \"The New York Times\".\n\nIn 2012, \"The New York Times\" introduced a Chinese-language news site, cn.nytimes.com, with content created by staff based in Shanghai, Beijing and Hong Kong, though the server was placed outside of China to avoid censorship issues. In March 2013, \"The New York Times\" and National Film Board of Canada announced a partnership titled \"A Short History of the Highrise\", which will create four short documentaries for the Internet about life in highrise buildings as part of the NFB's \"Highrise\" project, utilizing images from the newspaper's photo archives for the first three films, and user-submitted images for the final film. The third project in the series, \"A Short History of the Highrise\", won a Peabody Award in 2013.\n\nFalling print advertising revenue and projections of continued decline resulted in a \"metered paywall\" being instituted in 2011, regarded as modestly successful after garnering several hundred thousand subscriptions and about $100million in revenue . As announced in March 2011, the paywall would charge frequent readers for access to its online content. Readers would be able to access up to 20 articles each month without charge. (Although beginning in April 2012, the number of free-access articles was halved to just ten articles per month.) Any reader who wanted to access more would have to pay for a digital subscription. This plan would allow free access for occasional readers, but produce revenue from \"heavy\" readers. Digital subscriptions rates for four weeks range from $15 to $35 depending on the package selected, with periodic new subscriber promotions offering four-week all-digital access for as low as 99¢. Subscribers to the paper's print edition get full access without any additional fee. Some content, such as the front page and section fronts remained free, as well as the Top News page on mobile apps. In January 2013, \"The New York Times\" Public Editor Margaret M. Sullivan announced that for the first time in many decades, the paper generated more revenue through subscriptions than through advertising. In December 2017, the number of free articles per month was reduced from ten to five, as the first change to the metered paywall since 2012. An executive of \"The New York Times Company\" stated that the decision was motivated by \"an all-time high\" in the demand for journalism.\n\nThe newspaper's website was hacked on August 29, 2013, by the Syrian Electronic Army, a hacking group that supports the government of Syrian President Bashar al-Assad. The SEA managed to penetrate the paper's domain name registrar, Melbourne IT, and alter DNS records for \"The New York Times\", putting some of its websites out of service for hours.\n\nThe food section is supplemented on the web by properties for home cooks and for out-of-home dining. \"New York Times\" Cooking (cooking.nytimes.com; also available via iOS app) provides access to more than 17,000 recipes on file as of November 2016, and availability of saving recipes from other sites around the web. The newspaper's restaurant search (nytimes.com/reviews/dining) allows online readers to search NYC area restaurants by cuisine, neighborhood, price, and reviewer rating. \"The New York Times\" has also published several cookbooks, including \"The Essential New York Times Cookbook: Classic Recipes for a New Century\", published in late 2010.\n\nAs of December 2017, the \"New York Times\" has 3.5 million paid subscriptions and more than 130 million monthly readers, more than double its audience two years previously.\n\nThe \"Times Reader\" is a digital version of \"The New York Times\". It was created via a collaboration between the newspaper and Microsoft. \"Times Reader\" takes the principles of print journalism and applies them to the technique of online reporting. \"Times Reader\" uses a series of technologies developed by Microsoft and their Windows Presentation Foundation team. It was announced in Seattle in April 2006, by Arthur Ochs Sulzberger Jr., Bill Gates, and Tom Bodkin. In 2009, the \"Times Reader\" 2.0 was rewritten in Adobe AIR. In December 2013, the newspaper announced that the Times Reader app would be discontinued on January 6, 2014, urging readers of the app to instead begin using the subscription-only \"Today's Paper\" app.\n\nIn 2008, \"The New York Times\" created an app for the iPhone and iPod Touch which allowed users to download articles to their mobile device enabling them to read the paper even when they were unable to receive a signal. In April 2010, \"The New York Times\" announced it would begin publishing daily content through an iPad app. , \"The New York Times\" iPad app is ad-supported and available for free without a paid subscription, but translated into a subscription-based model in 2011.\n\nIn 2010, the newspaper also launched an app for Android smartphones, followed later by an app for Windows Phones.\n\n\"The New York Times\" began producing podcasts in 2006. Among the early podcasts were \"Inside The Times\" and \"Inside The New York Times Book Review\". Several of the \"Times\" podcasts were cancelled in 2012. The \"Times\" returned to launching new podcasts in 2016, including \"Modern Love\" with WBUR. On January 30, 2017, \"The New York Times\" launched a news podcast, \"The Daily\".\n\nIn June 2012, \"The New York Times\" launched its first official foreign-language variant, cn.nytimes.com, in Chinese, viewable in both traditional and simplified Chinese characters. The project was led by Craig S. Smith on the business side and Philip P. Pan on the editorial side.\n\nThe site's initial success was interrupted in October that year following the publication of an investigative article by David Barboza about the finances of Chinese Premier Wen Jiabao's family. In retaliation for the article, the Chinese government blocked access to both nytimes.com and cn.nytimes.com inside the People's Republic of China (PRC).\n\nDespite Chinese government interference, however, the Chinese-language operations have continued to develop, adding a second site, cn.nytstyle.com, iOS and Android apps and newsletters, all of which are accessible inside the PRC. The China operations also produce three print publications in Chinese. Traffic to cn.nytimes.com, meanwhile, has risen due to the widespread use of VPN technology in the PRC and to a growing Chinese audience outside mainland China. \"New York Times\" articles are also available to users in China via the use of mirror websites, apps, domestic newspapers, and social media. The Chinese platforms now represent one of \"The New York Times\" top five digital markets globally. The editor-in-chief of the Chinese platforms is Ching-Ching Ni.\n\nThe TimesMachine is a web-based archive of scanned issues of \"The New York Times\" from 1851 through 2002.\n\nUnlike \"The New York Times\" online archive, the TimesMachine presents scanned images of the actual newspaper. All non-advertising content can be displayed on a per-story basis in a separate PDF display page and saved for future reference. The archive is available to \"New York Times\" subscribers, home delivery and/or digital.\n\nBecause of holidays, no editions were printed on November 23, 1851; January 2, 1852; July 4, 1852; January 2, 1853; and January 1, 1854.\n\nBecause of strikes, the regular edition of \"The New York Times\" was not printed during the following periods:\n\n\"The New York Times\" editorial page is often regarded as liberal. In mid-2004, the newspaper's then public editor (ombudsman), Daniel Okrent, wrote that \"the Op-Ed page editors do an evenhanded job of representing a range of views in the essays from outsiders they publish – but you need an awfully heavy counterweight to balance a page that also bears the work of seven opinionated columnists, only two of whom could be classified as conservative (and, even then, of the conservative subspecies that supports legalization of gay unions and, in the case of William Safire, opposes some central provisions of the Patriot Act.\"\n\n\"The New York Times\" has not endorsed a Republican Party member for president since Dwight D. Eisenhower in 1956; since 1960, it has endorsed the Democratic Party nominee in every presidential election (see New York Times presidential endorsements). However, the \"Times\" did endorse incumbent Republican mayors of New York City Rudy Giuliani in 1997 and Michael Bloomberg in 2005 and 2009. The \"Times\" also endorsed Republican New York state governor George Pataki for re-election in 2002.\n\n\"The New York Times\" was criticized for the work of reporter Walter Duranty, who served as its Moscow bureau chief from 1922 through 1936. Duranty wrote a series of stories in 1931 on the Soviet Union and won a Pulitzer Prize for his work at that time; however, he has been criticized for his denial of widespread famine, most particularly the Ukrainian famine in the 1930s. In 2003, after the Pulitzer Board began a renewed inquiry, the \"Times\" hired Mark von Hagen, professor of Russian history at Columbia University, to review Duranty's work. Von Hagen found Duranty's reports to be unbalanced and uncritical, and that they far too often gave voice to Stalinist propaganda. In comments to the press he stated, \"For the sake of The New York Times' honor, they should take the prize away.\"\n\nOn November 14, 2001, in \"The New York Times\" 150th anniversary issue, former executive editor Max Frankel wrote that before and during World War II, the \"NY Times\" had maintained a consistent policy to minimize reports on the Holocaust in their news pages. Laurel Leff, associate professor of journalism at Northeastern University, concluded that the newspaper had downplayed the Third Reich targeting of Jews for genocide. Her 2005 book \"Buried by the Times\" documents the paper's tendency before, during and after World War II to place deep inside its daily editions the news stories about the ongoing persecution and extermination of Jews, while obscuring in those stories the special impact of the Nazis' crimes on Jews in particular. Leff attributes this dearth in part to the complex personal and political views of the newspaper's Jewish publisher, Arthur Hays Sulzberger, concerning Jewishness, antisemitism, and Zionism.\n\nDuring the war, \"The New York Times\" journalist William L. Laurence was \"on the payroll of the War Department\".\n\nIn the mid to late 1950s, \"fashion writer[s]... were required to come up every month with articles whose total column-inches reflected the relative advertising strength of every [\"department\" or \"specialty\"] store [\"assigned\" to a writer]... The monitor of all this was... the advertising director [of the \"NYT\"]... \" However, within this requirement, story ideas may have been the reporters' and editors' own.\n\n\"The New York Times\" supported the 2003 invasion of Iraq. On May 26, 2004, a year after the war started, the newspaper asserted that some of its articles had not been as rigorous as they should have been, and were insufficiently qualified, frequently overly dependent upon information from Iraqi exiles desiring regime change. Reporter Judith Miller retired after criticisms that her reporting of the lead-up to the Iraq War was factually inaccurate and overly favorable to the Bush administration's position, for which \"The New York Times\" later apologized. One of Miller's prime sources was Ahmed Chalabi, an Iraqi expatriate who returned to Iraq after the U.S. invasion and held a number of governmental positions culminating in acting oil minister and deputy prime minister from May 2005 until May 2006.\n\nIn May 2003, \"The New York Times\" reporter Jayson Blair was forced to resign from the newspaper after he was caught plagiarizing and fabricating elements of his stories. Some critics contended that African-American Blair's race was a major factor in his hiring and in \"The New York Times\" initial reluctance to fire him.\n\nThe newspaper was criticized for largely reporting the prosecutors' version of events in the 2006 Duke lacrosse case. Suzanne Smalley of \"Newsweek\" criticized the newspaper for its \"credulous\" coverage of the charges of rape against Duke University lacrosse players. Stuart Taylor, Jr. and KC Johnson, in their book \"Until Proven Innocent: Political Correctness and the Shameful Injustices of the Duke Lacrosse Rape Case\", write: \"at the head of the guilt-presuming pack, \"The New York Times\" vied in a race to the journalistic bottom with trash-TV talk shows.\"\n\nA 2003 study in \"The Harvard International Journal of Press/Politics\" concluded that \"The New York Times\" reporting was more favorable to Israelis than to Palestinians. A 2002 study published in the journal \"Journalism\" examined Middle East coverage of the Second Intifada over a one-month period in the \"Times\", \"Washington Post\" and \"Chicago Tribune\". The study authors said that the \"Times\" was \"the most slanted in a pro-Israeli direction\" with a bias \"reflected ... in its use of headlines, photographs, graphics, sourcing practices and lead paragraphs.\"\n\nFor its coverage of the Israeli–Palestinian conflict, some (such as Ed Koch) have claimed that the paper is pro-Palestinian, while others (such as As'ad AbuKhalil) have insisted that it is pro-Israel. \"The Israel Lobby and U.S. Foreign Policy\", by political science professors John Mearsheimer and Stephen Walt, alleges that \"The New York Times\" sometimes criticizes Israeli policies but is not even-handed and is generally pro-Israel. On the other hand, the Simon Wiesenthal Center has criticized \"The New York Times\" for printing cartoons regarding the Israeli-Palestinian conflict that were claimed to be anti-Semitic.\n\nIsraeli Prime Minister Benjamin Netanyahu rejected a proposal to write an article for the paper on grounds of lack of objectivity. A piece in which Thomas Friedman commented that praise awarded to Netanyahu during a speech at congress was \"paid for by the Israel lobby\" elicited an apology and clarification from its writer.\n\n\"The New York Times\" public editor Clark Hoyt concluded in his January 10, 2009, column, \"Though the most vociferous supporters of Israel and the Palestinians do not agree, I think \"The New York Times\", largely barred from the battlefield and reporting amid the chaos of war, has tried its best to do a fair, balanced and complete job— and has largely succeeded.\"\n\nIn February 2009, a \"Village Voice\" music blogger accused the newspaper of using \"chintzy, ad-hominem allegations\" in an article on British Tamil music artist M.I.A. concerning her activism against the Sinhala-Tamil conflict in Sri Lanka. M.I.A. criticized the paper in January 2010 after a travel piece rated post-conflict Sri Lanka the \"#1 place to go in 2010\". In June 2010, \"The New York Times Magazine\" published a correction on its cover article of M.I.A., acknowledging that the interview conducted by current \"W\" editor and then-\"Times Magazine\" contributor Lynn Hirschberg contained a recontextualization of two quotes. In response to the piece, M.I.A. broadcast Hirschberg's phone number and secret audio recordings from the interview via her Twitter and website.\n\n\"The New York Times\" was criticized for the 13-month delay of the December 2005 story revealing the U.S. National Security Agency warrantless surveillance program. Ex-NSA officials blew the whistle on the program to journalists James Risen and Eric Lichtblau, who presented an investigative article to the newspaper in November 2004, weeks before America's presidential election. Contact with former agency officials began the previous summer.\n\nFormer \"The New York Times\" executive editor Bill Keller decided not to report the piece after being pressured by the Bush administration and being advised not to do so by \"New York Times\" Washington bureau chief Philip Taubman. Keller explained the silence's rationale in an interview with the newspaper in 2013, stating \"Three years after 9/11, we, as a country, were still under the influence of that trauma, and we, as a newspaper, were not immune\".\n\nIn 2014, \"PBS Frontline\" interviewed Risen and Lichtblau, who said that the newspaper's plan was to not publish the story at all. \"The editors were furious at me\", Risen said to the program. \"They thought I was being insubordinate.\" Risen wrote a book about the mass surveillance revelations after \"The New York Times\" declined the piece's publication, and only released it after Risen told them that he would publish the book. Another reporter told NPR that the newspaper \"avoided disaster\" by ultimately publishing the story.\n\nOn June 16, 2015, \"The New York Times\" published an article reporting the deaths of six Irish students staying in Berkeley, California when the balcony they were standing on collapsed, the paper's story insinuating that they were to blame for the collapse. The paper stated that the behavior of Irish students coming to the US on J1 visas was an \"embarrassment to Ireland\". The Irish Taoiseach and former President of Ireland criticized the newspaper for \"being insensitive and inaccurate\" in its handling of the story.\n\nIn May 2015, a \"New York Times\" exposé by Sarah Maslin Nir on the working conditions of manicurists in New York City and elsewhere and the health hazards to which they are exposed attracted wide attention, resulting in emergency workplace enforcement actions by New York governor Andrew M. Cuomo. In July 2015, the story's claims of widespread illegally low wages were challenged by former \"New York Times\" reporter Richard Bernstein, in the \"New York Review of Books\". Bernstein, whose wife owns two nail salons, asserted that such illegally low wages were inconsistent with his personal experience, and were not evidenced by ads in the Chinese-language papers cited by the story. \"The New York Times\" editorial staff subsequently answered Bernstein's criticisms with examples of several published ads and stating that his response was industry advocacy. The independent \"NYT\" Public Editor also reported that she had previously corresponded with Bernstein and looked into his complaints, and expressed her belief that the story's reporting was sound.\n\nIn September and October 2015, nail salon owners and workers protested at \"The New York Times\" offices several times, in response to the story and the ensuing New York State crackdown. In October 2015, \"Reason\" magazine published a three part re-reporting of the story by Jim Epstein, charging that the series was filled with misquotes and factual errors respecting both its claims of illegally low wages and health hazards. Epstein additionally argued that \"The New York Times\" had mistranslated the ads cited in its answer to Bernstein, and that those ads actually validated Bernstein's argument. In November 2015, \"The New York Times\" public editor concluded that the exposé's \"findings, and the language used to express them, should have been dialed back — in some instances substantially\" and recommended that \"The Times write further follow-up stories, including some that re-examine its original findings and that take on the criticism from salon owners and others — not defensively but with an open mind.\"\n\nA 2015 study found that \"The New York Times\" fed into an overarching tendency towards national bias. During the Iranian nuclear crisis the newspaper minimized the \"negative processes\" of the United States while overemphasizing similar processes of Iran. This tendency was shared by other papers such as \"The Guardian\", \"Tehran Times\", and the Fars News Agency, while Xinhua News Agency was found to be more neutral while at the same time mimicking the foreign policy of the Peoples' Republic of China.\n\nIn April 2016, two black female employees in their sixties filed a federal class action lawsuit against The New York Times Company CEO Mark Thompson and chief revenue officer Meredith Levien, claiming age, gender, and racial discrimination. The plaintiffs claim that the \"Times\" advertising department favored younger white employees over older black employees in making firing and promotion decisions. The \"Times\" said that the suit was \"entirely without merit\" and was \"a series of recycled, scurrilous and unjustified attacks.\"\n\n\"New York Times\" public editor (ombudsman) Elizabeth Spayd wrote in 2016 that \"Conservatives and even many moderates, see in \"The Times\" a blue-state worldview\" and accuse it of harboring a liberal bias. Spayd did not analyze the substance of the claim, but did opine that the \"Times\" is \"part of a fracturing media environment that reflects a fractured country. That in turn leads liberals and conservatives toward separate news sources.\" \"Times\" executive editor Dean Baquet stated that he does not believe coverage has a liberal bias, but that: \"We have to be really careful that people feel like they can see themselves in \"The New York Times\". I want us to be perceived as fair and honest to the world, not just a segment of it. It's a really difficult goal. Do we pull it off all the time? No.\"\n\n\"Times\" public editor Arthur Brisbane wrote in 2012: \"When The Times covers a national presidential campaign, I have found that the lead editors and reporters are disciplined about enforcing fairness and balance, and usually succeed in doing so. Across the paper's many departments, though, so many share a kind of political and cultural progressivism — for lack of a better term — that this worldview virtually bleeds through the fabric of \"The Times\".\"\n\nIn mid-2004, the newspaper's then-public editor Daniel Okrent, wrote an opinion piece in which he said that \"The New York Times\" did have a liberal bias in news coverage of certain social issues such as abortion and same-sex marriage. He stated that this bias reflected the paper's cosmopolitanism, which arose naturally from its roots as a hometown paper of New York City. He wrote, \"if you're examining the paper's coverage of these subjects from a perspective that is neither urban nor Northeastern nor culturally seen-it-all; if you are among the groups \"The Times\" treats as strange objects to be examined on a laboratory slide (devout Catholics, gun owners, Orthodox Jews, Texans); if your value system wouldn't wear well on a composite \"New York Times\" journalist, then a walk through this paper can make you feel you're traveling in a strange and forbidding world.\" Okrent wrote that the \"Time\"s Arts & Leisure; the Sunday \"Times Magazine\", and Culture coverage trend to the left.\n\nIn December 2004, a University of California, Los Angeles study by former fellows of a conservative think tank gave \"The New York Times\" a score of 73.7 on a 100-point scale, with 0 being most conservative and 100 being most liberal, making it the second-most liberal major newspaper in the study after \"The Wall Street Journal\" (85.1). The validity of the study has been questioned, however. The liberal watchdog group Media Matters for America pointed out potential conflicts of interest with the author's funding, and political scientists, such as Brendan Nyhan, cited flaws in the study's methodology.\n\nDonald Trump has frequently criticized \"The New York Times\" on his Twitter account before and during his presidency; since November 2015, Trump has referred to the \"Times\" as \"the failing \"New York Times\"\" in a series of tweets. Despite Trump's criticism, \"New York Times\" editor Mark Thompson noted that the paper had enjoyed soaring digital readership, with the fourth quarter of 2016 seeing the highest number of new digital subscribers to the newspaper since 2011.\n\nCritic Matt Taibbi accused \"The New York Times\" of favoring Hillary Clinton over Bernie Sanders in the paper's news coverage of the 2016 Democratic presidential primaries. Responding to the complaints of many readers, \"New York Times\" public editor Margaret Sullivan wrote that \"\"The Times\" has not ignored Mr. Sanders's campaign, but it hasn't always taken it very seriously. The tone of some stories is regrettably dismissive, even mocking at times. Some of that is focused on the candidate's age, appearance and style, rather than what he has to say.\" \"Times\" senior editor Carolyn Ryan defended both the volume of \"New York Times\" coverage (noting that Sanders had received about the same amount of article coverage as Jeb Bush and Marco Rubio) and its tone.\n\nThe \"Times\" has developed a national and international \"reputation for thoroughness\" over time. Among journalists, the paper is held in high regard; a 1999 survey of newspaper editors conducted by the \"Columbia Journalism Review\" found that the \"Times\" was the \"best\" American paper, ahead of \"The Washington Post\", \"The Wall Street Journal\", and \"Los Angeles Times\". The \"Times\" also was ranked #1 in a 2011 \"quality\" ranking of U.S. newspapers by Daniel de Vise of \"The Washington Post\"; the objective ranking took into account the number of recent Pulitzer Prizes won, circulation, and perceived Web site quality. A 2012 report in WNYC called the \"Times\" \"the most respected newspaper in the world.\"\n\nNevertheless, like many other U.S. media sources, the \"Times\" had suffered from a decline in public perceptions of credibility in the U.S. from 2004 to 2012. A Pew Research Center survey in 2012 asked respondents about their views on credibility of various news organizations. Among respondents who gave a rating, 49% said that they believed \"all or most\" of the \"Times\"s reporting, while 50% disagreed. A large percentage (19%) of respondents were unable to rate believability. The \"Times\"s score was comparable to that of \"USA Today\". Media analyst Brooke Gladstone of WNYC's \"On the Media\", writing for \"The New York Times\", says that the decline in U.S. public trust of the mass media can be explained (1) by the rise of the polarized Internet-driven news; (2) by a decline in trust in U.S. institutions more generally; and (3) by the fact that \"Americans say they want accuracy and impartiality, but the polls suggest that, actually, most of us are seeking affirmation.\"\n\n\"The New York Times\" has won 122 Pulitzer Prizes, more than any other newspaper. The prize is awarded for excellence in journalism in a range of categories.\n\nIt has also (as of 2014) won three Peabody Awards (and jointly received two).\n\n\n\nOfficial \"NYT\" websites\nUnofficial \"NYT\"-related websites\n",
                "The New York Times Company\n\nThe New York Times Company is an American media company which publishes its namesake, \"The New York Times\". Arthur Ochs Sulzberger Jr. has served as chairman since 1997. It is headquartered in Manhattan, New York.\n\nThe company was founded by Henry Jarvis Raymond and George Jones in New York City. The first edition of the newspaper \"The New York Times\", published on September 18, 1851, stated: \"We publish today the first issue of the New-York Daily Times, and we intend to issue it every morning (Sundays excepted) for an indefinite number of years to come.\"\n\nThe company moved into the cable channel industry purchasing a 40% interest in the Popcorn Channel, a theatrical movie preview and local movie times, in November 1994.\n\nThe company completed its purchase of \"The Washington Post\" 50 percent interest in the \"International Herald Tribune\" (\"IHT\") for US$65 million on January 1, 2003, becoming the sole owner.\n\nOn March 18, 2005, the company acquired About.com, an online provider of consumer information for US$410 million. In 2005, the company reported revenues of US$3.4 billion to its investors.\n\nThe Times, on August 25, 2006, acquired Baseline StudioSystems, an online database and research service on the film and television industries for US$35 million.\n\nThe company announced on September 12, 2006, its decision to sell its Broadcast Media Group, consisting of \"nine network-affiliated television stations, their related Web sites and the digital operating center\" \"The New York Times\" reported on January 4, 2007, that the company had reached an agreement to sell all nine local television stations to the private equity firm Oak Hill Capital Partners, which then created a holding company for the stations, Local TV LLC. The company announced that it had finalized the sale of its Broadcast Media Group on May 7, 2007, for \"approximately $575 million.\"\n\nThe company moved from 229 West 43rd Street to The New York Times Building at 620 Eighth Avenue, on the west side of Times Square, between 40th and 41st streets across from the Port Authority of New York & New Jersey Bus Terminal.\n\nOn July 14, 2009, the company announced that WQXR was to be sold to WNYC, which moved the station to 105.9 FM and began to operate the station as non-commercial on October 8, 2009. This US$45 million transaction, which involved Univision Radio's WCAA moving to the 96.3 FM frequency from 105.9 FM, ended the Times' 65-year-long ownership of the station.\n\nIn December 2011, the company sold its Regional Media Group to Halifax Media Group, owners of \"The Daytona Beach News-Journal\", for $143 million. \"The Boston Globe\" and \"The Telegram & Gazette\" of Worcester were not part of the sale. In 2011, the Times sold Baseline StudioSystems back to its original owners, Laurie S. Silvers and Mitchell Rubenstein, majority shareholders of Project Hollywood LLC.\n\nFacing falling revenue from print advertising in its flagship publication in 2011, \"The New York Times\", the company introduced a paywall to its website. As of 2012, it has been modestly successful, garnering several hundred thousand subscriptions and about $100 million in annual revenue.\n\nIn 2013, the New York Times Company sold \"The Boston Globe\" and other New England media properties to John W. Henry, the principal owner of the Boston Red Sox. According to the Times Company, the move was made in order to focus more on its core brands.\n\nThe paper bought AM radio station WQXR (1560kHz) in 1944. Its \"sister\" FM station, WQXQ, would become WQXR-FM (96.3MHz). Branded as \"The Radio Stations of \"The New York Times\"\", its classical music radio format was simulcast on both the AM & FM frequencies until December 1992, when the big-band and pop standards music format of station WNEW (1130kHz – now WBBR/\"Bloomberg Radio\") was transferred to and adopted by WQXR; in recognition of the format change, WQXR changed its call letters to WQEW (a \"hybrid\" combination of \"WQXR\" and \"WNEW\"). By 1999, \"The New York Times\" was leasing WQEW to ABC Radio for its \"Radio Disney\" format. In 2007, WQEW was finally purchased by Disney; in late 2014, it was sold to Family Radio (a religious radio network) and became WFME. On July 14, 2009, it was announced that WQXR-FM would be sold to the WNYC radio group who, on October 8, 2009, moved the station from 96.3 to 105.9MHz (swapping frequencies with Spanish-language station WXNY-FM, which wanted the more powerful transmitter to increase its coverage) and began operating it as a non-commercial, public radio station. After the purchase, WQXR-FM retained the classical music format, whereas WNYC-FM (93.9MHz) abandoned it, switching to a talk radio format.\n\nAlongside its namesake newspaper, the company also owns \"the New York Times International Edition\" and their related digital properties including NYTimes.com, as well as various brand-related properties.\n\nSince 1967, the company has been listed on the New York Stock Exchange under the symbol NYT. Of the two categories of stock, Class A and Class B, the former is publicly traded and the latter is held privately --- largely (nearly 90%) by the descendants of Adolph Ochs, who purchased \"The New York Times\" newspaper in 1896. \n\nOn January 20, 2009, \"The New York Times\" reported that its parent company, The New York Times Company, had reached an agreement to borrow $250million from Carlos Slim, a Mexican businessman and the world's second richest person, \"to help the newspaper company finance its businesses\". The New York Times Company later repaid that loan ahead of schedule. Since then, Slim has bought large quantities of the company's Class A shares, which are available for purchase by the public and offer less control over the company than Class B shares, which are privately held. Slim's investments in the company included large purchases of Class A shares in 2011, when he increased his stake in the company to 8.1% of Class A shares, and again in 2015, when he exercised stock options—acquired as part of a repayment plan on the 2009 loan—to purchase 15.9million Class A shares, making him the largest shareholder. As of March 7, 2016, Slim owned 17.4% of the company's Class A shares, according to annual filings submitted by the company.\n\nAlthough Slim is the largest shareholder in the company, his investment does not give him the ability to control the newspaper, as his stake allows him to vote only for Class A directors, who compose just a third of the company's board. According to the company's 2016 annual filings, Slim did not own any of the company's Class B shares.\n\nAt the April 2005 board meeting, Class B shareholders elected nine of the fourteen directors of the company.\n\nThe company sponsors a series of national and local awards designed to highlight the achievements of individuals and organizations in different realms.\n\nIn 2007, it inaugurated its first Nonprofit Excellence Award, awarded to four organizations \"for the excellence of their management practices\". Only nonprofits in New York City, Long Island, or Westchester were eligible.\n\nJointly with the Carnegie Corporation of New York and the American Library Association, The New York Times Company sponsors an award to honor librarians \"for service to their communities.\" The \"I Love My Librarian!\" award was given to ten recipients in December 2008, and presented by The New York Times Company president and CEO Janet L. Robinson, Carnegie Corporation president Vartan Gregorian, and Jim Rettig, president of the American Library Association.\n\nIn May 2009, the company launched The New York Times Outstanding Playwright Award to honor an American playwright who had recently had his or her professional debut in New York. The first winner was Tarell Alvin McCraney for his play \"The Brothers Size\". In 2010, Dan LeFranc won for his play \"Sixty Miles to Silver Lake\".\n\n",
                "WQXR-FM\n\nWQXR-FM (105.9 FM) is an American classical radio station licensed to Newark, New Jersey and serving the New York City metropolitan area. It is the most-listened-to classical-music station in the United States, with an average quarter-hour audience of 63,000.\n\nIt is owned by the nonprofit New York Public Radio, which also operates WNYC (820 AM and 93.9 FM) and the four-station New Jersey Public Radio group. New York Public Radio acquired WQXR on July 14, 2009, as part of a three-way trade which also involved The New York Times Companythe previous owners of WQXRand Univision Radio. WQXR-FM broadcasts from studios and offices located in the Hudson Square neighborhood in Lower Manhattan, New York City, and the transmitter is located atop the Empire State Building.\n\nAt 8:00 p.m. on October 8, 2009, Univision's WCAA moved to the 96.3 FM frequency while WQXR-FM moved to 105.9 FM, becoming a non-commercial radio station run by New York Public Radio. Within that next week WCAA, now on 96.3, changed its call letters to WXNY-FM.\n\nWQXR once used a translator in Asbury Park, New Jersey that transmitted its programming at 96.7 FM, however the owners of the translator sold it and the new owners moved it out of Asbury Park, forcing WQXR to no longer broadcast in the area on that frequency.\n\nWQXR gained former stand-alone station WDFH as a repeater on July 29, 2013, changing its call-letters to the current WQXW.\n\nFull-powered simulcasts\nTranslators\nWQXR-FM is the outgrowth of a \"high-fidelity\" AM station, WQXR (1560 AM), which was founded in 1936 by John V. L. Hogan and Elliott Sanger. Hogan began this station as a mechanical television station, W2XR, which went on the air on March 26, 1929.\n\nThe radio station broadcast mainly classical music recordings. One of the station's listeners was the inventor of frequency modulation, Edwin Howard Armstrong. When Armstrong put his experimental FM station, W2XMN, on the air, he arranged to rebroadcast some of WQXR's programming. This ended in 1939, when Hogan and Sanger put their own experimental FM station on the air, W2XQR, just down the dial from Armstrong at 42.3 MHz.\n\nWhen the Federal Communications Commission began licensing commercial FM stations, W2XQR moved to 45.9 MHz and became W59NY; the special FM call signs were later dropped and the station became WQXQ.\n\nIn 1944, Hogan and Sanger sold their holding company, Interstate Broadcasting Company, to the New York Times Company. When the FM band was moved from 42–50 MHz to its present frequency range of 88–108 MHz in 1945, WQXQ moved to 97.7 MHz. Within a few years, the station had adopted its current callsign, WQXR-FM, and its frequency for the next 64 years, 96.3 MHz.\nWQXR was the first AM station in New York to experiment with broadcasting in stereo, beginning in 1952. During some of its live concerts, it used two microphones positioned six feet apart. The microphone on the right led to its AM feed, and the one on the left to its FM feed, so a listener could position two radios six feet apart, one tuned to 1560 and the other to 96.3, and listen in stereo.\n\nDuring the 1950s, WQXR-FM's programming was also heard on the Rural Radio Network on several stations in Upstate New York, including ones targeting Buffalo, Rochester, Syracuse and Albany. This ended when the RRN stations were sold to Pat Robertson's Christian Broadcasting Network. Both the AM and FM sides continued to simulcast each other until 1965, when the FCC began requiring commonly owned AM and FM stations in large markets to broadcast separate programming for at least part of the day. WQXR-FM concentrated on longer Classical works, while WQXR (AM) aired lighter Classical music and talk programs produced in conjunction with \"The New York Times\". While this plan gave Classical music fans two options, it also increased expenses for the stations.\n\nIn 1962, the QXR network was purchased by Novo Industrial Corporation but WQXR remained under the New York Times Company ownership.\nAfter briefly attempting to sell the WQXR stations in 1971, \"The New York Times\" was able to get a waiver of the simulcasting rules. The stations continued to duplicate each other until 1992, when the AM side changed its programming from classical to popular standards, becoming WQEW (it is now WFME). In 1998, the \"Times\" entered into a long-term lease for WQEW with ABC, a move which brought Radio Disney to New York City. The Times Company also included a purchase clause in the lease contract, and ABC exercised the option in 2007. This left WQXR-FM as the \"Times\" 's lone radio station and, following a sale of its group of television stations to the Local TV LLC company that same year, the Times Company's sole remaining broadcasting property.\n\nOn July 14, 2009, the New York Times Company announced it was trading the 96.3 frequency to Univision Radio in return for the 105.9 frequency of Univision's WCAA. The sale was slated to close in the second half of 2009. At 8 p.m. on October 8, 2009, WCAA and WQXR traded frequencies.\n\nThe frequency swap was part of a three-way deal between Univision, the New York Times Company and New York Public Radio. Univision paid the New York Times Company $33.5 million to trade broadcasting licenses with the Times. New York Public Radio then paid the New York Times Company $11.5 million for 105.9 FM’s license, equipment and the WQXR call letters, music library and website. As a result of the deal, WQXR became a non-commercial public radio station operated by New York Public Radio and now runs three on-air pledge drives a year.\n\nWQXR-FM has less range and population coverage on 105.9 than it had with its old signal on 96.3. WQXR-FM's old and new signals both radiate from the same FM master antenna atop the Empire State Building; but while WQXR-FM's old signal is 6,000 watts ERP (effective radiated power—the energy concentrated toward the horizon), its current signal is 610 watts. The calculated signal strength of the current signal at 30 miles (covering about 14.5 million people) is less than the previous 96.3 FM signal at 42 miles (covering about 17.1 million people). Further compromising coverage is Hartford's WHCN, which also broadcasts on 105.9 MHz. While WHCN has a directional signal with reduced wattage toward WQXR's transmitter, the two stations do interfere with each other where their signals overlap.\n\nWQXR operates a translator station: 103.7 in Highland, New York. It once formerly had 96.7 in Asbury Park, New Jersey until an unknown owner of the frequency sold it and the new owner moved it out of Asbury Park, forcing WQXR to no longer broadcast at that frequency. WQXR's audio is carried over WNYC-FM's HD2 channel at 93.9 FM, and over Time Warner Cable television channel 590 in the Hudson Valley, New York. On July 29, 2013, WQXR began broadcasting on the former WDFH, now WQXW (90.3 FM) in Ossining, New York, covering northern and central Westchester County.\n\nWorldwide, WQXR-FM's standard programming is streamed on its webcast, and the station also streams its HD2 channel called Q2, focusing on classical works by living composers. Q2's daily playlist is called \"Living Music, Living Composers\". The station also has a streaming only channel, called Operavore and dedicated to opera music, which was launched in 2012.\n\n\n\n"
            ],
            "good_message": false,
            "loggedData": [],
            "offensive": null,
            "researchTask": true,
            "selectedData": [],
            "shown_passages": [
                [
                    "The New York Times",
                    [
                        "The New York Times (sometimes abbreviated as The\" \"NYT or The Times) is an American newspaper based in New York City with worldwide influence and readership.",
                        "Founded in 1851, the paper has won 122 Pulitzer Prizes, more than any other newspaper.",
                        "As of September 2016, it had the largest combined print-and-digital circulation of any daily newspaper in the United States.",
                        "\"The New York Times\" is ranked 18th in the world by circulation.",
                        "The paper is owned by The New York Times Company, which is publicly traded but primarily controlled by the Ochs-Sulzberger family through a dual-class share structure."
                    ]
                ],
                [
                    "The New York Times Company",
                    [
                        "The New York Times Company is an American media company which publishes its namesake, \"The New York Times\".",
                        "Arthur Ochs Sulzberger Jr. has served as chairman since 1997.",
                        "It is headquartered in Manhattan, New York.",
                        "The company was founded by Henry Jarvis Raymond and George Jones in New York City.",
                        "The first edition of the newspaper \"The New York Times\", published on September 18, 1851, stated: \"We publish today the first issue of the New-York Daily Times, and we intend to issue it every morning (Sundays excepted) for an indefinite number of years to come.\"",
                        "The company moved into the cable channel industry purchasing a 40% interest in the Popcorn Channel, a theatrical movie preview and local movie times, in November 1994."
                    ]
                ],
                [
                    "WQXR-FM",
                    [
                        "WQXR-FM (105.9 FM) is an American classical radio station licensed to Newark, New Jersey and serving the New York City metropolitan area.",
                        "It is the most-listened-to classical-music station in the United States, with an average quarter-hour audience of 63,000.",
                        "It is owned by the nonprofit New York Public Radio, which also operates WNYC (820 AM and 93.9 FM) and the four-station New Jersey Public Radio group.",
                        "New York Public Radio acquired WQXR on July 14, 2009, as part of a three-way trade which also involved The New York Times Companythe previous owners of WQXRand Univision Radio.",
                        "WQXR-FM broadcasts from studios and offices located in the Hudson Square neighborhood in Lower Manhattan, New York City, and the transmitter is located atop the Empire State Building."
                    ]
                ]
            ],
            "speaker": "0_Wizard",
            "suggestionsConfig": "show",
            "suggestionsNumber": 5,
            "suggestionsShown": true,
            "text": "It is owned by The New York Times Company.",
            "textboxFilled": 0,
            "time": 1580233235.0901632,
            "timeTaken": 3.43,
            "totalTasks": 120,
            "turn": 7
        },
        {
            "checked_passage": {
                "no_passages_used": "no_passages_used"
            },
            "checked_sentence": {
                "no_passages_used": "no_passages_used"
            },
            "clicked_passages": {},
            "context": {
                "conversation": [
                    "I enjoy working for the New York Times.",
                    "That is the newspaper based in New York City.",
                    "Do you know when the paper was first founded?",
                    "Yes it was founded in 1851.",
                    "Wow thats a long time. Who actually owns the paper?",
                    "It is owned by The New York Times Company."
                ],
                "index": 29
            },
            "contextCount": 0,
            "full_passages": [
                "The New York Times Company\n\nThe New York Times Company is an American media company which publishes its namesake, \"The New York Times\". Arthur Ochs Sulzberger Jr. has served as chairman since 1997. It is headquartered in Manhattan, New York.\n\nThe company was founded by Henry Jarvis Raymond and George Jones in New York City. The first edition of the newspaper \"The New York Times\", published on September 18, 1851, stated: \"We publish today the first issue of the New-York Daily Times, and we intend to issue it every morning (Sundays excepted) for an indefinite number of years to come.\"\n\nThe company moved into the cable channel industry purchasing a 40% interest in the Popcorn Channel, a theatrical movie preview and local movie times, in November 1994.\n\nThe company completed its purchase of \"The Washington Post\" 50 percent interest in the \"International Herald Tribune\" (\"IHT\") for US$65 million on January 1, 2003, becoming the sole owner.\n\nOn March 18, 2005, the company acquired About.com, an online provider of consumer information for US$410 million. In 2005, the company reported revenues of US$3.4 billion to its investors.\n\nThe Times, on August 25, 2006, acquired Baseline StudioSystems, an online database and research service on the film and television industries for US$35 million.\n\nThe company announced on September 12, 2006, its decision to sell its Broadcast Media Group, consisting of \"nine network-affiliated television stations, their related Web sites and the digital operating center\" \"The New York Times\" reported on January 4, 2007, that the company had reached an agreement to sell all nine local television stations to the private equity firm Oak Hill Capital Partners, which then created a holding company for the stations, Local TV LLC. The company announced that it had finalized the sale of its Broadcast Media Group on May 7, 2007, for \"approximately $575 million.\"\n\nThe company moved from 229 West 43rd Street to The New York Times Building at 620 Eighth Avenue, on the west side of Times Square, between 40th and 41st streets across from the Port Authority of New York & New Jersey Bus Terminal.\n\nOn July 14, 2009, the company announced that WQXR was to be sold to WNYC, which moved the station to 105.9 FM and began to operate the station as non-commercial on October 8, 2009. This US$45 million transaction, which involved Univision Radio's WCAA moving to the 96.3 FM frequency from 105.9 FM, ended the Times' 65-year-long ownership of the station.\n\nIn December 2011, the company sold its Regional Media Group to Halifax Media Group, owners of \"The Daytona Beach News-Journal\", for $143 million. \"The Boston Globe\" and \"The Telegram & Gazette\" of Worcester were not part of the sale. In 2011, the Times sold Baseline StudioSystems back to its original owners, Laurie S. Silvers and Mitchell Rubenstein, majority shareholders of Project Hollywood LLC.\n\nFacing falling revenue from print advertising in its flagship publication in 2011, \"The New York Times\", the company introduced a paywall to its website. As of 2012, it has been modestly successful, garnering several hundred thousand subscriptions and about $100 million in annual revenue.\n\nIn 2013, the New York Times Company sold \"The Boston Globe\" and other New England media properties to John W. Henry, the principal owner of the Boston Red Sox. According to the Times Company, the move was made in order to focus more on its core brands.\n\nThe paper bought AM radio station WQXR (1560kHz) in 1944. Its \"sister\" FM station, WQXQ, would become WQXR-FM (96.3MHz). Branded as \"The Radio Stations of \"The New York Times\"\", its classical music radio format was simulcast on both the AM & FM frequencies until December 1992, when the big-band and pop standards music format of station WNEW (1130kHz – now WBBR/\"Bloomberg Radio\") was transferred to and adopted by WQXR; in recognition of the format change, WQXR changed its call letters to WQEW (a \"hybrid\" combination of \"WQXR\" and \"WNEW\"). By 1999, \"The New York Times\" was leasing WQEW to ABC Radio for its \"Radio Disney\" format. In 2007, WQEW was finally purchased by Disney; in late 2014, it was sold to Family Radio (a religious radio network) and became WFME. On July 14, 2009, it was announced that WQXR-FM would be sold to the WNYC radio group who, on October 8, 2009, moved the station from 96.3 to 105.9MHz (swapping frequencies with Spanish-language station WXNY-FM, which wanted the more powerful transmitter to increase its coverage) and began operating it as a non-commercial, public radio station. After the purchase, WQXR-FM retained the classical music format, whereas WNYC-FM (93.9MHz) abandoned it, switching to a talk radio format.\n\nAlongside its namesake newspaper, the company also owns \"the New York Times International Edition\" and their related digital properties including NYTimes.com, as well as various brand-related properties.\n\nSince 1967, the company has been listed on the New York Stock Exchange under the symbol NYT. Of the two categories of stock, Class A and Class B, the former is publicly traded and the latter is held privately --- largely (nearly 90%) by the descendants of Adolph Ochs, who purchased \"The New York Times\" newspaper in 1896. \n\nOn January 20, 2009, \"The New York Times\" reported that its parent company, The New York Times Company, had reached an agreement to borrow $250million from Carlos Slim, a Mexican businessman and the world's second richest person, \"to help the newspaper company finance its businesses\". The New York Times Company later repaid that loan ahead of schedule. Since then, Slim has bought large quantities of the company's Class A shares, which are available for purchase by the public and offer less control over the company than Class B shares, which are privately held. Slim's investments in the company included large purchases of Class A shares in 2011, when he increased his stake in the company to 8.1% of Class A shares, and again in 2015, when he exercised stock options—acquired as part of a repayment plan on the 2009 loan—to purchase 15.9million Class A shares, making him the largest shareholder. As of March 7, 2016, Slim owned 17.4% of the company's Class A shares, according to annual filings submitted by the company.\n\nAlthough Slim is the largest shareholder in the company, his investment does not give him the ability to control the newspaper, as his stake allows him to vote only for Class A directors, who compose just a third of the company's board. According to the company's 2016 annual filings, Slim did not own any of the company's Class B shares.\n\nAt the April 2005 board meeting, Class B shareholders elected nine of the fourteen directors of the company.\n\nThe company sponsors a series of national and local awards designed to highlight the achievements of individuals and organizations in different realms.\n\nIn 2007, it inaugurated its first Nonprofit Excellence Award, awarded to four organizations \"for the excellence of their management practices\". Only nonprofits in New York City, Long Island, or Westchester were eligible.\n\nJointly with the Carnegie Corporation of New York and the American Library Association, The New York Times Company sponsors an award to honor librarians \"for service to their communities.\" The \"I Love My Librarian!\" award was given to ten recipients in December 2008, and presented by The New York Times Company president and CEO Janet L. Robinson, Carnegie Corporation president Vartan Gregorian, and Jim Rettig, president of the American Library Association.\n\nIn May 2009, the company launched The New York Times Outstanding Playwright Award to honor an American playwright who had recently had his or her professional debut in New York. The first winner was Tarell Alvin McCraney for his play \"The Brothers Size\". In 2010, Dan LeFranc won for his play \"Sixty Miles to Silver Lake\".\n\n",
                "The New York Times\n\nThe New York Times (sometimes abbreviated as The\" \"NYT or The Times) is an American newspaper based in New York City with worldwide influence and readership. Founded in 1851, the paper has won 122 Pulitzer Prizes, more than any other newspaper.\n\nAs of September 2016, it had the largest combined print-and-digital circulation of any daily newspaper in the United States. \"The New York Times\" is ranked 18th in the world by circulation.\n\nThe paper is owned by The New York Times Company, which is publicly traded but primarily controlled by the Ochs-Sulzberger family through a dual-class share structure. It has been owned by the family since 1896; A.G. Sulzberger the paper's publisher and, his father, Arthur Ochs Sulzberger Jr. the company's chairman, is the fourth and fifth generation of the family to helm the paper.\n\nNicknamed \"The Gray Lady\", \"The New York Times\" has long been regarded within the industry as a national \"newspaper of record\". The paper's motto, \"All the News That's Fit to Print\", appears in the upper left-hand corner of the front page.\n\nSince the mid-1970s, \"The New York Times\" has greatly expanded its layout and organization, adding special weekly sections on various topics supplementing the regular news, editorials, sports, and features. Since 2008, \"The New York Times\" has been organized into the following sections: News, Editorials/Opinions-Columns/Op-Ed, New York (metropolitan), Business, Sports of The Times, Arts, Science, Styles, Home, Travel, and other features. On Sunday, \"The New York Times\" is supplemented by the \"Sunday Review\" (formerly the \"Week in Review\"), \"The New York Times Book Review\", \"The New York Times Magazine\" and \"\" (T is published 13 times a year). \"The New York Times\" stayed with the broadsheet full page set-up (as some others have changed into a tabloid lay-out) and an eight-column format for several years, after most papers switched to six, and was one of the last newspapers to adopt color photography, especially on the front page.\n\n\"The New York Times\" was founded as the New-York Daily Times on September 18, 1851. Founded by journalist and politician Henry Jarvis Raymond (1820–1869), and former banker George Jones, the \"Times\" was published by Raymond, Jones & Company (which raised about $70,000 initially). Early investors in the company were Edwin B. Morgan, Christopher Morgan, and Edward B. Wesley. Sold for a penny (equivalent to cents today), the inaugural edition attempted to address various speculations on its purpose and positions that preceded its release:\nIn 1852, the newspaper started a western division, \"The Times of California\", which arrived whenever a mail boat got to California. However, when local California newspapers came into prominence, the effort failed.\n\nThe newspaper shortened its name to The New-York Times on September 14, 1857. It dropped the hyphen in the city name on December 1, 1896. On April 21, 1861, \"The New York Times\" began publishing a Sunday edition to offer daily coverage of the Civil War. One of the earliest public controversies it was involved with was the Mortara Affair, the subject of twenty editorials it published alone.\n\nThe main office of \"The New York Times\" was attacked during the New York Draft Riots. The riots, sparked by the beginning of drafting for the Union Army, beginning on July 13, 1863. On \"Newspaper Row\", across from City Hall, Henry Raymond stopped the rioters with Gatling guns (early machine guns), one of which he manned himself. The mob diverted, and attacked the headquarters of abolitionist publisher Horace Greeley's \"New York Tribune\" until forced to flee by the Brooklyn City Police, who had crossed the East River to help the Manhattan authorities.\n\nIn 1869, Raymond died, and George Jones took over as publisher.\n\nThe newspaper's influence grew during 1870–1871 when it published a series of exposés on William Tweed, leader of the city's Democratic Party—popularly known as \"Tammany Hall\" (from its early 19th century meeting headquarters)—that led to the end of the Tweed Ring's domination of New York's City Hall. Tweed offered \"The New York Times\" five million dollars (equivalent to more than 100 million dollars today) to not publish the story. \n\nIn the 1880s, \"The New York Times\" transitioned gradually from editorially supporting Republican Party candidates to becoming more politically independent and analytical. In 1884, the paper supported Democrat Grover Cleveland (former Mayor of Buffalo and Governor of New York State) in his first presidential campaign. While this move cost \"The New York Times\" a portion of its readership among its more progressive and Republican readers (the revenue declined from $188,000 to $56,000 from 1883-1884\")\", the paper eventually regained most of its lost ground within a few years.\n\nAfter George Jones died in 1891, Charles Ransom Miller and other \"New York Times\" editors raised $1million dollars to buy the Times printing it under the New York Times Publishing Company. However, the newspaper was financially crippled by the Panic of 1893, and by 1896, the newspaper had a circulation of less than 9,000, and was losing $1,000 a day. That year, controlling interest in it was gained by Adolph Ochs, publisher of the \"Chattanooga Times\" for $75,000.\n\nShortly after assuming control of the paper, Ochs coined the paper's slogan, \"All The News That's Fit To Print\". The slogan has appeared in the paper since September 1896, and has been printed in a box in the upper left hand corner of the front page since early 1897. It was a jab at competing papers such as Joseph Pulitzer's \"New York World\" and William Randolph Hearst's \"New York Journal\" which were now being known for a lurid, sensationalist and often inaccurate reporting of facts and opinions known by the end of the century as \"yellow journalism\". Under Ochs' guidance, \"The New York Times\" achieved international scope, circulation, and reputation (the Sunday circulation went from under 9,000 in 1896 to 780,000 in 1934). In 1904, \"The New York Times\", along with \"The Times\" received the first on-the-spot wireless telegraph transmission from a naval battle, a report of the destruction of the Imperial Russian Navy's Baltic Fleet at the Battle of Port Arthur in the Straits of Tsushima off the eastern coast of Korea in the Yellow Sea in the western Pacific Ocean after just sailing across the globe from Europe from the press-boat \"Haimun\" during the Russo-Japanese War. In 1910, the first air delivery of \"The New York Times\" to Philadelphia began. \"The New York Times\" first trans-Atlantic delivery by air to London occurred in 1919 by dirigible. In 1920, a \"4 A.M. Airplane Edition\" was sent by plane to Chicago so it could be in the hands of Republican convention delegates by evening.\n\nIn the 1940s, the paper extended its breadth and reach. The crossword began appearing regularly in 1942, and the fashion section in 1946. \"The New York Times\" began an international edition in 1946. The international edition stopped publishing in 1967, when \"The New York Times\" joined the owners of the \"New York Herald Tribune\" and \"The Washington Post\" to publish the \"International Herald Tribune\" in Paris.\n\nThe paper's involvement in a 1964 libel case helped bring one of the key United States Supreme Court decisions supporting freedom of the press, \"New York Times Co. v. Sullivan\". In it, the United States Supreme Court established the \"actual malice\" standard for press reports about public officials or public figures to be considered defamatory or libelous. The malice standard requires the plaintiff in a defamation or libel case prove the publisher of the statement knew the statement was false or acted in reckless disregard of its truth or falsity. Because of the high burden of proof on the plaintiff, and difficulty in proving malicious intent, such cases by public figures rarely succeed.\n\nIn 1971, the \"Pentagon Papers\", a secret United States Department of Defense history of the United States' political and military involvement in the Vietnam War from 1945 to 1967, were given (\"leaked\") to Neil Sheehan of \"The New York Times\" by former State Department official Daniel Ellsberg, with his friend Anthony Russo assisting in copying them. \"The New York Times\" began publishing excerpts as a series of articles on June 13. Controversy and lawsuits followed. The papers revealed, among other things, that the government had deliberately expanded its role in the war by conducting air strikes over Laos, raids along the coast of North Vietnam, and offensive actions taken by U.S. Marines well before the public was told about the actions, all while President Lyndon B. Johnson had been promising not to expand the war. The document increased the credibility gap for the U.S. government, and hurt efforts by the Nixon administration to fight the ongoing war.\n\nWhen \"The New York Times\" began publishing its series, President Richard Nixon became incensed. His words to National Security Advisor Henry Kissinger included \"People have gotta be put to the torch for this sort of thing...\" and \"Let's get the son-of-a-bitch in jail.\" After failing to get \"The New York Times\" to stop publishing, Attorney General John Mitchell and President Nixon obtained a federal court injunction that \"The New York Times\" cease publication of excerpts. The newspaper appealed and the case began working through the court system. On June 18, 1971, \"The Washington Post\" began publishing its own series. Ben Bagdikian, a \"Post\" editor, had obtained portions of the papers from Ellsberg. That day the \"Post\" received a call from the Assistant Attorney General, William Rehnquist, asking them to stop publishing. When the \"Post\" refused, the U.S. Justice Department sought another injunction. The U.S. District court judge refused, and the government appealed. On June 26, 1971, the U.S. Supreme Court agreed to take both cases, merging them into \"New York Times Co. v. United States\" 403 US 713. On June 30, 1971, the Supreme Court held in a 6–3 decision that the injunctions were unconstitutional prior restraints and that the government had not met the burden of proof required. The justices wrote nine separate opinions, disagreeing on significant substantive issues. While it was generally seen as a victory for those who claim the First Amendment enshrines an absolute right to free speech, many felt it a lukewarm victory, offering little protection for future publishers when claims of national security were at stake.\n\nIn the 1970s, the paper introduced a number of new lifestyle sections including Weekend and Home, with the aim of attracting more advertisers and readers. Many criticized the move for betraying the paper's mission.\n\nOn September 7, 1976, the paper switched from an eight-column format to a six-column format. The overall page width stayed the same, with each column becoming wider. On September 14, 1987, the \"Times\" printed the heaviest ever newspaper, at over and 1,612 pages.\n\nThe \"Times\" was one of the last newspapers to adopt color photography, with the first color photograph on the front page appearing on October 16, 1997.\n\n\"The New York Times\" switched to a digital production process sometime before 1980, but only began preserving the resulting digital text that year.\n\nIn September 2008, \"The New York Times\" announced that it would be combining certain sections effective October 6, 2008, in editions printed in the New York metropolitan area. The changes folded the Metro Section into the main International / National news section and combined Sports and Business (except Saturday through Monday, when Sports is still printed as a standalone section). This change also included having the name of the Metro section be called New York outside of the Tri-State Area. The presses used by \"The New York Times\" allow four sections to be printed simultaneously; as the paper had included more than four sections all days except Saturday, the sections had to be printed separately in an early press run and collated together. The changes will allow \"The New York Times\" to print in four sections Monday through Wednesday, in addition to Saturday. \"The New York Times\" announcement stated that the number of news pages and employee positions will remain unchanged, with the paper realizing cost savings by cutting overtime expenses.\n\nIn 2009, the newspaper began production of local inserts in regions outside of the New York area. Beginning October 16, 2009, a two-page \"Bay Area\" insert was added to copies of the Northern California edition on Fridays and Sundays. The newspaper commenced production of a similar Friday and Sunday insert to the Chicago edition on November 20, 2009. The inserts consist of local news, policy, sports, and culture pieces, usually supported by local advertisements.\n\nFollowing industry trends, its weekday circulation had fallen in 2009 to fewer than one million.\n\nIn August 2007, the paper reduced the physical size of its print edition, cutting the page width from to a . This followed similar moves by a roster of other newspapers in the previous ten years, including \"USA Today\", \"The Wall Street Journal\", and \"The Washington Post\". The move resulted in a 5% reduction in news space, but (in an era of dwindling circulation and significant advertising revenue losses) also saved about $12million a year.\n\nBecause of its steadily declining sales attributed to the rise of online alternative media and social media, the newspaper has been going through a downsizing for several years, offering buyouts to workers and cutting expenses, in common with a general trend among print news media.\n\nIn December 2012, the \"Times\" published \"Snow Fall\", a six-part article about the 2012 Tunnel Creek avalanche which integrated videos, photos, and interactive graphics and was hailed as a watershed moment for online journalism.\n\nIn 2016, reporters for the newspaper were reportedly the target of cyber security breaches. The Federal Bureau of Investigation was reportedly investigating the attacks. The cyber security breaches have been described as possibly being related to cyberattacks that targeted other institutions, such as the Democratic National Committee.\n\nThe newspaper's first building was located at 113 Nassau Street in New York City. In 1854, it moved to 138 Nassau Street, and in 1858 to 41 Park Row, making it the first newspaper in New York City housed in a building built specifically for its use.\n\nThe newspaper moved its headquarters to the Times Tower, located at 1475 Broadway in 1904, in an area called Longacre Square, that was later renamed Times Square in honor of the newspaper. The top of the building now known as One Times Square is the site of the New Year's Eve tradition of lowering a lighted ball, which was started by the paper. The building is also notable for its electronic news ticker popularly known as \"The Zipper\" where headlines crawl around the outside of the building. It is still in use, but has been operated by Dow Jones & Company since 1995. After nine years in its Times Square tower the newspaper had an annex built at 229 West 43rd Street. After several expansions, the 43rd Street building became the newspaper's main headquarters in 1960 and the Times Tower on Broadway was sold the following year. It served as the newspaper's main printing plant until 1997, when the newspaper opened a state-of-the-art printing plant in the College Point section of the borough of Queens.\n\nA decade later, \"The New York Times\" moved its newsroom and businesses headquarters from West 43rd Street to a new tower at 620 Eighth Avenue between West 40th and 41st Streets, in Manhattan directly across Eighth Avenue from the Port Authority Bus Terminal. The new headquarters for the newspaper, known officially as The New York Times Building but unofficially called the new \"Times Tower\" by many New Yorkers, is a skyscraper designed by Renzo Piano.\n\nDiscriminatory practices restricting women in editorial positions were previously employed by the paper. The newspaper's first general woman reporter was Jane Grant, who described her experience afterwards. She wrote, \"In the beginning I was charged not to reveal the fact that a female had been hired\". Other reporters nicknamed her Fluff and she was subjected to considerable hazing. Because of her gender, promotions were out of the question, according to the then-managing editor. She was there for fifteen years, interrupted by World War I.\n\nIn 1935, Anne McCormick wrote to Arthur Hays Sulzberger, \"I hope you won't expect me to revert to 'woman's-point-of-view' stuff.\" Later, she interviewed major political leaders and appears to have had easier access than her colleagues did. Even those who witnessed her in action were unable to explain how she got the interviews she did. Clifton Daniel said, \"[After World War II,] I'm sure Adenauer called her up and invited her to lunch. She never had to grovel for an appointment.\" Covering world leaders' speeches after World War II at the National Press Club was limited to men by a Club rule. When women were eventually allowed in to hear the speeches, they still were not allowed to ask the speakers questions, although men were allowed and did ask, even though some of the women had won Pulitzer Prizes for prior work. \"Times\" reporter Maggie Hunter refused to return to the Club after covering one speech on assignment. Nan Robertson's article on the Union Stock Yards, Chicago, was read aloud as anonymous by a professor, who then said, \"'It will come as a surprise to you, perhaps, that the reporter is a \"girl,\" he began... [G]asps; amazement in the ranks. 'She had used all her senses, not just her eyes, to convey the smell and feel of the stockyards. She chose a difficult subject, an offensive subject. Her imagery was strong enough to revolt you.'\" \"The New York Times\" hired Kathleen McLaughlin after ten years at the \"Chicago Tribune\", where \"[s]he did a series on maids, going out herself to apply for housekeeping jobs.\"\n\n\"The New York Times\" has had one slogan. Since 1896, the newspaper's slogan has been \"All the News That's Fit to Print.\" In 1896, Adolph Ochs held a competition to attempt to find a replacement slogan, offering a $100 prize for the best one. Entries included \"News, Not Nausea\"; \"In One Word: Adequate\"; \"News Without Noise\"; \"Out Heralds The Herald, Informs The World, and Extinguishes The Sun\"; \"The Public Press is a Public Trust\"; and the winner of the competition, \"All the world's news, but not a school for scandal.\" On May 10, 1960, Wright Patman asked the FTC to investigate whether \"The New York Times's\" slogan was misleading or false advertising. Within 10 days, the FTC responded that it was not.\n\nAgain in 1996, a competition was held to find a new slogan, this time for NYTimes.com. Over 8,000 entries were submitted. Again however, \"All the News That's Fit to Print,\" was found to be the best.\n\nIn addition to its New York City headquarters, the paper has newsrooms in London and Hong Kong. Its Paris newsroom, which had been the headquarters of the paper's international edition, was closed in 2016, although the city remains home to a news bureau and an advertising office. The paper also has an editing and wire service center in Gainesville, Florida.\n\nAs of 2013, the newspaper had 6 news bureaus in the New York region, 14 elsewhere in the United States, and 24 in other countries.\n\nIn 2009, Russ Stanton, editor of the \"Los Angeles Times\", a competitor, stated that the newsroom of \"The New York Times\" was twice the size of the \"Los Angeles Times\", which had a newsroom of 600 at the time.\n\nIn 1896, Adolph Ochs bought \"The New York Times\", a money-losing newspaper, and formed the New York Times Company. The Ochs-Sulzberger family, one of the United States' newspaper dynasties, has owned \"The New York Times\" ever since. The publisher went public on January 14, 1969, trading at $42 a share on the American Stock Exchange. After this, the family continued to exert control through its ownership of the vast majority of Class B voting shares. Class A shareholders are permitted restrictive voting rights while Class B shareholders are allowed open voting rights.\n\nThe Ochs-Sulzberger family trust controls roughly 88 percent of the company's class B shares. Any alteration to the dual-class structure must be ratified by six of eight directors who sit on the board of the Ochs-Sulzberger family trust. The Trust board members are Daniel H. Cohen, James M. Cohen, Lynn G. Dolnick, Susan W. Dryfoos, Michael Golden, Eric M. A. Lax, Arthur O. Sulzberger, Jr. and Cathy J. Sulzberger.\n\nTurner Catledge, the top editor at \"The New York Times\" from 1952 to 1968, wanted to hide the ownership influence. Arthur Sulzberger routinely wrote memos to his editor, each containing suggestions, instructions, complaints, and orders. When Catledge would receive these memos he would erase the publisher's identity before passing them to his subordinates. Catledge thought that if he removed the publisher's name from the memos it would protect reporters from feeling pressured by the owner.\n\nThe position of public editor was established in 2003 to \"investigate matters of journalistic integrity\"; each public editor was to serve a two-year term. The post \"was established to receive reader complaints and question Times journalists on how they make decisions.\" The impetus for the creation of the public editor position was the Jayson Blair affair. Public editors were: Daniel Okrent (2003–2005), Byron Calame (2005–2007), Clark Hoyt (2007–2010) (served an extra year), Arthur S. Brisbane (2010–2012), Margaret Sullivan (2012–2016) (served a four-year term), and Elizabeth Spayd (2016–2017). In 2017, the Times eliminated the position of public editor.\n\nWhen referring to people, \"The New York Times\" generally uses honorifics, rather than unadorned last names (except in the sports pages, Book Review and Magazine).\n\n\"The New York Times\" printed a display advertisement on its first page on January 6, 2009, breaking tradition at the paper. The advertisement, for CBS, was in color and ran the entire width of the page. The newspaper promised it would place first-page advertisements on only the lower half of the page.\n\nIn August 2014, \"The Times\" decided to use the word \"torture\" to describe incidents in which interrogators \"inflicted pain on a prisoner in an effort to get information.\" This was a shift from the paper's previous practice of describe such practices as \"harsh\" or \"brutal\" interrogations.\n\nThe paper maintains a strict profanity policy. A 2007 review of a concert by punk band Fucked Up, for example, completely avoided mention of the group's name. However, the \"Times\" has on occasion published unfiltered video content that includes profanity and slurs where it has determined that such video has news value. During the 2016 U.S. presidential election campaign, the \"Times\" did print the words \"fuck\" and \"pussy,\" among others, when reporting on the vulgar statements made by Donald Trump in a 2005 recording. \"Times\" politics editor Carolyn Ryan said: \"It's a rare thing for us to use this language in our stories, even in quotes, and we discussed it at length,\" ultimately deciding to publish it because of its news value and because \"[t]o leave it out or simply describe it seemed awkward and less than forthright to us, especially given that we would be running a video that showed our readers exactly what was said.\"\n\nIn the absence of a major headline, the day's most important story generally appears in the top-right column, on the main page. The typefaces used for the headlines are custom variations of Cheltenham. The running text is set at 8.7 point Imperial.\n\nThe newspaper is organized in three sections, including the magazine.\n\nSome sections, such as Metro, are only found in the editions of the paper distributed in the New York–New Jersey–Connecticut Tri-state area and not in the national or Washington, D.C. editions. Aside from a weekly roundup of reprints of editorial cartoons from other newspapers, \"The New York Times\" does not have its own staff editorial cartoonist, nor does it feature a comics page or Sunday comics section.\n\nFrom 1851 to 2017, \"The New York Times\" published around 60,000 print issues containing about 3.5 million pages and 15 million articles.\n\nLike most other American newspapers, \"The New York Times\" has experienced a decline in circulation. Its printed weekday circulation dropped by percent to 571,500 copies from 2005 to 2016.\n\n\"The New York Times International Edition\" is a print version of the paper tailored for readers outside the United States. Formerly a joint venture with \"The Washington Post\" named \"The International Herald Tribune\", \"The New York Times\" took full ownership of the paper in 2002 and has gradually integrated it more closely into its domestic operations.\n\n\"The New York Times\" began publishing daily on the World Wide Web on January 22, 1996, \"offering readers around the world immediate access to most of the daily newspaper's contents.\" The website had 555million pageviews in March 2005. The domain \"nytimes.com\" attracted at least 146million visitors annually by 2008 according to a Compete.com study. In March 2009, \"The New York Times\" Web site ranked 59th by number of unique visitors, with over 20million unique visitors, making it the most visited newspaper site with more than twice the number of unique visitors as the next most popular site. , nytimes.com produced 22 of the 50 most popular newspaper blogs. NYTimes.com was ranked 118 in the world, and 32 in the U.S. by Alexa on June 4, 2017.\n\nIn September 2005, the paper decided to begin subscription-based service for daily columns in a program known as \"TimesSelect\", which encompassed many previously free columns. Until being discontinued two years later, \"TimesSelect\" cost $7.95 per month or $49.95 per year, though it was free for print copy subscribers and university students and faculty. To avoid this charge, bloggers often reposted TimesSelect material, and at least one site once compiled links of reprinted material. On September 17, 2007, \"The New York Times\" announced that it would stop charging for access to parts of its Web site, effective at midnight the following day, reflecting a growing view in the industry that subscription fees cannot outweigh the potential ad revenue from increased traffic on a free site. In addition to opening almost the entire site to all readers, \"The New York Times\" news archives from 1987 to the present are available at no charge, as well as those from 1851 to 1922, which are in the public domain. Access to the \"Premium Crosswords\" section continues to require either home delivery or a subscription for $6.95 per month or $39.95 per year. \"Times\" columnists including Nicholas Kristof and Thomas Friedman had criticized \"TimesSelect\", with Friedman going so far as to say \"I hate it. It pains me enormously because it's cut me off from a lot, a lot of people, especially because I have a lot of people reading me overseas, like in India ... I feel totally cut off from my audience.\"\n\n\"The New York Times\" was made available on the iPhone and iPod Touch in 2008, and on the iPad mobile devices in 2010. It was also the first newspaper to offer a video game as part of its editorial content, \"Food Import Folly\" by Persuasive Games. In 2010, \"The New York Times\" editors collaborated with students and faculty from New York University's Studio 20 Journalism Masters program to launch and produce \"The Local East Village\", a hyperlocal blog designed to offer news \"by, for and about the residents of the East Village\". That same year, reCAPTCHA helped to digitize old editions of \"The New York Times\".\n\nIn 2012, \"The New York Times\" introduced a Chinese-language news site, cn.nytimes.com, with content created by staff based in Shanghai, Beijing and Hong Kong, though the server was placed outside of China to avoid censorship issues. In March 2013, \"The New York Times\" and National Film Board of Canada announced a partnership titled \"A Short History of the Highrise\", which will create four short documentaries for the Internet about life in highrise buildings as part of the NFB's \"Highrise\" project, utilizing images from the newspaper's photo archives for the first three films, and user-submitted images for the final film. The third project in the series, \"A Short History of the Highrise\", won a Peabody Award in 2013.\n\nFalling print advertising revenue and projections of continued decline resulted in a \"metered paywall\" being instituted in 2011, regarded as modestly successful after garnering several hundred thousand subscriptions and about $100million in revenue . As announced in March 2011, the paywall would charge frequent readers for access to its online content. Readers would be able to access up to 20 articles each month without charge. (Although beginning in April 2012, the number of free-access articles was halved to just ten articles per month.) Any reader who wanted to access more would have to pay for a digital subscription. This plan would allow free access for occasional readers, but produce revenue from \"heavy\" readers. Digital subscriptions rates for four weeks range from $15 to $35 depending on the package selected, with periodic new subscriber promotions offering four-week all-digital access for as low as 99¢. Subscribers to the paper's print edition get full access without any additional fee. Some content, such as the front page and section fronts remained free, as well as the Top News page on mobile apps. In January 2013, \"The New York Times\" Public Editor Margaret M. Sullivan announced that for the first time in many decades, the paper generated more revenue through subscriptions than through advertising. In December 2017, the number of free articles per month was reduced from ten to five, as the first change to the metered paywall since 2012. An executive of \"The New York Times Company\" stated that the decision was motivated by \"an all-time high\" in the demand for journalism.\n\nThe newspaper's website was hacked on August 29, 2013, by the Syrian Electronic Army, a hacking group that supports the government of Syrian President Bashar al-Assad. The SEA managed to penetrate the paper's domain name registrar, Melbourne IT, and alter DNS records for \"The New York Times\", putting some of its websites out of service for hours.\n\nThe food section is supplemented on the web by properties for home cooks and for out-of-home dining. \"New York Times\" Cooking (cooking.nytimes.com; also available via iOS app) provides access to more than 17,000 recipes on file as of November 2016, and availability of saving recipes from other sites around the web. The newspaper's restaurant search (nytimes.com/reviews/dining) allows online readers to search NYC area restaurants by cuisine, neighborhood, price, and reviewer rating. \"The New York Times\" has also published several cookbooks, including \"The Essential New York Times Cookbook: Classic Recipes for a New Century\", published in late 2010.\n\nAs of December 2017, the \"New York Times\" has 3.5 million paid subscriptions and more than 130 million monthly readers, more than double its audience two years previously.\n\nThe \"Times Reader\" is a digital version of \"The New York Times\". It was created via a collaboration between the newspaper and Microsoft. \"Times Reader\" takes the principles of print journalism and applies them to the technique of online reporting. \"Times Reader\" uses a series of technologies developed by Microsoft and their Windows Presentation Foundation team. It was announced in Seattle in April 2006, by Arthur Ochs Sulzberger Jr., Bill Gates, and Tom Bodkin. In 2009, the \"Times Reader\" 2.0 was rewritten in Adobe AIR. In December 2013, the newspaper announced that the Times Reader app would be discontinued on January 6, 2014, urging readers of the app to instead begin using the subscription-only \"Today's Paper\" app.\n\nIn 2008, \"The New York Times\" created an app for the iPhone and iPod Touch which allowed users to download articles to their mobile device enabling them to read the paper even when they were unable to receive a signal. In April 2010, \"The New York Times\" announced it would begin publishing daily content through an iPad app. , \"The New York Times\" iPad app is ad-supported and available for free without a paid subscription, but translated into a subscription-based model in 2011.\n\nIn 2010, the newspaper also launched an app for Android smartphones, followed later by an app for Windows Phones.\n\n\"The New York Times\" began producing podcasts in 2006. Among the early podcasts were \"Inside The Times\" and \"Inside The New York Times Book Review\". Several of the \"Times\" podcasts were cancelled in 2012. The \"Times\" returned to launching new podcasts in 2016, including \"Modern Love\" with WBUR. On January 30, 2017, \"The New York Times\" launched a news podcast, \"The Daily\".\n\nIn June 2012, \"The New York Times\" launched its first official foreign-language variant, cn.nytimes.com, in Chinese, viewable in both traditional and simplified Chinese characters. The project was led by Craig S. Smith on the business side and Philip P. Pan on the editorial side.\n\nThe site's initial success was interrupted in October that year following the publication of an investigative article by David Barboza about the finances of Chinese Premier Wen Jiabao's family. In retaliation for the article, the Chinese government blocked access to both nytimes.com and cn.nytimes.com inside the People's Republic of China (PRC).\n\nDespite Chinese government interference, however, the Chinese-language operations have continued to develop, adding a second site, cn.nytstyle.com, iOS and Android apps and newsletters, all of which are accessible inside the PRC. The China operations also produce three print publications in Chinese. Traffic to cn.nytimes.com, meanwhile, has risen due to the widespread use of VPN technology in the PRC and to a growing Chinese audience outside mainland China. \"New York Times\" articles are also available to users in China via the use of mirror websites, apps, domestic newspapers, and social media. The Chinese platforms now represent one of \"The New York Times\" top five digital markets globally. The editor-in-chief of the Chinese platforms is Ching-Ching Ni.\n\nThe TimesMachine is a web-based archive of scanned issues of \"The New York Times\" from 1851 through 2002.\n\nUnlike \"The New York Times\" online archive, the TimesMachine presents scanned images of the actual newspaper. All non-advertising content can be displayed on a per-story basis in a separate PDF display page and saved for future reference. The archive is available to \"New York Times\" subscribers, home delivery and/or digital.\n\nBecause of holidays, no editions were printed on November 23, 1851; January 2, 1852; July 4, 1852; January 2, 1853; and January 1, 1854.\n\nBecause of strikes, the regular edition of \"The New York Times\" was not printed during the following periods:\n\n\"The New York Times\" editorial page is often regarded as liberal. In mid-2004, the newspaper's then public editor (ombudsman), Daniel Okrent, wrote that \"the Op-Ed page editors do an evenhanded job of representing a range of views in the essays from outsiders they publish – but you need an awfully heavy counterweight to balance a page that also bears the work of seven opinionated columnists, only two of whom could be classified as conservative (and, even then, of the conservative subspecies that supports legalization of gay unions and, in the case of William Safire, opposes some central provisions of the Patriot Act.\"\n\n\"The New York Times\" has not endorsed a Republican Party member for president since Dwight D. Eisenhower in 1956; since 1960, it has endorsed the Democratic Party nominee in every presidential election (see New York Times presidential endorsements). However, the \"Times\" did endorse incumbent Republican mayors of New York City Rudy Giuliani in 1997 and Michael Bloomberg in 2005 and 2009. The \"Times\" also endorsed Republican New York state governor George Pataki for re-election in 2002.\n\n\"The New York Times\" was criticized for the work of reporter Walter Duranty, who served as its Moscow bureau chief from 1922 through 1936. Duranty wrote a series of stories in 1931 on the Soviet Union and won a Pulitzer Prize for his work at that time; however, he has been criticized for his denial of widespread famine, most particularly the Ukrainian famine in the 1930s. In 2003, after the Pulitzer Board began a renewed inquiry, the \"Times\" hired Mark von Hagen, professor of Russian history at Columbia University, to review Duranty's work. Von Hagen found Duranty's reports to be unbalanced and uncritical, and that they far too often gave voice to Stalinist propaganda. In comments to the press he stated, \"For the sake of The New York Times' honor, they should take the prize away.\"\n\nOn November 14, 2001, in \"The New York Times\" 150th anniversary issue, former executive editor Max Frankel wrote that before and during World War II, the \"NY Times\" had maintained a consistent policy to minimize reports on the Holocaust in their news pages. Laurel Leff, associate professor of journalism at Northeastern University, concluded that the newspaper had downplayed the Third Reich targeting of Jews for genocide. Her 2005 book \"Buried by the Times\" documents the paper's tendency before, during and after World War II to place deep inside its daily editions the news stories about the ongoing persecution and extermination of Jews, while obscuring in those stories the special impact of the Nazis' crimes on Jews in particular. Leff attributes this dearth in part to the complex personal and political views of the newspaper's Jewish publisher, Arthur Hays Sulzberger, concerning Jewishness, antisemitism, and Zionism.\n\nDuring the war, \"The New York Times\" journalist William L. Laurence was \"on the payroll of the War Department\".\n\nIn the mid to late 1950s, \"fashion writer[s]... were required to come up every month with articles whose total column-inches reflected the relative advertising strength of every [\"department\" or \"specialty\"] store [\"assigned\" to a writer]... The monitor of all this was... the advertising director [of the \"NYT\"]... \" However, within this requirement, story ideas may have been the reporters' and editors' own.\n\n\"The New York Times\" supported the 2003 invasion of Iraq. On May 26, 2004, a year after the war started, the newspaper asserted that some of its articles had not been as rigorous as they should have been, and were insufficiently qualified, frequently overly dependent upon information from Iraqi exiles desiring regime change. Reporter Judith Miller retired after criticisms that her reporting of the lead-up to the Iraq War was factually inaccurate and overly favorable to the Bush administration's position, for which \"The New York Times\" later apologized. One of Miller's prime sources was Ahmed Chalabi, an Iraqi expatriate who returned to Iraq after the U.S. invasion and held a number of governmental positions culminating in acting oil minister and deputy prime minister from May 2005 until May 2006.\n\nIn May 2003, \"The New York Times\" reporter Jayson Blair was forced to resign from the newspaper after he was caught plagiarizing and fabricating elements of his stories. Some critics contended that African-American Blair's race was a major factor in his hiring and in \"The New York Times\" initial reluctance to fire him.\n\nThe newspaper was criticized for largely reporting the prosecutors' version of events in the 2006 Duke lacrosse case. Suzanne Smalley of \"Newsweek\" criticized the newspaper for its \"credulous\" coverage of the charges of rape against Duke University lacrosse players. Stuart Taylor, Jr. and KC Johnson, in their book \"Until Proven Innocent: Political Correctness and the Shameful Injustices of the Duke Lacrosse Rape Case\", write: \"at the head of the guilt-presuming pack, \"The New York Times\" vied in a race to the journalistic bottom with trash-TV talk shows.\"\n\nA 2003 study in \"The Harvard International Journal of Press/Politics\" concluded that \"The New York Times\" reporting was more favorable to Israelis than to Palestinians. A 2002 study published in the journal \"Journalism\" examined Middle East coverage of the Second Intifada over a one-month period in the \"Times\", \"Washington Post\" and \"Chicago Tribune\". The study authors said that the \"Times\" was \"the most slanted in a pro-Israeli direction\" with a bias \"reflected ... in its use of headlines, photographs, graphics, sourcing practices and lead paragraphs.\"\n\nFor its coverage of the Israeli–Palestinian conflict, some (such as Ed Koch) have claimed that the paper is pro-Palestinian, while others (such as As'ad AbuKhalil) have insisted that it is pro-Israel. \"The Israel Lobby and U.S. Foreign Policy\", by political science professors John Mearsheimer and Stephen Walt, alleges that \"The New York Times\" sometimes criticizes Israeli policies but is not even-handed and is generally pro-Israel. On the other hand, the Simon Wiesenthal Center has criticized \"The New York Times\" for printing cartoons regarding the Israeli-Palestinian conflict that were claimed to be anti-Semitic.\n\nIsraeli Prime Minister Benjamin Netanyahu rejected a proposal to write an article for the paper on grounds of lack of objectivity. A piece in which Thomas Friedman commented that praise awarded to Netanyahu during a speech at congress was \"paid for by the Israel lobby\" elicited an apology and clarification from its writer.\n\n\"The New York Times\" public editor Clark Hoyt concluded in his January 10, 2009, column, \"Though the most vociferous supporters of Israel and the Palestinians do not agree, I think \"The New York Times\", largely barred from the battlefield and reporting amid the chaos of war, has tried its best to do a fair, balanced and complete job— and has largely succeeded.\"\n\nIn February 2009, a \"Village Voice\" music blogger accused the newspaper of using \"chintzy, ad-hominem allegations\" in an article on British Tamil music artist M.I.A. concerning her activism against the Sinhala-Tamil conflict in Sri Lanka. M.I.A. criticized the paper in January 2010 after a travel piece rated post-conflict Sri Lanka the \"#1 place to go in 2010\". In June 2010, \"The New York Times Magazine\" published a correction on its cover article of M.I.A., acknowledging that the interview conducted by current \"W\" editor and then-\"Times Magazine\" contributor Lynn Hirschberg contained a recontextualization of two quotes. In response to the piece, M.I.A. broadcast Hirschberg's phone number and secret audio recordings from the interview via her Twitter and website.\n\n\"The New York Times\" was criticized for the 13-month delay of the December 2005 story revealing the U.S. National Security Agency warrantless surveillance program. Ex-NSA officials blew the whistle on the program to journalists James Risen and Eric Lichtblau, who presented an investigative article to the newspaper in November 2004, weeks before America's presidential election. Contact with former agency officials began the previous summer.\n\nFormer \"The New York Times\" executive editor Bill Keller decided not to report the piece after being pressured by the Bush administration and being advised not to do so by \"New York Times\" Washington bureau chief Philip Taubman. Keller explained the silence's rationale in an interview with the newspaper in 2013, stating \"Three years after 9/11, we, as a country, were still under the influence of that trauma, and we, as a newspaper, were not immune\".\n\nIn 2014, \"PBS Frontline\" interviewed Risen and Lichtblau, who said that the newspaper's plan was to not publish the story at all. \"The editors were furious at me\", Risen said to the program. \"They thought I was being insubordinate.\" Risen wrote a book about the mass surveillance revelations after \"The New York Times\" declined the piece's publication, and only released it after Risen told them that he would publish the book. Another reporter told NPR that the newspaper \"avoided disaster\" by ultimately publishing the story.\n\nOn June 16, 2015, \"The New York Times\" published an article reporting the deaths of six Irish students staying in Berkeley, California when the balcony they were standing on collapsed, the paper's story insinuating that they were to blame for the collapse. The paper stated that the behavior of Irish students coming to the US on J1 visas was an \"embarrassment to Ireland\". The Irish Taoiseach and former President of Ireland criticized the newspaper for \"being insensitive and inaccurate\" in its handling of the story.\n\nIn May 2015, a \"New York Times\" exposé by Sarah Maslin Nir on the working conditions of manicurists in New York City and elsewhere and the health hazards to which they are exposed attracted wide attention, resulting in emergency workplace enforcement actions by New York governor Andrew M. Cuomo. In July 2015, the story's claims of widespread illegally low wages were challenged by former \"New York Times\" reporter Richard Bernstein, in the \"New York Review of Books\". Bernstein, whose wife owns two nail salons, asserted that such illegally low wages were inconsistent with his personal experience, and were not evidenced by ads in the Chinese-language papers cited by the story. \"The New York Times\" editorial staff subsequently answered Bernstein's criticisms with examples of several published ads and stating that his response was industry advocacy. The independent \"NYT\" Public Editor also reported that she had previously corresponded with Bernstein and looked into his complaints, and expressed her belief that the story's reporting was sound.\n\nIn September and October 2015, nail salon owners and workers protested at \"The New York Times\" offices several times, in response to the story and the ensuing New York State crackdown. In October 2015, \"Reason\" magazine published a three part re-reporting of the story by Jim Epstein, charging that the series was filled with misquotes and factual errors respecting both its claims of illegally low wages and health hazards. Epstein additionally argued that \"The New York Times\" had mistranslated the ads cited in its answer to Bernstein, and that those ads actually validated Bernstein's argument. In November 2015, \"The New York Times\" public editor concluded that the exposé's \"findings, and the language used to express them, should have been dialed back — in some instances substantially\" and recommended that \"The Times write further follow-up stories, including some that re-examine its original findings and that take on the criticism from salon owners and others — not defensively but with an open mind.\"\n\nA 2015 study found that \"The New York Times\" fed into an overarching tendency towards national bias. During the Iranian nuclear crisis the newspaper minimized the \"negative processes\" of the United States while overemphasizing similar processes of Iran. This tendency was shared by other papers such as \"The Guardian\", \"Tehran Times\", and the Fars News Agency, while Xinhua News Agency was found to be more neutral while at the same time mimicking the foreign policy of the Peoples' Republic of China.\n\nIn April 2016, two black female employees in their sixties filed a federal class action lawsuit against The New York Times Company CEO Mark Thompson and chief revenue officer Meredith Levien, claiming age, gender, and racial discrimination. The plaintiffs claim that the \"Times\" advertising department favored younger white employees over older black employees in making firing and promotion decisions. The \"Times\" said that the suit was \"entirely without merit\" and was \"a series of recycled, scurrilous and unjustified attacks.\"\n\n\"New York Times\" public editor (ombudsman) Elizabeth Spayd wrote in 2016 that \"Conservatives and even many moderates, see in \"The Times\" a blue-state worldview\" and accuse it of harboring a liberal bias. Spayd did not analyze the substance of the claim, but did opine that the \"Times\" is \"part of a fracturing media environment that reflects a fractured country. That in turn leads liberals and conservatives toward separate news sources.\" \"Times\" executive editor Dean Baquet stated that he does not believe coverage has a liberal bias, but that: \"We have to be really careful that people feel like they can see themselves in \"The New York Times\". I want us to be perceived as fair and honest to the world, not just a segment of it. It's a really difficult goal. Do we pull it off all the time? No.\"\n\n\"Times\" public editor Arthur Brisbane wrote in 2012: \"When The Times covers a national presidential campaign, I have found that the lead editors and reporters are disciplined about enforcing fairness and balance, and usually succeed in doing so. Across the paper's many departments, though, so many share a kind of political and cultural progressivism — for lack of a better term — that this worldview virtually bleeds through the fabric of \"The Times\".\"\n\nIn mid-2004, the newspaper's then-public editor Daniel Okrent, wrote an opinion piece in which he said that \"The New York Times\" did have a liberal bias in news coverage of certain social issues such as abortion and same-sex marriage. He stated that this bias reflected the paper's cosmopolitanism, which arose naturally from its roots as a hometown paper of New York City. He wrote, \"if you're examining the paper's coverage of these subjects from a perspective that is neither urban nor Northeastern nor culturally seen-it-all; if you are among the groups \"The Times\" treats as strange objects to be examined on a laboratory slide (devout Catholics, gun owners, Orthodox Jews, Texans); if your value system wouldn't wear well on a composite \"New York Times\" journalist, then a walk through this paper can make you feel you're traveling in a strange and forbidding world.\" Okrent wrote that the \"Time\"s Arts & Leisure; the Sunday \"Times Magazine\", and Culture coverage trend to the left.\n\nIn December 2004, a University of California, Los Angeles study by former fellows of a conservative think tank gave \"The New York Times\" a score of 73.7 on a 100-point scale, with 0 being most conservative and 100 being most liberal, making it the second-most liberal major newspaper in the study after \"The Wall Street Journal\" (85.1). The validity of the study has been questioned, however. The liberal watchdog group Media Matters for America pointed out potential conflicts of interest with the author's funding, and political scientists, such as Brendan Nyhan, cited flaws in the study's methodology.\n\nDonald Trump has frequently criticized \"The New York Times\" on his Twitter account before and during his presidency; since November 2015, Trump has referred to the \"Times\" as \"the failing \"New York Times\"\" in a series of tweets. Despite Trump's criticism, \"New York Times\" editor Mark Thompson noted that the paper had enjoyed soaring digital readership, with the fourth quarter of 2016 seeing the highest number of new digital subscribers to the newspaper since 2011.\n\nCritic Matt Taibbi accused \"The New York Times\" of favoring Hillary Clinton over Bernie Sanders in the paper's news coverage of the 2016 Democratic presidential primaries. Responding to the complaints of many readers, \"New York Times\" public editor Margaret Sullivan wrote that \"\"The Times\" has not ignored Mr. Sanders's campaign, but it hasn't always taken it very seriously. The tone of some stories is regrettably dismissive, even mocking at times. Some of that is focused on the candidate's age, appearance and style, rather than what he has to say.\" \"Times\" senior editor Carolyn Ryan defended both the volume of \"New York Times\" coverage (noting that Sanders had received about the same amount of article coverage as Jeb Bush and Marco Rubio) and its tone.\n\nThe \"Times\" has developed a national and international \"reputation for thoroughness\" over time. Among journalists, the paper is held in high regard; a 1999 survey of newspaper editors conducted by the \"Columbia Journalism Review\" found that the \"Times\" was the \"best\" American paper, ahead of \"The Washington Post\", \"The Wall Street Journal\", and \"Los Angeles Times\". The \"Times\" also was ranked #1 in a 2011 \"quality\" ranking of U.S. newspapers by Daniel de Vise of \"The Washington Post\"; the objective ranking took into account the number of recent Pulitzer Prizes won, circulation, and perceived Web site quality. A 2012 report in WNYC called the \"Times\" \"the most respected newspaper in the world.\"\n\nNevertheless, like many other U.S. media sources, the \"Times\" had suffered from a decline in public perceptions of credibility in the U.S. from 2004 to 2012. A Pew Research Center survey in 2012 asked respondents about their views on credibility of various news organizations. Among respondents who gave a rating, 49% said that they believed \"all or most\" of the \"Times\"s reporting, while 50% disagreed. A large percentage (19%) of respondents were unable to rate believability. The \"Times\"s score was comparable to that of \"USA Today\". Media analyst Brooke Gladstone of WNYC's \"On the Media\", writing for \"The New York Times\", says that the decline in U.S. public trust of the mass media can be explained (1) by the rise of the polarized Internet-driven news; (2) by a decline in trust in U.S. institutions more generally; and (3) by the fact that \"Americans say they want accuracy and impartiality, but the polls suggest that, actually, most of us are seeking affirmation.\"\n\n\"The New York Times\" has won 122 Pulitzer Prizes, more than any other newspaper. The prize is awarded for excellence in journalism in a range of categories.\n\nIt has also (as of 2014) won three Peabody Awards (and jointly received two).\n\n\n\nOfficial \"NYT\" websites\nUnofficial \"NYT\"-related websites\n",
                "The New York Times Building\n\nThe New York Times Building is a skyscraper on the west side of Midtown Manhattan, New York City that was completed in 2007. Its chief tenant is The New York Times Company, publisher of \"The New York Times\" as well as the \"International New York Times\", and other newspapers. Construction was by a joint venture of The New York Times Company, Forest City Ratner (Forest City Enterprises's New York subsidiary), and ING Real Estate.\n\nThe original newspaper headquarters in 1851 were at 113 Nassau Street, in a little building that stood until fairly recently, then up the street a few years later at 138 Nassau Street. In 1858, the Times then moved to a five-story edifice at 41 Park Row; thirty years later, partially in response to a new tower erected by the competing \"Tribune\", it commissioned a new 13-story building at the same site, one that remains in use by Pace University. In 1904, again partially in response to the Herald Square headquarters of another competitor, the paper moved to perhaps its most famous location, the Times Tower, altering the name of the surrounding area from Longacre Square to Times Square. The slender tower was so constricted in space that the paper outgrew it within a decade and, in 1913, moved into the Times Annex, 229 West 43rd Street, where it remained for almost a century.\n\nThe project was announced on December 13, 2001: a 52-story tower on the east side of Eighth Avenue between 40th and 41st Street across from the Port Authority of New York & New Jersey Bus Terminal. The project, in conjunction with the Hearst Tower, represents the further westward expansion of Midtown along Eighth Avenue, a corridor that had seen no construction since the 1989 completion of One Worldwide Plaza. In addition, the new building—called by many New Yorkers \"The New Times Tower\"—keeps the paper in the Times Square area, its namesake.\n\nThe site for the building was obtained by the Empire State Development Corporation (ESDC) through eminent domain. With a mandate to acquire and redevelop blighted properties in Times Square, ten buildings were condemned by the ESDC and purchased from their owners. Some owners sued, asserting that the area was no longer blighted, but lost in court. Once the site was assembled, it was leased to The New York Times Company and Forest City Ratner for $85.6 million over 99 years (considerably below market value).Additionally, The New York Times Company received $26.1 million in tax breaks. The \"Times\" itself occupies 628,000 square feet on the 2nd to 21st floors, with the remainder leased to tenants.\n\nOn December 16, 2016, \"The New York Times\" announced that it was vacating at least 8 of the floors in order to generate rental income.\n\nThe tower was designed by Renzo Piano Building Workshop and FXFOWLE Architects, with Gensler providing interior design. The lighting design for the building's nighttime identity was designed by the Office for Visual Interaction Inc. The tower rises from the street to its roof, with the exterior curtain wall extending higher to , and a mast rising to . , the building was tied with the Chrysler Building as the fourth-tallest building in New York City, due to the unfinished One World Trade Center exceeding their height. The tower is also the tenth-tallest building in the United States.\n\nThe steel-framed building, cruciform in plan, has a screen of \" (41.3 mm) ceramic rods mounted on the exterior of the glass curtain wall on the east, west and south facades. The rod spacing increases from the base to the top, adding transparency as the building rises. The steel framing and bracing is exposed at the four corner \"notches\" of the building.\n\nThe new building is promoted as a green structure. The design incorporates numerous environmentally sustainable features for increased energy efficiency. The double skin curtain wall, automated louver shading system, dimmable lighting system, underfloor air distribution system and cogeneration are the main sustainable design features.\n\nThe use of floor-to-ceiling glass maximizes light and views for people inside and outside the building. The horizontal white ceramic rods on the building facade, which are spaced to allow occupants to have unobstructed views while both seated and standing, act as an aesthetic veil and a sun shade. They are made of aluminum silicate, an extremely dense and high-quality ceramic chosen for its durability and cost-effectiveness. Glazed with a finish similar to the material used on terra cotta to reflect light, self-clean, and resist weather, the rods change color with the sun and weather. Additionally, the automated louver shades move in response to the position of the sun and inputs from sensors, blocking light to reduce glare or allowing it to enter at times of less direct sunlight. The moveable shades reduce energy consumption about 13% by reducing solar heat gain by 30%.\n\nIn a normal office building, lights usually consume about 44% of total energy. The NYT building uses less, because its design, lighting, and shading systems allow the sun to be the main source of light. Each of the more than 18,000 electrical ballasts in the lighting system contains a computer chip, allowing each room's lights to brighten or dim depending on whether a room is occupied and how bright the sun is.\n\nThe New York Times Company utilizes the underfloor air distribution system which strives for better indoor air quality, thermal comfort as well as energy saving. The conditioned air from the air handler is delivered through an air highway system that circumnavigates the service core and then into the six zoned off underfloor low pressure zones for distribution across the floor plate to all floor diffusers. Around each perimeter a series of fan power boxes control the temperature in the space. If heating is required then the fans turn on delivering heat via a hot water coil. In cooling mode, the fans operate to give increased ventilation around the perimeter spaces. Fresh air is supplied by two air handling units on the 28th floor sending fresh treated outside air to each floor where a constant volume VAV controls the amount of air entering the system. The air is then conditioned for proper cooling and humidity before being sent to the air highway. The New York Times Building also benefits from other general UFAD advantages. In the open plan office space that was implemented in occupied space from 2nd to 21st floors to enhance daylighting and outdoor views, the system provides flexibility to place the required equipment anywhere on the raised integrated service plenum. When departments or occupants needed to be reconfigured, the raised floor also enables maintenance to carry out changes at relatively lower expense.\n\nThe New York Times Building incorporates a cogeneration plant to supply its 24-7 operation and 40% of the power used. Located in a mechanical room at the far end of the podium’s top floor, the plant consists of two natural gas fired reciprocating engine driven generators with a total generating capacity of 1.5 MW of electrical power. It recovers the heat produced by combustion and converts the heat into usable energy in the form of hot water. The recovered hot water serves as the building’s heating loop to provide warmth during the winter and functions as a refrigeration machine to provide cooling during the summer. The power from the grid is solely used by the building as a backup source.\n\nIn excess of 95% of the structural steel was recycled. The building, like many in Midtown Manhattan, has no on-site parking, with most employees arriving by public transit.\n\nA team of researchers at the Lawrence Berkeley National Lab and Center for the Built Environment monitored the building’s performance for a year and compared the results with buildings that meet with the standard building efficiency codes. They found that the New York Times Building significantly reduced annual electricity, cut heating energy use by more than 50% and decrease the peak electric demand as well. \"It is essential to start with a sound, integrated building design, and then to pay attention to details such as procurement of building equipment, and verifying the proper performance of the equipment after it is installed. The Times Company did its homework in 2004, well before construction began on the building, evaluating and optimizing the shading and daylighting technologies,\" concluded the Berkeley researchers.\n\nIn the summer of 2008, three men illegally climbed the external facade of The New York Times Building within a month of each other, with the first two on the same day. The three climbers were not associated with one another.\n\nOn June 5, 2008, a professional climber, Alain Robert, dubbed \"The French Spiderman,\" climbed the north side of The New York Times Building. He was able to scale the building from first floor all the way to the roof. During his climb, Robert attached a fluorescent green neon sign to the building that read \"Global warming kills more people than a 9/11 every week\". Robert also wore a t-shirt promoting the website \"The Solution is Simple\". Robert was met on the roof by the NYPD emergency service unit team where he was put in a harness to ensure he did not fall and placed under arrest. Later that day, a second climber scaled the western face of the building. He also was arrested for climbing the building facade after reaching the roof. The climber, 32-year-old Brooklyn resident Renaldo Clarke, was wearing a T-shirt with the words \"Malaria No More\" written on it.\n\nThe third climber was David Malone, 29, from Connecticut, who also scaled the west side of the building on July 9, 2008. Unlike the two previous climbers, Malone did not attempt to make it to the roof. He hung a banner around the fifth floor upon the first \"T\" of The New York Times sign, that had a picture of Osama Bin Laden holding Bush like a puppet—\"Bin Laden's Plan\" (the title of his book and website). He then climbed higher, stopping at the 11th floor, and remained hanging on the building for four hours before being arrested. Malone said he was protesting Al Qaeda's \"crusader baiting\", and \"intentional provocation of the U.S.\" On Saturday March 24, 2012, a homeless man was caught climbing the building. He made it to the 5th floor before getting stuck, and was eventually arrested.\n\n"
            ],
            "good_message": false,
            "loggedData": [
                {
                    "curr_message": " The New York Times Company is an American media company which publishes its namesake, \"The New York Times\".",
                    "last_key_pressed": "<<selected>>"
                }
            ],
            "offensive": null,
            "researchTask": true,
            "selectedData": [
                "The New York Times Company is an American media company which publishes its namesake, \"The New York Times\"."
            ],
            "shown_passages": [
                [
                    "The New York Times Company",
                    [
                        "The New York Times Company is an American media company which publishes its namesake, \"The New York Times\".",
                        "Arthur Ochs Sulzberger Jr. has served as chairman since 1997.",
                        "It is headquartered in Manhattan, New York.",
                        "The company was founded by Henry Jarvis Raymond and George Jones in New York City.",
                        "The first edition of the newspaper \"The New York Times\", published on September 18, 1851, stated: \"We publish today the first issue of the New-York Daily Times, and we intend to issue it every morning (Sundays excepted) for an indefinite number of years to come.\"",
                        "The company moved into the cable channel industry purchasing a 40% interest in the Popcorn Channel, a theatrical movie preview and local movie times, in November 1994."
                    ]
                ],
                [
                    "The New York Times",
                    [
                        "The New York Times (sometimes abbreviated as The\" \"NYT or The Times) is an American newspaper based in New York City with worldwide influence and readership.",
                        "Founded in 1851, the paper has won 122 Pulitzer Prizes, more than any other newspaper.",
                        "As of September 2016, it had the largest combined print-and-digital circulation of any daily newspaper in the United States.",
                        "\"The New York Times\" is ranked 18th in the world by circulation.",
                        "The paper is owned by The New York Times Company, which is publicly traded but primarily controlled by the Ochs-Sulzberger family through a dual-class share structure."
                    ]
                ],
                [
                    "The New York Times Building",
                    [
                        "The New York Times Building is a skyscraper on the west side of Midtown Manhattan, New York City that was completed in 2007.",
                        "Its chief tenant is The New York Times Company, publisher of \"The New York Times\" as well as the \"International New York Times\", and other newspapers.",
                        "Construction was by a joint venture of The New York Times Company, Forest City Ratner (Forest City Enterprises's New York subsidiary), and ING Real Estate.",
                        "The original newspaper headquarters in 1851 were at 113 Nassau Street, in a little building that stood until fairly recently, then up the street a few years later at 138 Nassau Street."
                    ]
                ]
            ],
            "speaker": "0_Wizard",
            "suggestionsConfig": "show",
            "suggestionsNumber": 5,
            "suggestionsShown": true,
            "text": " The New York Times Company is an American media company which publishes its namesake, \"The New York Times\".",
            "textboxFilled": 0,
            "time": 1580233254.4470031,
            "timeTaken": 19.519,
            "totalTasks": 120,
            "turn": 8
        },
        {
            "checked_passage": {
                "no_passages_used": "no_passages_used"
            },
            "checked_sentence": {
                "no_passages_used": "no_passages_used"
            },
            "clicked_passages": {},
            "context": {
                "conversation": [
                    "Good Morning.  I'm thinking of getting a job in the court system.",
                    "ARe you planning on working on a specific level, there are three?",
                    "I hadn't thought of that.  What levels are there to choose from?",
                    "The Supreme Court of the United States is the court of last resort, so it is the higher one."
                ],
                "index": 1
            },
            "contextCount": 0,
            "full_passages": [
                "Supreme court\n\nA supreme court is the highest court within the hierarchy of courts in many legal jurisdictions. Other descriptions for such courts include court of last resort, apex court, and highest (or final) court of appeal. Broadly speaking, the decisions of a supreme court are not subject to further review by any other court. Supreme courts typically function primarily as appellate courts, hearing appeals from decisions of lower trial courts, or from intermediate-level appellate courts.\n\nHowever, not all highest courts are named as such. Civil law states tend not to have a single highest court. Additionally, the highest court in some jurisdictions is not named the \"Supreme Court\", for example, the High Court of Australia; this is because decisions by the High Court could formerly be appealed to the Privy Council. On the other hand, in some places the court named the \"Supreme Court\" is not in fact the highest court; examples include the New York Supreme Court, the Supreme Courts of several Canadian provinces/territories and the former Supreme Court of Judicature of England and Wales and Supreme Court of Judicature of Northern Ireland, which are all subordinate to higher courts of appeal.\n\nSome countries have multiple \"supreme courts\" whose respective jurisdictions have different geographical extents, or which are restricted to particular areas of law. Some countries with a federal system of government may have both a federal supreme court (such as the Supreme Court of the United States), and supreme courts for each member state (such as the Supreme Court of Nevada), with the former having jurisdiction over the latter only to the extent that the federal constitution extends federal law over state law. However, other federations, such as Canada, may have a supreme court of general jurisdiction, able to decide any question of law. Jurisdictions with a civil law system often have a hierarchy of administrative courts separate from the ordinary courts, headed by a supreme administrative court as is the case in the Netherlands. A number of jurisdictions also maintain a separate constitutional court (first developed in the Czechoslovak Constitution of 1920), such as Austria, France, Germany, Luxembourg, Portugal, Russia, Spain and South Africa. Within the former British Empire, the highest court within a colony was often called the \"Supreme Court\", even though appeals could be made from that court to the United Kingdom's Privy Council (based in London). A number of Commonwealth jurisdictions retain this system, but many others have reconstituted their own highest court as a court of last resort, with the right of appeal to the Privy Council being abolished.\n\nIn jurisdictions using a common law system, the doctrine of \"stare decisis\" applies, whereby the principles applied by the supreme court in its decisions are binding upon all lower courts; this is intended to apply a uniform interpretation and implementation of the law. In civil law jurisdictions the doctrine of \"stare decisis\" is not generally considered to apply, so the decisions of the supreme court are not necessarily binding beyond the immediate case before it; however, in practice the decisions of the supreme court usually provide a very strong precedent, or \"jurisprudence constante\", for both itself and all lower courts.\n\nThe Supreme Court of Bangladesh is created by the provisions of the Constitution of Bangladesh, 1972. There are two Divisions of the Supreme Court, i.e. (a) Appellate Division and (b) High Court Division. Appellate Division is the highest Court of Appeal and usually does not exercise the powers of a court of first instance. Whereas, the High Court Division is a Court of first instance in writ/judicial review, company and admiralty matters.\n\nThe Supreme Court of Canada was established in 1875 but only became the highest court in the country in 1949 when the right of appeal to the Judicial Committee of the Privy Council was abolished. This court hears appeals from the courts of appeal from the provinces and territories, and also appeals from the Federal Court of Appeal. The Supreme Court is a \"General Court of Appeal.\" It can decide any question of law considered by the lower courts, including constitutional law, federal law, and provincial law. The court's decisions are final and binding on the federal courts and the courts from all provinces and territories. The title \"Supreme\" can be confusing because, for example, the Supreme Court of British Columbia does not have the final say and controversial cases heard there often get appealed in higher courts - it is in fact one of the lower courts in such a process.\n\nIn Hong Kong, the Supreme Court of Hong Kong (now known as the High Court of Hong Kong) was the final court of appeal during its colonial times which ended with transfer of sovereignty in 1997. The final adjudication power, as in any other British Colonies, rested with the Judicial Committee of the Privy Council (JCPC) in London, United Kingdom. Now the power of final adjudication is vested in the Court of Final Appeal created in 1997. Under the Basic Law, its constitution, the territory remains a common law jurisdiction. Consequently, judges from other common law jurisdictions (including England and Wales) can be recruited and continue to serve in the judiciary according to Article 92 of the Basic Law. On the other hand, the power of interpretation of the Basic Law itself is vested in the Standing Committee of the National People's Congress (NPCSC) in Beijing (without retroactive effect), and the courts are authorised to interpret the Basic Law when trying cases, in accordance with Article 158 of the Basic Law. This arrangement became controversial in light of the right of abode issue in 1999, raising concerns for judicial independence.\n\nIn India, the Supreme Court of India was created on January 28, 1950 after adoption of the Constitution.\nArticle 141 of the Constitution of India states that the law declared by Supreme Court is to be binding on all Courts within the territory of India. It is the highest court in India and has ultimate judicial authority to interpret the Constitution and decide questions of national law (including local bylaws). The Supreme Court is also vested with the power of judicial review to ensure the application of the rule of law.\n\nNote that within the constitutional framework of India, Jammu and Kashmir (J&K) has a special status vis-a-vis the other states of India. Article 370 of the Constitution of India carves out certain exceptions for J&K. However, the Constitution (Application to Jammu and Kashmir) Order 1954 makes Article 141 applicable to the state of J&K and hence law declared by the Supreme Court of India is equally applicable to all courts of J&K including the High Court.\n\nThe Supreme Court is the highest court in the Republic of Ireland. It has authority to interpret the constitution, and strike down laws and activities of the state that it finds to be unconstitutional. It is also the highest authority in the interpretation of the law. Constitutionally it must have authority to interpret the constitution but its further appellate jurisdiction from lower courts is defined by law. The Irish Supreme Court consists of its presiding member, the Chief Justice, and seven other judges. Judges of the Supreme Court are appointed by the President in accordance with the binding advice of the Government. The Supreme Court sits in the Four Courts in Dublin.\n\nIsrael's Supreme Court is at the head of the court system in the State of Israel. It is the highest judicial instance. The Supreme Court sits in Jerusalem. The area of its jurisdiction is the entire State. A ruling of the Supreme Court is binding upon every court, other than the Supreme Court itself. The Israeli supreme court is both an appellate court and the high court of justice. As an appellate court, the Supreme Court considers cases on appeal (both criminal and civil) on judgments and other decisions of the District Courts. It also considers appeals on judicial and quasi-judicial decisions of various kinds, such as matters relating to the legality of Knesset elections and disciplinary rulings of the Bar Association. As the High Court of Justice (Hebrew: Beit Mishpat Gavoha Le'Zedek בית משפט גבוה לצדק; also known by its initials as Bagatz בג\"ץ), the Supreme Court rules as a court of first instance, primarily in matters regarding the legality of decisions of State authorities: Government decisions, those of local authorities and other bodies and persons performing public functions under the law, and direct challenges to the constitutionality of laws enacted by the Knesset. The court has broad discretionary authority to rule on matters in which it considers it necessary to grant relief in the interests of justice, and which are not within the jurisdiction of another court or tribunal. The High Court of Justice grants relief through orders such as injunction, mandamus and Habeas Corpus, as well as through declaratory judgments. The Supreme Court can also sit at a further hearing on its own judgment. In a matter on which the Supreme Court has ruled - whether as a court of appeals or as the High Court of Justice - with a panel of three or more justices, it may rule at a further hearing with a panel of a larger number of justices. A further hearing may be held if the Supreme Court makes a ruling inconsistent with a previous ruling or if the Court deems that the importance, difficulty or novelty of a ruling of the Court justifies such hearing. The Supreme Court also holds the unique power of being able to order \"trial de novo\" (a retrial).\n\nIn Nauru, there is no single highest court for all types of cases. The Supreme Court has final jurisdiction on constitutional matters, but any other case may be appealed further to the Appellate Court. In addition, an agreement between Nauru and Australia in 1976 provides for appeals from the Supreme Court of Nauru to the High Court of Australia in both criminal and civil cases, with the notable exception of constitutional cases.\n\nIn New Zealand, the right of appeal to the Privy Council was abolished following the passing of the Supreme Court Act (2003). A right of appeal to the Privy Council remains for criminal cases which were decided before the Supreme Court was created, but it is likely that the successful appeal by Mark Lundy to the Privy Council in 2013 will be the last appeal to the Board from New Zealand.\n\nThe new Supreme Court of New Zealand was officially established at the beginning of 2004, although it did not come into operation until July. The High Court of New Zealand was until 1980 known as the Supreme Court. The Supreme Court has a purely appellate jurisdiction and hears appeals from the Court of Appeal of New Zealand. In some cases, an appeal may be removed directly to the Supreme Court from the High Court. For certain cases, particularly cases which commenced in the District Court, a lower court (typically the High Court or the Court of Appeal) may be the court of final jurisdiction.\n\nThe Supreme Court has been the apex court for Pakistan since the declaration of the republic in 1956 (previously the Privy Council had that function). The Supreme Court has the final say on matters of constitutional law, federal law or on matters of mixed federal and provincial competence. It can hear appeals on matters of provincial competence only if a matter of a constitutional nature is raised.\n\nWith respect to Pakistan's territories (i.e. FATA, Azad Kashmir, Northern Areas and Islamabad Capital Territory (ICT)) the Supreme Court's jurisdiction is rather limited and varies from territory to territory; it can hear appeals only of a constitutional nature from FATA and Northern Areas, while ICT generally functions the same as provinces. Azad Kashmir has its own courts system and the constitution of Pakistan does not apply to it as such; appeals from Azad Kashmir relate to its relationship with Pakistan.\n\nThe provinces have their own courts system, with the High Court as the apex court, except insofar as where an appeal can go to the Supreme Court as mentioned above.\n\nThe Supreme Court of the United Kingdom is the ultimate court for criminal and civil matters in England, Wales and Northern Ireland and for civil matters in Scotland. (The supreme court for criminal matters in Scotland is the High Court of Justiciary.) The Supreme Court was established by the Constitutional Reform Act 2005 with effect from 1 October 2009, replacing and assuming the judicial functions of the House of Lords. Devolution issues under the Scotland Act 1998, Government of Wales Act and Northern Ireland Act were also transferred to the new Supreme Court by the Constitutional Reform Act, from the Judicial Committee of the Privy Council.\n\nIn respect of Community Law the Supreme Court is subject to the decisions of the European Court of Justice. Since there can be no appeal from the Supreme Court, there is an interlocutory procedure by which the Supreme Court may refer to the European Court questions of European law which arise in cases before it, and obtain a definitive ruling before the Supreme Court gives its judgment.\n\nThe Supreme Court shares its members and accommodation at the Middlesex Guildhall in London with the Judicial Committee of the Privy Council which hears final appeals from certain smaller Commonwealth countries, admiralty cases, and certain appeals from the ecclesiastical courts and statutory private jurisdictions, such as professional and academic bodies.\n\n(The Constitutional Reform Act also renamed the \"Supreme Court of Judicature of Northern Ireland\" to the Court of Judicature, and the rarely cited \"Supreme Court of Judicature for England and Wales\" as the Senior Courts of England and Wales).\n\nThe Supreme Court was set up in 2009; until then the House of Lords was the ultimate court in addition to being a legislative body, and the Lord Chancellor, with legislative and executive functions, was also a senior judge in the House of Lords.\n\nThe Supreme Court of the United States, established in 1789, is the highest Federal court in the United States, with powers of judicial review first asserted in \"Calder v. Bull\" (1798) in Justice Iredell's dissenting opinion. The power was later given binding authority by Justice Marshall in \"Marbury v. Madison\" (1803). There are currently nine seats on the US Supreme Court.\n\nEach U.S. state has a state supreme court, which is the highest authority interpreting that state's law and administering that state's judiciary. Two states, Oklahoma and Texas, each have two separate highest courts that respectively specialize in criminal cases and civil cases. Although Delaware has a specialized court, the Court of Chancery, to hear cases in equity, it is not a supreme court because the Delaware Supreme Court has appellate jurisdiction over it.\n\nThe titles of state supreme court vary, which can cause confusion between jurisdictions because one state may use a name for its highest court that another uses for a lower court. In New York, Maryland, and the District of Columbia the highest court is called the Court of Appeals, a name used by many states for their intermediate appellate courts. Further, trial courts of general jurisdiction in New York are called the Supreme Court, and the intermediate appellate court is called the Supreme Court, Appellate Division. In West Virginia, the highest court of the state is the Supreme Court of Appeals. In Maine and Massachusetts the highest court is styled the \"Supreme Judicial Court\"; the last is the oldest appellate court of continuous operation in the Western Hemisphere.\n\nThe Roman law and the Corpus Juris Civilis are generally held to be the historical model for civil law. From the late 18th century onwards, civil law jurisdictions began to codify their laws, most of all in civil codes.\n\nIn Austria, the Austrian Constitution of 1920 (based on a draft by Hans Kelsen) introduced judicial review of legislative acts for their constitutionality. This function is performed by the Constitutional Court (\"Verfassungsgerichtshof\"), which is also charged with the review of administrative acts on whether they violate constitutionally guaranteed rights.\nOther than that, administrative acts are reviewed by the Administrative Court (\"Verwaltungsgerichtshof\"). The Supreme Court (\"Oberste Gerichtshof (OGH)\"), stands at the top of Austria's system of \"ordinary courts\" (\"ordentliche Gerichte\") as the final instance in issues of private law and criminal law.\n\nIn Brazil, the Supreme Federal Tribunal (\"Supremo Tribunal Federal\") is the highest court. It is both the constitutional court and the court of last resort in Brazilian law. It only reviews cases that may be unconstitutional or final \"habeas corpus\" pleads for criminal cases. It also judges, in original jurisdiction, cases involving members of congress, senators, ministers of state, members of the high courts and the President and Vice-President of the Republic. The Superior Court of Justice (\"Tribunal Superior de Justiça\") reviews State and Federal Circuit courts decisions for civil law and criminal law cases, when dealing with federal law or conflicting rulings. The Superior Labour Tribunal (\"Tribunal Superior do Trabalho\") reviews cases involving labour law. The Superior Electoral Tribunal (\"Tribunal Superior Eleitoral\") is the court of last resort of electoral law, and also oversees general elections. The Superior Military Tribunal (\"Tribunal Superior Militar\") is the highest court in matters of federal military law.\n\nIn Croatia, the supreme jurisdiction is given to the Supreme Court, which secures a uniform application of laws. The Constitutional Court exists to verify constitutionality of laws and regulations, as well as decide on individual complaints on decisions on governmental bodies. It also decides on jurisdictional disputes between the legislative, executive and judicial branches.\n\nIn Denmark, all ordinary courts have original jurisdiction to hear all types of cases, including cases of a constitutional or administrative nature. As a result, there exists no special constitutional court, and therefore final jurisdiction is vested with the Danish Supreme Court (\"Højesteret\") which was established 14 February 1661 by king Frederik III.\n\nIn France, supreme appellate jurisdiction is divided among three judicial bodies:\n\nWhen there is jurisdictional dispute between judicial and administrative courts: the Court of Arbitration (\"Tribunal des conflits\"), which is empanelled half from the Court of Cassation and half from the Council of State and presided over by the Minister of Justice, is called together to settle the dispute or hand down a final decision.\n\nThe High Court (\"Haute Cour\") exists only to impeach the President of the French Republic in case of \"breach of his duties patently incompatible with his continuing in office\". Since a constitutional amendment of 2007, the French Constitution states that the High Court is composed of all members of both Houses of Parliament. As of 2012, it has never been convened.\n\nIn Germany, there is no \"de jure\" single supreme court. Instead, cases are handled by numerous federal courts, depending on their nature.\n\nFinal interpretation of the German Constitution, the \"Grundgesetz\", is the task of the \"Bundesverfassungsgericht\" (Federal Constitutional Court), which is the \"de facto\" highest German court, as it can declare both federal and state legislation ineffective, and has the power to overrule decisions of all other federal courts, despite not being a regular court of appeals on itself in the German court system. It is also the only court possessing the power and authority to outlaw political parties, if it is deemed that these parties have repeatedly violated articles of the Constitution.\n\nWhen it comes to civil and criminal cases, the \"Bundesgerichtshof\" (Federal Court of Justice) is at the top of the hierarchy of courts. The other branches of the German judicial system each have their own appellate systems, each topped by a high court; these are the \"Bundessozialgericht\" (Federal Social Court) for matters of social security, the \"Bundesarbeitsgericht\" (Federal Labour Court) for employment and labour, the \"Bundesfinanzhof\" (Federal Fiscal Court) for taxation and financial issues, and the \"Bundesverwaltungsgericht\" (Federal Administrative Court) for administrative law. The so-called \"Gemeinsamer Senat der Obersten Gerichtshöfe\" (Joint Senate of the Supreme Courts) is not a supreme court in itself, but an ad-hoc body that is convened in only when one supreme court intends to diverge from another supreme court's legal opinion or when a certain case exceeds the authority of one court. As the courts have well-defined areas of responsibility, situations like these are rather rare and so, the Joint Senate gathers very infrequently, and only to consider matters which are mostly definitory.\n\nIn the Netherlands, the Supreme Court of the Netherlands is the highest court. Its decisions, known as \"arresten\", are absolutely final. The court is banned from testing legislation against the constitution, pursuant to the principle of the sovereignty of the States-General; the court can, however, test legislation against some treaties. Also, the ordinary courts in the Netherlands, including the Hoge Raad, do not deal with administrative law, which is dealt with in separate administrative courts, the highest of which is the Council of State (Raad van State)\n\nThe Supreme Court of Iceland (, lit. \"Highest Court of Iceland\") was founded under Act No. 22/1919 and held its first session on 16 February 1920. The Court holds the highest judicial power in Iceland, where the court system has two levels.\n\nThe Supreme Court of India, also known colloquially as the 'apex court', is the highest judicial body in the Republic of India. Any decision taken by it is final and binding, and can only be modified in some cases (death sentence, etc.) by the President of India. It has several jurisdiction like \n1. Original\n2.Appellate \n3. Advisory\n\nIt is also known as court of records, i. e. all judgements are recorded and printed. These are cited in lower courts as case - law in various cases.\n\nItaly follows the French system of different supreme courts.\n\nThe Italian court of last resort for most disputes is the \"Corte Suprema di Cassazione\". There is also a separate constitutional court, the \"Corte costituzionale\", which has a duty of judicial review, and which can strike down legislation as being in conflict with the Constitution.\n\nIn Japan, the Supreme Court of Japan is called (Saikō-Saibansho; called 最高裁 Saikō-Sai for short), located in Chiyoda, Tokyo, and is the highest court in Japan. It has ultimate judicial authority within Japan to interpret the Constitution and decide questions of national law (including local by laws). It has the power of judicial review (i.e., it can declare Acts of Diet and Local Assembly, and administrative actions, unconstitutional).\n\nIn Luxembourg, challenges on the conformity of the law to the Constitution are brought before the \"Cour Constitutionnelle\" (Constitutional Court). — The most used and common procedure to present these challenges is by way of the \"\"question préjudicielle\"\" (prejudicial question). The Court of last resort for civil and criminal proceedings is the \"\"Cour de Cassation\"\".\nFor administrative proceedings the highest court is the \"\"Cour Administrative\"\" (Administrative Court).\n\nThe supreme court of Macau is the Court of Final Appeal (; ).\n\nWhile the Philippines is generally considered a civil law nation, its Supreme Court is heavily modelled after the American Supreme Court. This can be attributed to the fact that the Philippines was colonized by both Spain and the United States, and the system of laws of both nations strongly influenced the development of Philippine laws and jurisprudence. Even as the body of Philippine laws remain mostly codified, the Philippine Civil Code expressly recognizes that decisions of the Supreme Court \"form part of the law of the land\", belonging to the same class as statutes. The 1987 Philippine Constitution also explicitly grants to the Supreme Court the power of judicial review over laws and executive actions. The Supreme Court is composed of 1 Chief Justice and 14 Associate Justices. The court sits either en banc or in divisions, depending on the nature of the case to be decided.\n\nIn the judicial system of mainland China the highest court of appeal is the Supreme People's Court. This supervises the administration of justice by all subordinate \"local\" and \"special\" people's courts, and is the court of last resort for the whole People's Republic of China except for Macau and Hong Kong\n\nIn Portugal, there are several supreme courts, each with a specific jurisdiction:\n\nUntil 2003, a fifth supreme court also existed for the military jurisdiction, this being the Supreme Military Court (\"Supremo Tribunal Militar\"). Presently, in time of peace, the supreme court for military justice matters is the Supreme Court of Justice, which now includes four military judges.\n\nIn the Republic of China (Taiwan), there are three different courts of last resort:\n\nThe Council of Grand Justices, consisting of 15 justices and mainly dealing with constitutional issues, is the counterpart of constitutional courts in some countries.\n\nAll three courts are directly under the Judicial Yuan, whose president also serves as Chief Justice in the Council of Grand Justices.\n\nFounded by papal bull in 1532, the Court of Session is the supreme civil court of Scotland, and the High Court of Justiciary is the supreme criminal court. However, the absolute highest court (excluding criminal matters) is the Supreme Court of the United Kingdom.\n\nSpanish Supreme Court is the highest court for all cases in Spain (both private and public). Only those cases related to human rights can be appealed at the Constitutional Court (which also decides about acts accordance with Spanish Constitution). \nIn Spain, high courts cannot create binding precedents; however, lower rank courts usually observe Supreme Court interpretations. In most private law cases, two Supreme Court judgements supporting a claim are needed to appeal at the Supreme Court.\nFive sections form the Spanish Supreme court:\n\nIn Sweden, the Supreme Court and the Supreme Administrative Court respectively function as the highest courts of the land. The Supreme Administrative Court considers cases concerning disputes between individuals and administrative organs, as well as disputes among administrative organs, while the Supreme Court considers all other cases. The judges are appointed by the Government. In most cases, the Supreme Courts will only grant leave to appeal a case (\"prövningstillstånd\") if the case involves setting a precedent in the interpretation of the law. Exceptions are issues where the Supreme Court is the court of first instance. Such cases include an application for a retrial of a criminal case in the light of new evidence, and prosecutions made against an incumbent minister of the Government for severe neglect of duty. If a lower court has to try a case which involves a question where there is no settled interpretation of the law, it can also refer the question to the relevant Supreme Court for an answer.\n\nIn Switzerland, the Federal Supreme Court of Switzerland is the final court of appeals. Due to Switzerland's system of direct democracy, it has no authority to review the constitutionality of federal statutes, but the people can strike down a proposed law by referendum. According to settled case law, however, the Court is authorised to review the compliance of all Swiss law with certain categories of international law, especially the European Convention of Human Rights.\n\nIn Sri Lanka, the Supreme Court of Sri Lanka was created in 1972 after the adoption of a new Constitution. The Supreme Court is the highest and final superior court of record and is empowered to exercise its powers, subject to the provisions of the Constitution. The court rulings take precedence over all lower Courts. The Sri Lanka judicial system is complex blend of both common-law and civil-law. In some cases such as capital punishment, the decision may be passed on to the President of the Republic for clemency petitions. However, when there is 2/3 majority in the parliament in favour of president (as with present), the supreme court and its judges' powers become nullified as they could be fired from their positions according to the Constitution, if the president wants. Therefore, in such situations, Civil law empowerment vanishes.\n\nIn South Africa, a \"two apex\" system existed from 1994 to 2013. The Supreme Court of Appeal (SCA) was created in 1994 and replaced the Appellate Division of the Supreme Court of South Africa as the highest court of appeal in non-constitutional matters. The SCA is subordinate to the Constitutional Court, which is the highest court in matters involving the interpretation and application of the Constitution. But in August 2013 the Constitution was amended to make the Constitutional Court the country's single apex court, superior to the SCA in all matters, both constitutional and non-constitutional.\n\nHistorically, citizens appealed directly to the King along his route to places out of the Palace. A Thai King would adjudicate all disputes. During the reign of King Chulalongkorn, an official department for appeals was set up, and, after Thailand adopted a western-styled government, Thai Supreme Court was established in 1891.\n\nAt present, the Supreme Court of Thailand retains the important status as the highest court of justice in the country. Operating separately from the Administrative Court and the Constitutional Court, the judgement of the Supreme Court is considered as final.\n\nIn the United Arab Emirates, the Federal Supreme Court of the United Arab Emirates was created in 1973 after the adoption of the Constitution. The Supreme Court is the highest and final superior court of record and is empowered to exercise its powers, subject to the provisions of the Constitution. The court rulings take precedence over all lower Courts. The Emirati judicial system is complex blend of both Islamic law and civil law. In some cases such as capital punishment, the decision may be passed on to the President of the country (currently Khalifa bin Zayed Al Nahyan).\n\n\nLaw of Indonesia at the national level is based on a combination of civil law from the tradition of Roman-Dutch law and customary law from the tradition of Adat. Law in regional jurisdictions can vary from province to province, including even Sharia law, for example Islamic criminal law in Aceh, though even at the national level, individual justices can cite sharia or other forms of non-Dutch law in their legal opinions.\n\nThe Supreme Court of Indonesia is the main judicial arm of the state, functioning as the final court of appeal as well as a means to re-open cases previously closed. The Supreme Court, which consists of a total of 51 justices, also oversees the regional high courts. It was founded at the country's independence in 1945.\n\nThe Constitutional Court of Indonesia, on the other hand, is a part of the judicial branch tasked with review of bills and government actions for constitutionality, as well as regulation of the interactions between various arms of the state. The constitutional amendment to establish the court was passed in 2001, and the court itself was established in 2003. The Constitutional Court consists of nine justices serving nine year terms, and they're appointed in tandem by the Supreme Court, the President of Indonesia and the People's Representative Council.\n\nIn most nations with constitutions modelled after the Soviet Union, the legislature was given the power of being the \"court of last resort\". In the People's Republic of China, the final power to interpret the law is vested in the Standing Committee of the National People's Congress (NPCSC). This power includes the power to interpret the basic laws of Hong Kong and Macau, the constitutional documents of the two special administrative regions which are common law and Portuguese-based legal system jurisdictions respectively. This power is a legislative power and not a judicial one in that an interpretation by the NPCSC does not affect cases which have already been decided.\n\n",
                "Precedent\n\nIn legal systems based on common law, a precedent, or authority, is a principle or rule established in a previous legal case that is either binding on or persuasive for a court or other tribunal when deciding subsequent cases with similar issues or facts . Common law legal systems place great value on deciding cases according to consistent principled rules so that similar facts will yield similar and predictable outcomes, and observance of precedent is the mechanism by which that goal is attained. The principle by which judges are bound to precedents is known as stare decisis. Black's Law Dictionary defines \"precedent\" as a \"rule of law established for the first time by a court for a particular type of case and thereafter referred to in deciding similar cases\". Common law precedent is a third kind of law, on equal footing with statutory law (statutes and codes enacted by legislative bodies), and delegated legislation (in U.K. parlance) or regulatory law (in U.S. parlance) (regulations promulgated by executive branch agencies).\n\nCase law, in common law jurisdictions, is the set of decisions of adjudicatory tribunals or other rulings that can be cited as precedent. In most countries, including most European countries, the term is applied to any set of rulings on law which is guided by previous rulings, for example, previous decisions of a government agency.\n\nEssential to the development of case law is the publication and indexing of decisions for use by lawyers, courts and the general public, in the form of law reports. While all decisions are precedent (though at varying levels of authority as discussed throughout this article), some become \"leading cases\" or \"landmark decisions\" that are cited especially often.\n\n\"Stare decisis\" (Anglo-Latin pronunciation: ) is a legal principle by which judges are obligated to respect the precedent established by prior decisions. The words originate from the phrasing of the principle in the Latin maxim Stare decisis et non quieta movere: \"to stand by decisions and not disturb the undisturbed\". In a legal context, this is understood to mean that courts should generally abide by precedent and not disturb settled matters. The principle of \"stare decisis\" can be divided into two components.\n\nThe first is the rule that a decision made by a superior court, or by the same court in an earlier decision, is binding precedent that the court itself and all its inferior courts are obligated to follow. The second is the principle that a court should not overturn its own precedent unless there is a strong reason to do so and should be guided by principles from lateral and inferior courts. The second principle, regarding persuasive precedent, is an advisory one that courts can and do ignore occasionally.\n\nIn the common law tradition, courts decide the law applicable to a case by interpreting statutes and applying precedent which record how and why prior cases have been decided. Unlike most civil law systems, common law systems follow the doctrine of \"stare decisis\", by which most courts are bound by their own previous decisions in similar cases, and all lower courts should make decisions consistent with previous decisions of higher courts. For example, in England, the High Court and the Court of Appeal are each bound by their own previous decisions, but the Supreme Court of the United Kingdom is able to deviate from its earlier decisions, although in practice it rarely does so.\n\nGenerally speaking, higher courts do not have direct oversight over day-to-day proceedings in lower courts, in that they cannot reach out on their own initiative (\"sua sponte\") at any time to reverse or overrule judgments of the lower courts. Normally, the burden rests with litigants to appeal rulings (including those in clear violation of established case law) to the higher courts. If a judge acts against precedent and the case is not appealed, the decision will stand.\n\nA lower court may not rule against a binding precedent, even if the lower court feels that the precedent is unjust; the lower court may only express the hope that a higher court or the legislature will reform the rule in question. If the court believes that developments or trends in legal reasoning render the precedent unhelpful, and wishes to evade it and help the law evolve, the court may either hold that the precedent is inconsistent with subsequent authority, or that the precedent should be \"distinguished\" by some material difference between the facts of the cases. If that judgment goes to appeal, the appellate court will have the opportunity to review both the precedent and the case under appeal, perhaps overruling the previous case law by setting a new precedent of higher authority. This may happen several times as the case works its way through successive appeals. Lord Denning, first of the High Court of Justice, later of the Court of Appeal, provided a famous example of this evolutionary process in his development of the concept of estoppel starting in the \"High Trees\" case: \"Central London Property Trust Ltd v. High Trees House Ltd\" [1947] K.B. 130.\n\nJudges may refer to various types of persuasive authority to reach a decision in a case. Widely cited non-binding sources include legal encyclopedias such as \"Corpus Juris Secundum\" and \"Halsbury's Laws of England\", or the published work of the Law Commission or the American Law Institute. Some bodies are given statutory powers to issue Guidance with persuasive authority or similar statutory effect, such as the Highway Code.\n\nIn federal or multi-jurisdictional law systems there may exist conflicts between the various lower appellate courts. Sometimes these differences may not be resolved and it may be necessary to distinguish how the law is applied in one district, province, division or appellate department. Usually only an appeal accepted by the court of last resort will resolve such differences and, for many reasons, such appeals are often not granted.\n\nAny court may seek to distinguish its present case from that of a binding precedent, in order to reach a different conclusion. The validity of such a distinction may or may not be accepted on appeal. An appellate court may also propound an entirely new and different analysis from that of junior courts, and may or may not be bound by its own previous decisions, or in any case may distinguish the decisions based on significant differences in the facts applicable to each case. Or, a court may view the matter before it as one of \"first impression,\" not governed by any controlling precedent.\n\nWhere there are several members of a court, there may be one or more judgments given; only the ratio decidendi of the majority can constitute a binding precedent, but all may be cited as persuasive, or their reasoning may be adopted in argument. Quite apart from the rules of precedent, the weight actually given to any reported judgment may depend on the reputation of both the court and the judges.\n\nGenerally, a common law court system has trial courts, intermediate appellate courts and a supreme court. The inferior courts conduct almost all trial proceedings. The inferior courts are bound to obey precedent established by the appellate court for their jurisdiction, and all supreme court precedent.\n\nThe Supreme Court of California's explanation of this principle is that\n\nAn Intermediate state appellate court is generally bound to follow the decisions of the highest court of that state.\n\nThe application of the doctrine of \"stare decisis\" from a superior court to an inferior court is sometimes called \"vertical stare decisis\".\n\nThe idea that a judge is bound by (or at least should respect) decisions of earlier judges of similar or coordinate level is called horizontal \"stare decisis\".\n\nIn the United States federal court system, the intermediate appellate courts are divided into thirteen \"circuits,\" each covering some range of territory ranging in size from the District of Columbia alone up to seven states. Each panel of judges on the court of appeals for a circuit is bound to obey the prior appellate decisions of the same circuit. Precedent of a United States court of appeals may be overruled only by the court \"en banc,\" that is, a session of all the active appellate judges of the circuit, or by the United States Supreme Court, not simply by a different three-judge panel.\n\nWhen a court binds itself, this application of the doctrine of precedent is sometimes called \"horizontal stare decisis\". The state of New York has a similar appellate structure as it is divided into four appellate departments supervised by the final New York Court of Appeals. Decisions of one appellate department are not binding upon another, and in some cases the departments differ considerably on interpretations of law.\n\nIn federal systems the division between federal and state law may result in complex interactions. In the United States, state courts are not considered inferior to federal courts but rather constitute a parallel court system.\n\n\nIn practice, however, judges in one system will almost always choose to follow relevant case law in the other system to prevent divergent results and to minimize forum shopping.\n\nPrecedent that must be applied or followed is known as \"binding precedent\" (alternately \"metaphorically precedent\", \"mandatory\" or \"binding authority\", etc.). Under the doctrine of \"stare decisis\", a lower court must honor findings of law made by a higher court that is within the appeals path of cases the court hears. In state and federal courts in the United States of America, jurisdiction is often divided geographically among local trial courts, several of which fall under the territory of a regional appeals court. All appellate courts fall under a highest court (sometimes but not always called a \"supreme court\"). By definition, decisions of lower courts are not binding on courts higher in the system, nor are appeals court decisions binding on local courts that fall under a different appeals court. Further, courts must follow their own proclamations of law made earlier on other cases, and honor rulings made by other courts in disputes among the parties before them pertaining to the same pattern of facts or events, unless they have a strong reason to change these rulings (see Law of the case re: a court's previous holding being binding precedent for that court).\n\nIn law, a binding precedent (also known as a mandatory precedent or binding authority) is a precedent which must be followed by all lower courts under common law legal systems. In English law it is usually created by the decision of a higher court, such as the Supreme Court of the United Kingdom, which took over the judicial functions of the House of Lords in 2009. In Civil law and pluralist systems precedent is not binding but case law is taken into account by the courts.\n\nBinding precedent relies on the legal principle of \"stare decisis\". \"Stare decisis\" means to stand by things decided. It ensures certainty and consistency in the application of law. Existing binding precedent from past cases are applied in principle to new situations by analogy.\n\nOne law professor has described mandatory precedent as follows:\n\nIn extraordinary circumstances a higher court may overturn or overrule mandatory precedent, but will often attempt to distinguish the precedent before overturning it, thereby limiting the scope of the precedent.\n\nUnder the U.S. legal system, courts are set up in a hierarchy. At the top of the federal or national system is the Supreme Court, and underneath are lower federal courts. The state court systems have hierarchy structures similar to that of the federal system.\n\nThe U.S. Supreme Court has final authority on questions about the meaning of federal law, including the U.S. Constitution. For example, when the Supreme Court says that the First Amendment applies in a specific way to suits for slander, then every court is bound by that precedent in its interpretation of the First Amendment as it applies to suits for slander. If a lower court judge disagrees with a higher court precedent on what the First Amendment should mean, the lower court judge must rule according to the binding precedent. Until the higher court changes the ruling (or the law itself is changed), the binding precedent is authoritative on the meaning of the law.\n\nLower courts are bound by the precedent set by higher courts within their region. Thus, a federal district court that falls within the geographic boundaries of the Third Circuit Court of Appeals (the mid-level appeals court that hears appeals from district court decisions from Delaware, New Jersey, Pennsylvania, and the Virgin Islands) is bound by rulings of the Third Circuit Court, but not by rulings in the Ninth Circuit (Alaska, Arizona, California, Guam, Hawaii, Idaho, Montana, Nevada, Northern Mariana Islands, Oregon, and Washington), since the Circuit Courts of Appeals have jurisdiction defined by geography. The Circuit Courts of Appeals can interpret the law how they want, so long as there is no binding Supreme Court precedent. One of the common reasons the Supreme Court grants certiorari (that is, they agree to hear a case) is if there is a conflict among the circuit courts as to the meaning of a federal law.\n\nThere are three elements needed for a precedent to work. Firstly, the hierarchy of the courts needs to be accepted, and an efficient system of law reporting. 'A balance must be struck between the need on one side for the legal certainty resulting from the binding effect of previous decisions, and on the other side the avoidance of undue restriction on the proper development of the law (1966 Practice Statement (Judicial Precedent) by Lord Gardiner L.C.)'.\n\nJudges are bound by the law of binding precedent in England and Wales and other common law jurisdictions. This is a distinctive feature of the English legal system. In Scotland and many countries throughout the world, particularly in mainland Europe, civil law means that judges take case law into account in a similar way, but are not obliged to do so and are required to consider the precedent in terms of principle. Their fellow judges' decisions may be persuasive but are not binding. Under the English legal system, judges are not necessarily entitled to make their own decisions about the development or interpretations of the law. They may be bound by a decision reached in a previous case. Two facts are crucial to determining whether a precedent is binding:\n\n\"Super \"stare decisis\"\" is a term used for important precedent that is resistant or immune from being overturned, without regard to whether correctly decided in the first place. It may be viewed as one extreme in a range of precedential power, or alternatively, to express a belief, or a critique of that belief, that some decisions should not be overturned.\n\nIn 1976, Richard Posner and William Landes coined the term \"super-precedent,\" in an article they wrote about testing theories of precedent by counting citations. Posner and Landes used this term to describe the influential effect of a cited decision. The term \"super-precedent\" later became associated with different issue: the difficulty of overturning a decision. In 1992, Rutgers professor Earl Maltz criticized the Supreme Court's decision in \"Planned Parenthood v. Casey\" for endorsing the idea that if one side can take control of the Court on an issue of major national importance (as in \"Roe v. Wade\"), that side can protect its position from being reversed \"by a kind of super-stare decisis\". The controversial idea that some decisions are virtually immune from being overturned, regardless of whether they were decided correctly in the first place, is the idea to which the term \"super \"stare decisis\"\" now usually refers.\n\nThe concept of super-\"stare decisis\" (or \"super-precedent\") was mentioned during the interrogations of Chief Justice John Roberts and Justice Samuel Alito before the Senate Judiciary Committee. Prior to the commencement of the Roberts hearings, the chair of that committee, Senator Arlen Specter of Pennsylvania, wrote an op/ed in the \"New York Times\" referring to \"Roe\" as a \"super-precedent\". He revisited this concept during the hearings, but neither Roberts nor Alito endorsed the term or the concept.\n\nPersuasive precedent (also persuasive authority) is precedent or other legal writing that is not binding precedent but that is useful or relevant and that may guide the judge in making the decision in a current case. Persuasive precedent includes cases decided by lower courts, by peer or higher courts from other geographic jurisdictions, cases made in other parallel systems (for example, military courts, administrative courts, indigenous/tribal courts, state courts versus federal courts in the United States), statements made in dicta, treatises or academic law reviews, and in some exceptional circumstances, cases of other nations, treaties, world judicial bodies, etc.\n\nIn a \"case of first impression\", courts often rely on persuasive precedent from courts in other jurisdictions that have previously dealt with similar issues. Persuasive precedent may become binding through its adoption by a higher court.\n\nIn civil law and pluralist systems, as under Scots law, precedent is not binding but case law is taken into account by the courts.\n\nA lower court's opinion may be considered as persuasive authority if the judge believes they have applied the correct legal principle and reasoning.\n\nA court may consider the ruling of a higher court that is not binding. For example, a district court in the United States First Circuit could consider a ruling made by the United States Court of Appeals for the Ninth Circuit as persuasive authority.\n\nCourts may consider rulings made in other courts that are of equivalent authority in the legal system. For example, an appellate court for one district could consider a ruling issued by an appeals court in another district.\n\nCourts may consider \"obiter dicta\" in opinions of higher courts. Dicta of a higher court, though not binding, will often be persuasive to lower courts. The phrase \"obiter dicta\" is usually translated as \"other things said\", but due to the high number of judges and individual concurring opinions, it is often hard to distinguish from the \"ratio decidendi\" (reason for the decision). For these reasons, the obiter dicta may often be taken into consideration by a court. A litigant may also consider \"obiter dicta\" if a court has previously signaled that a particular legal argument is weak and may even warrant sanctions if repeated.\n\nA case decided by a multi-judge panel could result in a split decision. While only the majority opinion is considered precedential, an outvoted judge can still publish a dissenting opinion. Common patterns for dissenting opinions include:\nA judge in a subsequent case, particularly in a different jurisdiction, could find the dissenting judge's reasoning persuasive. In the jurisdiction of the original decision, however, a judge should only overturn the holding of a court lower or equivalent in the hierarchy. A district court, for example, could not rely on a Supreme Court dissent as a basis to depart from the reasoning of the majority opinion. However, lower courts occasionally cite dissents, either for a limiting principle on the majority, or for propositions that are not stated in the majority opinion and not inconsistent with that majority, or to explain a disagreement with the majority and to urge reform (while following the majority in the outcome).\n\nCourts may consider the writings of eminent legal scholars in treatises, restatements of the law, and law reviews. The extent to which judges find these types of writings persuasive will vary widely with elements such as the reputation of the author and the relevance of the argument.\n\nThe courts of England and Wales are free to consider decisions of other jurisdictions, and give them whatever persuasive weight the English court sees fit, even though these other decisions are not binding precedent. Jurisdictions that are closer to modern English common law are more likely to be given persuasive weight (for example Commonwealth states such as Canada, Australia, or New Zealand). Persuasive weight might be given to other common law courts, such as from the United States, most often where the American courts have been particularly innovative, e.g. in product liability and certain areas of contract law.\n\nIn the United States, in the late 20th and early 21st centuries, the concept of a U.S. court considering foreign law or precedent has been considered controversial by some parties. The Supreme Court splits on this issue. This critique is recent, as in the early history of the United States, citation of English authority was ubiquitous. One of the first acts of many of the new state legislatures was to adopt the body of English common law into the law of the state. See here. Citation to English cases was common through the 19th and well into the 20th centuries. Even in the late 20th and early 21st centuries, it is relatively uncontroversial for American state courts to rely on English decisions for matters of pure common (i.e. judge-made) law. \n\nWithin the federal legal systems of several common-law countries, and most especially the United States, it is relatively common for the distinct lower-level judicial systems (e.g. state courts in the United States and Australia, provincial courts in Canada) to regard the decisions of other jurisdictions within the same country as persuasive precedent. Particularly in the United States, the adoption of a legal doctrine by a large number of other state judiciaries is regarded as highly persuasive evidence that such doctrine is preferred. A good example is the adoption in Tennessee of comparative negligence (replacing contributory negligence as a complete bar to recovery) by the 1992 Tennessee Supreme Court decision \"McIntyre v. Balentine\" (by this point all US jurisdictions save Tennessee, five other states, and the District of Columbia had adopted comparative negligence schemes). Moreover, in American law, the \"Erie\" doctrine requires federal courts sitting in diversity actions to apply state substantive law, but in a manner consistent with how the court believes the state's highest court would rule in that case. Since such decisions are not binding on state courts, but are often very well-reasoned and useful, state courts cite federal interpretations of state law fairly often as persuasive precedent, although it is also fairly common for a state high court to reject a federal court's interpretation of its jurisprudence.\n\nNon-publication of opinions, or unpublished opinions, are those decisions of courts that are not available for citation as precedent because the judges making the opinion deem the case as having less precedential value. Selective publication is the legal process which a judge or justices of a court decide whether a decision is to be or not published in a reporter. \"Unpublished\" federal appellate decisions are published in the Federal Appendix. Depublication is the power of a court to make a previously published order or opinion unpublished.\n\nLitigation that is settled out of court generates no written decision, and thus has no precedential effect. As one practical effect, the U.S. Department of Justice settles many cases against the federal government simply to avoid creating adverse precedent.\n\nSeveral rules may cause a decision to apply as narrow \"precedent\" to preclude future legal positions of the specific parties to a case, even if a decision is non-precedential with respect to all other parties.\n\nOnce a case is decided, the same plaintiff cannot sue the same defendant again on any claim arising out of the same facts. The law requires plaintiffs to put all issues on the table in a single case, not split the case. For example, in a case of an auto accident, the plaintiff cannot sue first for property damage, and then personal injury in a separate case. This is called \"res judicata\" or claim preclusion (\"'Res judicata'\" is the traditional name going back centuries; the name shifted to \"claim preclusion\" in the United States over the late 20th century). Claim preclusion applies whether the plaintiff wins or loses the earlier case, even if the later case raises a different legal theory, even the second claim is unknown at the time of the first case. Exceptions are extremely limited, for example if the two claims for relief must necessarily be brought in different courts (for example, one claim might be exclusively federal, and the other exclusively state).\n\nOnce a case is finally decided, any issues decided in the previous case may be binding against the party that lost the issue in later cases, even in cases involving other parties. For example, if a first case decides that a party was negligent, then other plaintiffs may rely on that earlier determination in later cases, and need not re-prove the issue of negligence. For another example, if a patent is shown to be invalid in a case against one accused infringer, that same patent is invalid against all other accused infringers—invalidity need not be re-proved. Again, there are limits and exceptions on this principle. The principle is called collateral estoppel or issue preclusion.\n\nWithin a single case, once there's been a first appeal, both the lower court and the appellate court itself will not further review the same issue, and will not re-review an issue that could have been appealed in the first appeal. Exceptions are limited to three \"exceptional circumstances:\" (1) when substantially different evidence is raised at a subsequent trial, (2) when the law changes after the first appeal, for example by a decision of a higher court, or (3) when a decision is clearly erroneous and would result in a manifest injustice. This principle is called \"law of the case\".\n\nOn many questions, reasonable people may differ. When two of those people are judges, the tension among two lines of precedent may be resolved as follows.\n\nIf the two courts are in separate, parallel jurisdictions, there is no conflict, and two lines of precedent may persist. Courts in one jurisdiction are influenced by decisions in others, and notably better rules may be adopted over time.\n\nCourts try to formulate the common law as a \"seamless web\" so that principles in one area of the law apply to other areas. However, this principle does not apply uniformly. Thus, a word may have different definitions in different areas of the law, or different rules may apply so that a question has different answers in different legal contexts. Judges try to minimize these conflicts, but they arise from time to time, and under principles of 'stare decisis', may persist for some time.\n\nA matter of first impression (known as \"primae impressionis\" in Latin) is a legal case in which there is no binding authority on the matter presented. Such a case can set forth a completely original issue of law for decision by the courts. A first impression case may be a first impression in only a particular jurisdiction. In that situation, courts will look to holdings of other jurisdictions for persuasive authority.\n\nIn the latter meaning, the case in question cannot be decided through referring to and/or relying on precedent. Since the legal issue under consideration has never been decided by an appeals court and, therefore, there is no precedent for the court to follow, the court uses analogies from prior rulings by appeals courts, refers to commentaries and articles by legal scholars, and applies its own logic. In cases of first impression, the trial judge will often ask both sides' attorneys for legal briefs.\n\nIn some situations, a case of first impression may exist in a jurisdiction until a reported appellate court decision is rendered.\n\nThe different roles of case law in civil law and common law traditions create differences in the way that courts render decisions. Common law courts generally explain in detail the legal rationale behind their decisions, with citations of both legislation and previous relevant judgments, and often an exegesis of the wider legal principles. These are called \"ratio decidendi\" and constitute a precedent binding on other courts; further analyses not strictly necessary to the determination of the current case are called \"obiter dicta\", which have persuasive authority but are not technically binding. By contrast, decisions in civil law jurisdictions are generally very short, referring only to statutes. The reason for this difference is that these civil law jurisdictions apply legislative positivism — a form of extreme legal positivism — which holds that legislation is the only valid source of law because it has been voted on democratically; thus, it is not the judiciary's role to create law, but rather to interpret and apply statute, and therefore their decisions must reflect that.\n\n\"Stare decisis\" is not usually a doctrine used in civil law systems, because it violates the legislative positivist principle that only the legislature may make law. Instead, the civil law system relies on the doctrine of \"jurisprudence constante\", according to which if a court has adjudicated a consistent line of cases that arrive at the same holdings using sound reasoning, then the previous decisions are highly persuasive but not controlling on issues of law. This doctrine is similar to \"stare decisis\" insofar as it dictates that a court's decision must condone a cohesive and predictable result. In theory, lower courts are generally not bound by the precedents of higher courts. In practice, the need for predictability means that lower courts generally defer to the precedent of higher courts. As a result, the precedent of courts of last resort, such as the French Cassation Court and the Council of State, is recognized as being \"de facto\" binding on lower courts.\n\nThe doctrine of \"jurisprudence constante\" also influences how court decisions are structured. In general, court decisions of common law jurisdictions give a sufficient \"ratio decidendi\" as to guide future courts. The ratio is used to justify a court decision on the basis of previous case law as well as to make it easier to use the decision as a precedent for future cases. By contrast, court decisions in some civil law jurisdictions (most prominently France) tend to be extremely brief, mentioning only the relevant legislation and codal provisions and not going into the \"ratio decidendi\" in any great detail. This is the result of the legislative positivist view that the court is only interpreting the legislature's intent and therefore detailed exposition is unnecessary. Because of this, \"ratio decidendi\" is carried out by legal academics (doctrinal writers) who provide the explanations that in common law jurisdictions would be provided by the judges themselves.\n\nIn other civil law jurisdictions, such as the German-speaking countries, \"ratio decidendi\" tend to be much more developed than in France, and courts will frequently cite previous cases and doctrinal writers. However, some courts (such as German courts) have less emphasis on the particular facts of the case than common law courts, but have more emphasis on the discussion of various doctrinal arguments and on finding what the correct interpretation of the law is.\n\nThe mixed systems of the Nordic countries are sometimes considered a branch of the civil law, but they are sometimes counted as separate from the civil law tradition. In Sweden, for instance, case law arguably plays a more important role than in some of the continental civil law systems. The two highest courts, the Supreme Court (\"Högsta domstolen\") and the Supreme Administrative Court (\"Högsta förvaltningsdomstolen\"), have the right to set precedent which has persuasive authority on all future application of the law. Appellate courts, be they judicial (\"hovrätter\") or administrative (\"kammarrätter\"), may also issue decisions that act as guides for the application of the law, but these decisions are persuasive, not controlling, and may therefore be overturned by higher courts.\n\nSome mixed systems, such as Scots law in Scotland, South-African law, and the law of Quebec and Louisiana, do not fit into the civil vs. common law dichotomy because they mix portions of both. Such systems may have been heavily influenced by the common law tradition; however, their private law is firmly rooted in the civil law tradition. Because of their position between the two main systems of law, these types of legal systems are sometimes referred to as \"mixed\" systems of law. Louisiana courts, for instance, operate under both \"stare decisis\" and \"jurisprudence constante\". In South Africa, the precedent of higher courts is absolutely or fully binding on lower courts, whereas the precedent of lower courts only has persuasive authority on higher courts; horizontally, precedent is \"prima facie\" or presumptively binding between courts.\n\nLaw professors in common law traditions play a much smaller role in developing case law than professors in civil law traditions. Because court decisions in civil law traditions are brief and not amenable to establishing precedent, much of the exposition of the law in civil law traditions is done by academics rather than by judges; this is called doctrine and may be published in treatises or in journals such as \"Recueil Dalloz\" in France. Historically, common law courts relied little on legal scholarship; thus, at the turn of the twentieth century, it was very rare to see an academic writer quoted in a legal decision (except perhaps for the academic writings of prominent judges such as Coke and Blackstone). Today academic writers are often cited in legal argument and decisions as persuasive authority; often, they are cited when judges are attempting to implement reasoning that other courts have not yet adopted, or when the judge believes the academic's restatement of the law is more compelling than can be found in precedent. Thus common law systems are adopting one of the approaches long common in civil law jurisdictions.\n\nJustice Louis Brandeis, in a heavily footnoted dissent to \"Burnet v. Coronado Oil & Gas Co.\", 285 U.S. 393, 405-411 (1932), explained (citations and quotations omitted):\n\nThe United States Court of Appeals for the Third Circuit has stated:\n\nThe United States Court of Appeals for the Ninth Circuit has stated:\n\nJustice McHugh of the High Court of Australia in relation to precedents remarked in \"Perre v Apand\":\n\nPrecedent viewed against passing time can serve to establish trends, thus indicating the next logical step in evolving interpretations of the law. For instance, if immigration has become more and more restricted under the law, then the next legal decision on that subject may serve to restrict it further still. The existence of submerged precedent (reasoned opinions not made available through conventional legal research sources) has been identified as a potentially distorting force in the evolution of law.\n\nScholars have recently attempted to apply network theory to precedent in order to establish which precedent is most important or authoritative, and how the court's interpretations and priorities have changed over time.\n\nEarly English common law did not have or require the \"stare decisis\" doctrine for a range of legal and technological reasons:\n\nThese features changed over time, opening the door to the doctrine of \"stare decisis\":\n\n\"Stare decisis\" applies to the holding of a case, rather than to obiter dicta (\"things said by the way\"). As the United States Supreme Court has put it: \"dicta may be followed if sufficiently persuasive but are not binding.\"\n\nIn the United States Supreme Court, the principle of stare decisis is most flexible in constitutional cases:\n\nFor example, in the years 1946–1992, the U.S. Supreme Court reversed itself in about 130 cases. The U.S. Supreme Court has further explained as follows:\n\nThe United States Supreme Court has stated that where a court gives multiple reasons for a given result, each alternative reason that is \"explicitly\" labeled by the court as an \"independent\" ground for the decision is not treated as \"simply a dictum\".\n\nThe doctrine of binding precedent or \"stare decisis\" is basic to the English legal system. Special features of the English legal system include the following:\n\nThe British House of Lords, as the court of last appeal outside Scotland before it was replaced by the UK Supreme Court, was not strictly bound to always follow its own decisions until the case \"London Street Tramways v London County Council [1898] AC 375\". After this case, once the Lords had given a ruling on a point of law, the matter was closed unless and until Parliament made a change by statute. This is the most strict form of the doctrine of \"stare decisis\" (one not applied, previously, in common law jurisdictions, where there was somewhat greater flexibility for a court of last resort to review its own precedent).\n\nThis situation changed, however, after the issuance of the Practice Statement of 1966. It enabled the House of Lords to adapt English law to meet changing social conditions. In \"R v G & R\" 2003, the House of Lords overruled its decision in \"Caldwell\" 1981, which had allowed the Lords to establish mens rea (\"guilty mind\") by measuring a defendant's conduct against that of a \"reasonable person,\" regardless of the defendant's actual state of mind.\n\nHowever, the Practice Statement has been seldom applied by the House of Lords, usually only as a last resort. As of 2005, the House of Lords has rejected its past decisions no more than 20 times. They are reluctant to use it because they fear to introduce uncertainty into the law. In particular, the Practice Statement stated that the Lords would be especially reluctant to overrule themselves in criminal cases because of the importance of certainty of that law. The first case involving criminal law to be overruled with the Practice Statement was \"Anderton v Ryan\" (1985), which was overruled by \"R v Shivpuri\" (1986), two decades after the Practice Statement. Remarkably, the precedent overruled had been made only a year before, but it had been criticised by several academic lawyers. As a result, Lord Bridge stated he was \"undeterred by the consideration that the decision in \"Anderton v Ryan\" was so recent. The Practice Statement is an effective abandonment of our pretention to infallibility. If a serious error embodied in a decision of this House has distorted the law, the sooner it is corrected the better.\" Still, the House of Lords has remained reluctant to overrule itself in some cases; in \"R v Kansal\" (2002), the majority of House members adopted the opinion that \"R v Lambert\" had been wrongly decided and agreed to depart from their earlier decision.\n\nA precedent does not bind a court if it finds there was a lack of care in the original \"Per Incuriam\". For example, if a statutory provision or precedent had not been brought to the previous court's attention before its decision, the precedent would not be binding.\n\nOne of the most important roles of precedent is to resolve ambiguities in other legal texts, such as constitutions, statutes, and regulations. The process involves, first and foremost, consultation of the plain language of the text, as enlightened by the legislative history of enactment, subsequent precedent, and experience with various interpretations of similar texts.\n\nA judge's normal aids include access to all previous cases in which a precedent has been set, and a good English dictionary.\n\nJudges and barristers in the U.K use three primary rules for interpreting the law.\n\nUnder the literal rule, the judge should do what the actual legislation states rather than trying to do what the judge thinks that it means. The judge should use the plain everyday ordinary meaning of the words, even if this produces an unjust or undesirable outcome. A good example of problems with this method is \"R v Maginnis\" (1987), in which several judges in separate opinions found several different dictionary meanings of the word \"supply\". Another example is \"Fisher v Bell\", where it was held that a shopkeeper who placed an illegal item in a shop window with a price tag did not make an offer to sell it, because of the specific meaning of \"offer for sale\" in contract law. As a result of this case, Parliament amended the statute concerned to end this discrepancy.\n\nThe golden rule is used when use of the literal rule would obviously create an absurd result. The court must find genuine difficulties before it declines to use the literal rule. There are two ways in which the golden rule can be applied: the narrow method, and the broad method. Under the narrow method, when there are apparently two contradictory meanings to a word used in a legislative provision or it is ambiguous, the least absurd is to be used. For example, in \"Adler v George\" (1964), the defendant was found guilty under the Official Secrets Act of 1920. The act said it was an offence to obstruct HM Forces in the vicinity of a prohibited place. Adler argued that he was not in the \"vicinity\" of a prohibited place but was actually \"in\" a prohibited place. The court chose not to accept the wording literally. Under the broad method, the court may reinterpret the law at will when it is clear that there is only one way to read the statute. This occurred in \"Re Sigsworth\" (1935) where a man who murdered his mother was forbidden from inheriting her estate, despite a statute to the contrary.\n\nThe mischief rule is the most flexible of the interpretation methods. Stemming from \"Heydon's Case\" (1584), it allows the court to enforce what the statute is intended to remedy rather than what the words actually say. For example, in \"Corkery v Carpenter\" (1950), a man was found guilty of being drunk in charge of a carriage, although in fact he only had a bicycle.\n\nIn the United States, the courts have stated consistently that the text of the statute is read as it is written, using the ordinary meaning of the words of the statute.\n\nHowever, most legal texts have some lingering ambiguity—inevitably, situations arise in which the words chosen by the legislature do not address the precise facts in issue, or there is some tension among two or more statutes. In such cases, a court must analyze the various available sources, and reach a resolution of the ambiguity. The \"Canons of statutory construction\" are discussed in a separate article. Once the ambiguity is resolved, that resolution has binding effect as described in the rest of this article.\n\nAlthough inferior courts are bound in theory by superior court precedent, in practice a judge may believe that justice requires an outcome at some variance with precedent, and may distinguish the facts of the individual case on reasoning that does not appear in the binding precedent. On appeal, the appellate court may either adopt the new reasoning, or reverse on the basis of precedent. On the other hand, if the losing party does not appeal (typically because of the cost of the appeal), the lower court decision may remain in effect, at least as to the individual parties.\n\nOccasionally, a lower court judge explicitly states personal disagreement with the judgment he or she has rendered, but that he or she is required to do so by binding precedent. Note that inferior courts cannot evade binding precedent of superior courts, but a court can depart from its own prior decisions.\n\nIn the United States, \"stare decisis\" can interact in counterintuitive ways with the federal and state court systems. On an issue of federal law, a state court is not bound by an interpretation of federal law at the district or circuit level, but is bound by an interpretation by the United States Supreme Court. On an interpretation of state law, whether common law or statutory law, the federal courts are bound by the interpretation of a state court of last resort, and are required normally to defer to the precedent of intermediate state courts as well.\n\nCourts may choose to obey precedent of international jurisdictions, but this is not an application of the doctrine of \"stare decisis\", because foreign decisions are not binding. Rather, a foreign decision that is obeyed on the basis of the soundness of its reasoning will be called \"persuasive authority\" — indicating that its effect is limited to the persuasiveness of the reasons it provides.\n\nOriginalism is an approach to interpretation of a legal text in which controlling weight is given to the intent of the original authors (at least the intent as inferred by a modern judge). In contrast, a non-originalist looks at other cues to meaning, including the current meaning of the words, the pattern and trend of other judicial decisions, changing context and improved scientific understanding, observation of practical outcomes and \"what works,\" contemporary standards of justice, and \"stare decisis\". Both are directed at \"interpreting\" the text, not changing it—interpretation is the process of resolving ambiguity and choosing from among possible meanings, not changing the text.\n\nThe two approaches look at different sets of underlying facts that may or may not point in the same direction--\"stare decisis\" gives most weight to the newest understanding of a legal text, while originalism gives most weight to the oldest. While they don't necessarily reach different results in every case, the two approaches are in direct tension. Originalists such as Justice Antonin Scalia argue that \"\"Stare decisis\" is not usually a doctrine used in civil law systems, because it violates the principle that only the legislature may make law.\" Justice Scalia argues that America is a civil law nation, not a common law nation. By principle, originalists are generally unwilling to defer to precedent when precedent seems to come into conflict with the originalist's own interpretation of the Constitutional text or inferences of original intent (even in situations where there is no original source statement of that original intent). However, there is still room within an originalist paradigm for \"stare decisis\"; whenever the plain meaning of the text has alternative constructions, past precedent is generally considered a valid guide, with the qualifier being that it cannot change what the text actually says.\n\nOriginalists vary in the degree to which they defer to precedent. In his confirmation hearings, Justice Clarence Thomas answered a question from Senator Strom Thurmond, qualifying his willingness to change precedent in this way:\n\nPossibly he has changed his mind, or there are a very large body of cases which merit \"the additional step\" of ignoring the doctrine; according to Scalia, \"Clarence Thomas doesn't believe in stare decisis, period. If a constitutional line of authority is wrong, he would say, let's get it right.\"\n\nProfessor Caleb Nelson, a former clerk for Justice Thomas and law professor at the University of Virginia, has elaborated on the role of \"stare decisis\" in originalist jurisprudence:\n\nThere are disadvantages and advantages of binding precedent, as noted by scholars and jurists.\n\nIn a 1997 book, attorney Michael Trotter blamed over-reliance by American lawyers on binding and persuasive authority, rather than the merits of the case at hand, as a major factor behind the escalation of legal costs during the 20th century. He argued that courts should ban the citation of persuasive precedent from outside their jurisdiction, with two exceptions:\n\nThe disadvantages of \"stare decisis\" include its rigidity, the complexity of learning law, the differences between some cases may be very small and appear illogical, and the slow growth or incremental changes to the law that are in need of major overhaul.\n\nAn argument often used against the system is that it is undemocratic as it allows judges, which may or may not be elected, to make law.\n\nRegarding constitutional interpretations, there is concern that over-reliance on the doctrine of \"stare decisis\" can be subversive. An erroneous precedent may at first be only slightly inconsistent with the Constitution, and then this error in interpretation can be propagated and increased by further precedent until a result is obtained that is greatly different from the original understanding of the Constitution. \"Stare decisis\" is not mandated by the Constitution, and if it causes unconstitutional results then the historical evidence of original understanding can be re-examined. In this opinion, predictable fidelity to the Constitution is more important than fidelity to unconstitutional precedent. See also the living tree doctrine.\n\nA counter-argument (in favor of the advantages of \"stare decisis\") is that if the legislature wishes to alter the case law (other than constitutional interpretations) by statute, the legislature is empowered to do so. Critics sometimes accuse particular judges of applying the doctrine selectively, invoking it to support precedent that the judge supported anyway, but ignoring it in order to change precedent with which the judge disagreed.\n\nThere is much discussion about the virtue of using \"stare decisis\". Supporters of the system, such as minimalists, argue that obeying precedent makes decisions \"predictable\". For example, a business person can be reasonably assured of predicting a decision where the facts of his or her case are sufficiently similar to a case decided previously. This parallels the arguments against retroactive (ex post facto) laws banned by the U.S. Constitution.\n\n",
                "Last resort rule\n\nIn Constitutional law, the Last Resort Rule is a largely prudential rule which gives a federal court the power to avoid a constitutional issue in some circumstances. This rule dictates that, even if all other jurisdictional and justiciability obstacles are surmounted, federal courts still must avoid a constitutional issue if there is any other ground upon which to render a final judgment. The last resort rule can function as a distinct barrier to Constitutional avoidance. It is articulated by Justice Brandeis in Ashwander v. Tennessee Valley Authority.\n\nBrandeis cited two examples in Ashwander of the \"most varied application\" of the last resort rule. First, as between two potential grounds, one involving a constitutional question, the other a question of statutory construction or general law, the Court will only decide the latter. To the extent the question involves statutory construction and a plausible interpretation of the statute might obviate the need for constitutional review, this example replicates the seventh rule of the avoidance doctrine.\n\nTo illustrate this first application, Brandeis relied primarily on Siler v. Louisville & Nashville Railroad Co. In Siler, a railroad company challenged an order by the Kentucky railroad commission setting maximum rates on commodities transported by rail within the state. The company asserted a takings claim and a Commerce Clause claim under the United States Constitution, as well as state law claims, including a claim that the commission had exceeded its statutory authorization in making such an order. The Supreme Court upheld the lower federal court's order enjoining enforcement of the maximum rate order. The Court indicated, however, that the lower court should have enjoined the rate order on state law grounds, without reaching the federal constitutional grounds.\n\nThe Court in Siler confirmed that once the lower court properly determined that it had federal question jurisdiction, the court had the right to decide either all questions or only the state law questions. The Siler Court stated that where a case can be decided without reference to questions arising under the federal Constitution, that course is \"usually pursued and is not departed from without important reasons.\" The Court declared it better to decide the case with regard to the construction of the state statute, and the authority therein given to the Commission to make the order in question, rather than to unnecessarily decide the various constitutional questions appearing in the record.\n\nThe Siler Court offered no case precedent or doctrinal ground for this policy decision. The discretionary nature of the Court's decision limits the extent to which Siler serves as a primary basis for an absolute last resort rule. After recognizing the lower court's authority to decide the constitutional questions, the Court decided to follow the \"usual course\" of avoiding such questions if questions of local law would resolve the dispute. This purely prudential formulation of the rule allows courts to dispense with the rule for \"important reasons.\" 1 Although Brandeis prefaced his avoidance doctrine discussion in Ashwander by casting the seven rules as prudential, his formulation of the last resort rule omits this \"important reasons\" qualification. Thus, an evaluation of the proper scope of the last resort rule requires a determination of whether the qualifying phrase should be employed, or whether the rule should be viewed as an absolute.\n\n\"Pullman abstention\" represents the most prominent development of this initial application of the last resort rule after Ashwander. \n\nThe second application Brandeis furnished to demonstrate the last resort rule in Ashwander is the adequate and independent state ground doctrine: \"Appeals [to the United States Supreme Court] from the highest court of a state challenging its decision of a question under the Federal Constitution are frequently dismissed because the judgment can be sustained on an independent state ground.\" When reviewing judgments of state courts, the United States Supreme Court only reviews questions of federal law. The Court will decline to hear a case if an adequate and independent state ground supports the judgment of the state court. The Court reasons that, if a state ground independently supports the judgment, a decision by the Court on federal law grounds will have no effect on the outcome of the case and will amount to an advisory opinion.\n\nThe Supreme Court has provided six closely related justifications for the general doctrine of avoiding constitutional questions, noting their grounding in \"the unique place and character . . . of judicial review of governmental action for constitutionality.\"\n\nThe Court has often called judicial review of legislative acts the most important and delicate of its responsibilities. The Court's characterization of judicial review of legislative acts as a \"delica[te]\" function, \"particularly in view of possible consequences for others stemming also from constitutional roots,\" fundamentally justifies the general avoidance doctrine. An evaluation of the force of this assertion as a justification for avoiding constitutional questions must be linked to evaluation of a second justification offered for the avoidance doctrine, that such review is a \"final\" function. If the Court renders a final, binding conclusion as to constitutional interpretation each time it speaks on a constitutional issue, the arduous task of amending the Constitution may provide the only counter to the Court's ruling. If, however, the Court acts as more or less an equal participant with other political actors in an ongoing dialogue, those other non-judicial actors can reinterpret and reapply a constitutional provision.\n\nThe avoidance doctrine is also premised on \"the inherent limitations of the judicial process, arising especially from its largely negative character and limited resources of enforcement.\" Additionally, federal courts are vulnerable to the extent their jurisdiction and the work of their judges are subject to control by the other branches. Proponents of avoidance techniques such as the last resort rule believe that the federal judiciary must exercise its powers cautiously to conserve the fragile credibility of the least dangerous branch. \n\nIn 1947, evaluating the avoidance doctrine generally, the Supreme Court speculated that to pursue another policy -- a policy of \"accelerated decision\" -- \"might have put an end to, or seriously impaired, the distinctively American institution of judicial review.\" The Court continued: \"It is not without significance for the [avoidance] policy's validity that the periods when the power [of judicial review of legislative acts] has been exercised most readily and broadly have been the ones in which this Court and the institution of judicial review had their stormiest experiences.\"\n\nAckerman notes that Bickel's countermajoritarian difficulty \"recalls the Old Court's long, and ultimately futile, judicial struggle against the New Deal.\" By using the last resort rule frequently, the Court can live with a constitutional problem and let a solution simmer until widespread acceptance is at hand. Bickel argued that the avoidance doctrine, by allowing the judiciary to render unpopular decisions cautiously, rather than suddenly or haphazardly, preserves judicial credibility and increases public acceptance of Court decisions. The last resort rule allows judges to determine when widespread acceptance is at hand or when more simmering is necessary.\n\nAnother justification for the avoidance doctrine is the \"paramount importance of constitutional adjudication in our system.\" This justification overlaps to some extent with the delicate and final nature of the constitutional function, discussed above, but it also implicates the role of constitutional rights. The Court sometimes claims that the ability to declare constitutional rights is the most important power the federal judiciary wields. But many individual rights depend on administrative and statutory claims. Justice Antonin Scalia has argued that not \"every constitutional claim is ipso facto more worthy, and every statutory claim less worthy, of judicial review.\" A decision by a court clarifying a statutory or procedural entitlement to relief may have a tremendous effect on a great number of individuals, or on the workings of an administrative agency.\n\nTwo forceful justifications for the avoidance doctrine are \"the necessity, if government is to function constitutionally, for each [branch] to keep within its power, including the courts\" and \"the consideration due to the judgment of other repositories of constitutional power [*1048] concerning the scope of their authority.\" These justifications are grounded in the separation of powers principle in a constitutional and prudential sense.\n\nIn addition to maintaining appropriate power relations among the national branches, the final two justifications for the avoidance doctrine also encompass federalism concerns. Federal courts must defer appropriately to the powers retained by states and their courts. This comity concern implicates two important applications of the last resort rule: Pullman abstention and the adequate and independent state ground doctrine.\nIn Railroad Commission of Texas v. Pullman Co., the Texas Railroad Commission issued an order requiring that white Pullman conductors, not black Pullman porters, operate sleeping cars. Several railroad companies, and the intervening Pullman porters, challenged the order as unauthorized by state law and unconstitutional under the Equal Protection, Due Process and Commerce Clauses of the federal Constitution. The Court acknowledged that the \"complaint of the Pullman porters undoubtedly tendered a substantial constitutional issue.\" But the Court avoided the issue by abstaining from decision. Justice Frankfurter wrote:\n\nIf the Texas Commission had acted beyond the scope of its authority, the order would be declared invalid under Texas law and no court would need to reach the equal protection issue. Finding the state law unclear, the Court balked at \"making a tentative answer\" regarding Texas law which the Texas Supreme Court could displace the next day. So the Court handed the politically explosive case to the state court for resolution of state law issues.\n\nToday, if a federal court were presented with a case identical to Pullman and the parties chose not to press the nonconstitutional claims, the court, relying on Zobrest, could reach the equal protection claim. Relying on Siler, the court could decide the state law issues itself; or, alternatively, it could apply Pullman abstention. The Court in Pullman used abstention both to avoid wasting federal resources on a \"tentative\" state law decision and to avoid the \"friction of a premature constitutional adjudication.\" Abstention furthered harmony between state and federal courts \"without the need of rigorous congressional restriction of those powers.\" \n\nIn contrast to Pullman abstention, one branch of the adequate and independent state ground doctrine constitutes appropriate application of the last resort rule. The branch dealing with parallel state and federal constitutional provisions has developed in a manner that accords sufficient regard for comity interests while preserving adequate federal court review of constitutional claims. The branch of the doctrine dealing with state procedural foreclosure, however, is more problematic.\n\nThe Supreme Court is entitled to review all federal issues, including constitutional issues, on appeal from a final judgment of the highest state courts in order to preserve federal supremacy and advance uniformity in federal law. The Court will refuse to hear a case, however, if an adequate and independent state ground supports the decision. By deferring to state court decisions based on an adequate and independent state ground, the doctrine addresses Brandeis' concern of federal judicial interference with state authority. The doctrine is generally grounded in efforts to avoid advisory opinions and unnecessary constitutional rulings, and the premise of according sufficient respect to the authority of state courts. It applies only when litigation begins in state courts rather than the lower federal courts. This could be because of a desire to proceed in state court or because Congress has limited the jurisdiction of the lower federal courts.\n\nThe first branch of the doctrine commonly applies where state and federal constitutional provisions are implicated. The application of the adequate and independent state ground doctrine in cases involving state procedural foreclosure is more troublesome. In such instances, failure to adhere to a state procedural rule is often deemed an adequate basis to avoid Supreme Court review of a federal constitutional claim. State procedural law is thus allowed to frustrate federal constitutional rights because of the decision to respect state procedural rules. The second major criticism of the adequate and independent state ground doctrine is that it contributes to inaccuracy and inconsistency in federal law because state courts' erroneous interpretations of federal law remain on the books as long as the judgment is supported by an adequate and independent state ground. The state court's rulings on federal law, however, arguably amount to no more than dicta because those rulings do not provide the basis for the judgment. The Supreme Court might promote uniformity by addressing constitutional claims even when a judgment is supported by adequate and independent state grounds. The ability of the Court, however, to promote uniformity effectively is questionable in our large nation. In any event, uniformity may not always be desirable. The constitutional dialogue may be advanced by a multiplicity of pronouncements from state and federal courts on federal constitutional law.\n\nLike Pullman abstention, the adequate and independent state ground doctrine may disrupt and delay the vindication of federal rights, and make litigation of federal rights less efficient because of prolonged state proceedings and federal review. These concerns are less troubling in this context because the litigation begins in state court. Either that choice is voluntary and litigants could avoid the adequate and independent state ground doctrine by going to federal court initially, or that initial choice is restricted by congressional jurisdictional allocations and deference to state law in particular areas. Congress can alter such allocations in light of restrictions imposed by the adequate and independent state ground doctrine.\n\nBecause the Supreme Court may ultimately review state decisions denying relief under the federal Constitution based on parallel state constitutional grounds, the first branch of the doctrine is a more easily justified application of the last resort rule.\n\n"
            ],
            "good_message": false,
            "loggedData": [],
            "offensive": null,
            "researchTask": true,
            "selectedData": [],
            "shown_passages": [
                [
                    "Supreme court",
                    [
                        "A supreme court is the highest court within the hierarchy of courts in many legal jurisdictions.",
                        "Other descriptions for such courts include court of last resort, apex court, and highest (or final) court of appeal.",
                        "Broadly speaking, the decisions of a supreme court are not subject to further review by any other court.",
                        "Supreme courts typically function primarily as appellate courts, hearing appeals from decisions of lower trial courts, or from intermediate-level appellate courts.",
                        "However, not all highest courts are named as such.",
                        "Civil law states tend not to have a single highest court.",
                        "Additionally, the highest court in some jurisdictions is not named the \"Supreme Court\", for example, the High Court of Australia; this is because decisions by the High Court could formerly be appealed to the Privy Council."
                    ]
                ],
                [
                    "Precedent",
                    [
                        "In legal systems based on common law, a precedent, or authority, is a principle or rule established in a previous legal case that is either binding on or persuasive for a court or other tribunal when deciding subsequent cases with similar issues or facts .",
                        "Common law legal systems place great value on deciding cases according to consistent principled rules so that similar facts will yield similar and predictable outcomes, and observance of precedent is the mechanism by which that goal is attained.",
                        "The principle by which judges are bound to precedents is known as stare decisis.",
                        "Black's Law Dictionary defines \"precedent\" as a \"rule of law established for the first time by a court for a particular type of case and thereafter referred to in deciding similar cases\"."
                    ]
                ],
                [
                    "Last resort rule",
                    [
                        "In Constitutional law, the Last Resort Rule is a largely prudential rule which gives a federal court the power to avoid a constitutional issue in some circumstances.",
                        "This rule dictates that, even if all other jurisdictional and justiciability obstacles are surmounted, federal courts still must avoid a constitutional issue if there is any other ground upon which to render a final judgment.",
                        "The last resort rule can function as a distinct barrier to Constitutional avoidance.",
                        "It is articulated by Justice Brandeis in Ashwander v. Tennessee Valley Authority.",
                        "Brandeis cited two examples in Ashwander of the \"most varied application\" of the last resort rule."
                    ]
                ]
            ],
            "speaker": "0_Wizard",
            "suggestionsConfig": "show",
            "suggestionsNumber": 5,
            "suggestionsShown": true,
            "text": "The Supreme Court of the United States is the court of last resort, so it is the higher one.",
            "textboxFilled": 0,
            "time": 1580233257.94714,
            "timeTaken": 3.385,
            "totalTasks": 120,
            "turn": 9
        },
        {
            "checked_passage": {
                "no_passages_used": "no_passages_used"
            },
            "checked_sentence": {
                "no_passages_used": "no_passages_used"
            },
            "clicked_passages": {},
            "context": {
                "conversation": [
                    "Good Morning.  I'm thinking of getting a job in the court system.",
                    "ARe you planning on working on a specific level, there are three?",
                    "I hadn't thought of that.  What levels are there to choose from?",
                    "The Supreme Court of the United States is the court of last resort, so it is the higher one."
                ],
                "index": 1
            },
            "contextCount": 0,
            "full_passages": [
                "Supreme court\n\nA supreme court is the highest court within the hierarchy of courts in many legal jurisdictions. Other descriptions for such courts include court of last resort, apex court, and highest (or final) court of appeal. Broadly speaking, the decisions of a supreme court are not subject to further review by any other court. Supreme courts typically function primarily as appellate courts, hearing appeals from decisions of lower trial courts, or from intermediate-level appellate courts.\n\nHowever, not all highest courts are named as such. Civil law states tend not to have a single highest court. Additionally, the highest court in some jurisdictions is not named the \"Supreme Court\", for example, the High Court of Australia; this is because decisions by the High Court could formerly be appealed to the Privy Council. On the other hand, in some places the court named the \"Supreme Court\" is not in fact the highest court; examples include the New York Supreme Court, the Supreme Courts of several Canadian provinces/territories and the former Supreme Court of Judicature of England and Wales and Supreme Court of Judicature of Northern Ireland, which are all subordinate to higher courts of appeal.\n\nSome countries have multiple \"supreme courts\" whose respective jurisdictions have different geographical extents, or which are restricted to particular areas of law. Some countries with a federal system of government may have both a federal supreme court (such as the Supreme Court of the United States), and supreme courts for each member state (such as the Supreme Court of Nevada), with the former having jurisdiction over the latter only to the extent that the federal constitution extends federal law over state law. However, other federations, such as Canada, may have a supreme court of general jurisdiction, able to decide any question of law. Jurisdictions with a civil law system often have a hierarchy of administrative courts separate from the ordinary courts, headed by a supreme administrative court as is the case in the Netherlands. A number of jurisdictions also maintain a separate constitutional court (first developed in the Czechoslovak Constitution of 1920), such as Austria, France, Germany, Luxembourg, Portugal, Russia, Spain and South Africa. Within the former British Empire, the highest court within a colony was often called the \"Supreme Court\", even though appeals could be made from that court to the United Kingdom's Privy Council (based in London). A number of Commonwealth jurisdictions retain this system, but many others have reconstituted their own highest court as a court of last resort, with the right of appeal to the Privy Council being abolished.\n\nIn jurisdictions using a common law system, the doctrine of \"stare decisis\" applies, whereby the principles applied by the supreme court in its decisions are binding upon all lower courts; this is intended to apply a uniform interpretation and implementation of the law. In civil law jurisdictions the doctrine of \"stare decisis\" is not generally considered to apply, so the decisions of the supreme court are not necessarily binding beyond the immediate case before it; however, in practice the decisions of the supreme court usually provide a very strong precedent, or \"jurisprudence constante\", for both itself and all lower courts.\n\nThe Supreme Court of Bangladesh is created by the provisions of the Constitution of Bangladesh, 1972. There are two Divisions of the Supreme Court, i.e. (a) Appellate Division and (b) High Court Division. Appellate Division is the highest Court of Appeal and usually does not exercise the powers of a court of first instance. Whereas, the High Court Division is a Court of first instance in writ/judicial review, company and admiralty matters.\n\nThe Supreme Court of Canada was established in 1875 but only became the highest court in the country in 1949 when the right of appeal to the Judicial Committee of the Privy Council was abolished. This court hears appeals from the courts of appeal from the provinces and territories, and also appeals from the Federal Court of Appeal. The Supreme Court is a \"General Court of Appeal.\" It can decide any question of law considered by the lower courts, including constitutional law, federal law, and provincial law. The court's decisions are final and binding on the federal courts and the courts from all provinces and territories. The title \"Supreme\" can be confusing because, for example, the Supreme Court of British Columbia does not have the final say and controversial cases heard there often get appealed in higher courts - it is in fact one of the lower courts in such a process.\n\nIn Hong Kong, the Supreme Court of Hong Kong (now known as the High Court of Hong Kong) was the final court of appeal during its colonial times which ended with transfer of sovereignty in 1997. The final adjudication power, as in any other British Colonies, rested with the Judicial Committee of the Privy Council (JCPC) in London, United Kingdom. Now the power of final adjudication is vested in the Court of Final Appeal created in 1997. Under the Basic Law, its constitution, the territory remains a common law jurisdiction. Consequently, judges from other common law jurisdictions (including England and Wales) can be recruited and continue to serve in the judiciary according to Article 92 of the Basic Law. On the other hand, the power of interpretation of the Basic Law itself is vested in the Standing Committee of the National People's Congress (NPCSC) in Beijing (without retroactive effect), and the courts are authorised to interpret the Basic Law when trying cases, in accordance with Article 158 of the Basic Law. This arrangement became controversial in light of the right of abode issue in 1999, raising concerns for judicial independence.\n\nIn India, the Supreme Court of India was created on January 28, 1950 after adoption of the Constitution.\nArticle 141 of the Constitution of India states that the law declared by Supreme Court is to be binding on all Courts within the territory of India. It is the highest court in India and has ultimate judicial authority to interpret the Constitution and decide questions of national law (including local bylaws). The Supreme Court is also vested with the power of judicial review to ensure the application of the rule of law.\n\nNote that within the constitutional framework of India, Jammu and Kashmir (J&K) has a special status vis-a-vis the other states of India. Article 370 of the Constitution of India carves out certain exceptions for J&K. However, the Constitution (Application to Jammu and Kashmir) Order 1954 makes Article 141 applicable to the state of J&K and hence law declared by the Supreme Court of India is equally applicable to all courts of J&K including the High Court.\n\nThe Supreme Court is the highest court in the Republic of Ireland. It has authority to interpret the constitution, and strike down laws and activities of the state that it finds to be unconstitutional. It is also the highest authority in the interpretation of the law. Constitutionally it must have authority to interpret the constitution but its further appellate jurisdiction from lower courts is defined by law. The Irish Supreme Court consists of its presiding member, the Chief Justice, and seven other judges. Judges of the Supreme Court are appointed by the President in accordance with the binding advice of the Government. The Supreme Court sits in the Four Courts in Dublin.\n\nIsrael's Supreme Court is at the head of the court system in the State of Israel. It is the highest judicial instance. The Supreme Court sits in Jerusalem. The area of its jurisdiction is the entire State. A ruling of the Supreme Court is binding upon every court, other than the Supreme Court itself. The Israeli supreme court is both an appellate court and the high court of justice. As an appellate court, the Supreme Court considers cases on appeal (both criminal and civil) on judgments and other decisions of the District Courts. It also considers appeals on judicial and quasi-judicial decisions of various kinds, such as matters relating to the legality of Knesset elections and disciplinary rulings of the Bar Association. As the High Court of Justice (Hebrew: Beit Mishpat Gavoha Le'Zedek בית משפט גבוה לצדק; also known by its initials as Bagatz בג\"ץ), the Supreme Court rules as a court of first instance, primarily in matters regarding the legality of decisions of State authorities: Government decisions, those of local authorities and other bodies and persons performing public functions under the law, and direct challenges to the constitutionality of laws enacted by the Knesset. The court has broad discretionary authority to rule on matters in which it considers it necessary to grant relief in the interests of justice, and which are not within the jurisdiction of another court or tribunal. The High Court of Justice grants relief through orders such as injunction, mandamus and Habeas Corpus, as well as through declaratory judgments. The Supreme Court can also sit at a further hearing on its own judgment. In a matter on which the Supreme Court has ruled - whether as a court of appeals or as the High Court of Justice - with a panel of three or more justices, it may rule at a further hearing with a panel of a larger number of justices. A further hearing may be held if the Supreme Court makes a ruling inconsistent with a previous ruling or if the Court deems that the importance, difficulty or novelty of a ruling of the Court justifies such hearing. The Supreme Court also holds the unique power of being able to order \"trial de novo\" (a retrial).\n\nIn Nauru, there is no single highest court for all types of cases. The Supreme Court has final jurisdiction on constitutional matters, but any other case may be appealed further to the Appellate Court. In addition, an agreement between Nauru and Australia in 1976 provides for appeals from the Supreme Court of Nauru to the High Court of Australia in both criminal and civil cases, with the notable exception of constitutional cases.\n\nIn New Zealand, the right of appeal to the Privy Council was abolished following the passing of the Supreme Court Act (2003). A right of appeal to the Privy Council remains for criminal cases which were decided before the Supreme Court was created, but it is likely that the successful appeal by Mark Lundy to the Privy Council in 2013 will be the last appeal to the Board from New Zealand.\n\nThe new Supreme Court of New Zealand was officially established at the beginning of 2004, although it did not come into operation until July. The High Court of New Zealand was until 1980 known as the Supreme Court. The Supreme Court has a purely appellate jurisdiction and hears appeals from the Court of Appeal of New Zealand. In some cases, an appeal may be removed directly to the Supreme Court from the High Court. For certain cases, particularly cases which commenced in the District Court, a lower court (typically the High Court or the Court of Appeal) may be the court of final jurisdiction.\n\nThe Supreme Court has been the apex court for Pakistan since the declaration of the republic in 1956 (previously the Privy Council had that function). The Supreme Court has the final say on matters of constitutional law, federal law or on matters of mixed federal and provincial competence. It can hear appeals on matters of provincial competence only if a matter of a constitutional nature is raised.\n\nWith respect to Pakistan's territories (i.e. FATA, Azad Kashmir, Northern Areas and Islamabad Capital Territory (ICT)) the Supreme Court's jurisdiction is rather limited and varies from territory to territory; it can hear appeals only of a constitutional nature from FATA and Northern Areas, while ICT generally functions the same as provinces. Azad Kashmir has its own courts system and the constitution of Pakistan does not apply to it as such; appeals from Azad Kashmir relate to its relationship with Pakistan.\n\nThe provinces have their own courts system, with the High Court as the apex court, except insofar as where an appeal can go to the Supreme Court as mentioned above.\n\nThe Supreme Court of the United Kingdom is the ultimate court for criminal and civil matters in England, Wales and Northern Ireland and for civil matters in Scotland. (The supreme court for criminal matters in Scotland is the High Court of Justiciary.) The Supreme Court was established by the Constitutional Reform Act 2005 with effect from 1 October 2009, replacing and assuming the judicial functions of the House of Lords. Devolution issues under the Scotland Act 1998, Government of Wales Act and Northern Ireland Act were also transferred to the new Supreme Court by the Constitutional Reform Act, from the Judicial Committee of the Privy Council.\n\nIn respect of Community Law the Supreme Court is subject to the decisions of the European Court of Justice. Since there can be no appeal from the Supreme Court, there is an interlocutory procedure by which the Supreme Court may refer to the European Court questions of European law which arise in cases before it, and obtain a definitive ruling before the Supreme Court gives its judgment.\n\nThe Supreme Court shares its members and accommodation at the Middlesex Guildhall in London with the Judicial Committee of the Privy Council which hears final appeals from certain smaller Commonwealth countries, admiralty cases, and certain appeals from the ecclesiastical courts and statutory private jurisdictions, such as professional and academic bodies.\n\n(The Constitutional Reform Act also renamed the \"Supreme Court of Judicature of Northern Ireland\" to the Court of Judicature, and the rarely cited \"Supreme Court of Judicature for England and Wales\" as the Senior Courts of England and Wales).\n\nThe Supreme Court was set up in 2009; until then the House of Lords was the ultimate court in addition to being a legislative body, and the Lord Chancellor, with legislative and executive functions, was also a senior judge in the House of Lords.\n\nThe Supreme Court of the United States, established in 1789, is the highest Federal court in the United States, with powers of judicial review first asserted in \"Calder v. Bull\" (1798) in Justice Iredell's dissenting opinion. The power was later given binding authority by Justice Marshall in \"Marbury v. Madison\" (1803). There are currently nine seats on the US Supreme Court.\n\nEach U.S. state has a state supreme court, which is the highest authority interpreting that state's law and administering that state's judiciary. Two states, Oklahoma and Texas, each have two separate highest courts that respectively specialize in criminal cases and civil cases. Although Delaware has a specialized court, the Court of Chancery, to hear cases in equity, it is not a supreme court because the Delaware Supreme Court has appellate jurisdiction over it.\n\nThe titles of state supreme court vary, which can cause confusion between jurisdictions because one state may use a name for its highest court that another uses for a lower court. In New York, Maryland, and the District of Columbia the highest court is called the Court of Appeals, a name used by many states for their intermediate appellate courts. Further, trial courts of general jurisdiction in New York are called the Supreme Court, and the intermediate appellate court is called the Supreme Court, Appellate Division. In West Virginia, the highest court of the state is the Supreme Court of Appeals. In Maine and Massachusetts the highest court is styled the \"Supreme Judicial Court\"; the last is the oldest appellate court of continuous operation in the Western Hemisphere.\n\nThe Roman law and the Corpus Juris Civilis are generally held to be the historical model for civil law. From the late 18th century onwards, civil law jurisdictions began to codify their laws, most of all in civil codes.\n\nIn Austria, the Austrian Constitution of 1920 (based on a draft by Hans Kelsen) introduced judicial review of legislative acts for their constitutionality. This function is performed by the Constitutional Court (\"Verfassungsgerichtshof\"), which is also charged with the review of administrative acts on whether they violate constitutionally guaranteed rights.\nOther than that, administrative acts are reviewed by the Administrative Court (\"Verwaltungsgerichtshof\"). The Supreme Court (\"Oberste Gerichtshof (OGH)\"), stands at the top of Austria's system of \"ordinary courts\" (\"ordentliche Gerichte\") as the final instance in issues of private law and criminal law.\n\nIn Brazil, the Supreme Federal Tribunal (\"Supremo Tribunal Federal\") is the highest court. It is both the constitutional court and the court of last resort in Brazilian law. It only reviews cases that may be unconstitutional or final \"habeas corpus\" pleads for criminal cases. It also judges, in original jurisdiction, cases involving members of congress, senators, ministers of state, members of the high courts and the President and Vice-President of the Republic. The Superior Court of Justice (\"Tribunal Superior de Justiça\") reviews State and Federal Circuit courts decisions for civil law and criminal law cases, when dealing with federal law or conflicting rulings. The Superior Labour Tribunal (\"Tribunal Superior do Trabalho\") reviews cases involving labour law. The Superior Electoral Tribunal (\"Tribunal Superior Eleitoral\") is the court of last resort of electoral law, and also oversees general elections. The Superior Military Tribunal (\"Tribunal Superior Militar\") is the highest court in matters of federal military law.\n\nIn Croatia, the supreme jurisdiction is given to the Supreme Court, which secures a uniform application of laws. The Constitutional Court exists to verify constitutionality of laws and regulations, as well as decide on individual complaints on decisions on governmental bodies. It also decides on jurisdictional disputes between the legislative, executive and judicial branches.\n\nIn Denmark, all ordinary courts have original jurisdiction to hear all types of cases, including cases of a constitutional or administrative nature. As a result, there exists no special constitutional court, and therefore final jurisdiction is vested with the Danish Supreme Court (\"Højesteret\") which was established 14 February 1661 by king Frederik III.\n\nIn France, supreme appellate jurisdiction is divided among three judicial bodies:\n\nWhen there is jurisdictional dispute between judicial and administrative courts: the Court of Arbitration (\"Tribunal des conflits\"), which is empanelled half from the Court of Cassation and half from the Council of State and presided over by the Minister of Justice, is called together to settle the dispute or hand down a final decision.\n\nThe High Court (\"Haute Cour\") exists only to impeach the President of the French Republic in case of \"breach of his duties patently incompatible with his continuing in office\". Since a constitutional amendment of 2007, the French Constitution states that the High Court is composed of all members of both Houses of Parliament. As of 2012, it has never been convened.\n\nIn Germany, there is no \"de jure\" single supreme court. Instead, cases are handled by numerous federal courts, depending on their nature.\n\nFinal interpretation of the German Constitution, the \"Grundgesetz\", is the task of the \"Bundesverfassungsgericht\" (Federal Constitutional Court), which is the \"de facto\" highest German court, as it can declare both federal and state legislation ineffective, and has the power to overrule decisions of all other federal courts, despite not being a regular court of appeals on itself in the German court system. It is also the only court possessing the power and authority to outlaw political parties, if it is deemed that these parties have repeatedly violated articles of the Constitution.\n\nWhen it comes to civil and criminal cases, the \"Bundesgerichtshof\" (Federal Court of Justice) is at the top of the hierarchy of courts. The other branches of the German judicial system each have their own appellate systems, each topped by a high court; these are the \"Bundessozialgericht\" (Federal Social Court) for matters of social security, the \"Bundesarbeitsgericht\" (Federal Labour Court) for employment and labour, the \"Bundesfinanzhof\" (Federal Fiscal Court) for taxation and financial issues, and the \"Bundesverwaltungsgericht\" (Federal Administrative Court) for administrative law. The so-called \"Gemeinsamer Senat der Obersten Gerichtshöfe\" (Joint Senate of the Supreme Courts) is not a supreme court in itself, but an ad-hoc body that is convened in only when one supreme court intends to diverge from another supreme court's legal opinion or when a certain case exceeds the authority of one court. As the courts have well-defined areas of responsibility, situations like these are rather rare and so, the Joint Senate gathers very infrequently, and only to consider matters which are mostly definitory.\n\nIn the Netherlands, the Supreme Court of the Netherlands is the highest court. Its decisions, known as \"arresten\", are absolutely final. The court is banned from testing legislation against the constitution, pursuant to the principle of the sovereignty of the States-General; the court can, however, test legislation against some treaties. Also, the ordinary courts in the Netherlands, including the Hoge Raad, do not deal with administrative law, which is dealt with in separate administrative courts, the highest of which is the Council of State (Raad van State)\n\nThe Supreme Court of Iceland (, lit. \"Highest Court of Iceland\") was founded under Act No. 22/1919 and held its first session on 16 February 1920. The Court holds the highest judicial power in Iceland, where the court system has two levels.\n\nThe Supreme Court of India, also known colloquially as the 'apex court', is the highest judicial body in the Republic of India. Any decision taken by it is final and binding, and can only be modified in some cases (death sentence, etc.) by the President of India. It has several jurisdiction like \n1. Original\n2.Appellate \n3. Advisory\n\nIt is also known as court of records, i. e. all judgements are recorded and printed. These are cited in lower courts as case - law in various cases.\n\nItaly follows the French system of different supreme courts.\n\nThe Italian court of last resort for most disputes is the \"Corte Suprema di Cassazione\". There is also a separate constitutional court, the \"Corte costituzionale\", which has a duty of judicial review, and which can strike down legislation as being in conflict with the Constitution.\n\nIn Japan, the Supreme Court of Japan is called (Saikō-Saibansho; called 最高裁 Saikō-Sai for short), located in Chiyoda, Tokyo, and is the highest court in Japan. It has ultimate judicial authority within Japan to interpret the Constitution and decide questions of national law (including local by laws). It has the power of judicial review (i.e., it can declare Acts of Diet and Local Assembly, and administrative actions, unconstitutional).\n\nIn Luxembourg, challenges on the conformity of the law to the Constitution are brought before the \"Cour Constitutionnelle\" (Constitutional Court). — The most used and common procedure to present these challenges is by way of the \"\"question préjudicielle\"\" (prejudicial question). The Court of last resort for civil and criminal proceedings is the \"\"Cour de Cassation\"\".\nFor administrative proceedings the highest court is the \"\"Cour Administrative\"\" (Administrative Court).\n\nThe supreme court of Macau is the Court of Final Appeal (; ).\n\nWhile the Philippines is generally considered a civil law nation, its Supreme Court is heavily modelled after the American Supreme Court. This can be attributed to the fact that the Philippines was colonized by both Spain and the United States, and the system of laws of both nations strongly influenced the development of Philippine laws and jurisprudence. Even as the body of Philippine laws remain mostly codified, the Philippine Civil Code expressly recognizes that decisions of the Supreme Court \"form part of the law of the land\", belonging to the same class as statutes. The 1987 Philippine Constitution also explicitly grants to the Supreme Court the power of judicial review over laws and executive actions. The Supreme Court is composed of 1 Chief Justice and 14 Associate Justices. The court sits either en banc or in divisions, depending on the nature of the case to be decided.\n\nIn the judicial system of mainland China the highest court of appeal is the Supreme People's Court. This supervises the administration of justice by all subordinate \"local\" and \"special\" people's courts, and is the court of last resort for the whole People's Republic of China except for Macau and Hong Kong\n\nIn Portugal, there are several supreme courts, each with a specific jurisdiction:\n\nUntil 2003, a fifth supreme court also existed for the military jurisdiction, this being the Supreme Military Court (\"Supremo Tribunal Militar\"). Presently, in time of peace, the supreme court for military justice matters is the Supreme Court of Justice, which now includes four military judges.\n\nIn the Republic of China (Taiwan), there are three different courts of last resort:\n\nThe Council of Grand Justices, consisting of 15 justices and mainly dealing with constitutional issues, is the counterpart of constitutional courts in some countries.\n\nAll three courts are directly under the Judicial Yuan, whose president also serves as Chief Justice in the Council of Grand Justices.\n\nFounded by papal bull in 1532, the Court of Session is the supreme civil court of Scotland, and the High Court of Justiciary is the supreme criminal court. However, the absolute highest court (excluding criminal matters) is the Supreme Court of the United Kingdom.\n\nSpanish Supreme Court is the highest court for all cases in Spain (both private and public). Only those cases related to human rights can be appealed at the Constitutional Court (which also decides about acts accordance with Spanish Constitution). \nIn Spain, high courts cannot create binding precedents; however, lower rank courts usually observe Supreme Court interpretations. In most private law cases, two Supreme Court judgements supporting a claim are needed to appeal at the Supreme Court.\nFive sections form the Spanish Supreme court:\n\nIn Sweden, the Supreme Court and the Supreme Administrative Court respectively function as the highest courts of the land. The Supreme Administrative Court considers cases concerning disputes between individuals and administrative organs, as well as disputes among administrative organs, while the Supreme Court considers all other cases. The judges are appointed by the Government. In most cases, the Supreme Courts will only grant leave to appeal a case (\"prövningstillstånd\") if the case involves setting a precedent in the interpretation of the law. Exceptions are issues where the Supreme Court is the court of first instance. Such cases include an application for a retrial of a criminal case in the light of new evidence, and prosecutions made against an incumbent minister of the Government for severe neglect of duty. If a lower court has to try a case which involves a question where there is no settled interpretation of the law, it can also refer the question to the relevant Supreme Court for an answer.\n\nIn Switzerland, the Federal Supreme Court of Switzerland is the final court of appeals. Due to Switzerland's system of direct democracy, it has no authority to review the constitutionality of federal statutes, but the people can strike down a proposed law by referendum. According to settled case law, however, the Court is authorised to review the compliance of all Swiss law with certain categories of international law, especially the European Convention of Human Rights.\n\nIn Sri Lanka, the Supreme Court of Sri Lanka was created in 1972 after the adoption of a new Constitution. The Supreme Court is the highest and final superior court of record and is empowered to exercise its powers, subject to the provisions of the Constitution. The court rulings take precedence over all lower Courts. The Sri Lanka judicial system is complex blend of both common-law and civil-law. In some cases such as capital punishment, the decision may be passed on to the President of the Republic for clemency petitions. However, when there is 2/3 majority in the parliament in favour of president (as with present), the supreme court and its judges' powers become nullified as they could be fired from their positions according to the Constitution, if the president wants. Therefore, in such situations, Civil law empowerment vanishes.\n\nIn South Africa, a \"two apex\" system existed from 1994 to 2013. The Supreme Court of Appeal (SCA) was created in 1994 and replaced the Appellate Division of the Supreme Court of South Africa as the highest court of appeal in non-constitutional matters. The SCA is subordinate to the Constitutional Court, which is the highest court in matters involving the interpretation and application of the Constitution. But in August 2013 the Constitution was amended to make the Constitutional Court the country's single apex court, superior to the SCA in all matters, both constitutional and non-constitutional.\n\nHistorically, citizens appealed directly to the King along his route to places out of the Palace. A Thai King would adjudicate all disputes. During the reign of King Chulalongkorn, an official department for appeals was set up, and, after Thailand adopted a western-styled government, Thai Supreme Court was established in 1891.\n\nAt present, the Supreme Court of Thailand retains the important status as the highest court of justice in the country. Operating separately from the Administrative Court and the Constitutional Court, the judgement of the Supreme Court is considered as final.\n\nIn the United Arab Emirates, the Federal Supreme Court of the United Arab Emirates was created in 1973 after the adoption of the Constitution. The Supreme Court is the highest and final superior court of record and is empowered to exercise its powers, subject to the provisions of the Constitution. The court rulings take precedence over all lower Courts. The Emirati judicial system is complex blend of both Islamic law and civil law. In some cases such as capital punishment, the decision may be passed on to the President of the country (currently Khalifa bin Zayed Al Nahyan).\n\n\nLaw of Indonesia at the national level is based on a combination of civil law from the tradition of Roman-Dutch law and customary law from the tradition of Adat. Law in regional jurisdictions can vary from province to province, including even Sharia law, for example Islamic criminal law in Aceh, though even at the national level, individual justices can cite sharia or other forms of non-Dutch law in their legal opinions.\n\nThe Supreme Court of Indonesia is the main judicial arm of the state, functioning as the final court of appeal as well as a means to re-open cases previously closed. The Supreme Court, which consists of a total of 51 justices, also oversees the regional high courts. It was founded at the country's independence in 1945.\n\nThe Constitutional Court of Indonesia, on the other hand, is a part of the judicial branch tasked with review of bills and government actions for constitutionality, as well as regulation of the interactions between various arms of the state. The constitutional amendment to establish the court was passed in 2001, and the court itself was established in 2003. The Constitutional Court consists of nine justices serving nine year terms, and they're appointed in tandem by the Supreme Court, the President of Indonesia and the People's Representative Council.\n\nIn most nations with constitutions modelled after the Soviet Union, the legislature was given the power of being the \"court of last resort\". In the People's Republic of China, the final power to interpret the law is vested in the Standing Committee of the National People's Congress (NPCSC). This power includes the power to interpret the basic laws of Hong Kong and Macau, the constitutional documents of the two special administrative regions which are common law and Portuguese-based legal system jurisdictions respectively. This power is a legislative power and not a judicial one in that an interpretation by the NPCSC does not affect cases which have already been decided.\n\n",
                "Caribbean Court of Justice\n\nThe Caribbean Court of Justice (CCJ; ; ) is the judicial institution of the Caribbean Community (CARICOM). Established in 2001, it is based in Port of Spain, Trinidad and Tobago. The CCJ sits at 134 Henry Street in Port of Spain.\n\nThe Caribbean Court of Justice has two jurisdictions: an original jurisdiction and an appellate jurisdiction:\n\nIn the aftermath of the collapse of the Federation of the West Indies (and with it the Federal Supreme Court), which had lasted a mere four years, from 1958 to 1962, the Anglophone continental and insular Caribbean states formed the CARIFTA (the Caribbean Free Trade Association), with a view to maintaining an economic link among the various former and continuing colonies of the United Kingdom after the collapse of the political bond. On 1 August 1973, the successor to the CARIFTA, the Caribbean Community, better known by its acronym, CARICOM, came into being. The founding document of the CARICOM, the Treaty of Chaguaramas, was signed by the so-called \"Big Four\" states: Barbados, Jamaica, Guyana and Trinidad & Tobago, all of which had gained their political independence from the UK during the 1960s. This signing was the starter’s signal for a more mature, though at times slow and halting, process of regional integration among the states of the Commonwealth Caribbean.\n\nIn 2001, the Conference of Heads of Government of the Caribbean Community, at their 22nd meeting in Nassau, the Bahamas, signed the Revised Treaty of Chaguaramas (RTC), rebranding the Caribbean Community and Common Market to include the proposed CARICOM Single Market and Economy (CSME). The single market replacing the original Common Market aspect of the group.\n\nOriginally an Anglophone club, the admission of Dutch-speaking Suriname in 1995, and Créole-speaking Haiti (where French is the official language) in 2002 has somewhat modified the cultural and jurisprudential mix of the community.\n\nUnder the revised Treaty of Chaguaramas, and typical of similar international integrationist movements, the CARICOM has restructured itself to include such elements as are characteristic of the modern democratic state, viz., executive (CARICOM Heads of Government and the Community Council), legislative (Assembly of Caribbean Community Parliamentarians - established before the revised treaty and now moribund) and judicial (CCJ) arms.\n\nThe Caribbean Court of Justice (CCJ) is the Caribbean regional judicial tribunal established on 14 February 2001, by the Agreement Establishing the Caribbean Court of Justice. The agreement was signed on that date by the CARICOM states of: Antigua & Barbuda; Barbados; Belize; Grenada; Guyana; Jamaica; Saint Kitts & Nevis; Saint Lucia; Suriname; and Trinidad & Tobago. Two further states, Dominica and Saint Vincent & the Grenadines, signed the agreement on 15 February 2003, bringing the total number of signatories to 12. The Bahamas and Haiti, though full members of the CARICOM, are not yet signatories, and because of Montserrat's status as a British territory, they must await Instruments of Entrustment from the UK in order to ratify. The Agreement Establishing the Caribbean Court of Justice came into force on 23 July 2003, and the CCJ was inaugurated on 16 April 2005 in Port of Spain, Trinidad & Tobago, the seat of the Court.\n\nThe birth of the CCJ came after a long and arduous period of planning. In March 1970, the Organisation of Commonwealth Caribbean Bar Associations (OCCBA) first raised the issue of the need to replace the Judicial Committee of the Privy Council as the court of last resort for the Commonwealth Caribbean by a regional court of appeal. Again in Jamaica, in April 1970, at the Sixth Commonwealth Caribbean Heads of Government, the Jamaican delegation tabled a proposal on setting up a regional Court of Appeal and the heads further agreed to take action on relinquishing the Privy Council as the Anglophone Caribbean’s final appeal court and mandated a committee of CARICOM attorneys-general to further explore the question of the establishment of what was then being called a \"Caribbean Court of Appeal\".\n\nFurther to the perceived need for an indigenous, regional court as a tribunal of last resort in civil and criminal cases, other factors eventually led to the strong support for the creation of a judicial arm of the CARICOM. In 1972 consideration was being given by the OCCBA for the proposed Caribbean Court of Appeal to serve as both a municipal court of last resort and an international tribunal to adjudicate disputes between CARICOM member states. In 1989 the West Indian Commission established by the CARICOM heads of government endorsed this proposed hybrid jurisdiction without qualification. As Duke Pollard, then director of the Caricom Legislative Drafting Facility, wrote in 2000: \"the old Treaty of Chaguaramas provided for arbitration in the event of disputes concerning the interpretation and application of the Treaty. Unfortunately, however, the arbitral procedure was never used and serious disputes were never settled, thereby causing the integration movement to be hampered. Moreover, the rights and obligations created by the CSME are so important and extensive, relating to the establishment of economic enterprises, the provision of professional services, the movement of capital, the acquisition of land for the operation of businesses, that there is a clear need to have a permanent, central, regional institution to authoritatively and definitively pronounce on those rights and corresponding obligations. The Caribbean Court of Justice is intended to be such an authoritative institution.\"\n\nThe official inauguration was held in Queen's Hall, Port of Spain, Trinidad and Tobago, on Saturday 16 April 2005. The first case heard by the CCJ was in August 2005 and was to settle a \"decade-long\" libel court case from Barbados. Barbados and Guyana acceded to the CCJ's appellate jurisdiction in 2005, with Belize joining them in June 2010, and Dominica in March 2015.\n\nThe reasons given for the establishment of a supreme appellate court are many and varied, including a perceived regional disenfranchisement from the British Judicial Committee of the Privy Council.\n\nControversy surrounding the establishment of this court corresponds to two major events that made the Privy Council unpopular in the Caribbean region.\n\nThe British-based court has been perceived as having too much power in the Caribbean region. Several politicians also lamented that the Caribbean nations are the only remaining region of the former British Empire still to rely on the British court system for appeals.\n\nParadoxally, even as some within the Caribbean oppose switching from the Privy Council to the CCJ over fears of lessened impartiality by CCJ judges not as far removed from the region as the Privy Council judges, senior British legal figures (often members of the JCPC itself) have expressed support for a regional court for the Caribbean. As far back as 1828, the man responsible for remodelling the Judicial Committee of the Privy Council, Lord Brougham, had raised the issue of removing colonies from the Privy Council's jurisdiction. He opined that due to the distance of the colonies from the UK and the immense variety of matters arising from them which would be foreign to British habits, that any court in the UK would be extremely inadequate for the colonies.\n\nLord Brougham's sentiments were echoed nearly 200 years later in 2003 by Lord Hoffman, a Law Lord from 1995-2009, when he noted that although the Privy Council had done its best to serve the Caribbean and had effected improvements in the administration of justice, the remoteness of the court from the community served as a handicap. In his own view a local final court would be necessary and beneficial to transform society in partnership with the other two branches of government.\n\nIn 1990, Lord Wilberforce (Senior Law Lord from 1975–1982) and later in 1992 leading barrister Lord Gifford QC, both called on the Commonwealth Caribbean to establish its own regional and final court of appeal. In 1999, then Senior Law Lord, Lord Browne-Wilkinson described as burdensome the number of appeals in capital matters coming from the Caribbean to the Privy Council. He noted that such appeals occupied 25% of the Privy Council's time and he felt it was time for the Privy Council to be relieved of the Caribbean cases in order for the region to accede to full legal independence. Lord Browne-Wilkinson also advocated for the establishment of a regional Caribbean court of last resort.\n\nIn September 2009, Lord Phillips of Worth Matravers expressed sentiments close to those of Lord Browne-Wilkinson a decade earlier. Lord Phillips, the last Senior Law Lord and first President of the Supreme Court of the United Kingdom, said he would search for ways to curb the “disproportionate” time that he and his fellow senior justices spent on hearing legal appeals from independent Commonwealth countries to the Privy Council. He expressed concern that the new Supreme Court's judges would end up spending as much as 40% of their working time on Privy Council business and was intending to take some pressure off of the Supreme Court judges by drafting in lower tier judges from the Court of Appeal to sit on cases from Commonwealth countries. He also added that in an ideal world former Commonwealth countries would stop utilizing the Privy Council and instead set up their own final courts of appeal.\n\nIn October 2009, Lord Gifford at a reception in Kingston, Jamaica again expressed support for the replacement of the Privy Council by the CCJ. Lord Gifford noted that the CCJ would be more accessible, affordable and provide a better quality of justice for Jamaicans and other former British colonies in the Caribbean than the Privy Council. Lord Gifford expressed support for Lord Phillips' earlier comments, and hoped that they would serve to stir Jamaica and other Caribbean states to leave the Privy Council and join the CCJ. Lord Gifford also said that his arguments in support of the CCJ were strictly practical and not based on the Privy Council's composition or being a \"colonial relic\".\n\nThus it would seems that for at least some of members of the JCPC, geographical and psychological distance (often raised as necessary for greater objectivity and impartiality) does not seem to be an issue and what is more important is the need for the Caribbean (and other Commonwealth countries) to take care of its own affairs. In fact, the Privy Council has often been willing to accept findings by Caribbean courts on local matters because they recognize that such courts are more familiar with Caribbean matters.\n\nThe CCJ is intended to be a hybrid institution: a municipal court of last resort and an international court vested with original, compulsory and exclusive jurisdiction in respect of the interpretation and application of the Revised Treaty of Chaguaramas. In the exercise of this original jurisdiction, the CCJ discharges the functions of an international tribunal, applying rules of international law in respect of the interpretation and application of the treaty. The CCJ thus performs similarly to the European Court of Justice, EFTA Court, East African Court of Justice, the ECOWAS Community Court of Justice, the Andean Court of Justice and the International Court of Justice. In contrast to many general international tribunals or courts, the original jurisdiction of the CCJ is compulsory, requiring no pre-existing agreement.\n\nAs a municipal court of last resort, it exercises an appellate jurisdiction, as a final court of appeal for CARICOM member states, replacing the Judicial Committee of the Privy Council (JCPC) for Anglophone member states. In the exercise of its appellate jurisdiction, the CCJ hears appeals from common law courts within the jurisdictions of parties to the Agreement Establishing the CCJ, and is the highest municipal court in the region.\n\nWhile the CCJ has jurisdiction in all member states of the Agreement Establishing The Caribbean Court of Justice, the Agreement itself provides for the CCJ's jurisdiction to also be available to any other state within the Caribbean that CARICOM should choose invite to become a party to the Agreement. Thus the appellate jurisdiction of the Court, in particular, could be available to a non-CARICOM Caribbean state or to CARICOM's associate member states.\n\nUnlike some international courts (but similar to others such as the ECJ and EFTA Court), cases between member states, between CARICOM nationals, or between nationals and the state are all justiciable under the CCJ.\n\nAppellate decisions of the court are delivered with signed majority opinions, concurrences and dissenting opinions, as well as a record of which judges voted for the ruling and which voted against it. As a result, CCJ appellate opinions do not shield judges behind a singular and collective\n“voice of the court” as the ECJ and the CCJ's original opinions do, and the practice is in keeping with the normal procedures of municipal courts. This may actually aid in providing transparency to the regional court operating in an environment where many of its citizens are distrustful of their local judiciaries.\n\nBy contrast, judgments or advisory opinions under the original jurisdiction of the Court are published in a single judgment of the court once the majority of judges have reached a conclusion after final deliberation. No other judgments or opinions are permitted to be given or delivered. This is in keeping with the practice of the ECJ and EFTA Court as international courts.\n\nAlthough there is no ratio or quota for judges based on sex or nationality, most CCJ judges previously sat at a national level or previously taught law for 15 years or more. At least one member of the panel is required to be an expert in international law and one judge is also required to be from the civil law tradition, reflecting the presence of civil law jurisdictions such as Suriname and Haiti.\n\nThe Regional Judicial and Legal Services Commission (the RJLSC or the Commission) was established pursuant to Article V(1) of the Agreement Establishing the Caribbean Court of Justice. The Commission is composed of the following persons: the President of the Court who is also the Chairman of the Commission; two persons nominated jointly by the Organisation of the Commonwealth Caribbean Bar Association (OCCBA) and the Organisation of Eastern Caribbean States (OECS) Bar Association; one chairman of the Judicial Services Commission of a contracting state selected in rotation in the English alphabetical order for a period of three years; the Chairman of a Public Service Commission of a contracting state selected in rotation in the reverse English alphabetical order for a period of three years; two persons from civil society nominated jointly by the Secretary-General of the Caribbean Community and the Director General of the OECS for a period of three years following consultations with regional non-governmental organisations; two distinguished jurists nominated jointly by the Dean of the Faculty of Law of the University of the West Indies, the Deans of the Faculties of Law of any of the contracting states and the Chairman of the Council of Legal Education; and two persons nominated jointly by the Bar or Law Associations of the Contracting Parties.\n\nThe Commission itself has a number of responsibilities that help to ensure the independence and proper functioning of the CCJ. It is responsible for recommending the candidate to be the next President of the Court; for considering and appointing potential judges for the Court; for appointing the registrar, deputy registrars and other officials and employees as necessary and determining their terms of service and terminating their appointments; for exercising disciplinary control over the judges and starting the process for removing judges based on inability to perform or misbehaviour.\n\nUniquely among integrative courts, the CCJ is funded through an independent Caribbean Court of Justice Trust Fund. The Trust Fund was developed to ensure the financial independence of the Court from political interference. It was established with US $100 million from initial contributions of the member states by way of loans from the Caribbean Development Bank. The income from the Fund is expected to finance the expenditures of the Court (remuneration of judges and other employees, operation of the court) in perpetuity. This keeps the CCJ from depending on the largesse of governments and keeps it free from their administrative control. The CCJ Trust Fund is administered by a Board of Trustees drawn from various regional bodies including the following persons or their nominees: the secretary-general of the Caribbean Community; the vice-chancellor of the University of the West Indies; the president of the Insurance Association of the Caribbean; the chairman of the Association of Indigenous Banks of the Caribbean; the president of the Caribbean Institute of Chartered Accountants; the president of the Organisation of Commonwealth Caribbean Bar Associations; the chairman of the Conference of Heads of the Judiciary of Member States of the Caribbean Community; the president of the Caribbean Association of Industry and Commerce; and the president of the Caribbean Congress of Labour.\n\nThe framework of the CCJ provides many layers of protection from political pressure and influence in both the appellate and original jurisdictions:\n\n\nInitially being created as a replacement for the Privy Council or JCPC, and later being tasked with the original jurisdiction over the interpretation of the Revised Treaty of Chaguaramas, the CCJ replicates certain aspects of the British justice system while being divergent in other ways.\n\nBoth the CCJ and JCPC have a president of the Court and have a smaller panel of judges being called up for any one particular case from a larger pool of eligible judges. For the JCPC, five judges normally sit on appeals from Commonwealth countries, while three to five judges normally sit on the CCJ for cases, although at times all the eligible judges have sat for a case.\n\nA major difference though is in the pool of eligible justices from which the deciding panel is called. For the JCPC has no explicit limit on the number of eligible jurists, while for the CCJ the initial limit is nine judges other than the president (though this limit may be increased by the agreement of all the member states). The actual number of judges eligible for JCPC at any given time is actually difficult to determine with one 2009 estimate having ninety-five jurists, of which only three were Caribbean judges. The primary decision makers in the JCPC's pool are the privy councillors who also serve as judges on the Supreme Court of the United Kingdom and tend to be the only ones listed on the JCPC's website. No Caribbean judges, however, have sat on the JCPC since 2009 when the JCPC was co-located with the new UK Supreme Court.\n\nAs a result of the JCPC's employment of a large pool of jurists but usage of only a fraction of them, the JCPC has been criticized for the fact that decisions for any one case often depends on the judges called. The possible combinations of judges available means that different decisions can be rendered for very similar fact patterns in cases. This issue becomes particularly relevant when an appeal comes from a Caribbean country as it may be difficult for judgments to be handed down based on the nuances of Caribbean society when it is unlikely that a majority of judges on the panel for a case would come from the Caribbean. As one Caribbean lawyer lobbyist (initially an opponent of the CCJ) noted: \"What is the “reasonable man” test in the Caribbean? Acts of provocation in England and the Caribbean may not be the same... In the Caribbean, even express words may have different connotations. These are the types of questions that need to be discussed by an indigenous tribunal...\"\n\nAs comparable appellate courts, the CCJ and JCPC as outlined above have similar procedures, but there are major differences in both the time and money that would need to be spent by individual litigants and states in pursuing cases to either court.\n\nIndividual litigants are expected to almost always face reductions in the costs associated with pursuing their cases whenever a switch to the CCJ is made. For instance the cost of filing an appeal with the JCPC is more than five times greater than filing an appeal with the CCJ. For the JCPC, the filing of an application for permission to appeal along with the actual notice of appeal would have cost £220, or roughly US$350 in 2013, while the CCJ requires no payment for filing an application for permission to appeal and the cost for filing a notice of appeal was US$60. In 2015, the new filing fees for the JCPC were noted as ranging from between £400 to a £5,000 or roughly US$600 to US$7,500 while the comparable fees for the CCJ remained at US$60 thus making the cost of filing an appeal with the JCPC at least ten times greater than filing an appeal with the CCJ.\n\nAlthough both courts do allow appeals \"in forma pauperis\", waiving filing fees when they are deemed too burdensome on the individual litigant, the JCPC does so on a more limited basis.\n\nSome of the highest costs for litigants however arise when litigation of the case actually begins. In most cases, litigants will have to travel to the United Kingdom (UK) to pursue their cases before the JCPC. This may involve purchasing plane tickets and/or finding and hiring lawyers licensed in the UK. Additionally, Jamaican and Guyanese citizens are required to obtain visas before travelling to the UK, and for these citizens the cost of a UK visa would range from £85 to £737 (or US$131 to US$1,138) depending on the type of visitor visa applied for. Regardless of whether or not a visa is required, all litigants would also have to pay for accommodation and any other necessary expenses in the UK for the duration of the litigation. All of this adds up as a very expensive appeals process; one estimate placed the average total cost between US$57,000 and US$87,500. Given the generally low number of appeals coming from the smaller CARICOM states and sometimes from larger CARICOM states such as Jamaica, in effect the local courts of appeal are the courts of last resort for the majority of CARICOM litigants who cannot afford to take their appeals to the JCPC and must therefore be satisfied with the judgments of the local courts of appeal.\n\nAs a result, the JCPC has only really been accessible to either the very wealthy from the Caribbean or to certain inmates on death row who are able to secure \"pro bono\" legal service from British barristers. In the case of those nationals, such as Jamaicans, who also require visas in order to travel to the UK to pursue cases before the JCPC, there is the risk that the denial of the visa could negatively impact on the ability for their cases to be heard, thus further reducing potential accessibility (especially if the work load is not sufficient to justify the JCPC travelling to their jurisdiction instead). By contrast, the CCJ, in most cases, is a much less expensive option for litigants in all comparative costs (filing, airfare, accommodations, other expenses, etc.). As the CARICOM has successfully pushed for visa free access by CARICOM nationals (except Haitians for now) to other member states, no citizen of any country that currently has the JCPC as its final court would need to apply for and exhaust financial resources obtaining a visa to travel to the seat of the CCJ or to any other state where the CCJ may sit. And by virtue of distance, travel to the seat of the CCJ in Trinidad, is much cheaper than travel to the UK. In fact physical travel to the seat of the CCJ itself in some cases is not necessary as the court itself (like the JCPC) is itinerant and (unlike the JCPC), the CCJ makes extensive use of electronic and teleconferencing facilities to reduce the cost to litigants. The CCJ has an e-filing system (which has been hailed as \"impressive\") that makes provision for all court filing to be carried out electronically specifically in order to reduce to the cost to litigants of filing documents with the court and to keep its commitment of access to justice for all. The CCJ also utilizes the system to conduct hearings electronically, making use of teleconferencing equipment installed in all contracting states.\n\nIt is in the aspect of the CCJ and JCPC itinerancy that the costs to states (and further differences between the CCJ and JCPC) becomes apparent. Although it is not established to operate as an itinerant court, the JCPC has maintained that it is willing to consider sitting outside the UK, but only where it receives an official invitation to do so from the chief judge and the government of the country or territory concerned, and where the full costs of the JCPC (that is airfare, travel, accommodations and other relevant costs) are covered by the hosts, and where there is sufficient work to justify such a visit. Such sufficient work might involve hearing cases from other neighbouring or nearby territories or countries in which case litigants from the other territories would have to incur the cost of travel and litigation to the country actually hosting the JCPC.\n\nBy contrast, as expressly provided for in the Agreement establishing the CCJ, the CCJ is willing to sit in any country within its jurisdiction on a case by case basis, if doing so is necessary for evidence to be given in person and where video- or teleconferencing technology proves insufficient for the task and where the litigant may not be able to afford to appear before the seat of the court and thus be unable to adequately present his or her own case. When travelling to another country in its jurisdiction the costs are paid for by the CCJ itself including airfare, accommodations and any other expenses. The hosting state is expected to provide a location for the Court to sit (as with the JCPC) and to provide ground transportation and security for the Court (as with the JCPC). Thus far the CCJ has sat on cases in Barbados, Belize, Guyana, Jamaica and in its seat of Trinidad and Tobago.\n\nDue to the differences in costs, the JCPC has only been truly practically accessible to certain death row inmates or very wealthy individuals. Additionally, the JCPC does have jurisdictional limitations unrelated to the cost of appeal. The JCPC functions as a final appellate court in very restricted manner. Under the common law, the right of appeal does not exist for all cases and instead must be specially conferred. This is done consequently as appeals \"as of right\" and \"as of leave\" (where leave is required by the local Court of Appeal or the JCPC itself).\n\nFirstly, appeals to the JCPC in civil proceedings lie at the discretion of the local court where the case at hand is one of 'great general public importance or otherwise ought to be submitted to Her Majesty in Council for decision' and where an amount or in dispute or claim (including property) is of, or exceeds, the prescribed statutory value. In criminal matters, the JCPC will not intervene unless it can be demonstrated that some serious miscarriage of justice has occurred through violation of the principles of natural justice; violation the due process of law or other serious injustice. This is as a result of the JCPC not being designed to function as a second tier Court of Appeal to review the evidence of a given case.\n\nThe structural and practical limitations of JCPC appeals has meant that the range of precedent generated by the highest court for many Caribbean jurisdictions is confined to narrow categories, particularly capital punishment and high finance. The wide body of law between these categories, has often been left mainly to small domestic courts in the Caribbean. Thus different decisions have been more likely to be rendered for similar fact patterns, creating inconsistencies in how laws are interpreted across the region.\n\nThe CCJ's structure and appellate jurisdiction, however, address this issue by providing a forum for the creation of jurisprudence in the gap in Caribbean law where the JCPC was never able to rule upon while also ruling on the area of law the JCPC specializes in. In the three years following the CCJ's inauguration, civil appeals petitioned to the court outnumbered criminal appeals by nearly seven to one, with half of the civil appeals coming from appellants the CCJ deemed too poor to pay for the filing costs. By contrast, under the JCPC, civil appeals have never outnumbered criminal appeals. This combination of lower litigant cost for the CCJ, the Court's willingness to grant \"in forma pauperis\" and having a wider field of law to hear appeals on has enabled the CCJ to hear types of cases from the region that the JCPC has never known.\n\nAlthough limited to only four states in its appellate jurisdiction at the moment, so far citizens of those states have been accessing the Court more than they did the JCPC. For Barbados there were eight appeals heard by the JCPC in the five years immediately prior to Barbados' accession to the appellate jurisdiction of the CCJ. In the five years that followed immediate after the switch to the CCJ, twelve appeals were heard from Barbados. Belize saw appeals roughly twice per year to the JCPC before switching to the CCJ in 2010 and subsequently saw 12 appeals in the four years since the first appeal to the CCJ from Belize in mid 2011. While Guyana originally abolished appeals to the JCPC in 1970, since adopting the CCJ, appeals to that Court have been exponentially increasing.\n\nIn 2011, Bahamian Chief Justice Sir Michael Barnett said The Bahamas should eventually abandon the Privy Council as the final court of appeal and move toward the Caribbean Court of Justice (CCJ). While that decision would be up to the government of The Bahamas, Sir Michael said there is a “powerful argument to moving eventually toward the CCJ”.\n\n“Whether we do that now is a matter for political debate and a matter that [the government] will have to discuss and consider,” Sir Michael told the Nassau Guardian following the opening ceremony of the Caribbean Association of Judicial Officers Conference.\n\n“I have my own views and I think it’s almost a natural progression of our constitutional development that we move away from the Privy Council and I think the Caribbean Court of Justice is likely to be the alternative to the Privy Council. I think that as a part of our constitutional development it’s almost inevitable that we move away from the Privy Council like lots of other countries, including Australia and New Zealand.”\n\nSir Michael said while the Privy Council has been useful, the CCJ would better serve the country’s needs. \"It's a regional court but it's also part of our development as a nation that we look to our own court for the resolution of disputes.\"\n\nSome proponents in The Bahamas wishing to sever links with the Privy Council are in favour of joining the CCJ, perhaps by having a dual final court of appeal system in the country with the Privy Council for civil and commercial matters and the CCJ for criminal matters.\n\nBarbados recognizes the court for original and final jurisdictions. In 2003 the Parliament of Barbados passed the \"Caribbean Court of Justice Act\" and the \"Constitution (Amendment) Act\", and they were brought into force by Proclamation on 8 April 2005.\n\nBelizean legislation to recognize the CCJ was tied up for some years in partisan politics. In 2007, the People's United Party (PUP)-led government introduced the Caribbean Court of Justice Bill, but due to the opposition of United Democratic Party (UDP) members, it did not achieve the required three-fourths majority. This led to mutual recriminations, with PM Said Musa accusing the UDP of being anti-Caribbean, while the UDP complained of the PUP's attempts to tie the CCJ Bill to the Coast Guard Bill, which the UDP supported. The Belizean general election, 2008 resulted in the UDP taking power; new PM Dean Barrow then tabled the Belize Constitution (Seventh Amendment) Bill, which aside from replacing the Judicial Committee of the Privy Council with the CCJ, would also have removed the prohibition against dual citizens being elected to the National Assembly. This time the PUP blocked passage of the constitutional amendment until the dual citizenship provision was removed; after this was done, the bill passed in February 2010. After the passage of the bill, PM Barrow signed the order in May 2010 to abolish appeals to the Privy Council beginning on 1 June that year.\n\nThe Jamaica Labour Party resisted the full powers of the CCJ on the basis that it was a hanging court. In February, 2005, the Privy Council declared that the CCJ-related companion bills passed by the Jamaican Parliament in 2004 were unconstitutional and therefore void. The bills would have established the CCJ as the final court of appeal in Jamaica. The Privy Council sided with the appellants, including the Jamaican Council for Human Rights, the Jamaica Labour Party and others, ruling that to establish the CCJ as the country's final appeal court, without it being entrenched in the constitution would undermine the protection given to the Jamaican people by Chapter Seven of the Jamaican constitution. The court concluded that the procedure appropriate for an amendment of an entrenched provision — a referendum — should have been followed.\n\nIn January 2012, the new People's National Party government of Jamaica stated that it would be moving to have the CCJ serving in both the original and appellate jurisdictions for Jamaica in time for the 50th anniversary of Jamaica's independence in August. The Jamaica Labour Party, now in opposition, stated it has no issue with the government's plan and seems set to support the move despite strident objections in the past. In February, the foreign affairs minister of Jamaica has also called on Trinidad & Tobago to sign on to the court's appellate jurisdiction to mark that country's 50th anniversary of independence.\n\nIn May 2015, the Jamaican House of Representatives approved, with the necessary two-thirds majority, three bills that would end legal appeals to the Judicial Committee of the Privy Council and make the Caribbean Court of Justice as Jamaica's final Court of Appeal. The reform was debated by the Jamaican Senate, however, the government needed the support of at least one opposition senator for the measures to be approved by the required two-thirds majority. The 2016 general election was held without the issue being resolved and resulted in the defeat of the People's National Party government and the election of a new Jamaican Labour Party government, led by Andrew Holness, which opposes implementing the reform without a referendum. Holness's government has promised to hold a referendum on the question.\n\nIn late 2009, controversy arose over the fact that the CEO of a company involved in CCJ litigation was also the chairman of the Court's trust fund.\n\nIn April 2012, the prime minister of Trinidad and Tobago Kamla Persad-Bissessar announced in Parliament that it intended to abolish criminal appeals to the Privy Council in favour of the CCJ and would be tabling legislation to that effect. This follows a review of the situation conducted by the government after a commitment given at the last Caricom heads of government conference in Suriname in July 2011. Although the announcement had the general support of the Opposition leader Dr Keith Rowley, he expressed disappointment that the government was \"only going halfway\" by planning to adopt the CCJ for criminal appeals only while retaining the Privy Council for civil matters and cautioned that the move may not be legally possible under the relevant treaties. He said the opposition People's National Movement was fully supportive of adopting the CCJ as a final appeals court on all matters, both civil and criminal. It has been observed however that there is a precedent for the partial abolition of appeals to the Privy Council with Canada ending criminal appeals to the court in 1933 and civil appeals in 1949.\n\nIt is expected that the two Caribbean states that will have the most difficulty accessing the court will be Suriname which has a Dutch-based legal system, and Haiti which has a French-based legal system. All other member states have British-based legal systems with the CCJ itself being predominantly modeled after the British system.\n\nIn 2012, following the 54th meeting of the OECS Authority, it was agreed that although all OECS members are committed to acceding to the CCJ's appellate jurisdiction as soon as possible the differing constitutional provisions of each member state meant that simultaneous accession was no longer the preferred option. Dominica and St. Kitts & Nevis are the only members that would be able to take steps to accede to the CCJ's appellate jurisdiction during the course of 2012 as they only require a parliamentary majority to join up to the court. Grenada and Antigua & Barbuda would require referenda before being able to accede, while St. Lucia and St. Vincent & the Grenadines would need a parliamentary majority approving accession along with a judicial resolution.\n\nOn 29 January 2015, it was announced that Dominica would become the fourth CARICOM member state to accede to both the original and appellate jurisdictions of the CCJ by early February 2015. This was announced by Dominica's Prime Minister, Roosevelt Skerrit and follows on the formal approval received in 2014 from the British government that was required in order for Dominica to delink from the Privy Council. Dominica acceded to the CCJ in its appellate jurisdiction on 6 March 2015.\n\nIn July 2015, the St. Lucian government announced that intended to soon table legislation that would replace the Privy Council with the CCJ. Prime Minister Dr. Kenny Anthony noted that St. Lucia had a provision in its Constitution which was identical to a provision in the Constitution of Dominica which allowed that country to recently join the CCJ. Further noting that St. Lucia's Attorney General had received an advisory opinion from the Court of Appeal to a possible erroneous section that the provision in question had referred to; the Court of Appeal agreed by a 2-1 majority that there was indeed an error in the Constitution. On that basis the government plans to proceed with accession to the CCJ's appellate jurisdiction and it has formally written to the British government advising them that the government of St. Lucia wishes to delink from the Privy Council pursuant to the requirements of that section of the Constitution. Prime Minister Anthony anticipated opposition and possible legal challenges to this move, and stated his government had no problems with that, even suggesting it would be interesting to see what pronouncement the Privy Council would make on the non-binding advisory opinion of the Court of Appeal.\n\nAntigua and Barbuda began taking positive steps towards adopting the CCJ as its final appellate court when it launched a public education campaign on the CCJ in March 2016. The public education campaign and move towards acceding to the CCJ in the appellate jurisdiction has the support of both the Government and the Opposition and is expected to go on for three months ahead of a referendum on the issue likely to be held in June. Additionally three new pieces of legislation would be needed to facilitate the referendum on accession to the appellate jurisdiction - a Constitution Amendment Bill to amend the provisions of the Constition on the Supreme Court Order, an amendment to the Referendum Act, and an amendment to the Representation of the People Act. Two of these instruments were expected to be submitted to the Parliament for review and voting in late March 2016.\n\nOn 20 June 2016, the Parliament of Grenada passed legislation that would allow Grenada to accede to the CCJ's appellate jurisdiction. Before Grenada can accede however the bill will need to be approved by a simple majority in Senate and then a referendum will have to be held. No date had been announced for the referendum, but it is expected to be held by December 2016.\n\nThe Caribbean Court of Justice currently consists of 7 Judges (including the President), though under the Agreement establishing the court there can be a maximum of 10 Judges including the President. This limit may be increased by the agreement of all of the member states if necessary. The Judges, other than the President, are appointed or removed by a majority vote of the eleven member Regional Judicial and Legal Services Commission (RJLSC), which is also the body which must recommend a need of for an increase in the number of Judges (other than the President) before such an increase can be effected by the agreement of the member states. Removal of a Judge by the RJLSC occurs only after the question of the removal of a Judge has been referred by the RJLSC to a tribunal and the tribunal has subsequently advised that the Judge should be removed for misbehaviour or an inability to carry out the duties of a Judge.\n\nUnder the Agreement establishing the court, at least three judges of the full complement of ten are required to possess expertise in international law including international trade law and one judge is also required to be from the civil law tradition similar to such jurisdictions as Haiti and Suriname. Persons being appointed to the office of Judge (including the President) are supposed to have high moral character, intellectual and analytical ability, integrity, demonstrate sound judgment, and an understanding of people and society. The RJLSC appoints persons to be Judges (or recommends persons to be President of the Court) from candidates who have distinguished themselves in their careers either practicing or teaching law for at least fifteen years or being Judges of a court of unlimited jurisdiction in civil and criminal cases for at least five years. The candidates must have practiced or taught law, or been a judge, in at least one of the following:\n\n\nThe RJLSC does not and is not allowed to consider potential judges by recommendations from contracting member states, but only by a prospective judge’s individual application.\n\nOnce a judge is appointed, they are allowed to hold office until the age of 72, but are allowed to continue in office, if necessary for a further three months in order to deliver a judgment or to do any other thing in proceedings that he or she has heard. During the evolutionary phase of the Court (that is until the full complement of 9 Judges plus the President have been appointed), the RJLSC may extend the tenure of a Judge until the age of 75.\n\n\"As of :\"\n\n\"Past Judges:\"\n\nThe President of the CCJ is appointed or removed by the qualified super majority vote of three-quarters of the Contracting Parties on the recommendation of the RJLSC. The President may be removed by the Contracting Parties only on recommendation of the RJLSC and then only after the question of the removal of the President has been referred by the RJLSC to a tribunal and the tribunal has subsequently advised that the President should be removed for an inability to carry out the duties of President or for misbehaviour.\n\nThe President serves also as Chairman of the RJLSC and in the Court will preside over hearings and deliberations; direct the Court to sit in such number of divisions as he or she chooses; appoint one or more judges to determine interlocutory matters; and (in consultation with the five other Judges selected by him for the purpose) establish rules for the exercise of the original jurisdiction of the Court and rules for regulating the practice and procedure in the exercise of the appellate jurisdiction of the Court.\n\nThe President automatically takes precedence over all other Judges of the Court, with the seniority of the other Judges being determined by the dates of their appointment. In the event that the President is unable to perform the duties of office (or if there is a vacancy in the office of President), the most senior Judge shall perform the role of President and shall be appointed to perform that role by the Chairman of the Conference of Heads of Government of CARICOM until the President can resume those functions or, in the case of a vacancy in the Presidency, until someone has been appointed to and assumes the functions of the office. Where there is no difference in seniority among the Judges, one of the Judges will simply be selected by the Heads of Government to perform the role of President in the event of a vacancy in Presidency or the inability of the President to perform the functions of office.\n\nThe President may only serve for one, non-renewable 7-year term or until the age of 72 (whichever is earlier) but is allowed to continue in office, if necessary for a further three months in order to deliver a judgment or to do any other thing in proceedings that he or she has heard. As with the other Judges, during the evolutionary phase of the Court (that is until the full complement of 9 Judges plus the President have been appointed), the RJLSC may extend the tenure of the President until the age of 75 or until seven years in office have been reached, whichever comes first.\n\nArticle III of the Agreement establishing the CCJ provides that The Seat of the Court shall be in the territory of a Contracting Party as determined by a qualified majority of the Contracting Parties.\n\nIn 1999, Trinidad and Tobago signed an Agreement with the Caribbean Community establishing the seat of the CCJ and the offices of the RJLSC in that country. This followed from the decision of the Contracting Parties for Trinidad and Tobago to serve as the headquarters of the court in the 1990s and the promotion by Basdeo Panday (then Prime Minister of Trinidad and Tobago) of the CCJ and his desire to seek agreement with the Opposition to give effect to the Agreement establishing the CCJ and for Trinidad and Tobago to effectively act as the base for the Court.\n\nIn 2005, a broadly identical Agreement was signed between Trinidad and Tobago and the newly established CCJ and RJLSC establishing the seat of the CCJ and Offices of the RJLSC in Trinidad and Tobago as was required under Article III of the Agreement establishing the CCJ itself.\n\nWhile having a seat in Trinidad, the Court is also given the authority (under the same Article III of the Agreement establishing the CCJ) to sit, as circumstances warrant, in the territory of any other Contracting Party. This itinerant ability, coupled with the Court's use of electronic and teleconferencing facilities, makes travel to the seat of the Court unnecessary in some cases. This is especially true for cases where litigants may be unable to afford the cost of appearing before the seat of the Court and electronic and teleconferencing facilities are inadequate for the task. As a result of the CCJ's planned self-sufficiency in terms of funding, when the Court sits in another country in its jurisdiction, it pays the cost for travelling, accommodation and other expenses and only requires the host state to provide a location for the Court to sit and to provide security and ground transportation to and from the venue of the sitting. Up to May 2015, the CCJ has sat in Barbados, Belize, Guyana, Jamaica and Trinidad & Tobago. The Court itself views itinerant sittings as important to ensuring that the accessibility of itself and justice in general to the people it serves of the Caribbean Community.\n\n\n\n\n\n\n\n\n\n\n",
                "Precedent\n\nIn legal systems based on common law, a precedent, or authority, is a principle or rule established in a previous legal case that is either binding on or persuasive for a court or other tribunal when deciding subsequent cases with similar issues or facts . Common law legal systems place great value on deciding cases according to consistent principled rules so that similar facts will yield similar and predictable outcomes, and observance of precedent is the mechanism by which that goal is attained. The principle by which judges are bound to precedents is known as stare decisis. Black's Law Dictionary defines \"precedent\" as a \"rule of law established for the first time by a court for a particular type of case and thereafter referred to in deciding similar cases\". Common law precedent is a third kind of law, on equal footing with statutory law (statutes and codes enacted by legislative bodies), and delegated legislation (in U.K. parlance) or regulatory law (in U.S. parlance) (regulations promulgated by executive branch agencies).\n\nCase law, in common law jurisdictions, is the set of decisions of adjudicatory tribunals or other rulings that can be cited as precedent. In most countries, including most European countries, the term is applied to any set of rulings on law which is guided by previous rulings, for example, previous decisions of a government agency.\n\nEssential to the development of case law is the publication and indexing of decisions for use by lawyers, courts and the general public, in the form of law reports. While all decisions are precedent (though at varying levels of authority as discussed throughout this article), some become \"leading cases\" or \"landmark decisions\" that are cited especially often.\n\n\"Stare decisis\" (Anglo-Latin pronunciation: ) is a legal principle by which judges are obligated to respect the precedent established by prior decisions. The words originate from the phrasing of the principle in the Latin maxim Stare decisis et non quieta movere: \"to stand by decisions and not disturb the undisturbed\". In a legal context, this is understood to mean that courts should generally abide by precedent and not disturb settled matters. The principle of \"stare decisis\" can be divided into two components.\n\nThe first is the rule that a decision made by a superior court, or by the same court in an earlier decision, is binding precedent that the court itself and all its inferior courts are obligated to follow. The second is the principle that a court should not overturn its own precedent unless there is a strong reason to do so and should be guided by principles from lateral and inferior courts. The second principle, regarding persuasive precedent, is an advisory one that courts can and do ignore occasionally.\n\nIn the common law tradition, courts decide the law applicable to a case by interpreting statutes and applying precedent which record how and why prior cases have been decided. Unlike most civil law systems, common law systems follow the doctrine of \"stare decisis\", by which most courts are bound by their own previous decisions in similar cases, and all lower courts should make decisions consistent with previous decisions of higher courts. For example, in England, the High Court and the Court of Appeal are each bound by their own previous decisions, but the Supreme Court of the United Kingdom is able to deviate from its earlier decisions, although in practice it rarely does so.\n\nGenerally speaking, higher courts do not have direct oversight over day-to-day proceedings in lower courts, in that they cannot reach out on their own initiative (\"sua sponte\") at any time to reverse or overrule judgments of the lower courts. Normally, the burden rests with litigants to appeal rulings (including those in clear violation of established case law) to the higher courts. If a judge acts against precedent and the case is not appealed, the decision will stand.\n\nA lower court may not rule against a binding precedent, even if the lower court feels that the precedent is unjust; the lower court may only express the hope that a higher court or the legislature will reform the rule in question. If the court believes that developments or trends in legal reasoning render the precedent unhelpful, and wishes to evade it and help the law evolve, the court may either hold that the precedent is inconsistent with subsequent authority, or that the precedent should be \"distinguished\" by some material difference between the facts of the cases. If that judgment goes to appeal, the appellate court will have the opportunity to review both the precedent and the case under appeal, perhaps overruling the previous case law by setting a new precedent of higher authority. This may happen several times as the case works its way through successive appeals. Lord Denning, first of the High Court of Justice, later of the Court of Appeal, provided a famous example of this evolutionary process in his development of the concept of estoppel starting in the \"High Trees\" case: \"Central London Property Trust Ltd v. High Trees House Ltd\" [1947] K.B. 130.\n\nJudges may refer to various types of persuasive authority to reach a decision in a case. Widely cited non-binding sources include legal encyclopedias such as \"Corpus Juris Secundum\" and \"Halsbury's Laws of England\", or the published work of the Law Commission or the American Law Institute. Some bodies are given statutory powers to issue Guidance with persuasive authority or similar statutory effect, such as the Highway Code.\n\nIn federal or multi-jurisdictional law systems there may exist conflicts between the various lower appellate courts. Sometimes these differences may not be resolved and it may be necessary to distinguish how the law is applied in one district, province, division or appellate department. Usually only an appeal accepted by the court of last resort will resolve such differences and, for many reasons, such appeals are often not granted.\n\nAny court may seek to distinguish its present case from that of a binding precedent, in order to reach a different conclusion. The validity of such a distinction may or may not be accepted on appeal. An appellate court may also propound an entirely new and different analysis from that of junior courts, and may or may not be bound by its own previous decisions, or in any case may distinguish the decisions based on significant differences in the facts applicable to each case. Or, a court may view the matter before it as one of \"first impression,\" not governed by any controlling precedent.\n\nWhere there are several members of a court, there may be one or more judgments given; only the ratio decidendi of the majority can constitute a binding precedent, but all may be cited as persuasive, or their reasoning may be adopted in argument. Quite apart from the rules of precedent, the weight actually given to any reported judgment may depend on the reputation of both the court and the judges.\n\nGenerally, a common law court system has trial courts, intermediate appellate courts and a supreme court. The inferior courts conduct almost all trial proceedings. The inferior courts are bound to obey precedent established by the appellate court for their jurisdiction, and all supreme court precedent.\n\nThe Supreme Court of California's explanation of this principle is that\n\nAn Intermediate state appellate court is generally bound to follow the decisions of the highest court of that state.\n\nThe application of the doctrine of \"stare decisis\" from a superior court to an inferior court is sometimes called \"vertical stare decisis\".\n\nThe idea that a judge is bound by (or at least should respect) decisions of earlier judges of similar or coordinate level is called horizontal \"stare decisis\".\n\nIn the United States federal court system, the intermediate appellate courts are divided into thirteen \"circuits,\" each covering some range of territory ranging in size from the District of Columbia alone up to seven states. Each panel of judges on the court of appeals for a circuit is bound to obey the prior appellate decisions of the same circuit. Precedent of a United States court of appeals may be overruled only by the court \"en banc,\" that is, a session of all the active appellate judges of the circuit, or by the United States Supreme Court, not simply by a different three-judge panel.\n\nWhen a court binds itself, this application of the doctrine of precedent is sometimes called \"horizontal stare decisis\". The state of New York has a similar appellate structure as it is divided into four appellate departments supervised by the final New York Court of Appeals. Decisions of one appellate department are not binding upon another, and in some cases the departments differ considerably on interpretations of law.\n\nIn federal systems the division between federal and state law may result in complex interactions. In the United States, state courts are not considered inferior to federal courts but rather constitute a parallel court system.\n\n\nIn practice, however, judges in one system will almost always choose to follow relevant case law in the other system to prevent divergent results and to minimize forum shopping.\n\nPrecedent that must be applied or followed is known as \"binding precedent\" (alternately \"metaphorically precedent\", \"mandatory\" or \"binding authority\", etc.). Under the doctrine of \"stare decisis\", a lower court must honor findings of law made by a higher court that is within the appeals path of cases the court hears. In state and federal courts in the United States of America, jurisdiction is often divided geographically among local trial courts, several of which fall under the territory of a regional appeals court. All appellate courts fall under a highest court (sometimes but not always called a \"supreme court\"). By definition, decisions of lower courts are not binding on courts higher in the system, nor are appeals court decisions binding on local courts that fall under a different appeals court. Further, courts must follow their own proclamations of law made earlier on other cases, and honor rulings made by other courts in disputes among the parties before them pertaining to the same pattern of facts or events, unless they have a strong reason to change these rulings (see Law of the case re: a court's previous holding being binding precedent for that court).\n\nIn law, a binding precedent (also known as a mandatory precedent or binding authority) is a precedent which must be followed by all lower courts under common law legal systems. In English law it is usually created by the decision of a higher court, such as the Supreme Court of the United Kingdom, which took over the judicial functions of the House of Lords in 2009. In Civil law and pluralist systems precedent is not binding but case law is taken into account by the courts.\n\nBinding precedent relies on the legal principle of \"stare decisis\". \"Stare decisis\" means to stand by things decided. It ensures certainty and consistency in the application of law. Existing binding precedent from past cases are applied in principle to new situations by analogy.\n\nOne law professor has described mandatory precedent as follows:\n\nIn extraordinary circumstances a higher court may overturn or overrule mandatory precedent, but will often attempt to distinguish the precedent before overturning it, thereby limiting the scope of the precedent.\n\nUnder the U.S. legal system, courts are set up in a hierarchy. At the top of the federal or national system is the Supreme Court, and underneath are lower federal courts. The state court systems have hierarchy structures similar to that of the federal system.\n\nThe U.S. Supreme Court has final authority on questions about the meaning of federal law, including the U.S. Constitution. For example, when the Supreme Court says that the First Amendment applies in a specific way to suits for slander, then every court is bound by that precedent in its interpretation of the First Amendment as it applies to suits for slander. If a lower court judge disagrees with a higher court precedent on what the First Amendment should mean, the lower court judge must rule according to the binding precedent. Until the higher court changes the ruling (or the law itself is changed), the binding precedent is authoritative on the meaning of the law.\n\nLower courts are bound by the precedent set by higher courts within their region. Thus, a federal district court that falls within the geographic boundaries of the Third Circuit Court of Appeals (the mid-level appeals court that hears appeals from district court decisions from Delaware, New Jersey, Pennsylvania, and the Virgin Islands) is bound by rulings of the Third Circuit Court, but not by rulings in the Ninth Circuit (Alaska, Arizona, California, Guam, Hawaii, Idaho, Montana, Nevada, Northern Mariana Islands, Oregon, and Washington), since the Circuit Courts of Appeals have jurisdiction defined by geography. The Circuit Courts of Appeals can interpret the law how they want, so long as there is no binding Supreme Court precedent. One of the common reasons the Supreme Court grants certiorari (that is, they agree to hear a case) is if there is a conflict among the circuit courts as to the meaning of a federal law.\n\nThere are three elements needed for a precedent to work. Firstly, the hierarchy of the courts needs to be accepted, and an efficient system of law reporting. 'A balance must be struck between the need on one side for the legal certainty resulting from the binding effect of previous decisions, and on the other side the avoidance of undue restriction on the proper development of the law (1966 Practice Statement (Judicial Precedent) by Lord Gardiner L.C.)'.\n\nJudges are bound by the law of binding precedent in England and Wales and other common law jurisdictions. This is a distinctive feature of the English legal system. In Scotland and many countries throughout the world, particularly in mainland Europe, civil law means that judges take case law into account in a similar way, but are not obliged to do so and are required to consider the precedent in terms of principle. Their fellow judges' decisions may be persuasive but are not binding. Under the English legal system, judges are not necessarily entitled to make their own decisions about the development or interpretations of the law. They may be bound by a decision reached in a previous case. Two facts are crucial to determining whether a precedent is binding:\n\n\"Super \"stare decisis\"\" is a term used for important precedent that is resistant or immune from being overturned, without regard to whether correctly decided in the first place. It may be viewed as one extreme in a range of precedential power, or alternatively, to express a belief, or a critique of that belief, that some decisions should not be overturned.\n\nIn 1976, Richard Posner and William Landes coined the term \"super-precedent,\" in an article they wrote about testing theories of precedent by counting citations. Posner and Landes used this term to describe the influential effect of a cited decision. The term \"super-precedent\" later became associated with different issue: the difficulty of overturning a decision. In 1992, Rutgers professor Earl Maltz criticized the Supreme Court's decision in \"Planned Parenthood v. Casey\" for endorsing the idea that if one side can take control of the Court on an issue of major national importance (as in \"Roe v. Wade\"), that side can protect its position from being reversed \"by a kind of super-stare decisis\". The controversial idea that some decisions are virtually immune from being overturned, regardless of whether they were decided correctly in the first place, is the idea to which the term \"super \"stare decisis\"\" now usually refers.\n\nThe concept of super-\"stare decisis\" (or \"super-precedent\") was mentioned during the interrogations of Chief Justice John Roberts and Justice Samuel Alito before the Senate Judiciary Committee. Prior to the commencement of the Roberts hearings, the chair of that committee, Senator Arlen Specter of Pennsylvania, wrote an op/ed in the \"New York Times\" referring to \"Roe\" as a \"super-precedent\". He revisited this concept during the hearings, but neither Roberts nor Alito endorsed the term or the concept.\n\nPersuasive precedent (also persuasive authority) is precedent or other legal writing that is not binding precedent but that is useful or relevant and that may guide the judge in making the decision in a current case. Persuasive precedent includes cases decided by lower courts, by peer or higher courts from other geographic jurisdictions, cases made in other parallel systems (for example, military courts, administrative courts, indigenous/tribal courts, state courts versus federal courts in the United States), statements made in dicta, treatises or academic law reviews, and in some exceptional circumstances, cases of other nations, treaties, world judicial bodies, etc.\n\nIn a \"case of first impression\", courts often rely on persuasive precedent from courts in other jurisdictions that have previously dealt with similar issues. Persuasive precedent may become binding through its adoption by a higher court.\n\nIn civil law and pluralist systems, as under Scots law, precedent is not binding but case law is taken into account by the courts.\n\nA lower court's opinion may be considered as persuasive authority if the judge believes they have applied the correct legal principle and reasoning.\n\nA court may consider the ruling of a higher court that is not binding. For example, a district court in the United States First Circuit could consider a ruling made by the United States Court of Appeals for the Ninth Circuit as persuasive authority.\n\nCourts may consider rulings made in other courts that are of equivalent authority in the legal system. For example, an appellate court for one district could consider a ruling issued by an appeals court in another district.\n\nCourts may consider \"obiter dicta\" in opinions of higher courts. Dicta of a higher court, though not binding, will often be persuasive to lower courts. The phrase \"obiter dicta\" is usually translated as \"other things said\", but due to the high number of judges and individual concurring opinions, it is often hard to distinguish from the \"ratio decidendi\" (reason for the decision). For these reasons, the obiter dicta may often be taken into consideration by a court. A litigant may also consider \"obiter dicta\" if a court has previously signaled that a particular legal argument is weak and may even warrant sanctions if repeated.\n\nA case decided by a multi-judge panel could result in a split decision. While only the majority opinion is considered precedential, an outvoted judge can still publish a dissenting opinion. Common patterns for dissenting opinions include:\nA judge in a subsequent case, particularly in a different jurisdiction, could find the dissenting judge's reasoning persuasive. In the jurisdiction of the original decision, however, a judge should only overturn the holding of a court lower or equivalent in the hierarchy. A district court, for example, could not rely on a Supreme Court dissent as a basis to depart from the reasoning of the majority opinion. However, lower courts occasionally cite dissents, either for a limiting principle on the majority, or for propositions that are not stated in the majority opinion and not inconsistent with that majority, or to explain a disagreement with the majority and to urge reform (while following the majority in the outcome).\n\nCourts may consider the writings of eminent legal scholars in treatises, restatements of the law, and law reviews. The extent to which judges find these types of writings persuasive will vary widely with elements such as the reputation of the author and the relevance of the argument.\n\nThe courts of England and Wales are free to consider decisions of other jurisdictions, and give them whatever persuasive weight the English court sees fit, even though these other decisions are not binding precedent. Jurisdictions that are closer to modern English common law are more likely to be given persuasive weight (for example Commonwealth states such as Canada, Australia, or New Zealand). Persuasive weight might be given to other common law courts, such as from the United States, most often where the American courts have been particularly innovative, e.g. in product liability and certain areas of contract law.\n\nIn the United States, in the late 20th and early 21st centuries, the concept of a U.S. court considering foreign law or precedent has been considered controversial by some parties. The Supreme Court splits on this issue. This critique is recent, as in the early history of the United States, citation of English authority was ubiquitous. One of the first acts of many of the new state legislatures was to adopt the body of English common law into the law of the state. See here. Citation to English cases was common through the 19th and well into the 20th centuries. Even in the late 20th and early 21st centuries, it is relatively uncontroversial for American state courts to rely on English decisions for matters of pure common (i.e. judge-made) law. \n\nWithin the federal legal systems of several common-law countries, and most especially the United States, it is relatively common for the distinct lower-level judicial systems (e.g. state courts in the United States and Australia, provincial courts in Canada) to regard the decisions of other jurisdictions within the same country as persuasive precedent. Particularly in the United States, the adoption of a legal doctrine by a large number of other state judiciaries is regarded as highly persuasive evidence that such doctrine is preferred. A good example is the adoption in Tennessee of comparative negligence (replacing contributory negligence as a complete bar to recovery) by the 1992 Tennessee Supreme Court decision \"McIntyre v. Balentine\" (by this point all US jurisdictions save Tennessee, five other states, and the District of Columbia had adopted comparative negligence schemes). Moreover, in American law, the \"Erie\" doctrine requires federal courts sitting in diversity actions to apply state substantive law, but in a manner consistent with how the court believes the state's highest court would rule in that case. Since such decisions are not binding on state courts, but are often very well-reasoned and useful, state courts cite federal interpretations of state law fairly often as persuasive precedent, although it is also fairly common for a state high court to reject a federal court's interpretation of its jurisprudence.\n\nNon-publication of opinions, or unpublished opinions, are those decisions of courts that are not available for citation as precedent because the judges making the opinion deem the case as having less precedential value. Selective publication is the legal process which a judge or justices of a court decide whether a decision is to be or not published in a reporter. \"Unpublished\" federal appellate decisions are published in the Federal Appendix. Depublication is the power of a court to make a previously published order or opinion unpublished.\n\nLitigation that is settled out of court generates no written decision, and thus has no precedential effect. As one practical effect, the U.S. Department of Justice settles many cases against the federal government simply to avoid creating adverse precedent.\n\nSeveral rules may cause a decision to apply as narrow \"precedent\" to preclude future legal positions of the specific parties to a case, even if a decision is non-precedential with respect to all other parties.\n\nOnce a case is decided, the same plaintiff cannot sue the same defendant again on any claim arising out of the same facts. The law requires plaintiffs to put all issues on the table in a single case, not split the case. For example, in a case of an auto accident, the plaintiff cannot sue first for property damage, and then personal injury in a separate case. This is called \"res judicata\" or claim preclusion (\"'Res judicata'\" is the traditional name going back centuries; the name shifted to \"claim preclusion\" in the United States over the late 20th century). Claim preclusion applies whether the plaintiff wins or loses the earlier case, even if the later case raises a different legal theory, even the second claim is unknown at the time of the first case. Exceptions are extremely limited, for example if the two claims for relief must necessarily be brought in different courts (for example, one claim might be exclusively federal, and the other exclusively state).\n\nOnce a case is finally decided, any issues decided in the previous case may be binding against the party that lost the issue in later cases, even in cases involving other parties. For example, if a first case decides that a party was negligent, then other plaintiffs may rely on that earlier determination in later cases, and need not re-prove the issue of negligence. For another example, if a patent is shown to be invalid in a case against one accused infringer, that same patent is invalid against all other accused infringers—invalidity need not be re-proved. Again, there are limits and exceptions on this principle. The principle is called collateral estoppel or issue preclusion.\n\nWithin a single case, once there's been a first appeal, both the lower court and the appellate court itself will not further review the same issue, and will not re-review an issue that could have been appealed in the first appeal. Exceptions are limited to three \"exceptional circumstances:\" (1) when substantially different evidence is raised at a subsequent trial, (2) when the law changes after the first appeal, for example by a decision of a higher court, or (3) when a decision is clearly erroneous and would result in a manifest injustice. This principle is called \"law of the case\".\n\nOn many questions, reasonable people may differ. When two of those people are judges, the tension among two lines of precedent may be resolved as follows.\n\nIf the two courts are in separate, parallel jurisdictions, there is no conflict, and two lines of precedent may persist. Courts in one jurisdiction are influenced by decisions in others, and notably better rules may be adopted over time.\n\nCourts try to formulate the common law as a \"seamless web\" so that principles in one area of the law apply to other areas. However, this principle does not apply uniformly. Thus, a word may have different definitions in different areas of the law, or different rules may apply so that a question has different answers in different legal contexts. Judges try to minimize these conflicts, but they arise from time to time, and under principles of 'stare decisis', may persist for some time.\n\nA matter of first impression (known as \"primae impressionis\" in Latin) is a legal case in which there is no binding authority on the matter presented. Such a case can set forth a completely original issue of law for decision by the courts. A first impression case may be a first impression in only a particular jurisdiction. In that situation, courts will look to holdings of other jurisdictions for persuasive authority.\n\nIn the latter meaning, the case in question cannot be decided through referring to and/or relying on precedent. Since the legal issue under consideration has never been decided by an appeals court and, therefore, there is no precedent for the court to follow, the court uses analogies from prior rulings by appeals courts, refers to commentaries and articles by legal scholars, and applies its own logic. In cases of first impression, the trial judge will often ask both sides' attorneys for legal briefs.\n\nIn some situations, a case of first impression may exist in a jurisdiction until a reported appellate court decision is rendered.\n\nThe different roles of case law in civil law and common law traditions create differences in the way that courts render decisions. Common law courts generally explain in detail the legal rationale behind their decisions, with citations of both legislation and previous relevant judgments, and often an exegesis of the wider legal principles. These are called \"ratio decidendi\" and constitute a precedent binding on other courts; further analyses not strictly necessary to the determination of the current case are called \"obiter dicta\", which have persuasive authority but are not technically binding. By contrast, decisions in civil law jurisdictions are generally very short, referring only to statutes. The reason for this difference is that these civil law jurisdictions apply legislative positivism — a form of extreme legal positivism — which holds that legislation is the only valid source of law because it has been voted on democratically; thus, it is not the judiciary's role to create law, but rather to interpret and apply statute, and therefore their decisions must reflect that.\n\n\"Stare decisis\" is not usually a doctrine used in civil law systems, because it violates the legislative positivist principle that only the legislature may make law. Instead, the civil law system relies on the doctrine of \"jurisprudence constante\", according to which if a court has adjudicated a consistent line of cases that arrive at the same holdings using sound reasoning, then the previous decisions are highly persuasive but not controlling on issues of law. This doctrine is similar to \"stare decisis\" insofar as it dictates that a court's decision must condone a cohesive and predictable result. In theory, lower courts are generally not bound by the precedents of higher courts. In practice, the need for predictability means that lower courts generally defer to the precedent of higher courts. As a result, the precedent of courts of last resort, such as the French Cassation Court and the Council of State, is recognized as being \"de facto\" binding on lower courts.\n\nThe doctrine of \"jurisprudence constante\" also influences how court decisions are structured. In general, court decisions of common law jurisdictions give a sufficient \"ratio decidendi\" as to guide future courts. The ratio is used to justify a court decision on the basis of previous case law as well as to make it easier to use the decision as a precedent for future cases. By contrast, court decisions in some civil law jurisdictions (most prominently France) tend to be extremely brief, mentioning only the relevant legislation and codal provisions and not going into the \"ratio decidendi\" in any great detail. This is the result of the legislative positivist view that the court is only interpreting the legislature's intent and therefore detailed exposition is unnecessary. Because of this, \"ratio decidendi\" is carried out by legal academics (doctrinal writers) who provide the explanations that in common law jurisdictions would be provided by the judges themselves.\n\nIn other civil law jurisdictions, such as the German-speaking countries, \"ratio decidendi\" tend to be much more developed than in France, and courts will frequently cite previous cases and doctrinal writers. However, some courts (such as German courts) have less emphasis on the particular facts of the case than common law courts, but have more emphasis on the discussion of various doctrinal arguments and on finding what the correct interpretation of the law is.\n\nThe mixed systems of the Nordic countries are sometimes considered a branch of the civil law, but they are sometimes counted as separate from the civil law tradition. In Sweden, for instance, case law arguably plays a more important role than in some of the continental civil law systems. The two highest courts, the Supreme Court (\"Högsta domstolen\") and the Supreme Administrative Court (\"Högsta förvaltningsdomstolen\"), have the right to set precedent which has persuasive authority on all future application of the law. Appellate courts, be they judicial (\"hovrätter\") or administrative (\"kammarrätter\"), may also issue decisions that act as guides for the application of the law, but these decisions are persuasive, not controlling, and may therefore be overturned by higher courts.\n\nSome mixed systems, such as Scots law in Scotland, South-African law, and the law of Quebec and Louisiana, do not fit into the civil vs. common law dichotomy because they mix portions of both. Such systems may have been heavily influenced by the common law tradition; however, their private law is firmly rooted in the civil law tradition. Because of their position between the two main systems of law, these types of legal systems are sometimes referred to as \"mixed\" systems of law. Louisiana courts, for instance, operate under both \"stare decisis\" and \"jurisprudence constante\". In South Africa, the precedent of higher courts is absolutely or fully binding on lower courts, whereas the precedent of lower courts only has persuasive authority on higher courts; horizontally, precedent is \"prima facie\" or presumptively binding between courts.\n\nLaw professors in common law traditions play a much smaller role in developing case law than professors in civil law traditions. Because court decisions in civil law traditions are brief and not amenable to establishing precedent, much of the exposition of the law in civil law traditions is done by academics rather than by judges; this is called doctrine and may be published in treatises or in journals such as \"Recueil Dalloz\" in France. Historically, common law courts relied little on legal scholarship; thus, at the turn of the twentieth century, it was very rare to see an academic writer quoted in a legal decision (except perhaps for the academic writings of prominent judges such as Coke and Blackstone). Today academic writers are often cited in legal argument and decisions as persuasive authority; often, they are cited when judges are attempting to implement reasoning that other courts have not yet adopted, or when the judge believes the academic's restatement of the law is more compelling than can be found in precedent. Thus common law systems are adopting one of the approaches long common in civil law jurisdictions.\n\nJustice Louis Brandeis, in a heavily footnoted dissent to \"Burnet v. Coronado Oil & Gas Co.\", 285 U.S. 393, 405-411 (1932), explained (citations and quotations omitted):\n\nThe United States Court of Appeals for the Third Circuit has stated:\n\nThe United States Court of Appeals for the Ninth Circuit has stated:\n\nJustice McHugh of the High Court of Australia in relation to precedents remarked in \"Perre v Apand\":\n\nPrecedent viewed against passing time can serve to establish trends, thus indicating the next logical step in evolving interpretations of the law. For instance, if immigration has become more and more restricted under the law, then the next legal decision on that subject may serve to restrict it further still. The existence of submerged precedent (reasoned opinions not made available through conventional legal research sources) has been identified as a potentially distorting force in the evolution of law.\n\nScholars have recently attempted to apply network theory to precedent in order to establish which precedent is most important or authoritative, and how the court's interpretations and priorities have changed over time.\n\nEarly English common law did not have or require the \"stare decisis\" doctrine for a range of legal and technological reasons:\n\nThese features changed over time, opening the door to the doctrine of \"stare decisis\":\n\n\"Stare decisis\" applies to the holding of a case, rather than to obiter dicta (\"things said by the way\"). As the United States Supreme Court has put it: \"dicta may be followed if sufficiently persuasive but are not binding.\"\n\nIn the United States Supreme Court, the principle of stare decisis is most flexible in constitutional cases:\n\nFor example, in the years 1946–1992, the U.S. Supreme Court reversed itself in about 130 cases. The U.S. Supreme Court has further explained as follows:\n\nThe United States Supreme Court has stated that where a court gives multiple reasons for a given result, each alternative reason that is \"explicitly\" labeled by the court as an \"independent\" ground for the decision is not treated as \"simply a dictum\".\n\nThe doctrine of binding precedent or \"stare decisis\" is basic to the English legal system. Special features of the English legal system include the following:\n\nThe British House of Lords, as the court of last appeal outside Scotland before it was replaced by the UK Supreme Court, was not strictly bound to always follow its own decisions until the case \"London Street Tramways v London County Council [1898] AC 375\". After this case, once the Lords had given a ruling on a point of law, the matter was closed unless and until Parliament made a change by statute. This is the most strict form of the doctrine of \"stare decisis\" (one not applied, previously, in common law jurisdictions, where there was somewhat greater flexibility for a court of last resort to review its own precedent).\n\nThis situation changed, however, after the issuance of the Practice Statement of 1966. It enabled the House of Lords to adapt English law to meet changing social conditions. In \"R v G & R\" 2003, the House of Lords overruled its decision in \"Caldwell\" 1981, which had allowed the Lords to establish mens rea (\"guilty mind\") by measuring a defendant's conduct against that of a \"reasonable person,\" regardless of the defendant's actual state of mind.\n\nHowever, the Practice Statement has been seldom applied by the House of Lords, usually only as a last resort. As of 2005, the House of Lords has rejected its past decisions no more than 20 times. They are reluctant to use it because they fear to introduce uncertainty into the law. In particular, the Practice Statement stated that the Lords would be especially reluctant to overrule themselves in criminal cases because of the importance of certainty of that law. The first case involving criminal law to be overruled with the Practice Statement was \"Anderton v Ryan\" (1985), which was overruled by \"R v Shivpuri\" (1986), two decades after the Practice Statement. Remarkably, the precedent overruled had been made only a year before, but it had been criticised by several academic lawyers. As a result, Lord Bridge stated he was \"undeterred by the consideration that the decision in \"Anderton v Ryan\" was so recent. The Practice Statement is an effective abandonment of our pretention to infallibility. If a serious error embodied in a decision of this House has distorted the law, the sooner it is corrected the better.\" Still, the House of Lords has remained reluctant to overrule itself in some cases; in \"R v Kansal\" (2002), the majority of House members adopted the opinion that \"R v Lambert\" had been wrongly decided and agreed to depart from their earlier decision.\n\nA precedent does not bind a court if it finds there was a lack of care in the original \"Per Incuriam\". For example, if a statutory provision or precedent had not been brought to the previous court's attention before its decision, the precedent would not be binding.\n\nOne of the most important roles of precedent is to resolve ambiguities in other legal texts, such as constitutions, statutes, and regulations. The process involves, first and foremost, consultation of the plain language of the text, as enlightened by the legislative history of enactment, subsequent precedent, and experience with various interpretations of similar texts.\n\nA judge's normal aids include access to all previous cases in which a precedent has been set, and a good English dictionary.\n\nJudges and barristers in the U.K use three primary rules for interpreting the law.\n\nUnder the literal rule, the judge should do what the actual legislation states rather than trying to do what the judge thinks that it means. The judge should use the plain everyday ordinary meaning of the words, even if this produces an unjust or undesirable outcome. A good example of problems with this method is \"R v Maginnis\" (1987), in which several judges in separate opinions found several different dictionary meanings of the word \"supply\". Another example is \"Fisher v Bell\", where it was held that a shopkeeper who placed an illegal item in a shop window with a price tag did not make an offer to sell it, because of the specific meaning of \"offer for sale\" in contract law. As a result of this case, Parliament amended the statute concerned to end this discrepancy.\n\nThe golden rule is used when use of the literal rule would obviously create an absurd result. The court must find genuine difficulties before it declines to use the literal rule. There are two ways in which the golden rule can be applied: the narrow method, and the broad method. Under the narrow method, when there are apparently two contradictory meanings to a word used in a legislative provision or it is ambiguous, the least absurd is to be used. For example, in \"Adler v George\" (1964), the defendant was found guilty under the Official Secrets Act of 1920. The act said it was an offence to obstruct HM Forces in the vicinity of a prohibited place. Adler argued that he was not in the \"vicinity\" of a prohibited place but was actually \"in\" a prohibited place. The court chose not to accept the wording literally. Under the broad method, the court may reinterpret the law at will when it is clear that there is only one way to read the statute. This occurred in \"Re Sigsworth\" (1935) where a man who murdered his mother was forbidden from inheriting her estate, despite a statute to the contrary.\n\nThe mischief rule is the most flexible of the interpretation methods. Stemming from \"Heydon's Case\" (1584), it allows the court to enforce what the statute is intended to remedy rather than what the words actually say. For example, in \"Corkery v Carpenter\" (1950), a man was found guilty of being drunk in charge of a carriage, although in fact he only had a bicycle.\n\nIn the United States, the courts have stated consistently that the text of the statute is read as it is written, using the ordinary meaning of the words of the statute.\n\nHowever, most legal texts have some lingering ambiguity—inevitably, situations arise in which the words chosen by the legislature do not address the precise facts in issue, or there is some tension among two or more statutes. In such cases, a court must analyze the various available sources, and reach a resolution of the ambiguity. The \"Canons of statutory construction\" are discussed in a separate article. Once the ambiguity is resolved, that resolution has binding effect as described in the rest of this article.\n\nAlthough inferior courts are bound in theory by superior court precedent, in practice a judge may believe that justice requires an outcome at some variance with precedent, and may distinguish the facts of the individual case on reasoning that does not appear in the binding precedent. On appeal, the appellate court may either adopt the new reasoning, or reverse on the basis of precedent. On the other hand, if the losing party does not appeal (typically because of the cost of the appeal), the lower court decision may remain in effect, at least as to the individual parties.\n\nOccasionally, a lower court judge explicitly states personal disagreement with the judgment he or she has rendered, but that he or she is required to do so by binding precedent. Note that inferior courts cannot evade binding precedent of superior courts, but a court can depart from its own prior decisions.\n\nIn the United States, \"stare decisis\" can interact in counterintuitive ways with the federal and state court systems. On an issue of federal law, a state court is not bound by an interpretation of federal law at the district or circuit level, but is bound by an interpretation by the United States Supreme Court. On an interpretation of state law, whether common law or statutory law, the federal courts are bound by the interpretation of a state court of last resort, and are required normally to defer to the precedent of intermediate state courts as well.\n\nCourts may choose to obey precedent of international jurisdictions, but this is not an application of the doctrine of \"stare decisis\", because foreign decisions are not binding. Rather, a foreign decision that is obeyed on the basis of the soundness of its reasoning will be called \"persuasive authority\" — indicating that its effect is limited to the persuasiveness of the reasons it provides.\n\nOriginalism is an approach to interpretation of a legal text in which controlling weight is given to the intent of the original authors (at least the intent as inferred by a modern judge). In contrast, a non-originalist looks at other cues to meaning, including the current meaning of the words, the pattern and trend of other judicial decisions, changing context and improved scientific understanding, observation of practical outcomes and \"what works,\" contemporary standards of justice, and \"stare decisis\". Both are directed at \"interpreting\" the text, not changing it—interpretation is the process of resolving ambiguity and choosing from among possible meanings, not changing the text.\n\nThe two approaches look at different sets of underlying facts that may or may not point in the same direction--\"stare decisis\" gives most weight to the newest understanding of a legal text, while originalism gives most weight to the oldest. While they don't necessarily reach different results in every case, the two approaches are in direct tension. Originalists such as Justice Antonin Scalia argue that \"\"Stare decisis\" is not usually a doctrine used in civil law systems, because it violates the principle that only the legislature may make law.\" Justice Scalia argues that America is a civil law nation, not a common law nation. By principle, originalists are generally unwilling to defer to precedent when precedent seems to come into conflict with the originalist's own interpretation of the Constitutional text or inferences of original intent (even in situations where there is no original source statement of that original intent). However, there is still room within an originalist paradigm for \"stare decisis\"; whenever the plain meaning of the text has alternative constructions, past precedent is generally considered a valid guide, with the qualifier being that it cannot change what the text actually says.\n\nOriginalists vary in the degree to which they defer to precedent. In his confirmation hearings, Justice Clarence Thomas answered a question from Senator Strom Thurmond, qualifying his willingness to change precedent in this way:\n\nPossibly he has changed his mind, or there are a very large body of cases which merit \"the additional step\" of ignoring the doctrine; according to Scalia, \"Clarence Thomas doesn't believe in stare decisis, period. If a constitutional line of authority is wrong, he would say, let's get it right.\"\n\nProfessor Caleb Nelson, a former clerk for Justice Thomas and law professor at the University of Virginia, has elaborated on the role of \"stare decisis\" in originalist jurisprudence:\n\nThere are disadvantages and advantages of binding precedent, as noted by scholars and jurists.\n\nIn a 1997 book, attorney Michael Trotter blamed over-reliance by American lawyers on binding and persuasive authority, rather than the merits of the case at hand, as a major factor behind the escalation of legal costs during the 20th century. He argued that courts should ban the citation of persuasive precedent from outside their jurisdiction, with two exceptions:\n\nThe disadvantages of \"stare decisis\" include its rigidity, the complexity of learning law, the differences between some cases may be very small and appear illogical, and the slow growth or incremental changes to the law that are in need of major overhaul.\n\nAn argument often used against the system is that it is undemocratic as it allows judges, which may or may not be elected, to make law.\n\nRegarding constitutional interpretations, there is concern that over-reliance on the doctrine of \"stare decisis\" can be subversive. An erroneous precedent may at first be only slightly inconsistent with the Constitution, and then this error in interpretation can be propagated and increased by further precedent until a result is obtained that is greatly different from the original understanding of the Constitution. \"Stare decisis\" is not mandated by the Constitution, and if it causes unconstitutional results then the historical evidence of original understanding can be re-examined. In this opinion, predictable fidelity to the Constitution is more important than fidelity to unconstitutional precedent. See also the living tree doctrine.\n\nA counter-argument (in favor of the advantages of \"stare decisis\") is that if the legislature wishes to alter the case law (other than constitutional interpretations) by statute, the legislature is empowered to do so. Critics sometimes accuse particular judges of applying the doctrine selectively, invoking it to support precedent that the judge supported anyway, but ignoring it in order to change precedent with which the judge disagreed.\n\nThere is much discussion about the virtue of using \"stare decisis\". Supporters of the system, such as minimalists, argue that obeying precedent makes decisions \"predictable\". For example, a business person can be reasonably assured of predicting a decision where the facts of his or her case are sufficiently similar to a case decided previously. This parallels the arguments against retroactive (ex post facto) laws banned by the U.S. Constitution.\n\n"
            ],
            "good_message": false,
            "loggedData": [
                {
                    "curr_message": " Other descriptions for such courts include court of last resort, apex court, and highest (or final) court of appeal.",
                    "last_key_pressed": "<<selected>>"
                }
            ],
            "offensive": null,
            "researchTask": true,
            "selectedData": [
                "Other descriptions for such courts include court of last resort, apex court, and highest (or final) court of appeal."
            ],
            "shown_passages": [
                [
                    "Supreme court",
                    [
                        "A supreme court is the highest court within the hierarchy of courts in many legal jurisdictions.",
                        "Other descriptions for such courts include court of last resort, apex court, and highest (or final) court of appeal.",
                        "Broadly speaking, the decisions of a supreme court are not subject to further review by any other court.",
                        "Supreme courts typically function primarily as appellate courts, hearing appeals from decisions of lower trial courts, or from intermediate-level appellate courts.",
                        "However, not all highest courts are named as such.",
                        "Civil law states tend not to have a single highest court.",
                        "Additionally, the highest court in some jurisdictions is not named the \"Supreme Court\", for example, the High Court of Australia; this is because decisions by the High Court could formerly be appealed to the Privy Council."
                    ]
                ],
                [
                    "Caribbean Court of Justice",
                    [
                        "The Caribbean Court of Justice (CCJ; ; ) is the judicial institution of the Caribbean Community (CARICOM).",
                        "Established in 2001, it is based in Port of Spain, Trinidad and Tobago.",
                        "The CCJ sits at 134 Henry Street in Port of Spain.",
                        "The Caribbean Court of Justice has two jurisdictions: an original jurisdiction and an appellate jurisdiction:  In the aftermath of the collapse of the Federation of the West Indies (and with it the Federal Supreme Court), which had lasted a mere four years, from 1958 to 1962, the Anglophone continental and insular Caribbean states formed the CARIFTA (the Caribbean Free Trade Association), with a view to maintaining an economic link among the various former and continuing colonies of the United Kingdom after the collapse of the political bond."
                    ]
                ],
                [
                    "Precedent",
                    [
                        "In legal systems based on common law, a precedent, or authority, is a principle or rule established in a previous legal case that is either binding on or persuasive for a court or other tribunal when deciding subsequent cases with similar issues or facts .",
                        "Common law legal systems place great value on deciding cases according to consistent principled rules so that similar facts will yield similar and predictable outcomes, and observance of precedent is the mechanism by which that goal is attained.",
                        "The principle by which judges are bound to precedents is known as stare decisis.",
                        "Black's Law Dictionary defines \"precedent\" as a \"rule of law established for the first time by a court for a particular type of case and thereafter referred to in deciding similar cases\"."
                    ]
                ]
            ],
            "speaker": "0_Wizard",
            "suggestionsConfig": "show",
            "suggestionsNumber": 5,
            "suggestionsShown": true,
            "text": " Other descriptions for such courts include court of last resort, apex court, and highest (or final) court of appeal.",
            "textboxFilled": 0,
            "time": 1580233287.1551855,
            "timeTaken": 29.275,
            "totalTasks": 120,
            "turn": 10
        },
        {
            "checked_passage": {
                "no_passages_used": "no_passages_used"
            },
            "checked_sentence": {
                "no_passages_used": "no_passages_used"
            },
            "clicked_passages": {},
            "context": {
                "conversation": [
                    "I know I have to take vitamin C but why actually?",
                    "Well it is an essential vitamin used in the repair of soft tissue in the body.",
                    "How can I get it naturally as I don't like these supplements.",
                    "People always think of orange juice, but it is available in such much more like broccoli, sprouts, peppers, strawberries and all citrus fruits.",
                    "Wow thats a lot of options and I do like oranges but sprouts I hate. What happens if you don't have enough Vitamin C?",
                    "It can cause scurvy. You may have seen it on films with sailors years ago.  Its swolen bleeding gums, unhealing wounds, it sounds horrible!"
                ],
                "index": 9
            },
            "contextCount": 0,
            "full_passages": [
                "Scurvy\n\nScurvy is a disease resulting from a lack of vitamin C. Early symptoms include weakness, feeling tired, and sore arms and legs. Without treatment, decreased red blood cells, gum disease, changes to hair, and bleeding from the skin may occur. As scurvy worsens there can be poor wound healing, personality changes, and finally death from infection or bleeding.\nTypically, scurvy is caused by a lack of vitamin C in the diet. It takes at least a month of little to no vitamin C before symptoms occur. In modern times, it occurs most commonly in people with mental disorders, unusual eating habits, alcoholism, and old people who live alone. Other risk factors include intestinal malabsorption and dialysis. Humans and certain other animals require vitamin C in their diets to make the building blocks for collagen. Diagnosis typically is based on physical signs, X-rays, and improvement after treatment.\nTreatment is with vitamin C supplements taken by mouth. Improvement often begins in a few days with complete recovery in a few weeks. Sources of vitamin C in the diet include citrus fruit and a number of vegetables such as tomatoes and potatoes. Cooking often decreases vitamin C in foods.\nScurvy currently is rare. It occurs more often in the developing world in association with malnutrition. Rates among refugees are reported at 5% to 45%. Scurvy was described as early as the time of ancient Egypt. It was a limiting factor in long distance sea travel, often killing large numbers of people. A Scottish surgeon in the Royal Navy, James Lind, was the first to prove it could be treated with citrus fruit in a 1753 publication. His experiments represented the first controlled trial. It took another 40 years before the British Navy began giving out lemon juice routinely.\n\nEarly symptoms are malaise and lethargy. Even earlier might be a pain in a section of the gums which interferes with digestion. After 1–3 months, patients develop shortness of breath and bone pain. Myalgias may occur because of reduced carnitine production. Other symptoms include skin changes with roughness, easy bruising and petechiae, gum disease, loosening of teeth, poor wound healing, and emotional changes (which may appear before any physical changes). Dry mouth and dry eyes similar to Sjögren's syndrome may occur. In the late stages, jaundice, generalized edema, oliguria, neuropathy, fever, convulsions, and eventual death are frequently seen.\nScurvy or subclinical scurvy is caused by a deficiency of vitamin C. In modern Western societies, scurvy is rarely present in adults, although infants and elderly people are affected. Virtually all commercially available baby formulas contain added vitamin C, preventing infantile scurvy. Human breast milk contains sufficient vitamin C, if the mother has an adequate intake. Commercial milk is pasteurized, a heating process that destroys the natural vitamin C content of the milk.\n\nScurvy is one of the accompanying diseases of malnutrition (other such micronutrient deficiencies are beriberi or pellagra) and thus is still widespread in areas of the world depending on external food aid. Although rare, there are also documented cases of scurvy due to poor dietary choices by people living in industrialized nations.\n\nAscorbic acid is needed for a variety of biosynthetic pathways, by accelerating hydroxylation and amidation reactions. In the synthesis of collagen, ascorbic acid is required as a cofactor for prolyl hydroxylase and lysyl hydroxylase. These two enzymes are responsible for the hydroxylation of the proline and lysine amino acids in collagen. Hydroxyproline and hydroxylysine are important for stabilizing collagen by cross-linking the propeptides in collagen. Defective collagen fibrillogenesis impairs wound healing. Collagen is an important part of bone, so bone formation is affected. Defective connective tissue leads to fragile capillaries, resulting in abnormal bleeding. Untreated scurvy is invariably fatal.\n\nDiagnosis typically is based on physical signs, X-rays, and improvement after treatment.\n\nScurvy can be prevented by a diet that includes vitamin C-rich foods such as bell peppers (sweet peppers), blackcurrants, broccoli, chili peppers, guava, kiwifruit, and parsley. Other sources rich in vitamin C are fruits such as lemons, oranges, papaya, and strawberries. It is also found in vegetables, such as brussels sprouts, cabbage, potatoes, and spinach. Some fruits and vegetables not high in vitamin C may be pickled in lemon juice, which is high in vitamin C. Though redundant in the presence of a balanced diet, various nutritional supplements are available that provide ascorbic acid well in excess of that required to prevent scurvy.\n\nSome animal products, including liver, Muktuk (whale skin), oysters, and parts of the central nervous system, including the adrenal medulla, brain, and spinal cord, contain large amounts of vitamin C, and can even be used to treat scurvy. Fresh meat from animals which make their own vitamin C (which most animals do) contains enough vitamin C to prevent scurvy, and even partly treat it. In some cases (notably French soldiers eating fresh horse meat), it was discovered that meat alone, even partly cooked meat, could alleviate scurvy. Conversely, in other cases, a meat-only diet could cause scurvy.\n\nScott's 1902 Antarctic expedition used lightly fried seal meat and liver, whereby complete recovery from incipient scurvy was reported to have taken less than two weeks.\n\nHippocrates documented scurvy as a disease, and Egyptians have recorded its symptoms as early as 1550 BCE. The knowledge that consuming foods containing vitamin C is a cure for scurvy has been repeatedly rediscovered and forgotten into the early 20th century.\n\nIn the 13th century, the Crusaders frequently suffered from scurvy. In the 1497 expedition of Vasco de Gama, the curative effects of citrus fruit were already known and confirmed by Pedro Álvares Cabral and his crew in 1507.\n\nThe Portuguese planted fruit trees and vegetables in Saint Helena, a stopping point for homebound voyages from Asia, and left their sick, suffering from scurvy and other ailments, to be taken home, if they recovered, by the next ship.\n\nIn 1500, one of the pilots of Cabral's fleet bound for India noted that in Malindi, its king offered the expedition fresh supplies such as lambs, chickens, and ducks, along with lemons and oranges, due to which \"some of our ill were cured of scurvy\".\n\nUnfortunately, these travel accounts did not stop further maritime tragedies caused by scurvy, first because of the lack of communication between travelers and those responsible for their health, and because fruits and vegetables could not be kept for long on ships.\n\nIn 1536, the French explorer Jacques Cartier, exploring the St. Lawrence River, used the local natives' knowledge to save his men who were dying of scurvy. He boiled the needles of the arbor vitae tree (Eastern White Cedar) to make a tea that was later shown to contain 50 mg of vitamin C per 100 grams. Such treatments were not available aboard ship, where the disease was most common.\nIn February 1601, Captain James Lancaster, while sailing to Sumatra, landed on the northern coast to specifically obtain lemons and oranges for his crew to stop scurvy. Captain Lancaster conducted an experiment using four ships under his command. One ship's crew received routine doses of lemon juice while the other three ships did not receive any such treatment. As a result, members of the non-treated ships started to become ill, contracting scurvy with many dying as a result.\n\nDuring the Age of Exploration (between 1500 and 1800), it has been estimated that scurvy killed at least two million sailors. Jonathan Lamb wrote: \"In 1499, Vasco da Gama lost 116 of his crew of 170; In 1520, Magellan lost 208 out of 230;...all mainly to scurvy.\"\n\nIn 1593, Admiral Sir Richard Hawkins advocated drinking orange and lemon juice as a means of preventing scurvy.\n\nIn 1614, John Woodall, Surgeon General of the East India Company, published \"The Surgion's Mate\" as a handbook for apprentice surgeons aboard the company's ships. He repeated the experience of mariners that the cure for scurvy was fresh food or, if not available, oranges, lemons, limes, and tamarinds. He was, however, unable to explain the reason why, and his assertion had no impact on the opinions of the influential physicians who ran the medical establishment that scurvy was a digestive complaint.\n\nA 1707 handwritten book by Mrs. Ebot Mitchell, discovered in a house in Hasfield, Gloucestershire, contains a \"Recp.t for the Scurvy\" that consisted of extracts from various plants mixed with a plentiful supply of orange juice, white wine or beer.\n\nIn 1734, the Leiden-based physician Johann Bachstrom published a book on scurvy in which he stated, \"scurvy is solely owing to a total abstinence from fresh vegetable food, and greens; which is alone the primary cause of the disease\", and urged the use of fresh fruit and vegetables as a cure.\n\nHowever, it was not until 1747 that James Lind formally demonstrated that scurvy could be treated by supplementing the diet with citrus fruit, in one of the first ever reported controlled clinical experiments in the history of medicine. In 1753, Lind published \"A Treatise of the Scurvy\", in which he explained the details of his clinical trial\",\" but it occupied only a few paragraphs in a work that was long and complex and had little impact. In fact, Lind himself never actively promoted lemon juice as a single ‘cure’. He shared medical opinion at the time that scurvy had multiple causes – notably hard work, bad water, and the consumption of salt meat in a damp atmosphere which inhibited healthful perspiration and normal excretion - and therefore required multiple solutions. He was also sidetracked\nby the possibilities of producing a concentrated ‘rob’ of lemon juice by boiling it. Unfortunately this process\ndestroyed the vitamin C and was unsuccessful.\n\nDuring the 18th century, disease killed more British sailors than enemy action. It was mainly by scurvy that George Anson, in his celebrated voyage of 1740–1744, lost nearly two-thirds of his crew (1300 out of 2000) within the first 10 months of the voyage. The Royal Navy enlisted 184,899 sailors during the Seven Years' War; 133,708 of these were \"missing\" or died from disease, and scurvy was the leading cause.\n\nAlthough throughout this period sailors and naval surgeons were increasingly convinced that citrus fruits could cure scurvy, the classically trained physicians who ran the medical establishment dismissed this evidence as mere anecdote which did not conform to current theories of disease. Literature championing the cause of citrus juice, therefore, had no practical impact. Medical theory was based on the assumption that scurvy was a disease of internal putrefaction brought on by faulty digestion caused by the hardships of life at sea and the naval diet. Although this basic idea was given different emphases by successive theorists, the remedies they advocated (and which the navy accepted) amounted to little more than the consumption of ‘fizzy drinks’ to activate the digestive system, the most extreme of which was the regular consumption of ‘elixir of vitriol’ – sulphuric acid taken with spirits and barley water, and laced with spices.\n\nIn 1764, a new variant appeared. Advocated by Dr David McBride and Sir John Pringle, Surgeon General of the Army and later President of the Royal Society, this idea was that scurvy was the result of a lack of ‘fixed air’ in the tissues which could be prevented by drinking infusions of malt and wort whose fermentation within the body would stimulate digestion and restore the missing gases. These ideas received wide and influential backing, when James Cook set off to circumnavigate the world (1768–1771) in HM Bark \"Endeavour\", malt and wort were top of the list of the remedies he was ordered to investigate. The others were beer, sour crout and Lind's ‘rob’. The list did not include lemons.\n\nCook did not lose a single man to scurvy, and his report came down in favour of malt and wort, although it is now clear that the reason for the health of his crews on this and other voyages was Cook's regime of shipboard cleanliness, enforced by strict discipline, as well as frequent replenishment of fresh food and green stuffs. Another rule implemented by Cook was his prohibition of the consumption of salt fat skimmed from the ship's copper boiling pans, then a common practice in the Navy. In contact with air the copper formed compounds that prevented the absorption of vitamins by the intestines.\n\nThe first major long distance expedition that experienced virtually no scurvy was that of the Spanish naval officer Alessandro Malaspina, 1789–1794. Malaspina's medical officer, Pedro González, was convinced that fresh oranges and lemons were essential for preventing scurvy. Only one outbreak occurred, during a 56-day trip across the open sea. Five sailors came down with symptoms, one seriously. After three days at Guam all five were healthy again. Spain's large empire and many ports of call made it easier to acquire fresh fruit.\n\nAlthough towards the end of the century MacBride's theories were being challenged, the medical establishment in Britain remained wedded to the notion that scurvy was a disease of internal ‘putrefaction’ and the Sick and Hurt Board, run by administrators, felt obliged to follow its advice. Within the Royal Navy, however, opinion – strengthened by first-hand experience of the use of lemon juice at the siege of Gibraltar and during Admiral Rodney's expedition to the Caribbean – had become increasingly convinced of its efficacy. This was reinforced by the writings of experts like Gilbert Blane and Thomas Trotter and by the reports of up-and-coming naval commanders.\n\nWith the coming of war in 1793, the need to eliminate scurvy acquired a new urgency. But the first initiative came not from the medical establishment but from the admirals. Ordered to lead an expedition against Mauritius, Rear Admiral Gardner was uninterested in the wort, malt and elixir of vitriol which were still being issued to ships of the Royal Navy, and demanded that he be supplied with lemons, to counteract scurvy on the voyage. Members of the Sick and Hurt Board, recently augmented by two practical naval surgeons, supported the request, and the Admiralty ordered that it be done. There was, however, a last minute change of plan. The expedition against Mauritius was cancelled. On 2 May 1794, only HMS \"Suffolk\" and two sloops under Commodore Peter Rainier sailed for the east with an outward bound convoy, but the warships were fully supplied with lemon juice and the sugar with which it had to be mixed. Then in March 1795, came astonishing news. \"Suffolk\" had arrived in India after a four-month voyage without a trace of scurvy and with a crew that was healthier than when it set out.\n\nThe effect was immediate. Fleet commanders clamoured also to be supplied with lemon juice, and by June the Admiralty acknowledged the groundswell of demand in the navy had agreed to a proposal from the Sick and Hurt Board that lemon juice and sugar should in future be issued as a daily ration to the crews of all warships.\n\nIt took a few years before the method of distribution to all ships in the fleet had been perfected and the supply of the huge quantities of lemon juice required to be secured, but by 1800, the system was in place and functioning. This led to a remarkable health improvement among the sailors and consequently played a critical role in gaining the advantage in naval battles against enemies who had yet to introduce the measures.\n\nThe surgeon-in-chief of Napoleon's army at the Siege of Alexandria (1801), Baron Dominique-Jean Larrey, wrote in his memoirs that the consumption of horse meat helped the French to curb an epidemic of scurvy. The meat was cooked but was freshly obtained from young horses bought from Arabs, and was nevertheless effective. This helped to start the 19th-century tradition of horse meat consumption in France.\n\nLauchlin Rose patented a method used to preserve citrus juice without alcohol in 1867, creating a concentrated drink known as Rose's lime juice. The Merchant Shipping Act of 1867 required all ships of the Royal Navy and Merchant Navy to provide a daily lime ration to sailors to prevent scurvy. The product became nearly ubiquitous, hence the term \"limey\", first for British sailors, then for English immigrants within the former British colonies (particularly America, New Zealand and South Africa), and finally, in old American slang, all British people.\n\nThe plant \"Cochlearia officinalis\", also known as \"Common Scurvygrass\", acquired its common name from the observation that it cured scurvy, and it was taken on board ships in dried bundles or distilled extracts. Its very bitter taste was usually disguised with herbs and spices; however, this did not prevent scurvygrass drinks and sandwiches from becoming a popular fad in the UK until the middle of the nineteenth century, when citrus fruits became more readily available.\n\nWest Indian limes began to supplement lemons, when Spain's alliance with France against Britain in the Napoleonic Wars made the supply of Mediterranean lemons problematic, and because they were more easily obtained from Britain's Caribbean colonies and were believed to be more effective because they were more acidic. It was the acid, not the (then-unknown) Vitamin C that was believed to cure scurvy. In fact, the West Indian limes were significantly lower in Vitamin C than the previous lemons and further were not served fresh but rather as lime juice, which had been exposed to light and air, and piped through copper tubing, all of which significantly reduced the Vitamin C. Indeed, a 1918 animal experiment using representative samples of the Navy and Merchant Marine's lime juice showed that it had virtually no antiscorbutic power at all.\n\nThe belief that scurvy was fundamentally a nutritional deficiency, best treated by consumption of fresh food, particularly fresh citrus or fresh meat, was not universal in the 19th and early 20th centuries, and thus sailors and explorers continued to suffer from scurvy into the 20th century. For example, the Belgian Antarctic Expedition of 1897–1899 became seriously affected by scurvy when its leader, Adrien de Gerlache, initially discouraged his men from eating penguin and seal meat.\n\nIn the Royal Navy's Arctic expeditions in the 19th century it was widely believed that scurvy was prevented by good hygiene on board ship, regular exercise, and maintaining the morale of the crew, rather than by a diet of fresh food. Navy expeditions continued to be plagued by scurvy even while fresh (not jerked or tinned) meat was well known as a practical antiscorbutic among civilian whalers and explorers in the Arctic. Even cooking fresh meat did not entirely destroy its antiscorbutic properties, especially as many cooking methods failed to bring all the meat to high temperature.\n\nThe confusion is attributed to a number of factors:\nIn the resulting confusion, a new hypothesis was proposed, following the new germ theory of disease – that scurvy was caused by ptomaine, a waste product of bacteria, particularly in tainted tinned meat.\n\nInfantile scurvy emerged in the late 19th century because children were being fed pasteurized cow's milk, particularly in the urban upper class. While pasteurization killed bacteria, it also destroyed vitamin C. This was eventually resolved by supplementing with onion juice or cooked potatoes.\n\nBy the early 20th century, when Robert Falcon Scott made his first expedition to the Antarctic (1901–1904), the prevailing theory was that scurvy was caused by \"ptomaine poisoning\", particularly in tinned meat. However, Scott discovered that a diet of fresh meat from Antarctic seals cured scurvy before any fatalities occurred.\n\nIn 1907, an animal model which would eventually help to isolate and identify the \"antiscorbutic factor\" was discovered. Axel Holst and Theodor Frølich, two Norwegian physicians studying shipboard beriberi contracted by ship's crews in the Norwegian Fishing Fleet, wanted a small test mammal to substitute for the pigeons then used in beriberi research. They fed guinea pigs their test diet of grains and flour, which had earlier produced beriberi in their pigeons, and were surprised when classic scurvy resulted instead. This was a serendipitous choice of animal. Until that time, scurvy had not been observed in any organism apart from humans and had been considered an exclusively human disease. Certain birds, mammals, and fish are susceptible to scurvy, but pigeons are unaffected, since they can synthesize ascorbic acid internally. Holst and Frølich found they could cure scurvy in guinea pigs with the addition of various fresh foods and extracts. This discovery of an animal experimental model for scurvy, which was made even before the essential idea of \"vitamins\" in foods had been put forward, has been called the single most important piece of vitamin C research.\n\nVilhjalmur Stefansson, an arctic explorer who had lived among the Inuit, proved that the all-meat diet they consumed did not lead to vitamin deficiencies. He participated in a study in New York's Bellevue Hospital in February 1928, where he and a companion ate only meat for a year while under close medical observation, yet remained in good health.\n\nIn 1927, Hungarian biochemist Szent-Györgyi isolated a compound he called \"hexuronic acid\". Szent-Györgyi suspected hexuronic acid, which he had isolated from adrenal glands, to be the antiscorbutic agent, but he could not prove it without an animal-deficiency model. In 1932, the connection between hexuronic acid and scurvy was finally proven by American researcher Charles Glen King of the University of Pittsburgh. King's laboratory was given some hexuronic acid by Szent-Györgyi and soon established that it was the sought-after anti-scorbutic agent. Because of this, hexuronic acid was subsequently renamed \"ascorbic acid.\"\n\nRates of scurvy in most of the world are low. Those most commonly affected are malnourished people in the developing world and the homeless. There have been outbreaks of the condition in refugee camps. Case reports in the developing world of those with poorly healing wounds have occurred.\n\nNotable human dietary studies of experimentally induced scurvy have been conducted on conscientious objectors during WWII in Britain and on Iowa state prisoner volunteers in the late 1960s. These studies both found that all obvious symptoms of scurvy previously induced by an experimental scorbutic diet with extremely low vitamin C content could be completely reversed by additional vitamin C supplementation of only 10 mg per day. In these experiments, no clinical difference was noted between men given 70 mg vitamin C per day (which produced blood levels of vitamin C of about 0.55 mg/dl, about 1/3 of tissue saturation levels), and those given 10 mg per day (which produced lower blood levels). Men in the prison study developed the first signs of scurvy about 4 weeks after starting the vitamin C-free diet, whereas in the British study, six to eight months were required, possibly because the subjects were pre-loaded with a 70 mg/day supplement for six weeks before the scorbutic diet was fed.\n\nMen in both studies, on a diet devoid or nearly devoid of vitamin C, had blood levels of vitamin C too low to be accurately measured when they developed signs of scurvy, and in the Iowa study, at this time were estimated (by labeled vitamin C dilution) to have a body pool of less than 300 mg, with daily turnover of only 2.5 mg/day.\n\nAlmost all plant and animal species synthesize vitamin C. Notable mammalian exceptions include most or all of the order Chiroptera (bats), and one of the two major primate suborders, the \"Anthropoidea\" (Haplorrhini) which include tarsiers, monkeys, and apes, including human beings. The Strepsirrhini (non-tarsier prosimians) can make their own vitamin C, and these include lemurs, lorises, pottos, and galagos. Ascorbic acid is also not synthesized by at least two species of Caviidae, the capybara and the guinea pig. There are known species of birds and fish that do not synthesize their own Vitamin C. All species that do not synthesize ascorbate require it in the diet. Deficiency causes scurvy in humans, and somewhat similar symptoms in other animals.\n\nIn babies, scurvy is sometimes referred to as Barlow's disease, named after Thomas Barlow, a British physician who described it in 1883. However, Barlow's disease or Barlow's syndrome may also refer to mitral valve prolapse, first described by John Brereton Barlow in 1966.\n\n",
                "'Round Springfield\n\n\"'Round Springfield\" is the 22nd episode of \"The Simpsons\" sixth season. It originally aired on the Fox network in the United States on April 30, 1995. In the episode, Bart is rushed to the hospital after eating a jagged metal Krusty-O and decides to sue Krusty the Clown. Whilst visiting Bart, Lisa meets her old mentor, jazz musician Bleeding Gums Murphy. She is saddened when she later learns that Murphy has died, and resolves to honor his memory. Steve Allen (as himself) and Ron Taylor (as Bleeding Gums Murphy) guest star, each in their second appearance on the show. Dan Higgins also returns as the writer and performer of all of Lisa and Bleeding Gums' saxophone solos.\n\nIt was written by Joshua Sternin and Jeffrey Ventimilia, based on a story idea by Al Jean and Mike Reiss and was the first episode directed by Steven Dean Moore. Jean and Reiss, who were previously the series' showrunners, returned to produce this episode (as well as \"A Star Is Burns\") in order to lessen the workload of the show's regular staff. They worked on it alongside the staff of \"The Critic\", the series they had left \"The Simpsons\" to create. The episode marks the first time in which a recurring character was killed off in the show, something the staff had considered for a while. The episode features numerous cultural references, including Carole King's song \"Jazzman\", the actor James Earl Jones and the \"Kimba the White Lion/The Lion King\" controversy.\n\nThe episode also features the phrase \"cheese-eating surrender monkeys\", used by Groundskeeper Willie to describe the French. The phrase has since entered the public lexicon. It has been used and referenced by journalists and academics; it appears in two Oxford quotation dictionaries.\n\nOn the day of a history test at school, Bart gets a stomachache after accidentally eating a jagged metal Krusty-O with his breakfast cereal. Lisa is the only one who believes him, however, and Homer and Marge send him to school anyway. Bart struggles through the test, and after finally convincing Mrs. Krabappel that he is actually ill, she lets him go and see the nurse. Bart collapses on the medical room floor, and is taken to Springfield General Hospital where he undergoes surgery from Dr. Hibbert and Dr. Nick, as it emerges he has appendicitis. While visiting Bart in the hospital, Lisa meets her hero, jazzman Bleeding Gums Murphy, in a bed in another ward.\n\nLater, Bleeding Gums lends Lisa his saxophone for her school recital. Returning to the hospital the next day, Lisa is saddened to learn that Bleeding Gums has died. Lisa is the only person who attends Bleeding Gums' funeral, and she vows to make sure that everyone in Springfield knows the name Bleeding Gums Murphy. Bart, meanwhile, sues Krusty the Clown and is given a $100,000 settlement. However, after Bart's attorney Lionel Hutz takes his \"legal fees\", Bart is left with only $500.\n\nStill stricken with grief, Lisa decides that the best way to honor Bleeding Gums' memory is by having his album played at the local jazz station. Lisa spots it in the Android Dungeon store for $250, but after hearing that Bleeding Gums is dead, Comic Book Guy doubles the price to $500. Bart then walks in with his $500 to buy the ultimate pog with Steve Allen's face. However, after remembering that Lisa was the only one who believed him about his stomachache, Bart decides to buy the album with his money. Lisa thanks him for it, and when the station plays one of Bleeding Gums's songs, Lisa is disappointed because the station's tiny range still prevents anyone from hearing it. Lightning then strikes the antenna, giving it extra power and projecting it into every radio in Springfield. Lisa is finally satisfied. After proclaiming \"that was for you Bleeding Gums\", she turns to leave, but not before Bleeding Gums appears from the heavens to tell Lisa that she had made \"an old jazz man happy\". After saying a final goodbye, Lisa and Bleeding Gums jam to \"Jazzman\" one last time.\n\n\"'Round Springfield\" was written by Joshua Sternin and Jeffrey Ventimilia, based on a story idea by Al Jean and Mike Reiss. It was the first episode directed by Steven Dean Moore. Due to Fox's demand for 24 to 25 episodes per season, which the production staff found impossible to meet, two episodes of each season were written and produced by former showrunners, to relieve the stress on \"The Simpsons\" writing staff. Jean and Reiss, who were showrunners for the show's third and fourth seasons, returned to produce this episode, as well as \"A Star Is Burns\", instead of the season's main showrunner David Mirkin. On both episodes, they were aided by the staff of \"The Critic\", the show the two left \"The Simpsons\" to create. Sternin and Ventimilia were writers on \"The Critic\" and were big fans of \"The Simpsons\", so were thrilled to be able to write an episode.\nThis episode marked the first time a recurring character has been killed off on the show. The writers and production team felt that it would be a good, emotional storyline, which, through Lisa, could focus on the theme of grief. They decided that it could not be one of the main characters; Jean joked that \"we wouldn't want it to be someone like Mr. Burns, that we'd obviously want to see in the show again\". Eventually, Jean decided on Bleeding Gums Murphy, a character introduced in the season one episode \"Moaning Lisa\"; a flashback to \"Moaning Lisa\" is featured in the episode. Murphy was a fairly minor character, only appearing in a couple of episodes, but he appeared in the show's opening sequence and remained there after this episode, until the opening was re-designed in season 20. Moore's first ever job on the show was in the animation department for \"Moaning Lisa\" so he \"appreciated\" being able to direct this episode. Reiss stated, \"I had been polling for years to kill Marge's mom but this was a better idea\". Actor Ron Taylor returned to guest star as Murphy in the episode. Comedian Steve Allen also made his second guest appearance on the show, having previously appeared in the episode \"Separate Vocations\".\n\nThe main story of the episode's first act sees Bart get appendicitis from eating a jagged metal Krusty-O. Mike Reiss's father, being a doctor, \"sort of\" acted as the medical consultant on this episode. He stated that \"you can't get\" appendicitis from eating a piece of metal, but the writers decided to do it anyway. In his flashback, Murphy is shown as having a \"$1,500 a day Fabergé egg habit\"; Jean \"didn't realize just how expensive\" Fabergé eggs actually were, so the joke does not make much sense.\n\nThe episode contains numerous references to popular culture. The title is a play on both the jazz standard \"'Round Midnight\" by Thelonious Monk and the similarly named film also about an unappreciated jazz musician. When a deceased Bleeding Gums Murphy appears to Lisa in a cloud towards the end of the episode, he is joined by Darth Vader, Mufasa, and James Earl Jones. Although all three roles were portrayed by Jones, the characters in this scene were impersonated by cast member Harry Shearer; Jones himself guest starred twice previously. Additionally, Mufasa accidentally mentions \"Kimba\" and corrects himself by saying \"Simba\". This is a reference to the debate regarding \"The Lion King\"s resemblance to the anime \"Kimba the White Lion\". Lisa and Bleeding Gums play Carole King's song \"Jazzman\" in this scene and in the hospital earlier in the episode. Bleeding Gums has to leave at the end of the scene because he has a date with the jazz singer Billie Holiday.\n\nAdditionally, Homer has a Starland Vocal Band tattoo on his arm, Bart considers buying a Steve Allen \"ultimate pog\", and the music heard just before Bart's operation is a parody of the theme music of \"ER\". Bleeding Gums appears on an episode of \"The Cosby Show\", a reference to Bill Cosby often getting jazz musicians he liked to appear on the show; in the episode, Cosby is voiced by \"The Simpsons\" regular Dan Castellaneta. Lionel Hutz's \"crack team of lawyers\", Robert Shaporo and Albert Dershman, are parodies of Robert Shapiro and Alan Dershowitz, two of the defense attorneys at the O. J. Simpson murder case. The three drive away in a white pickup truck, similar to the Ford Bronco that Al Cowlings and O.J. Simpson drove in their televised low-speed pursuit before Simpson's arrest.\n\nIn its original broadcast, \"'Round Springfield\" finished 60th in the ratings for the week of April 24 to April 30, 1995, with a Nielsen rating of 8.2. The episode was the fourth highest rated show on the Fox network that week. Mike Reiss and Al Jean thought that the episode would \"get a ton of awards\", and joked that this was why they opted to receive a story credit, which they usually would not. Ultimately it did not win any awards. Warren Martyn and Adrian Wood, the authors of the book \"I Can't Believe It's a Bigger and Better Updated Unofficial Simpsons Guide\", found that it was \"a real tear-jerker\" and praised Grampa believing everything he saw was death. In a DVD review of the sixth season, Ryan Keefer of DVD Verdict rated the episode a \"B\". Adam Finley of \"TV Squad\" praised the episode, noting its many \"great moments\" including \"Steve Allen pimping his books on TV: \"How to Make Love to Steve Allen\"; \"Happiness is a Naked Steve Allen\"; \"Journey to the Center of Steve Allen\"; \"The Joy of Cooking Steve Allen\"\" and \"Moe running a \"retox\" clinic right next to a detox clinic\". On the other hand, Colin Jacobson of DVD Movie Guide called the episode \"dull\", stating that \"some of the moments connected to Bart's illness are funny\", but that he \"really hate[s] that \"Jazzman\" song\" and dislikes \"the Bleeding Gums parts\".\n\nIn the episode, budget cuts at Springfield Elementary School force the janitor Groundskeeper Willie to be used as a French teacher. Expressing his disdain for the French, he exclaims to his class: \"Bonjour, you cheese-eatin' surrender monkeys.\" The quote, particularly the phrase \"cheese-eating surrender monkeys\", has since entered wider use. It was used particularly in the run-up to the war in Iraq, having been popularized by the conservative \"National Review\" journalist Jonah Goldberg, to describe European and especially French opposition to military action. A piece in \"The Guardian\" noted that the phrase was \"made acceptable in official diplomatic channels around the globe\". Ben Macintyre has written that the phrase is \"perhaps the most famous\" of the show's coinages and since Goldberg's usage it \"has gone on to become a journalistic cliché\".\n\nIt has subsequently been used by the \"New York Post\" (as \"Surrender Monkeys\") as the headline for its December 7, 2006, front page, referring to the Iraq Study Group and its recommendation that U.S. soldiers be withdrawn from Iraq by early 2008. The \"Daily Mail\" has used the phrase in reference to the French's \"attitude problem\", and the editor of \"Metro\" used it to describe the \"muted\" European reaction to the death of Osama bin Laden, while \"The Daily Telegraph\" has cited it in relation to Anglo-French military cooperation. The term has been used in books by commentator Laura Ingraham, and academics Stuart Croft, Stephen Chan, and Paul L. Moorcraft and Philip M. Taylor. Ned Sherrin included the quote in the \"Oxford Dictionary of Humorous Quotations\"; it was introduced in the third edition in 2005. It is also included in the \"Oxford Dictionary of Modern Quotations\". Douglas Coupland's 2009 novel \"Generation A\" refers to Groundskeeper Willie's use of the phrase.\n\nThe line was \"probably\" written by Ken Keeler during one of the episode's re-write sessions, although none of those present on the episode's DVD audio commentary could remember for sure. The writers were surprised it became as widely used as it did and never meant it as a political statement, merely as an \"obnoxious\" joke for Willie. The French dub of the show uses the line \"singes mangeurs de fromage\", omitting the word \"surrender\".\n\n",
                "Vitamin C\n\nVitamin C, also known as ascorbic acid and -ascorbic acid, is a vitamin found in food and used as a dietary supplement. The disease scurvy is prevented and treated with vitamin C-containing foods or dietary supplements. Evidence does not support use in the general population for the prevention of the common cold. There is, however, some evidence that regular use may shorten the length of colds. It is unclear if supplementation affects the risk of cancer, cardiovascular disease, or dementia. It may be taken by mouth or by injection.\nVitamin C is generally well tolerated. Large doses may cause gastrointestinal discomfort, headache, trouble sleeping, and flushing of the skin. Normal doses are safe during pregnancy. The United States Institute of Medicine recommends against taking large doses.\nVitamin C is an essential nutrient involved in the repair of tissue and the enzymatic production of certain neurotransmitters. It is required for the functioning of several enzymes and is important for immune system function. It also functions as an antioxidant. Foods containing vitamin C include citrus fruits, broccoli, Brussels sprouts, raw bell peppers, and strawberries. Prolonged storage or cooking may reduce vitamin C content in foods.\nVitamin C was discovered in 1912, isolated in 1928, and in 1933 was the first vitamin to be chemically produced. It is on the World Health Organization Model List of Essential Medicines, the most effective and safe medicines needed in a health system. Vitamin C is available as a generic medication and over-the-counter drug. In 2015, the wholesale cost in the developing world was less than 0.01 per tablet. Partly for its discovery, Albert Szent-Györgyi and Walter Norman Haworth were awarded 1937 Nobel Prizes in Physiology and Medicine and Chemistry, respectively.\nVitamin C is an essential nutrient for certain animals including humans. The term \"vitamin C\" encompasses several vitamers that have vitamin C activity in animals. Ascorbate salts such as sodium ascorbate and calcium ascorbate are used in some dietary supplements. These release ascorbate upon digestion. Ascorbate and ascorbic acid are both naturally present in the body, since the forms interconvert according to pH. Oxidized forms of the molecule such as dehydroascorbic acid are converted back to ascorbic acid by reducing agents.\n\nVitamin C is a cofactor in at least eight enzymatic reactions in animals (and humans) important in many essential functions, including wound healing. In humans, vitamin C deficiency compromises collagen synthesis, contributing to the more severe symptoms of scurvy. More generally, the biochemical role of vitamin C is to act as an antioxidant, a reducing agent, donating electrons to various enzymatic and non-enzymatic reactions. Doing so converts vitamin C to an oxidized state - either as semidehydroascorbic acid or dehydroascorbic acid. These compounds can be restored to a reduced state by glutathione and NADPH-dependent enzymatic mechanisms.\n\nIn plants, vitamin C is a substrate for ascorbate peroxidase. This enzyme utilizes ascorbate to neutralize toxic hydrogen peroxide (HO) by converting it to water (HO).\n\nScurvy is an avitaminosis resulting from lack of vitamin C, since without this vitamin, collagen made by the body is too unstable to perform its function.\n\nScurvy leads to the formation of brown spots on the skin, spongy gums, and bleeding from all mucous membranes. The spots are most abundant on the thighs and legs, and a person with the ailment looks pale, feels depressed, and is partially immobilized. In advanced scurvy there are open, suppurating wounds and loss of teeth and, eventually, death. The human body can store only a certain amount of vitamin C, and so the body stores are depleted if fresh supplies are not consumed. The time frame for onset of symptoms of scurvy in unstressed adults on a completely vitamin C free diet, however, may range from one month to more than six months, depending on previous loading of vitamin C.\n\nNotable human dietary studies of experimentally induced scurvy have been conducted on conscientious objectors during World War II in Britain and on Iowa state prisoners in the late 1960s to the 1980s. These studies both found that all obvious symptoms of scurvy previously induced by an experimental scorbutic diet with extremely low vitamin C content could be completely reversed by additional vitamin C supplementation of only 10 mg a day. In these experiments, there was no clinical difference noted between men given 70 mg vitamin C per day (which produced blood level of vitamin C of about 0.55 mg/dl, about 1/3 of tissue saturation levels) and those given 10 mg per day. Men in the prison study developed the first signs of scurvy about four weeks after starting the vitamin C-free diet, whereas in the British study, six to eight months were required, possibly due to the pre-loading of this group with a 70 mg/day supplement for six weeks before the scorbutic diet was fed.\n\nMen in both studies on a diet devoid, or nearly devoid, of vitamin C had blood levels of vitamin C too low to be accurately measured when they developed signs of scurvy, and in the Iowa study, at this time were estimated (by labeled vitamin C dilution) to have a body pool of less than 300 mg, with daily turnover of only 2.5 mg/day, implying an instantaneous half-life of 83 days by this time (elimination constant of 4 months).\n\nVitamin C has a definitive role in treating scurvy, which is a disease caused by vitamin C deficiency. Beyond that, a role for vitamin C as prevention or treatment for various diseases is disputed, with reviews reporting conflicting results. A 2012 Cochrane review reported no effect of vitamin C supplementation on overall mortality. It is on the World Health Organization's List of Essential Medicines as one of the most effective and safe medicines needed in a health system.\n\nThe disease scurvy is caused by vitamin C deficiency and can be prevented and treated with vitamin C containing foods or dietary supplements. It takes at least a month of little to no vitamin C before symptoms occur. Early symptoms are malaise and lethargy, progressing to shortness of breath, bone pain, bleeding gums, susceptibility to bruising, poor wound healing, and finally fever, convulsions and eventual death. Until quite late in the disease the damage is reversible, as with vitamin C repletion, healthy collagen replaces the defective collagen. Treatment can be orally or by intramuscular or intravenous injection. Scurvy was known to Hippocrates in the classical era. The disease was shown to be prevented by citrus fruit in an early controlled trial by a Royal Navy surgeon, James Lind, in 1747, and from 1796 lemon juice was issued to all Royal Navy crewmen.\n\nThe effect of vitamin C on the common cold has been extensively researched. The earliest publication of a controlled clinical trial appears to be from 1945. Researchers continued to work on this question, but research interest and public interest spiked after Linus Pauling, two-time awardee of the Nobel Prize (Chemistry Prize, 1954, Peace Prize 1962), started publishing research on the topic and also published a book \"Vitamin C and the Common Cold\" in 1970. A revised and expanded edition \"Vitamin C, the Common Cold and the Flu\" was published in 1976.\n\nThe most recent meta-analysis, a Cochrane Review published in 2013, with inclusion criteria limited to trials that called for at least 200 mg/day, concluded that vitamin C taken on a regular basis was not effective in prevention of the common cold. Limiting inclusion to trials that called for at least 1000 mg/day made no difference. However, taking vitamin C on a regular basis did reduce average duration by 8% in adults and 14% in children, and also reduced severity of colds. A subset of trials reported that supplementation reduced the incidence of colds by half in marathon runners, skiers, or soldiers in subarctic conditions. Another subset of trials looked at therapeutic use, meaning that vitamin C was not started unless the people started to feel the beginnings of a cold. In these, vitamin C did not impact duration or severity. An earlier review stated that vitamin C did not prevent colds, did reduce duration, did not reduce severity. The authors of the Cochrane review concluded that \"...given the consistent effect of vitamin C on the duration and severity of colds in the regular supplementation studies, and the low cost and safety, it may be worthwhile for common cold patients to test on an individual basis whether therapeutic vitamin C is beneficial for them.\"\n\nVitamin C distributes readily in high concentrations into immune cells, has antimicrobial and natural killer cell activities, promotes lymphocyte proliferation, and is consumed quickly during infections, effects indicating a prominent role in immune system regulation. The European Food Safety Authority found a cause and effect relationship exists between the dietary intake of vitamin C and functioning of a normal immune system in adults and in children under three years of age.\n\nThere are two approaches to the question of whether vitamin C has an impact on cancer. First, within the normal range of dietary intake without additional dietary supplementation, are people who consume more vitamin C at lower risk for developing cancer, and if so, does an orally consumed supplement have the same benefit? Second, for people diagnosed with cancer, will large amounts of ascorbic acid administered intravenously treat the cancer, reduce the adverse effects of other treatments, and so prolong survival and improve quality of life? A 2013 Cochrane review found no evidence that vitamin C supplementation reduces the risk of lung cancer in healthy or high risk (smokers and asbestos-exposed) people. A 2014 meta-analysis found that vitamin C intake might protect against lung cancer risk. A second meta-analysis found no effect on the risk of prostate cancer. Two meta-analyses evaluated the effect of vitamin C supplementation on the risk of colorectal cancer. One found a weak association between vitamin C consumption and reduced risk, and the other found no effect of supplementation. A 2011 meta-analysis failed to find support for the prevention of breast cancer with vitamin C supplementation, but a second study concluded that vitamin C may be associated with increased survival in those already diagnosed.\n\nUnder the rubric of orthomolecular medicine, \"Intravenous vitamin C is a contentious adjunctive cancer therapy, widely used in naturopathic and integrative oncology settings.\" With oral administration absorption efficiency decreases as amounts increase. Intravenous administration bypasses this. Doing so makes it possible to achieve plasma concentrations of 5 to 10 millimoles/liter (mmol/L), which far exceed the approximately 0.2 mmol/L limit from oral consumption. The theories of mechanism are contradictory. At high tissue concentrations, ascorbic acid is described as acting as a pro-oxidant, generating hydrogen peroxide (HO) to kill tumor cells. The same literature claims that ascorbic acid acts as an antioxidant, thereby reducing the adverse effects of chemotherapy and radiation therapy. Research continues in this field, but a 2014 review concluded: \"Currently, the use of high-dose intravenous vitamin C [as an anticancer agent] cannot be recommended outside of a clinical trial.\" A 2015 review added: \"There is no high-quality evidence to suggest that ascorbate supplementation in cancer patients either enhances the antitumor effects of chemotherapy or reduces its toxicity. Evidence for ascorbate's anti-tumor effects was limited to case reports and observational and uncontrolled studies.\"\n\nA 2013 meta-analysis found no evidence that vitamin C supplementation reduces the risk of myocardial infarction, stroke, cardiovascular mortality, or all-cause mortality. However, a second analysis found an inverse relationship between circulating vitamin C levels or dietary vitamin C and the risk of stroke.\n\nA meta-analysis of 44 clinical trials has shown a significant positive effect of vitamin C on endothelial function when taken at doses greater than 500 mg per day. The endothelium is a layer of cells that line the interior surface of blood vessels. Endothelial dysfunction is implicated in many aspects of vascular diseases. The researchers noted that the effect of vitamin C supplementation appeared to be dependent on health status, with stronger effects in those at higher cardiovascular disease risk.\n\nStudies examining the effects of vitamin C intake on the risk of Alzheimer's disease have reached conflicting conclusions. Maintaining a healthy dietary intake is probably more important than supplementation for achieving any potential benefit. A 2010 review found no role for vitamin C supplementation in the treatment of rheumatoid arthritis. Vitamin C supplementation does not prevent or slow the progression of age-related cataract.\n\nMore than two to three grams may cause indigestion, particularly when taken on an empty stomach. However, taking vitamin C in the form of sodium ascorbate and calcium ascorbate may minimize this effect. Other symptoms reported for large dose include nausea, abdominal cramps and diarrhea. These effects are attributed to the osmotic effect of unabsorbed vitamin C passing through the intestine. In theory, high vitamin C intake may cause excessive absorption of iron. A summary of reviews of supplementation in healthy subjects did not report this problem, but left as untested the possibility that individuals with hereditary hemochromatosis might be adversely affected. There is a longstanding belief among the mainstream medical community that vitamin C increases risk of kidney stones. \"Reports of kidney stone formation associated with excess ascorbic acid intake are limited to individuals with renal disease\". Reviews state that \"data from epidemiological studies do not support an association between excess ascorbic acid intake and kidney stone formation in apparently healthy individuals\", although one large, multi-year trial did report a nearly two-fold increase in kidney stones in men who regularly consumed a vitamin C supplement. Vitamin C is a water-soluble vitamin, with dietary excesses not absorbed, and excesses in the blood rapidly excreted in the urine, so it exhibits remarkably low acute toxicity.\n\nRecommendations for vitamin C intake by adults have been set by various national agencies:\n\nIn 2000 the North American Dietary Reference Intake chapter on vitamin C updated the Recommended Dietary Allowance (RDA) to 90 milligrams per day for adult men and 75 mg/day for adult women, and set a Tolerable upper intake level (UL) for adults of 2,000 mg/day. The table shows RDAs for the United States and Canada for children, and for pregnant and lactating women. For the European Union, the EFSA set higher recommendations for adults, and also for children: 20 mg/day for ages 1–3, 30 mg/day for ages 4–6, 45 mg/day for ages 7–10, 70 mg/day for ages 11–14, 100 mg/day for males ages 15–17, 90 mg/day for females ages 15–17. For pregnancy 100 mg/day; for lactation 155 mg/day. India, on the other hand, has set recommendations much lower: 40 mg/day for ages 1 through adult, 60 mg/day for pregnancy, and 80 mg/day for lactation. Clearly, there is not consensus among countries.\n\nCigarette smokers and people exposed to secondhand smoke have lower plasma vitamin C levels than nonsmokers. The thinking is that inhalation of smoke causes oxidative damage, depleting this antioxidant vitamin.. The U.S. Institute of Medicine estimated that smokers need 35 mg more vitamin C per day than nonsmokers, but did not formally establish a higher RDA for smokers. One meta-analysis showed an inverse relationship between vitamin C intake and lung cancer, although it concluded that more research is needed to confirm this observation.\n\nThe U.S. National Center for Health Statistics conducts biannual National Health and Nutrition Examination Survey (NHANES) to assess the health and nutritional status of adults and children in the United States. Some results are reported as What We Eat In America. The 2013-2014 survey reported that for adults ages 20 years and older, men consumed on average 83.3 mg/d and women 75.1 mg/d. This means that half the women and more than half the men are not consuming the RDA for vitamin C. The same survey stated that about 30% of adults reported they consumed a vitamin C dietary supplement or a multi-vitamin/mineral supplement that included vitamin C, and that for these people total consumption was between 300 and 400 mg/d.\n\nIn 2000 the Institute of Medicine of the U.S. National Academy of Sciences set a Tolerable upper intake level (UL) for adults of 2,000 mg/day. The amount was chosen because human trials had reported diarrhea and other gastrointestinal disturbances at intakes of greater than 3,000 mg/day. This was the Lowest-Observed-Adverse-Effect Level (LOAEL), meaning that other adverse effects were observed at higher intakes. The European Food Safety Authority (EFSA) reviewed the safety question in 2006 and reached the conclusion that there was not sufficient evidence to set a UL for vitamin C. The Japan National Institute of Health and Nutrition reviewed the same question in 2010 and also reached the conclusion that there was not sufficient evidence to set a UL.\n\nFor U.S. food and dietary supplement labeling purposes the amount in a serving is expressed as a percent of Daily Value (%DV). For vitamin C labeling purposes 100% of the Daily Value was 60 mg, but as of May 27, 2016 it was revised to 90 mg to bring it into agreement with the RDA. A table of the old and new adult Daily Values is provided at Reference Daily Intake. Food and supplement companies have until January 1, 2020 to comply with the change. European Union regulations require that labels declare energy, protein, fat, saturated fat, carbohydrates, sugars, and salt. Voluntary nutrients may be shown if present in significant amounts. Instead of Daily Values, amounts are shown as percent of Reference Intakes (RIs). For vitamin C, 100% RI was set at 80 mg in 2011.\n\nThe richest natural sources are fruits and vegetables. Vitamin C is the most widely taken nutritional supplement and is available in a variety of forms, including tablets, drink mixes, and in capsules.\n\nVitamin C is absorbed by the intestines using a sodium-ion dependent channel. It is transported through the intestine via both glucose-sensitive and glucose-insensitive mechanisms. The presence of large quantities of sugar either in the intestines or in the blood can slow absorption.\n\nWhile plants are generally a good source of vitamin C, the amount in foods of plant origin depends on the precise variety of the plant, soil condition, climate where it grew, length of time since it was picked, storage conditions, and method of preparation.\n\nThe following table is approximate and shows the relative abundance in different raw plant sources. As some plants were analyzed fresh while others were dried (thus, artifactually increasing concentration of individual constituents like vitamin C), the data are subject to potential variation and difficulties for comparison. The amount is given in milligrams per 100 grams of fruit or vegetable:\n\nAnimal-sourced foods do not provide much vitamin C, and what there is, is destroyed by the heat of cooking. For example, raw chicken liver contains 17.9 mg/100 g, but fried, the content is reduced to 2.7 mg/100 g. Chicken eggs contain no vitamin C, raw or cooked. Vitamin C is present in human breast milk at 5.0 mg/100 g and 6.1 mg/100 g in one tested sample of infant formula, but cow's milk contains only 1.0 mg/ 100 g.\n\nVitamin C chemically decomposes under certain conditions, many of which may occur during the cooking of food. Vitamin C concentrations in various food substances decrease with time in proportion to the temperature at which they are stored and cooking can reduce the vitamin C content of vegetables by around 60% possibly partly due to increased enzymatic destruction as it may be more significant at sub-boiling temperatures. Longer cooking times also add to this effect, as will copper food vessels, which catalyse the decomposition.\n\nAnother cause of vitamin C being lost from food is leaching, where the water-soluble vitamin dissolves into the cooking water, which is later poured away and not consumed. However, vitamin C does not leach in all vegetables at the same rate; research shows broccoli seems to retain more than any other. Research has also shown that freshly cut fruits do not lose significant nutrients when stored in the refrigerator for a few days.\n\nVitamin C dietary supplements are available as tablets, capsules, drink mix packets, in multi-vitamin/mineral formulations, in antioxidant formulations, and as crystalline powder. Vitamin C is also added to some fruit juices and juice drinks. Tablet and capsule content ranges from 25 mg to 1500 mg per serving. The most commonly used supplement compounds are ascorbic acid, sodium ascorbate and calcium ascorbate. Vitamin C molecules can also be bound to the fatty acid palmitate, creating ascorbyl palmitate, or else incorporated into liposomes.\n\nIn 2014, the Canadian Food Inspection Agency evaluated the effect of fortification of foods with ascorbate in the guidance document, \"Foods to Which Vitamins, Mineral Nutrients and Amino Acids May or Must be Added\". Voluntary and mandatory fortification was described for various classes of foods. Among foods classified for mandatory fortification with vitamin C were fruit-flavored drinks, mixes, and concentrates, foods for a low-energy diet, meal replacement products, and evaporated milk.\n\nFrom the U.S. National Institutes of Health: [In humans] \"Approximately 70%–90% of vitamin C is absorbed at moderate intakes of 30–180 mg/day. However, at doses above 1,000 mg/day, absorption falls to less than 50%.\"\n\nAscorbic acid is absorbed in the body by both active transport and simple diffusion. Sodium-Dependent Active Transport—Sodium-Ascorbate Co-Transporters (SVCTs) and Hexose transporters (GLUTs)—are the two transporter proteins required for active absorption. SVCT1 and SVCT2 import the reduced form of ascorbate across plasma membranes. GLUT1 and GLUT3 are glucose transporters, and transfer only the dehydroascorbic acid (DHA) form of vitamin C. Although dehydroascorbic acid is absorbed in higher rate than ascorbate, the amount of dehydroascorbic acid found in plasma and tissues under normal conditions is low, as cells rapidly reduce dehydroascorbic acid to ascorbate.\n\nSVCTs appear to be the predominant system for vitamin C transport in the body, the notable exception being red blood cells, which lose SVCT proteins during maturation. In both vitamin C synthesizers (example: rat) and non-synthesizers (example: human) cells with few exceptions maintain ascorbic acid concentrations much higher than the approximately 50 micromoles/liter (µmol/L) found in plasma. For example, the ascorbic acid content of pituitary and adrenal glands can exceed 2,000 µmol/L, and muscle is at 200-300 µmol/L. The known coenzymatic functions of ascorbic acid do not require such high concentrations, so there may be other, as yet unknown functions. Consequences of all this organ content is that plasma vitamin C is not a good indicator of whole-body status, and people may vary in the amount of time needed to show symptoms of deficiency when consuming a diet very low in vitamin C.\n\nExcretion, can be as ascorbic acid, via urine. In humans, during times of low dietary intake, vitamin C is reabsorbed by the kidneys rather than excreted. Only when plasma concentrations are 1.4 mg/dL or higher does re-absorption decline and the excess amounts pass freely into the urine. This salvage process delays onset of deficiency. Ascorbic acid also converts (reversibly) to dehydroascorbate (DHA) and from that compound non-reversibly to 2,3-diketogluonate and then oxalate. These three compounds are also excreted via urine. Humans are better than guinea pigs at converting DHA back to ascorbate, and thus take much longer to become vitamin C deficient.\n\nAscorbic acid performs numerous physiological functions in the human body. These functions include the synthesis of collagen, carnitine, and neurotransmitters; the synthesis and catabolism of tyrosine; and the metabolism of microsome. During biosynthesis ascorbate acts as a reducing agent, donating electrons and preventing oxidation to keep iron and copper atoms in their reduced states.\n\nVitamin C acts as an electron donor for eight enzymes:\n\n\nThe name \"vitamin C\" always refers to the -enantiomer of ascorbic acid and its oxidized forms, such as dehydroascorbate (DHA). Therefore, unless written otherwise, \"ascorbate\" and \"ascorbic acid\" refer in the nutritional literature to -ascorbate and -ascorbic acid respectively. Ascorbic acid is a weak sugar acid structurally related to glucose. In biological systems, ascorbic acid can be found only at low pH, but in solutions above pH 5 is predominantly found in the ionized form, ascorbate. All of these molecules have vitamin C activity and thus are used synonymously with vitamin C, unless otherwise specified.\n\nNumerous analytical methods have been developed for ascorbic acid detection. For example, vitamin C content of a food sample such as fruit juice can be calculated by measuring the volume of the sample required to decolorize a solution of dichlorophenolindophenol (DCPIP) and then calibrating the results by comparison with a known concentration of vitamin C.\n\nSimple tests are available to measure the levels of vitamin C in the urine and in serum or blood plasma. However these reflect recent dietary intake rather than total body content. It has been observed that while serum or blood plasma concentrations follow a circadian rhythm or reflect short-term dietary impact, content within tissues is more stable and can give a better view of the availability of ascorbate within the entire organism. However, very few hospital laboratories are adequately equipped and trained to carry out such detailed analyses.\n\nThe vast majority of animals and plants are able to synthesize vitamin C, through a sequence of enzyme-driven steps, which convert monosaccharides to vitamin C. In plants, this is accomplished through the conversion of mannose or galactose to ascorbic acid. In some animals, glucose needed to produce ascorbate in the liver (in mammals and perching birds) is extracted from glycogen; ascorbate synthesis is a glycogenolysis-dependent process.\n\nAmong the mammals that have lost the ability to synthesize vitamin C are simians and tarsiers, which together make up one of two major primate suborders, Haplorrhini. This group includes humans. The other more primitive primates (Strepsirrhini) have the ability to make vitamin C. Synthesis does not occur in a number of species (perhaps all species) in the small rodent family Caviidae that includes guinea pigs and capybaras, but occurs in other rodents (rats and mice do not need vitamin C in their diet, for example).\n\nIn reptiles and birds the biosynthesis is carried out in the kidneys. A number of species of passerine birds also do not synthesize, but not all of them, and those that do not are not clearly related; there is a theory that the ability was lost separately a number of times in birds. In particular, the ability to synthesize vitamin C is presumed to have been lost and then later re-acquired in at least two cases. The ability to synthesize vitamin C has also been lost in about 96% of fish (the teleosts).\n\nMost tested families of bats (order Chiroptera), including major insect and fruit-eating bat families, cannot synthesize vitamin C. A trace of gulonolactone oxidase (GULO) was detected in only 1 of 34 bat species tested, across the range of 6 families of bats tested. There are at least two species of bats, frugivorous bat (\"Rousettus leschenaultii\") and insectivorous bat (\"Hipposideros armiger\"), that retain (or regained) their ability of vitamin C production.\n\nThese animals all lack the -gulonolactone oxidase (GULO) enzyme, which is required in the last step of vitamin C synthesis. The genomes of these species contain GULO as pseudogenes, which serve as insight into the evolutionary past of the species.\n\nSome of these species (including humans) are able to make do with the lower levels available from their diets by recycling oxidised vitamin C.\n\nMost simians consume the vitamin in amounts 10 to 20 times higher than that recommended by governments for humans. This discrepancy constitutes much of the basis of the controversy on current recommended dietary allowances. It is countered by arguments that humans are very good at conserving dietary vitamin C, and are able to maintain blood levels of vitamin C comparable with simians on a far smaller dietary intake, perhaps by recycling oxidized vitamin C.\n\nIn vertebrates that can synthesize ascorbic acid, the biosynthesis pathway starts with glucose, either taking place in the liver for mammals and some birds, or the kidneys for amphibians, reptiles and some birds. The pathway is the same. Several enzymes catalyze steps from D-glucose to D-glucuronate. Next, the enzyme glucuronate reductase converts D-glucuronate to L-gluconate. Then the enzyme gulonolactonase converts L-gluconate to L-gulonolactone. The final enzymatic conversion is by the enzyme L-gulonolactone oxidase (GLO), to 2-keto-gulonolactone. From this compound, the last step is a spontaneous, i.e., non-enzymatic conversion to ascorbic acid (vitamin C). GLO is the enzyme that is absent in animal species unable to synthesize vitamin C.\n\nAll plants synthesize ascorbic acid. Ascorbic acid functions as a cofactor for enzymes involved in photosynthesis, synthesis of plant hormones, as an antioxidant and also regenerator of other antioxidants. Plants use multiple pathways to synthesize vitamin C. The major pathway starts with glucose, fructose or mannose (all simple sugars) and proceeds to L-galactose, L-galaconolactone and ascorbic acid. There is feedback regulation in place, in that the presence of ascorbic acid suppresses enzymes in the synthesis pathway. This process follows a diurnal rhythm, so that enzyme expression peaks in the morning to support biosynthesis later on when mid-day sunlight intensity demands high ascorbic acid concentrations. Minor pathways may be specific to certain parts of plants; these can be either identical to the vertebrate pathway (including the GLO enzyme), or start with inositol and get to ascorbic acid via L-galactonic acid to L-galactonolactone.\n\nAscorbic acid is a common enzymatic cofactor in mammals used in the synthesis of collagen, as well as a powerful reducing agent capable of rapidly scavenging a number of reactive oxygen species (ROS). Given that ascorbate has these important functions, it is surprising that the ability to synthesize this molecule has not always been conserved. In fact, anthropoid primates, \"Cavia porcellus\" (guinea pigs), teleost fishes, most bats, and some Passeriform birds have all independently lost the ability to internally synthesize Vitamin C in either the kidney or the liver. In all of the cases where genomic analysis was done on an ascorbic acid auxotroph, the origin of the change was found to be a result of loss-of-function mutations in the gene that codes for L-Gulono-γ-lactone oxidase, the enzyme that catalyzes the last step of the ascorbic acid pathway outlined above.\n\nIn the case of the simians, it is thought that the loss of the ability to make vitamin C may have occurred much farther back in evolutionary history than the emergence of humans or even apes, since it evidently occurred soon after the appearance of the first primates, yet sometime after the split of early primates into the two major suborders Haplorrhini (which cannot make vitamin C) and its sister suborder of non-tarsier prosimians, the Strepsirrhini (\"wet-nosed\" primates), which retained the ability to make vitamin C. According to molecular clock dating, these two suborder primate branches parted ways about 63 to 60 million years ago. Approximately three to five million years later (58 million years ago), only a short time afterward from an evolutionary perspective, the infraorder Tarsiiformes, whose only remaining family is that of the tarsier (Tarsiidae), branched off from the other haplorrhines. Since tarsiers also cannot make vitamin C, this implies the mutation had already occurred, and thus must have occurred between these two marker points (63 to 58 million years ago).\n\nOne explanation for the repeated loss of the ability to synthesize vitamin C is that it was the result of genetic drift; assuming that the diet was rich in vitamin C, natural selection would not act to preserve it.\n\nSome scientists have suggested that loss of the vitamin C biosynthesis pathway may have played a role in rapid evolutionary changes, leading to hominids and the emergence of human beings. According to this theory, the loss of ascorbic acid's anti-oxidizing properties would have led to an increase in free radicals in the body. Free radicals are known to increase the frequency of genetic mutations, which would subsequently increase the speed of evolution.\n\nIt has also been noted that the loss of the ability to synthesize ascorbate strikingly parallels the inability to break down uric acid, also a characteristic of primates. Uric acid and ascorbate are both strong reducing agents. This has led to the suggestion that, in higher primates, uric acid has taken over some of the functions of ascorbate.\n\nVitamin C is produced from glucose by two main routes. The Reichstein process, developed in the 1930s, uses a single pre-fermentation followed by a purely chemical route. The modern two-step fermentation process, originally developed in China in the 1960s, uses additional fermentation to replace part of the later chemical stages. Both processes yield approximately 60% vitamin C from the glucose feed.\n\nWorld production of synthesized vitamin C was estimated at approximately 110,000 tonnes annually in 2000. Traditionally, the main producers were BASF/Takeda, DSM, Merck and the China Pharmaceutical Group Ltd. of the People's Republic of China. By 2008 only the DSM plant in Scotland remained operational outside of China because of the strong price competition from China.\n\nThe world price of vitamin C rose sharply in 2008 partly as a result of rises in basic food prices but also in anticipation of a stoppage of the two Chinese plants, situated at Shijiazhuang near Beijing, as part of a general shutdown of polluting industry in China over the period of the Olympic games. Production resumed after the Olympics, but then five Chinese manufacturers met in 2010, among them Northeast Pharmaceutical Group and North China Pharmaceutical Group, and agreed to temporarily stop production in order to maintain prices. In 2011 an American suit was filed against four Chinese companies that allegedly colluded to limit production and fix prices of vitamin C in the United States. The companies did not deny the accusation but say in their defense that the Chinese government compelled them to act in this way. In January 2012 a United States judge ruled that the Chinese companies can be sued in the U.S. by buyers acting as a group. A verdict was reached in March 2013 imposing a $147.8 million fine. This verdict was reversed by the 2nd U.S. Circuit Court of Appeals in New York, on the grounds that China formally advised the Court that its laws required the vitamin C makers to violate the Sherman Act, a U.S. antitrust law. In June 2017 the U.S. Supreme Court announced that it would consider an appeal filed to reverse the lower court decision.\n\nThe need to include fresh plant food or raw animal flesh in the diet to prevent disease was known from ancient times. Native people living in marginal areas incorporated this into their medicinal lore. For example, spruce needles were used in temperate zones in infusions, or the leaves from species of drought-resistant trees in desert areas. In 1536, the French explorers Jacques Cartier and Daniel Knezevic, exploring the St. Lawrence River, used the local natives' knowledge to save his men who were dying of scurvy. He boiled the needles of the arbor vitae tree to make a tea that was later shown to contain 50 mg of vitamin C per 100 grams.\n\nIn the 1497 expedition of Vasco de Gama, the curative effects of citrus fruit were known. The Portuguese planted fruit trees and vegetables in Saint Helena, a stopping point for homebound voyages from Asia, and left their sick to be taken home by the next ship.\n\nAuthorities occasionally recommended plant food to prevent scurvy during long sea voyages. John Woodall, the first surgeon to the British East India Company, recommended the preventive and curative use of lemon juice in his 1617 book, \"The Surgeon's Mate\". In 1734, the Dutch writer Johann Bachstrom gave the firm opinion that \"\"scurvy is solely owing to a total abstinence from fresh vegetable food, and greens.\"\"\n\nScurvy had long been a principal killer of sailors during the long sea voyages. According to Jonathan Lamb, \"In 1499, Vasco da Gama lost 116 of his crew of 170; In 1520, Magellan lost 208 out of 230;...all mainly to scurvy.\"\n\nThe first attempt to give scientific basis for the cause of this disease was by a ship's surgeon in the Royal Navy, James Lind. While at sea in May 1747, Lind provided some crew members with two oranges and one lemon per day, in addition to normal rations, while others continued on cider, vinegar, sulfuric acid or seawater, along with their normal rations, in one of the world's first controlled experiments. The results showed that citrus fruits prevented the disease. Lind published his work in 1753 in his \"Treatise on the Scurvy\".\n\nFresh fruit was expensive to keep on board, whereas boiling it down to juice allowed easy storage but destroyed the vitamin (especially if boiled in copper kettles). It was 1796 before the British navy adopted lemon juice as standard issue at sea. In 1845, ships in the West Indies were provided with lime juice instead, and in 1860 lime juice was used throughout the Royal Navy, giving rise to the American use of the nickname \"limey\" for the British. Captain James Cook had previously demonstrated the advantages of carrying \"Sour krout\" on board, by taking his crews to the Hawaiian Islands without losing any of his men to scurvy. For this, the British Admiralty awarded him a medal.\n\nThe name \"antiscorbutic\" was used in the eighteenth and nineteenth centuries for foods known to prevent scurvy. These foods included lemons, limes, oranges, sauerkraut, cabbage, malt, and portable soup. In 1928, the Canadian Arctic anthropologist Vilhjalmur Stefansson showed that the Inuit avoid scurvy on a diet of largely raw meat. Later studies on traditional food diets of the Yukon First Nations, Dene, Inuit, and Métis of Northern Canada showed that their daily intake of vitamin C averaged between 52 and 62 mg/day, comparable with the Estimated Average Requirement.\n\nVitamin C was discovered in 1912, isolated in 1928 and synthesized in 1933, making it the first vitamin to be synthesized. Shortly thereafter Tadeus Reichstein succeeded in synthesizing the vitamin in bulk by what is now called the Reichstein process. This made possible the inexpensive mass-production of vitamin C. In 1934 Hoffmann–La Roche trademarked synthetic vitamin C under the brand name Redoxon and began to market it as a dietary supplement.\n\nIn 1907 a laboratory animal model which would help to identify the antiscorbutic factor was discovered by the Norwegian physicians Axel Holst and Theodor Frølich, who when studying shipboard beriberi, fed guinea pigs their test diet of grains and flour and were surprised when scurvy resulted instead of beriberi. By luck, this species did not make its own vitamin C, whereas mice and rats do. In 1912, the Polish biochemist Casimir Funk developed the concept of vitamins. One of these was thought to be the anti-scorbutic factor. In 1928, this was referred to as \"water-soluble C,\" although its chemical structure had not been determined.\n\nFrom 1928 to 1932, Albert Szent-Györgyi and Joseph L. Svirbely's Hungarian team, and Charles Glen King's American team, identified the anti-scorbutic factor. Szent-Györgyi isolated hexuronic acid from animal adrenal glands, and suspected it to be the antiscorbutic factor. In late 1931, Szent-Györgyi gave Svirbely the last of his adrenal-derived hexuronic acid with the suggestion that it might be the anti-scorbutic factor. By the spring of 1932, King's laboratory had proven this, but published the result without giving Szent-Györgyi credit for it. This led to a bitter dispute over priority. In 1933, Walter Norman Haworth chemically identified the vitamin as -hexuronic acid, proving this by synthesis in 1933. Haworth and Szent-Györgyi proposed that L-hexuronic acid be named a-scorbic acid, and chemically -ascorbic acid, in honor of its activity against scurvy. The term's etymology is from Latin, \"a-\" meaning away, or off from, while -scorbic is from Medieval Latin \"scorbuticus\" (pertaining to scurvy), cognate with Old Norse \"skyrbjugr\", French \"scorbut\", Dutch \"scheurbuik\" and Low German \"scharbock\". Partly for this discovery, Szent-Györgyi was awarded the 1937 Nobel Prize in Medicine, and Haworth shared that year's Nobel Prize in Chemistry.\n\nIn 1957, J.J. Burns showed that some mammals are susceptible to scurvy as their liver does not produce the enzyme -gulonolactone oxidase, the last of the chain of four enzymes that synthesize vitamin C. American biochemist Irwin Stone was the first to exploit vitamin C for its food preservative properties. He later developed the theory that humans possess a mutated form of the -gulonolactone oxidase coding gene.\n\nIn 2008, researchers at the University of Montpellier discovered that in humans and other primates the red blood cells have evolved a mechanism to more efficiently utilize the vitamin C present in the body by recycling oxidized -dehydroascorbic acid (DHA) back into ascorbic acid for reuse by the body. The mechanism was not found to be present in mammals that synthesize their own vitamin C.\n\nVitamin C megadosage is a term describing the consumption or injection of vitamin C in doses comparable to or higher than the amounts produced by the livers of mammals which are able to synthesize vitamin C. The theory behind this, although not the actual term, was described in 1970 in an article by Linus Pauling, a famous scientist who had been awarded the Nobel Prize in Chemistry in 1954. Briefly, his position was that for optimal health, humans should be consuming at least 2,300 mg/day to compensate for the inability to synthesize vitamin C. The recommendation also fell into the consumption range for gorillas - a non-synthesizing near-relative to humans. A second argument for high intake is that serum ascorbic acid concentrations increase as intake increases until it plateaus at about 190 to 200 micromoles per liter (µmol/L) once consumption exceeds 1,250 milligrams. As noted, government recommendations are a range of 40 to 110 mg/day and normal plasma is approximately 50 µmol/L, so 'normal' is about 25% of what can be achieved when oral consumption is in the proposed megadose range.\n\nLinus Pauling, famous awardee of the Nobel Prize for Chemistry, popularized the concept of high dose vitamin C as prevention and treatment of the common cold in 1970. A few years later he proposed that vitamin C would prevent cardiovascular disease, and that 10 grams/day, initially (10 days) administered intravenously and thereafter orally, would cure late-stage cancer. Mega-dosing with ascorbic acid has other champions, among them chemist Irwin Stone and the controversial Matthias Rath and Patrick Holford, who both have been accused of making unsubstantiated treatment claims for treating cancer and HIV infection.\n\nThe mega-dosing theory is to a large degree discredited. Modest benefits are demonstrated for the common cold. Benefits are not superior when supplement intakes of more than 1,000 mg/day are compared to intakes between 200 and 1,000 mg/day, and so not limited to the mega-dose range. The theory that large amounts of intravenous ascorbic acid can be used to treat late-stage cancer is - some forty years after Pauling's seminal paper - still considered unproven and still in need of high quality research. However, a lack of conclusive evidence has not stopped individual physicians from prescribing intravenous ascorbic acid to thousands of people with cancer.\n\nIn February 2011, the Swiss Post issued a postage stamp bearing a depiction of a model of a molecule of vitamin C to mark the International Year of Chemistry.\n\n"
            ],
            "good_message": false,
            "loggedData": [],
            "offensive": null,
            "researchTask": true,
            "selectedData": [],
            "shown_passages": [
                [
                    "Scurvy",
                    [
                        "Scurvy is a disease resulting from a lack of vitamin C. Early symptoms include weakness, feeling tired, and sore arms and legs.",
                        "Without treatment, decreased red blood cells, gum disease, changes to hair, and bleeding from the skin may occur.",
                        "As scurvy worsens there can be poor wound healing, personality changes, and finally death from infection or bleeding.",
                        "Typically, scurvy is caused by a lack of vitamin C in the diet.",
                        "It takes at least a month of little to no vitamin C before symptoms occur.",
                        "In modern times, it occurs most commonly in people with mental disorders, unusual eating habits, alcoholism, and old people who live alone."
                    ]
                ],
                [
                    "'Round Springfield",
                    [
                        "\"'Round Springfield\" is the 22nd episode of \"The Simpsons\" sixth season.",
                        "It originally aired on the Fox network in the United States on April 30, 1995.",
                        "In the episode, Bart is rushed to the hospital after eating a jagged metal Krusty-O and decides to sue Krusty the Clown.",
                        "Whilst visiting Bart, Lisa meets her old mentor, jazz musician Bleeding Gums Murphy.",
                        "She is saddened when she later learns that Murphy has died, and resolves to honor his memory.",
                        "Steve Allen (as himself) and Ron Taylor (as Bleeding Gums Murphy) guest star, each in their second appearance on the show.",
                        "Dan Higgins also returns as the writer and performer of all of Lisa and Bleeding Gums' saxophone solos."
                    ]
                ],
                [
                    "Vitamin C",
                    [
                        "Vitamin C, also known as ascorbic acid and -ascorbic acid, is a vitamin found in food and used as a dietary supplement.",
                        "The disease scurvy is prevented and treated with vitamin C-containing foods or dietary supplements.",
                        "Evidence does not support use in the general population for the prevention of the common cold.",
                        "There is, however, some evidence that regular use may shorten the length of colds.",
                        "It is unclear if supplementation affects the risk of cancer, cardiovascular disease, or dementia.",
                        "It may be taken by mouth or by injection.",
                        "Vitamin C is generally well tolerated.",
                        "Large doses may cause gastrointestinal discomfort, headache, trouble sleeping, and flushing of the skin."
                    ]
                ]
            ],
            "speaker": "0_Wizard",
            "suggestionsConfig": "show",
            "suggestionsNumber": 5,
            "suggestionsShown": true,
            "text": "It can cause scurvy. You may have seen it on films with sailors years ago.  Its swolen bleeding gums, unhealing wounds, it sounds horrible!",
            "textboxFilled": 0,
            "time": 1580233290.6987338,
            "timeTaken": 3.363,
            "totalTasks": 120,
            "turn": 11
        },
        {
            "checked_passage": {
                "no_passages_used": "no_passages_used"
            },
            "checked_sentence": {
                "no_passages_used": "no_passages_used"
            },
            "clicked_passages": {},
            "context": {
                "conversation": [
                    "I know I have to take vitamin C but why actually?",
                    "Well it is an essential vitamin used in the repair of soft tissue in the body.",
                    "How can I get it naturally as I don't like these supplements.",
                    "People always think of orange juice, but it is available in such much more like broccoli, sprouts, peppers, strawberries and all citrus fruits.",
                    "Wow thats a lot of options and I do like oranges but sprouts I hate. What happens if you don't have enough Vitamin C?",
                    "It can cause scurvy. You may have seen it on films with sailors years ago.  Its swolen bleeding gums, unhealing wounds, it sounds horrible!"
                ],
                "index": 9
            },
            "contextCount": 0,
            "full_passages": [
                "Scurvy\n\nScurvy is a disease resulting from a lack of vitamin C. Early symptoms include weakness, feeling tired, and sore arms and legs. Without treatment, decreased red blood cells, gum disease, changes to hair, and bleeding from the skin may occur. As scurvy worsens there can be poor wound healing, personality changes, and finally death from infection or bleeding.\nTypically, scurvy is caused by a lack of vitamin C in the diet. It takes at least a month of little to no vitamin C before symptoms occur. In modern times, it occurs most commonly in people with mental disorders, unusual eating habits, alcoholism, and old people who live alone. Other risk factors include intestinal malabsorption and dialysis. Humans and certain other animals require vitamin C in their diets to make the building blocks for collagen. Diagnosis typically is based on physical signs, X-rays, and improvement after treatment.\nTreatment is with vitamin C supplements taken by mouth. Improvement often begins in a few days with complete recovery in a few weeks. Sources of vitamin C in the diet include citrus fruit and a number of vegetables such as tomatoes and potatoes. Cooking often decreases vitamin C in foods.\nScurvy currently is rare. It occurs more often in the developing world in association with malnutrition. Rates among refugees are reported at 5% to 45%. Scurvy was described as early as the time of ancient Egypt. It was a limiting factor in long distance sea travel, often killing large numbers of people. A Scottish surgeon in the Royal Navy, James Lind, was the first to prove it could be treated with citrus fruit in a 1753 publication. His experiments represented the first controlled trial. It took another 40 years before the British Navy began giving out lemon juice routinely.\n\nEarly symptoms are malaise and lethargy. Even earlier might be a pain in a section of the gums which interferes with digestion. After 1–3 months, patients develop shortness of breath and bone pain. Myalgias may occur because of reduced carnitine production. Other symptoms include skin changes with roughness, easy bruising and petechiae, gum disease, loosening of teeth, poor wound healing, and emotional changes (which may appear before any physical changes). Dry mouth and dry eyes similar to Sjögren's syndrome may occur. In the late stages, jaundice, generalized edema, oliguria, neuropathy, fever, convulsions, and eventual death are frequently seen.\nScurvy or subclinical scurvy is caused by a deficiency of vitamin C. In modern Western societies, scurvy is rarely present in adults, although infants and elderly people are affected. Virtually all commercially available baby formulas contain added vitamin C, preventing infantile scurvy. Human breast milk contains sufficient vitamin C, if the mother has an adequate intake. Commercial milk is pasteurized, a heating process that destroys the natural vitamin C content of the milk.\n\nScurvy is one of the accompanying diseases of malnutrition (other such micronutrient deficiencies are beriberi or pellagra) and thus is still widespread in areas of the world depending on external food aid. Although rare, there are also documented cases of scurvy due to poor dietary choices by people living in industrialized nations.\n\nAscorbic acid is needed for a variety of biosynthetic pathways, by accelerating hydroxylation and amidation reactions. In the synthesis of collagen, ascorbic acid is required as a cofactor for prolyl hydroxylase and lysyl hydroxylase. These two enzymes are responsible for the hydroxylation of the proline and lysine amino acids in collagen. Hydroxyproline and hydroxylysine are important for stabilizing collagen by cross-linking the propeptides in collagen. Defective collagen fibrillogenesis impairs wound healing. Collagen is an important part of bone, so bone formation is affected. Defective connective tissue leads to fragile capillaries, resulting in abnormal bleeding. Untreated scurvy is invariably fatal.\n\nDiagnosis typically is based on physical signs, X-rays, and improvement after treatment.\n\nScurvy can be prevented by a diet that includes vitamin C-rich foods such as bell peppers (sweet peppers), blackcurrants, broccoli, chili peppers, guava, kiwifruit, and parsley. Other sources rich in vitamin C are fruits such as lemons, oranges, papaya, and strawberries. It is also found in vegetables, such as brussels sprouts, cabbage, potatoes, and spinach. Some fruits and vegetables not high in vitamin C may be pickled in lemon juice, which is high in vitamin C. Though redundant in the presence of a balanced diet, various nutritional supplements are available that provide ascorbic acid well in excess of that required to prevent scurvy.\n\nSome animal products, including liver, Muktuk (whale skin), oysters, and parts of the central nervous system, including the adrenal medulla, brain, and spinal cord, contain large amounts of vitamin C, and can even be used to treat scurvy. Fresh meat from animals which make their own vitamin C (which most animals do) contains enough vitamin C to prevent scurvy, and even partly treat it. In some cases (notably French soldiers eating fresh horse meat), it was discovered that meat alone, even partly cooked meat, could alleviate scurvy. Conversely, in other cases, a meat-only diet could cause scurvy.\n\nScott's 1902 Antarctic expedition used lightly fried seal meat and liver, whereby complete recovery from incipient scurvy was reported to have taken less than two weeks.\n\nHippocrates documented scurvy as a disease, and Egyptians have recorded its symptoms as early as 1550 BCE. The knowledge that consuming foods containing vitamin C is a cure for scurvy has been repeatedly rediscovered and forgotten into the early 20th century.\n\nIn the 13th century, the Crusaders frequently suffered from scurvy. In the 1497 expedition of Vasco de Gama, the curative effects of citrus fruit were already known and confirmed by Pedro Álvares Cabral and his crew in 1507.\n\nThe Portuguese planted fruit trees and vegetables in Saint Helena, a stopping point for homebound voyages from Asia, and left their sick, suffering from scurvy and other ailments, to be taken home, if they recovered, by the next ship.\n\nIn 1500, one of the pilots of Cabral's fleet bound for India noted that in Malindi, its king offered the expedition fresh supplies such as lambs, chickens, and ducks, along with lemons and oranges, due to which \"some of our ill were cured of scurvy\".\n\nUnfortunately, these travel accounts did not stop further maritime tragedies caused by scurvy, first because of the lack of communication between travelers and those responsible for their health, and because fruits and vegetables could not be kept for long on ships.\n\nIn 1536, the French explorer Jacques Cartier, exploring the St. Lawrence River, used the local natives' knowledge to save his men who were dying of scurvy. He boiled the needles of the arbor vitae tree (Eastern White Cedar) to make a tea that was later shown to contain 50 mg of vitamin C per 100 grams. Such treatments were not available aboard ship, where the disease was most common.\nIn February 1601, Captain James Lancaster, while sailing to Sumatra, landed on the northern coast to specifically obtain lemons and oranges for his crew to stop scurvy. Captain Lancaster conducted an experiment using four ships under his command. One ship's crew received routine doses of lemon juice while the other three ships did not receive any such treatment. As a result, members of the non-treated ships started to become ill, contracting scurvy with many dying as a result.\n\nDuring the Age of Exploration (between 1500 and 1800), it has been estimated that scurvy killed at least two million sailors. Jonathan Lamb wrote: \"In 1499, Vasco da Gama lost 116 of his crew of 170; In 1520, Magellan lost 208 out of 230;...all mainly to scurvy.\"\n\nIn 1593, Admiral Sir Richard Hawkins advocated drinking orange and lemon juice as a means of preventing scurvy.\n\nIn 1614, John Woodall, Surgeon General of the East India Company, published \"The Surgion's Mate\" as a handbook for apprentice surgeons aboard the company's ships. He repeated the experience of mariners that the cure for scurvy was fresh food or, if not available, oranges, lemons, limes, and tamarinds. He was, however, unable to explain the reason why, and his assertion had no impact on the opinions of the influential physicians who ran the medical establishment that scurvy was a digestive complaint.\n\nA 1707 handwritten book by Mrs. Ebot Mitchell, discovered in a house in Hasfield, Gloucestershire, contains a \"Recp.t for the Scurvy\" that consisted of extracts from various plants mixed with a plentiful supply of orange juice, white wine or beer.\n\nIn 1734, the Leiden-based physician Johann Bachstrom published a book on scurvy in which he stated, \"scurvy is solely owing to a total abstinence from fresh vegetable food, and greens; which is alone the primary cause of the disease\", and urged the use of fresh fruit and vegetables as a cure.\n\nHowever, it was not until 1747 that James Lind formally demonstrated that scurvy could be treated by supplementing the diet with citrus fruit, in one of the first ever reported controlled clinical experiments in the history of medicine. In 1753, Lind published \"A Treatise of the Scurvy\", in which he explained the details of his clinical trial\",\" but it occupied only a few paragraphs in a work that was long and complex and had little impact. In fact, Lind himself never actively promoted lemon juice as a single ‘cure’. He shared medical opinion at the time that scurvy had multiple causes – notably hard work, bad water, and the consumption of salt meat in a damp atmosphere which inhibited healthful perspiration and normal excretion - and therefore required multiple solutions. He was also sidetracked\nby the possibilities of producing a concentrated ‘rob’ of lemon juice by boiling it. Unfortunately this process\ndestroyed the vitamin C and was unsuccessful.\n\nDuring the 18th century, disease killed more British sailors than enemy action. It was mainly by scurvy that George Anson, in his celebrated voyage of 1740–1744, lost nearly two-thirds of his crew (1300 out of 2000) within the first 10 months of the voyage. The Royal Navy enlisted 184,899 sailors during the Seven Years' War; 133,708 of these were \"missing\" or died from disease, and scurvy was the leading cause.\n\nAlthough throughout this period sailors and naval surgeons were increasingly convinced that citrus fruits could cure scurvy, the classically trained physicians who ran the medical establishment dismissed this evidence as mere anecdote which did not conform to current theories of disease. Literature championing the cause of citrus juice, therefore, had no practical impact. Medical theory was based on the assumption that scurvy was a disease of internal putrefaction brought on by faulty digestion caused by the hardships of life at sea and the naval diet. Although this basic idea was given different emphases by successive theorists, the remedies they advocated (and which the navy accepted) amounted to little more than the consumption of ‘fizzy drinks’ to activate the digestive system, the most extreme of which was the regular consumption of ‘elixir of vitriol’ – sulphuric acid taken with spirits and barley water, and laced with spices.\n\nIn 1764, a new variant appeared. Advocated by Dr David McBride and Sir John Pringle, Surgeon General of the Army and later President of the Royal Society, this idea was that scurvy was the result of a lack of ‘fixed air’ in the tissues which could be prevented by drinking infusions of malt and wort whose fermentation within the body would stimulate digestion and restore the missing gases. These ideas received wide and influential backing, when James Cook set off to circumnavigate the world (1768–1771) in HM Bark \"Endeavour\", malt and wort were top of the list of the remedies he was ordered to investigate. The others were beer, sour crout and Lind's ‘rob’. The list did not include lemons.\n\nCook did not lose a single man to scurvy, and his report came down in favour of malt and wort, although it is now clear that the reason for the health of his crews on this and other voyages was Cook's regime of shipboard cleanliness, enforced by strict discipline, as well as frequent replenishment of fresh food and green stuffs. Another rule implemented by Cook was his prohibition of the consumption of salt fat skimmed from the ship's copper boiling pans, then a common practice in the Navy. In contact with air the copper formed compounds that prevented the absorption of vitamins by the intestines.\n\nThe first major long distance expedition that experienced virtually no scurvy was that of the Spanish naval officer Alessandro Malaspina, 1789–1794. Malaspina's medical officer, Pedro González, was convinced that fresh oranges and lemons were essential for preventing scurvy. Only one outbreak occurred, during a 56-day trip across the open sea. Five sailors came down with symptoms, one seriously. After three days at Guam all five were healthy again. Spain's large empire and many ports of call made it easier to acquire fresh fruit.\n\nAlthough towards the end of the century MacBride's theories were being challenged, the medical establishment in Britain remained wedded to the notion that scurvy was a disease of internal ‘putrefaction’ and the Sick and Hurt Board, run by administrators, felt obliged to follow its advice. Within the Royal Navy, however, opinion – strengthened by first-hand experience of the use of lemon juice at the siege of Gibraltar and during Admiral Rodney's expedition to the Caribbean – had become increasingly convinced of its efficacy. This was reinforced by the writings of experts like Gilbert Blane and Thomas Trotter and by the reports of up-and-coming naval commanders.\n\nWith the coming of war in 1793, the need to eliminate scurvy acquired a new urgency. But the first initiative came not from the medical establishment but from the admirals. Ordered to lead an expedition against Mauritius, Rear Admiral Gardner was uninterested in the wort, malt and elixir of vitriol which were still being issued to ships of the Royal Navy, and demanded that he be supplied with lemons, to counteract scurvy on the voyage. Members of the Sick and Hurt Board, recently augmented by two practical naval surgeons, supported the request, and the Admiralty ordered that it be done. There was, however, a last minute change of plan. The expedition against Mauritius was cancelled. On 2 May 1794, only HMS \"Suffolk\" and two sloops under Commodore Peter Rainier sailed for the east with an outward bound convoy, but the warships were fully supplied with lemon juice and the sugar with which it had to be mixed. Then in March 1795, came astonishing news. \"Suffolk\" had arrived in India after a four-month voyage without a trace of scurvy and with a crew that was healthier than when it set out.\n\nThe effect was immediate. Fleet commanders clamoured also to be supplied with lemon juice, and by June the Admiralty acknowledged the groundswell of demand in the navy had agreed to a proposal from the Sick and Hurt Board that lemon juice and sugar should in future be issued as a daily ration to the crews of all warships.\n\nIt took a few years before the method of distribution to all ships in the fleet had been perfected and the supply of the huge quantities of lemon juice required to be secured, but by 1800, the system was in place and functioning. This led to a remarkable health improvement among the sailors and consequently played a critical role in gaining the advantage in naval battles against enemies who had yet to introduce the measures.\n\nThe surgeon-in-chief of Napoleon's army at the Siege of Alexandria (1801), Baron Dominique-Jean Larrey, wrote in his memoirs that the consumption of horse meat helped the French to curb an epidemic of scurvy. The meat was cooked but was freshly obtained from young horses bought from Arabs, and was nevertheless effective. This helped to start the 19th-century tradition of horse meat consumption in France.\n\nLauchlin Rose patented a method used to preserve citrus juice without alcohol in 1867, creating a concentrated drink known as Rose's lime juice. The Merchant Shipping Act of 1867 required all ships of the Royal Navy and Merchant Navy to provide a daily lime ration to sailors to prevent scurvy. The product became nearly ubiquitous, hence the term \"limey\", first for British sailors, then for English immigrants within the former British colonies (particularly America, New Zealand and South Africa), and finally, in old American slang, all British people.\n\nThe plant \"Cochlearia officinalis\", also known as \"Common Scurvygrass\", acquired its common name from the observation that it cured scurvy, and it was taken on board ships in dried bundles or distilled extracts. Its very bitter taste was usually disguised with herbs and spices; however, this did not prevent scurvygrass drinks and sandwiches from becoming a popular fad in the UK until the middle of the nineteenth century, when citrus fruits became more readily available.\n\nWest Indian limes began to supplement lemons, when Spain's alliance with France against Britain in the Napoleonic Wars made the supply of Mediterranean lemons problematic, and because they were more easily obtained from Britain's Caribbean colonies and were believed to be more effective because they were more acidic. It was the acid, not the (then-unknown) Vitamin C that was believed to cure scurvy. In fact, the West Indian limes were significantly lower in Vitamin C than the previous lemons and further were not served fresh but rather as lime juice, which had been exposed to light and air, and piped through copper tubing, all of which significantly reduced the Vitamin C. Indeed, a 1918 animal experiment using representative samples of the Navy and Merchant Marine's lime juice showed that it had virtually no antiscorbutic power at all.\n\nThe belief that scurvy was fundamentally a nutritional deficiency, best treated by consumption of fresh food, particularly fresh citrus or fresh meat, was not universal in the 19th and early 20th centuries, and thus sailors and explorers continued to suffer from scurvy into the 20th century. For example, the Belgian Antarctic Expedition of 1897–1899 became seriously affected by scurvy when its leader, Adrien de Gerlache, initially discouraged his men from eating penguin and seal meat.\n\nIn the Royal Navy's Arctic expeditions in the 19th century it was widely believed that scurvy was prevented by good hygiene on board ship, regular exercise, and maintaining the morale of the crew, rather than by a diet of fresh food. Navy expeditions continued to be plagued by scurvy even while fresh (not jerked or tinned) meat was well known as a practical antiscorbutic among civilian whalers and explorers in the Arctic. Even cooking fresh meat did not entirely destroy its antiscorbutic properties, especially as many cooking methods failed to bring all the meat to high temperature.\n\nThe confusion is attributed to a number of factors:\nIn the resulting confusion, a new hypothesis was proposed, following the new germ theory of disease – that scurvy was caused by ptomaine, a waste product of bacteria, particularly in tainted tinned meat.\n\nInfantile scurvy emerged in the late 19th century because children were being fed pasteurized cow's milk, particularly in the urban upper class. While pasteurization killed bacteria, it also destroyed vitamin C. This was eventually resolved by supplementing with onion juice or cooked potatoes.\n\nBy the early 20th century, when Robert Falcon Scott made his first expedition to the Antarctic (1901–1904), the prevailing theory was that scurvy was caused by \"ptomaine poisoning\", particularly in tinned meat. However, Scott discovered that a diet of fresh meat from Antarctic seals cured scurvy before any fatalities occurred.\n\nIn 1907, an animal model which would eventually help to isolate and identify the \"antiscorbutic factor\" was discovered. Axel Holst and Theodor Frølich, two Norwegian physicians studying shipboard beriberi contracted by ship's crews in the Norwegian Fishing Fleet, wanted a small test mammal to substitute for the pigeons then used in beriberi research. They fed guinea pigs their test diet of grains and flour, which had earlier produced beriberi in their pigeons, and were surprised when classic scurvy resulted instead. This was a serendipitous choice of animal. Until that time, scurvy had not been observed in any organism apart from humans and had been considered an exclusively human disease. Certain birds, mammals, and fish are susceptible to scurvy, but pigeons are unaffected, since they can synthesize ascorbic acid internally. Holst and Frølich found they could cure scurvy in guinea pigs with the addition of various fresh foods and extracts. This discovery of an animal experimental model for scurvy, which was made even before the essential idea of \"vitamins\" in foods had been put forward, has been called the single most important piece of vitamin C research.\n\nVilhjalmur Stefansson, an arctic explorer who had lived among the Inuit, proved that the all-meat diet they consumed did not lead to vitamin deficiencies. He participated in a study in New York's Bellevue Hospital in February 1928, where he and a companion ate only meat for a year while under close medical observation, yet remained in good health.\n\nIn 1927, Hungarian biochemist Szent-Györgyi isolated a compound he called \"hexuronic acid\". Szent-Györgyi suspected hexuronic acid, which he had isolated from adrenal glands, to be the antiscorbutic agent, but he could not prove it without an animal-deficiency model. In 1932, the connection between hexuronic acid and scurvy was finally proven by American researcher Charles Glen King of the University of Pittsburgh. King's laboratory was given some hexuronic acid by Szent-Györgyi and soon established that it was the sought-after anti-scorbutic agent. Because of this, hexuronic acid was subsequently renamed \"ascorbic acid.\"\n\nRates of scurvy in most of the world are low. Those most commonly affected are malnourished people in the developing world and the homeless. There have been outbreaks of the condition in refugee camps. Case reports in the developing world of those with poorly healing wounds have occurred.\n\nNotable human dietary studies of experimentally induced scurvy have been conducted on conscientious objectors during WWII in Britain and on Iowa state prisoner volunteers in the late 1960s. These studies both found that all obvious symptoms of scurvy previously induced by an experimental scorbutic diet with extremely low vitamin C content could be completely reversed by additional vitamin C supplementation of only 10 mg per day. In these experiments, no clinical difference was noted between men given 70 mg vitamin C per day (which produced blood levels of vitamin C of about 0.55 mg/dl, about 1/3 of tissue saturation levels), and those given 10 mg per day (which produced lower blood levels). Men in the prison study developed the first signs of scurvy about 4 weeks after starting the vitamin C-free diet, whereas in the British study, six to eight months were required, possibly because the subjects were pre-loaded with a 70 mg/day supplement for six weeks before the scorbutic diet was fed.\n\nMen in both studies, on a diet devoid or nearly devoid of vitamin C, had blood levels of vitamin C too low to be accurately measured when they developed signs of scurvy, and in the Iowa study, at this time were estimated (by labeled vitamin C dilution) to have a body pool of less than 300 mg, with daily turnover of only 2.5 mg/day.\n\nAlmost all plant and animal species synthesize vitamin C. Notable mammalian exceptions include most or all of the order Chiroptera (bats), and one of the two major primate suborders, the \"Anthropoidea\" (Haplorrhini) which include tarsiers, monkeys, and apes, including human beings. The Strepsirrhini (non-tarsier prosimians) can make their own vitamin C, and these include lemurs, lorises, pottos, and galagos. Ascorbic acid is also not synthesized by at least two species of Caviidae, the capybara and the guinea pig. There are known species of birds and fish that do not synthesize their own Vitamin C. All species that do not synthesize ascorbate require it in the diet. Deficiency causes scurvy in humans, and somewhat similar symptoms in other animals.\n\nIn babies, scurvy is sometimes referred to as Barlow's disease, named after Thomas Barlow, a British physician who described it in 1883. However, Barlow's disease or Barlow's syndrome may also refer to mitral valve prolapse, first described by John Brereton Barlow in 1966.\n\n",
                "Vitamin C\n\nVitamin C, also known as ascorbic acid and -ascorbic acid, is a vitamin found in food and used as a dietary supplement. The disease scurvy is prevented and treated with vitamin C-containing foods or dietary supplements. Evidence does not support use in the general population for the prevention of the common cold. There is, however, some evidence that regular use may shorten the length of colds. It is unclear if supplementation affects the risk of cancer, cardiovascular disease, or dementia. It may be taken by mouth or by injection.\nVitamin C is generally well tolerated. Large doses may cause gastrointestinal discomfort, headache, trouble sleeping, and flushing of the skin. Normal doses are safe during pregnancy. The United States Institute of Medicine recommends against taking large doses.\nVitamin C is an essential nutrient involved in the repair of tissue and the enzymatic production of certain neurotransmitters. It is required for the functioning of several enzymes and is important for immune system function. It also functions as an antioxidant. Foods containing vitamin C include citrus fruits, broccoli, Brussels sprouts, raw bell peppers, and strawberries. Prolonged storage or cooking may reduce vitamin C content in foods.\nVitamin C was discovered in 1912, isolated in 1928, and in 1933 was the first vitamin to be chemically produced. It is on the World Health Organization Model List of Essential Medicines, the most effective and safe medicines needed in a health system. Vitamin C is available as a generic medication and over-the-counter drug. In 2015, the wholesale cost in the developing world was less than 0.01 per tablet. Partly for its discovery, Albert Szent-Györgyi and Walter Norman Haworth were awarded 1937 Nobel Prizes in Physiology and Medicine and Chemistry, respectively.\nVitamin C is an essential nutrient for certain animals including humans. The term \"vitamin C\" encompasses several vitamers that have vitamin C activity in animals. Ascorbate salts such as sodium ascorbate and calcium ascorbate are used in some dietary supplements. These release ascorbate upon digestion. Ascorbate and ascorbic acid are both naturally present in the body, since the forms interconvert according to pH. Oxidized forms of the molecule such as dehydroascorbic acid are converted back to ascorbic acid by reducing agents.\n\nVitamin C is a cofactor in at least eight enzymatic reactions in animals (and humans) important in many essential functions, including wound healing. In humans, vitamin C deficiency compromises collagen synthesis, contributing to the more severe symptoms of scurvy. More generally, the biochemical role of vitamin C is to act as an antioxidant, a reducing agent, donating electrons to various enzymatic and non-enzymatic reactions. Doing so converts vitamin C to an oxidized state - either as semidehydroascorbic acid or dehydroascorbic acid. These compounds can be restored to a reduced state by glutathione and NADPH-dependent enzymatic mechanisms.\n\nIn plants, vitamin C is a substrate for ascorbate peroxidase. This enzyme utilizes ascorbate to neutralize toxic hydrogen peroxide (HO) by converting it to water (HO).\n\nScurvy is an avitaminosis resulting from lack of vitamin C, since without this vitamin, collagen made by the body is too unstable to perform its function.\n\nScurvy leads to the formation of brown spots on the skin, spongy gums, and bleeding from all mucous membranes. The spots are most abundant on the thighs and legs, and a person with the ailment looks pale, feels depressed, and is partially immobilized. In advanced scurvy there are open, suppurating wounds and loss of teeth and, eventually, death. The human body can store only a certain amount of vitamin C, and so the body stores are depleted if fresh supplies are not consumed. The time frame for onset of symptoms of scurvy in unstressed adults on a completely vitamin C free diet, however, may range from one month to more than six months, depending on previous loading of vitamin C.\n\nNotable human dietary studies of experimentally induced scurvy have been conducted on conscientious objectors during World War II in Britain and on Iowa state prisoners in the late 1960s to the 1980s. These studies both found that all obvious symptoms of scurvy previously induced by an experimental scorbutic diet with extremely low vitamin C content could be completely reversed by additional vitamin C supplementation of only 10 mg a day. In these experiments, there was no clinical difference noted between men given 70 mg vitamin C per day (which produced blood level of vitamin C of about 0.55 mg/dl, about 1/3 of tissue saturation levels) and those given 10 mg per day. Men in the prison study developed the first signs of scurvy about four weeks after starting the vitamin C-free diet, whereas in the British study, six to eight months were required, possibly due to the pre-loading of this group with a 70 mg/day supplement for six weeks before the scorbutic diet was fed.\n\nMen in both studies on a diet devoid, or nearly devoid, of vitamin C had blood levels of vitamin C too low to be accurately measured when they developed signs of scurvy, and in the Iowa study, at this time were estimated (by labeled vitamin C dilution) to have a body pool of less than 300 mg, with daily turnover of only 2.5 mg/day, implying an instantaneous half-life of 83 days by this time (elimination constant of 4 months).\n\nVitamin C has a definitive role in treating scurvy, which is a disease caused by vitamin C deficiency. Beyond that, a role for vitamin C as prevention or treatment for various diseases is disputed, with reviews reporting conflicting results. A 2012 Cochrane review reported no effect of vitamin C supplementation on overall mortality. It is on the World Health Organization's List of Essential Medicines as one of the most effective and safe medicines needed in a health system.\n\nThe disease scurvy is caused by vitamin C deficiency and can be prevented and treated with vitamin C containing foods or dietary supplements. It takes at least a month of little to no vitamin C before symptoms occur. Early symptoms are malaise and lethargy, progressing to shortness of breath, bone pain, bleeding gums, susceptibility to bruising, poor wound healing, and finally fever, convulsions and eventual death. Until quite late in the disease the damage is reversible, as with vitamin C repletion, healthy collagen replaces the defective collagen. Treatment can be orally or by intramuscular or intravenous injection. Scurvy was known to Hippocrates in the classical era. The disease was shown to be prevented by citrus fruit in an early controlled trial by a Royal Navy surgeon, James Lind, in 1747, and from 1796 lemon juice was issued to all Royal Navy crewmen.\n\nThe effect of vitamin C on the common cold has been extensively researched. The earliest publication of a controlled clinical trial appears to be from 1945. Researchers continued to work on this question, but research interest and public interest spiked after Linus Pauling, two-time awardee of the Nobel Prize (Chemistry Prize, 1954, Peace Prize 1962), started publishing research on the topic and also published a book \"Vitamin C and the Common Cold\" in 1970. A revised and expanded edition \"Vitamin C, the Common Cold and the Flu\" was published in 1976.\n\nThe most recent meta-analysis, a Cochrane Review published in 2013, with inclusion criteria limited to trials that called for at least 200 mg/day, concluded that vitamin C taken on a regular basis was not effective in prevention of the common cold. Limiting inclusion to trials that called for at least 1000 mg/day made no difference. However, taking vitamin C on a regular basis did reduce average duration by 8% in adults and 14% in children, and also reduced severity of colds. A subset of trials reported that supplementation reduced the incidence of colds by half in marathon runners, skiers, or soldiers in subarctic conditions. Another subset of trials looked at therapeutic use, meaning that vitamin C was not started unless the people started to feel the beginnings of a cold. In these, vitamin C did not impact duration or severity. An earlier review stated that vitamin C did not prevent colds, did reduce duration, did not reduce severity. The authors of the Cochrane review concluded that \"...given the consistent effect of vitamin C on the duration and severity of colds in the regular supplementation studies, and the low cost and safety, it may be worthwhile for common cold patients to test on an individual basis whether therapeutic vitamin C is beneficial for them.\"\n\nVitamin C distributes readily in high concentrations into immune cells, has antimicrobial and natural killer cell activities, promotes lymphocyte proliferation, and is consumed quickly during infections, effects indicating a prominent role in immune system regulation. The European Food Safety Authority found a cause and effect relationship exists between the dietary intake of vitamin C and functioning of a normal immune system in adults and in children under three years of age.\n\nThere are two approaches to the question of whether vitamin C has an impact on cancer. First, within the normal range of dietary intake without additional dietary supplementation, are people who consume more vitamin C at lower risk for developing cancer, and if so, does an orally consumed supplement have the same benefit? Second, for people diagnosed with cancer, will large amounts of ascorbic acid administered intravenously treat the cancer, reduce the adverse effects of other treatments, and so prolong survival and improve quality of life? A 2013 Cochrane review found no evidence that vitamin C supplementation reduces the risk of lung cancer in healthy or high risk (smokers and asbestos-exposed) people. A 2014 meta-analysis found that vitamin C intake might protect against lung cancer risk. A second meta-analysis found no effect on the risk of prostate cancer. Two meta-analyses evaluated the effect of vitamin C supplementation on the risk of colorectal cancer. One found a weak association between vitamin C consumption and reduced risk, and the other found no effect of supplementation. A 2011 meta-analysis failed to find support for the prevention of breast cancer with vitamin C supplementation, but a second study concluded that vitamin C may be associated with increased survival in those already diagnosed.\n\nUnder the rubric of orthomolecular medicine, \"Intravenous vitamin C is a contentious adjunctive cancer therapy, widely used in naturopathic and integrative oncology settings.\" With oral administration absorption efficiency decreases as amounts increase. Intravenous administration bypasses this. Doing so makes it possible to achieve plasma concentrations of 5 to 10 millimoles/liter (mmol/L), which far exceed the approximately 0.2 mmol/L limit from oral consumption. The theories of mechanism are contradictory. At high tissue concentrations, ascorbic acid is described as acting as a pro-oxidant, generating hydrogen peroxide (HO) to kill tumor cells. The same literature claims that ascorbic acid acts as an antioxidant, thereby reducing the adverse effects of chemotherapy and radiation therapy. Research continues in this field, but a 2014 review concluded: \"Currently, the use of high-dose intravenous vitamin C [as an anticancer agent] cannot be recommended outside of a clinical trial.\" A 2015 review added: \"There is no high-quality evidence to suggest that ascorbate supplementation in cancer patients either enhances the antitumor effects of chemotherapy or reduces its toxicity. Evidence for ascorbate's anti-tumor effects was limited to case reports and observational and uncontrolled studies.\"\n\nA 2013 meta-analysis found no evidence that vitamin C supplementation reduces the risk of myocardial infarction, stroke, cardiovascular mortality, or all-cause mortality. However, a second analysis found an inverse relationship between circulating vitamin C levels or dietary vitamin C and the risk of stroke.\n\nA meta-analysis of 44 clinical trials has shown a significant positive effect of vitamin C on endothelial function when taken at doses greater than 500 mg per day. The endothelium is a layer of cells that line the interior surface of blood vessels. Endothelial dysfunction is implicated in many aspects of vascular diseases. The researchers noted that the effect of vitamin C supplementation appeared to be dependent on health status, with stronger effects in those at higher cardiovascular disease risk.\n\nStudies examining the effects of vitamin C intake on the risk of Alzheimer's disease have reached conflicting conclusions. Maintaining a healthy dietary intake is probably more important than supplementation for achieving any potential benefit. A 2010 review found no role for vitamin C supplementation in the treatment of rheumatoid arthritis. Vitamin C supplementation does not prevent or slow the progression of age-related cataract.\n\nMore than two to three grams may cause indigestion, particularly when taken on an empty stomach. However, taking vitamin C in the form of sodium ascorbate and calcium ascorbate may minimize this effect. Other symptoms reported for large dose include nausea, abdominal cramps and diarrhea. These effects are attributed to the osmotic effect of unabsorbed vitamin C passing through the intestine. In theory, high vitamin C intake may cause excessive absorption of iron. A summary of reviews of supplementation in healthy subjects did not report this problem, but left as untested the possibility that individuals with hereditary hemochromatosis might be adversely affected. There is a longstanding belief among the mainstream medical community that vitamin C increases risk of kidney stones. \"Reports of kidney stone formation associated with excess ascorbic acid intake are limited to individuals with renal disease\". Reviews state that \"data from epidemiological studies do not support an association between excess ascorbic acid intake and kidney stone formation in apparently healthy individuals\", although one large, multi-year trial did report a nearly two-fold increase in kidney stones in men who regularly consumed a vitamin C supplement. Vitamin C is a water-soluble vitamin, with dietary excesses not absorbed, and excesses in the blood rapidly excreted in the urine, so it exhibits remarkably low acute toxicity.\n\nRecommendations for vitamin C intake by adults have been set by various national agencies:\n\nIn 2000 the North American Dietary Reference Intake chapter on vitamin C updated the Recommended Dietary Allowance (RDA) to 90 milligrams per day for adult men and 75 mg/day for adult women, and set a Tolerable upper intake level (UL) for adults of 2,000 mg/day. The table shows RDAs for the United States and Canada for children, and for pregnant and lactating women. For the European Union, the EFSA set higher recommendations for adults, and also for children: 20 mg/day for ages 1–3, 30 mg/day for ages 4–6, 45 mg/day for ages 7–10, 70 mg/day for ages 11–14, 100 mg/day for males ages 15–17, 90 mg/day for females ages 15–17. For pregnancy 100 mg/day; for lactation 155 mg/day. India, on the other hand, has set recommendations much lower: 40 mg/day for ages 1 through adult, 60 mg/day for pregnancy, and 80 mg/day for lactation. Clearly, there is not consensus among countries.\n\nCigarette smokers and people exposed to secondhand smoke have lower plasma vitamin C levels than nonsmokers. The thinking is that inhalation of smoke causes oxidative damage, depleting this antioxidant vitamin.. The U.S. Institute of Medicine estimated that smokers need 35 mg more vitamin C per day than nonsmokers, but did not formally establish a higher RDA for smokers. One meta-analysis showed an inverse relationship between vitamin C intake and lung cancer, although it concluded that more research is needed to confirm this observation.\n\nThe U.S. National Center for Health Statistics conducts biannual National Health and Nutrition Examination Survey (NHANES) to assess the health and nutritional status of adults and children in the United States. Some results are reported as What We Eat In America. The 2013-2014 survey reported that for adults ages 20 years and older, men consumed on average 83.3 mg/d and women 75.1 mg/d. This means that half the women and more than half the men are not consuming the RDA for vitamin C. The same survey stated that about 30% of adults reported they consumed a vitamin C dietary supplement or a multi-vitamin/mineral supplement that included vitamin C, and that for these people total consumption was between 300 and 400 mg/d.\n\nIn 2000 the Institute of Medicine of the U.S. National Academy of Sciences set a Tolerable upper intake level (UL) for adults of 2,000 mg/day. The amount was chosen because human trials had reported diarrhea and other gastrointestinal disturbances at intakes of greater than 3,000 mg/day. This was the Lowest-Observed-Adverse-Effect Level (LOAEL), meaning that other adverse effects were observed at higher intakes. The European Food Safety Authority (EFSA) reviewed the safety question in 2006 and reached the conclusion that there was not sufficient evidence to set a UL for vitamin C. The Japan National Institute of Health and Nutrition reviewed the same question in 2010 and also reached the conclusion that there was not sufficient evidence to set a UL.\n\nFor U.S. food and dietary supplement labeling purposes the amount in a serving is expressed as a percent of Daily Value (%DV). For vitamin C labeling purposes 100% of the Daily Value was 60 mg, but as of May 27, 2016 it was revised to 90 mg to bring it into agreement with the RDA. A table of the old and new adult Daily Values is provided at Reference Daily Intake. Food and supplement companies have until January 1, 2020 to comply with the change. European Union regulations require that labels declare energy, protein, fat, saturated fat, carbohydrates, sugars, and salt. Voluntary nutrients may be shown if present in significant amounts. Instead of Daily Values, amounts are shown as percent of Reference Intakes (RIs). For vitamin C, 100% RI was set at 80 mg in 2011.\n\nThe richest natural sources are fruits and vegetables. Vitamin C is the most widely taken nutritional supplement and is available in a variety of forms, including tablets, drink mixes, and in capsules.\n\nVitamin C is absorbed by the intestines using a sodium-ion dependent channel. It is transported through the intestine via both glucose-sensitive and glucose-insensitive mechanisms. The presence of large quantities of sugar either in the intestines or in the blood can slow absorption.\n\nWhile plants are generally a good source of vitamin C, the amount in foods of plant origin depends on the precise variety of the plant, soil condition, climate where it grew, length of time since it was picked, storage conditions, and method of preparation.\n\nThe following table is approximate and shows the relative abundance in different raw plant sources. As some plants were analyzed fresh while others were dried (thus, artifactually increasing concentration of individual constituents like vitamin C), the data are subject to potential variation and difficulties for comparison. The amount is given in milligrams per 100 grams of fruit or vegetable:\n\nAnimal-sourced foods do not provide much vitamin C, and what there is, is destroyed by the heat of cooking. For example, raw chicken liver contains 17.9 mg/100 g, but fried, the content is reduced to 2.7 mg/100 g. Chicken eggs contain no vitamin C, raw or cooked. Vitamin C is present in human breast milk at 5.0 mg/100 g and 6.1 mg/100 g in one tested sample of infant formula, but cow's milk contains only 1.0 mg/ 100 g.\n\nVitamin C chemically decomposes under certain conditions, many of which may occur during the cooking of food. Vitamin C concentrations in various food substances decrease with time in proportion to the temperature at which they are stored and cooking can reduce the vitamin C content of vegetables by around 60% possibly partly due to increased enzymatic destruction as it may be more significant at sub-boiling temperatures. Longer cooking times also add to this effect, as will copper food vessels, which catalyse the decomposition.\n\nAnother cause of vitamin C being lost from food is leaching, where the water-soluble vitamin dissolves into the cooking water, which is later poured away and not consumed. However, vitamin C does not leach in all vegetables at the same rate; research shows broccoli seems to retain more than any other. Research has also shown that freshly cut fruits do not lose significant nutrients when stored in the refrigerator for a few days.\n\nVitamin C dietary supplements are available as tablets, capsules, drink mix packets, in multi-vitamin/mineral formulations, in antioxidant formulations, and as crystalline powder. Vitamin C is also added to some fruit juices and juice drinks. Tablet and capsule content ranges from 25 mg to 1500 mg per serving. The most commonly used supplement compounds are ascorbic acid, sodium ascorbate and calcium ascorbate. Vitamin C molecules can also be bound to the fatty acid palmitate, creating ascorbyl palmitate, or else incorporated into liposomes.\n\nIn 2014, the Canadian Food Inspection Agency evaluated the effect of fortification of foods with ascorbate in the guidance document, \"Foods to Which Vitamins, Mineral Nutrients and Amino Acids May or Must be Added\". Voluntary and mandatory fortification was described for various classes of foods. Among foods classified for mandatory fortification with vitamin C were fruit-flavored drinks, mixes, and concentrates, foods for a low-energy diet, meal replacement products, and evaporated milk.\n\nFrom the U.S. National Institutes of Health: [In humans] \"Approximately 70%–90% of vitamin C is absorbed at moderate intakes of 30–180 mg/day. However, at doses above 1,000 mg/day, absorption falls to less than 50%.\"\n\nAscorbic acid is absorbed in the body by both active transport and simple diffusion. Sodium-Dependent Active Transport—Sodium-Ascorbate Co-Transporters (SVCTs) and Hexose transporters (GLUTs)—are the two transporter proteins required for active absorption. SVCT1 and SVCT2 import the reduced form of ascorbate across plasma membranes. GLUT1 and GLUT3 are glucose transporters, and transfer only the dehydroascorbic acid (DHA) form of vitamin C. Although dehydroascorbic acid is absorbed in higher rate than ascorbate, the amount of dehydroascorbic acid found in plasma and tissues under normal conditions is low, as cells rapidly reduce dehydroascorbic acid to ascorbate.\n\nSVCTs appear to be the predominant system for vitamin C transport in the body, the notable exception being red blood cells, which lose SVCT proteins during maturation. In both vitamin C synthesizers (example: rat) and non-synthesizers (example: human) cells with few exceptions maintain ascorbic acid concentrations much higher than the approximately 50 micromoles/liter (µmol/L) found in plasma. For example, the ascorbic acid content of pituitary and adrenal glands can exceed 2,000 µmol/L, and muscle is at 200-300 µmol/L. The known coenzymatic functions of ascorbic acid do not require such high concentrations, so there may be other, as yet unknown functions. Consequences of all this organ content is that plasma vitamin C is not a good indicator of whole-body status, and people may vary in the amount of time needed to show symptoms of deficiency when consuming a diet very low in vitamin C.\n\nExcretion, can be as ascorbic acid, via urine. In humans, during times of low dietary intake, vitamin C is reabsorbed by the kidneys rather than excreted. Only when plasma concentrations are 1.4 mg/dL or higher does re-absorption decline and the excess amounts pass freely into the urine. This salvage process delays onset of deficiency. Ascorbic acid also converts (reversibly) to dehydroascorbate (DHA) and from that compound non-reversibly to 2,3-diketogluonate and then oxalate. These three compounds are also excreted via urine. Humans are better than guinea pigs at converting DHA back to ascorbate, and thus take much longer to become vitamin C deficient.\n\nAscorbic acid performs numerous physiological functions in the human body. These functions include the synthesis of collagen, carnitine, and neurotransmitters; the synthesis and catabolism of tyrosine; and the metabolism of microsome. During biosynthesis ascorbate acts as a reducing agent, donating electrons and preventing oxidation to keep iron and copper atoms in their reduced states.\n\nVitamin C acts as an electron donor for eight enzymes:\n\n\nThe name \"vitamin C\" always refers to the -enantiomer of ascorbic acid and its oxidized forms, such as dehydroascorbate (DHA). Therefore, unless written otherwise, \"ascorbate\" and \"ascorbic acid\" refer in the nutritional literature to -ascorbate and -ascorbic acid respectively. Ascorbic acid is a weak sugar acid structurally related to glucose. In biological systems, ascorbic acid can be found only at low pH, but in solutions above pH 5 is predominantly found in the ionized form, ascorbate. All of these molecules have vitamin C activity and thus are used synonymously with vitamin C, unless otherwise specified.\n\nNumerous analytical methods have been developed for ascorbic acid detection. For example, vitamin C content of a food sample such as fruit juice can be calculated by measuring the volume of the sample required to decolorize a solution of dichlorophenolindophenol (DCPIP) and then calibrating the results by comparison with a known concentration of vitamin C.\n\nSimple tests are available to measure the levels of vitamin C in the urine and in serum or blood plasma. However these reflect recent dietary intake rather than total body content. It has been observed that while serum or blood plasma concentrations follow a circadian rhythm or reflect short-term dietary impact, content within tissues is more stable and can give a better view of the availability of ascorbate within the entire organism. However, very few hospital laboratories are adequately equipped and trained to carry out such detailed analyses.\n\nThe vast majority of animals and plants are able to synthesize vitamin C, through a sequence of enzyme-driven steps, which convert monosaccharides to vitamin C. In plants, this is accomplished through the conversion of mannose or galactose to ascorbic acid. In some animals, glucose needed to produce ascorbate in the liver (in mammals and perching birds) is extracted from glycogen; ascorbate synthesis is a glycogenolysis-dependent process.\n\nAmong the mammals that have lost the ability to synthesize vitamin C are simians and tarsiers, which together make up one of two major primate suborders, Haplorrhini. This group includes humans. The other more primitive primates (Strepsirrhini) have the ability to make vitamin C. Synthesis does not occur in a number of species (perhaps all species) in the small rodent family Caviidae that includes guinea pigs and capybaras, but occurs in other rodents (rats and mice do not need vitamin C in their diet, for example).\n\nIn reptiles and birds the biosynthesis is carried out in the kidneys. A number of species of passerine birds also do not synthesize, but not all of them, and those that do not are not clearly related; there is a theory that the ability was lost separately a number of times in birds. In particular, the ability to synthesize vitamin C is presumed to have been lost and then later re-acquired in at least two cases. The ability to synthesize vitamin C has also been lost in about 96% of fish (the teleosts).\n\nMost tested families of bats (order Chiroptera), including major insect and fruit-eating bat families, cannot synthesize vitamin C. A trace of gulonolactone oxidase (GULO) was detected in only 1 of 34 bat species tested, across the range of 6 families of bats tested. There are at least two species of bats, frugivorous bat (\"Rousettus leschenaultii\") and insectivorous bat (\"Hipposideros armiger\"), that retain (or regained) their ability of vitamin C production.\n\nThese animals all lack the -gulonolactone oxidase (GULO) enzyme, which is required in the last step of vitamin C synthesis. The genomes of these species contain GULO as pseudogenes, which serve as insight into the evolutionary past of the species.\n\nSome of these species (including humans) are able to make do with the lower levels available from their diets by recycling oxidised vitamin C.\n\nMost simians consume the vitamin in amounts 10 to 20 times higher than that recommended by governments for humans. This discrepancy constitutes much of the basis of the controversy on current recommended dietary allowances. It is countered by arguments that humans are very good at conserving dietary vitamin C, and are able to maintain blood levels of vitamin C comparable with simians on a far smaller dietary intake, perhaps by recycling oxidized vitamin C.\n\nIn vertebrates that can synthesize ascorbic acid, the biosynthesis pathway starts with glucose, either taking place in the liver for mammals and some birds, or the kidneys for amphibians, reptiles and some birds. The pathway is the same. Several enzymes catalyze steps from D-glucose to D-glucuronate. Next, the enzyme glucuronate reductase converts D-glucuronate to L-gluconate. Then the enzyme gulonolactonase converts L-gluconate to L-gulonolactone. The final enzymatic conversion is by the enzyme L-gulonolactone oxidase (GLO), to 2-keto-gulonolactone. From this compound, the last step is a spontaneous, i.e., non-enzymatic conversion to ascorbic acid (vitamin C). GLO is the enzyme that is absent in animal species unable to synthesize vitamin C.\n\nAll plants synthesize ascorbic acid. Ascorbic acid functions as a cofactor for enzymes involved in photosynthesis, synthesis of plant hormones, as an antioxidant and also regenerator of other antioxidants. Plants use multiple pathways to synthesize vitamin C. The major pathway starts with glucose, fructose or mannose (all simple sugars) and proceeds to L-galactose, L-galaconolactone and ascorbic acid. There is feedback regulation in place, in that the presence of ascorbic acid suppresses enzymes in the synthesis pathway. This process follows a diurnal rhythm, so that enzyme expression peaks in the morning to support biosynthesis later on when mid-day sunlight intensity demands high ascorbic acid concentrations. Minor pathways may be specific to certain parts of plants; these can be either identical to the vertebrate pathway (including the GLO enzyme), or start with inositol and get to ascorbic acid via L-galactonic acid to L-galactonolactone.\n\nAscorbic acid is a common enzymatic cofactor in mammals used in the synthesis of collagen, as well as a powerful reducing agent capable of rapidly scavenging a number of reactive oxygen species (ROS). Given that ascorbate has these important functions, it is surprising that the ability to synthesize this molecule has not always been conserved. In fact, anthropoid primates, \"Cavia porcellus\" (guinea pigs), teleost fishes, most bats, and some Passeriform birds have all independently lost the ability to internally synthesize Vitamin C in either the kidney or the liver. In all of the cases where genomic analysis was done on an ascorbic acid auxotroph, the origin of the change was found to be a result of loss-of-function mutations in the gene that codes for L-Gulono-γ-lactone oxidase, the enzyme that catalyzes the last step of the ascorbic acid pathway outlined above.\n\nIn the case of the simians, it is thought that the loss of the ability to make vitamin C may have occurred much farther back in evolutionary history than the emergence of humans or even apes, since it evidently occurred soon after the appearance of the first primates, yet sometime after the split of early primates into the two major suborders Haplorrhini (which cannot make vitamin C) and its sister suborder of non-tarsier prosimians, the Strepsirrhini (\"wet-nosed\" primates), which retained the ability to make vitamin C. According to molecular clock dating, these two suborder primate branches parted ways about 63 to 60 million years ago. Approximately three to five million years later (58 million years ago), only a short time afterward from an evolutionary perspective, the infraorder Tarsiiformes, whose only remaining family is that of the tarsier (Tarsiidae), branched off from the other haplorrhines. Since tarsiers also cannot make vitamin C, this implies the mutation had already occurred, and thus must have occurred between these two marker points (63 to 58 million years ago).\n\nOne explanation for the repeated loss of the ability to synthesize vitamin C is that it was the result of genetic drift; assuming that the diet was rich in vitamin C, natural selection would not act to preserve it.\n\nSome scientists have suggested that loss of the vitamin C biosynthesis pathway may have played a role in rapid evolutionary changes, leading to hominids and the emergence of human beings. According to this theory, the loss of ascorbic acid's anti-oxidizing properties would have led to an increase in free radicals in the body. Free radicals are known to increase the frequency of genetic mutations, which would subsequently increase the speed of evolution.\n\nIt has also been noted that the loss of the ability to synthesize ascorbate strikingly parallels the inability to break down uric acid, also a characteristic of primates. Uric acid and ascorbate are both strong reducing agents. This has led to the suggestion that, in higher primates, uric acid has taken over some of the functions of ascorbate.\n\nVitamin C is produced from glucose by two main routes. The Reichstein process, developed in the 1930s, uses a single pre-fermentation followed by a purely chemical route. The modern two-step fermentation process, originally developed in China in the 1960s, uses additional fermentation to replace part of the later chemical stages. Both processes yield approximately 60% vitamin C from the glucose feed.\n\nWorld production of synthesized vitamin C was estimated at approximately 110,000 tonnes annually in 2000. Traditionally, the main producers were BASF/Takeda, DSM, Merck and the China Pharmaceutical Group Ltd. of the People's Republic of China. By 2008 only the DSM plant in Scotland remained operational outside of China because of the strong price competition from China.\n\nThe world price of vitamin C rose sharply in 2008 partly as a result of rises in basic food prices but also in anticipation of a stoppage of the two Chinese plants, situated at Shijiazhuang near Beijing, as part of a general shutdown of polluting industry in China over the period of the Olympic games. Production resumed after the Olympics, but then five Chinese manufacturers met in 2010, among them Northeast Pharmaceutical Group and North China Pharmaceutical Group, and agreed to temporarily stop production in order to maintain prices. In 2011 an American suit was filed against four Chinese companies that allegedly colluded to limit production and fix prices of vitamin C in the United States. The companies did not deny the accusation but say in their defense that the Chinese government compelled them to act in this way. In January 2012 a United States judge ruled that the Chinese companies can be sued in the U.S. by buyers acting as a group. A verdict was reached in March 2013 imposing a $147.8 million fine. This verdict was reversed by the 2nd U.S. Circuit Court of Appeals in New York, on the grounds that China formally advised the Court that its laws required the vitamin C makers to violate the Sherman Act, a U.S. antitrust law. In June 2017 the U.S. Supreme Court announced that it would consider an appeal filed to reverse the lower court decision.\n\nThe need to include fresh plant food or raw animal flesh in the diet to prevent disease was known from ancient times. Native people living in marginal areas incorporated this into their medicinal lore. For example, spruce needles were used in temperate zones in infusions, or the leaves from species of drought-resistant trees in desert areas. In 1536, the French explorers Jacques Cartier and Daniel Knezevic, exploring the St. Lawrence River, used the local natives' knowledge to save his men who were dying of scurvy. He boiled the needles of the arbor vitae tree to make a tea that was later shown to contain 50 mg of vitamin C per 100 grams.\n\nIn the 1497 expedition of Vasco de Gama, the curative effects of citrus fruit were known. The Portuguese planted fruit trees and vegetables in Saint Helena, a stopping point for homebound voyages from Asia, and left their sick to be taken home by the next ship.\n\nAuthorities occasionally recommended plant food to prevent scurvy during long sea voyages. John Woodall, the first surgeon to the British East India Company, recommended the preventive and curative use of lemon juice in his 1617 book, \"The Surgeon's Mate\". In 1734, the Dutch writer Johann Bachstrom gave the firm opinion that \"\"scurvy is solely owing to a total abstinence from fresh vegetable food, and greens.\"\"\n\nScurvy had long been a principal killer of sailors during the long sea voyages. According to Jonathan Lamb, \"In 1499, Vasco da Gama lost 116 of his crew of 170; In 1520, Magellan lost 208 out of 230;...all mainly to scurvy.\"\n\nThe first attempt to give scientific basis for the cause of this disease was by a ship's surgeon in the Royal Navy, James Lind. While at sea in May 1747, Lind provided some crew members with two oranges and one lemon per day, in addition to normal rations, while others continued on cider, vinegar, sulfuric acid or seawater, along with their normal rations, in one of the world's first controlled experiments. The results showed that citrus fruits prevented the disease. Lind published his work in 1753 in his \"Treatise on the Scurvy\".\n\nFresh fruit was expensive to keep on board, whereas boiling it down to juice allowed easy storage but destroyed the vitamin (especially if boiled in copper kettles). It was 1796 before the British navy adopted lemon juice as standard issue at sea. In 1845, ships in the West Indies were provided with lime juice instead, and in 1860 lime juice was used throughout the Royal Navy, giving rise to the American use of the nickname \"limey\" for the British. Captain James Cook had previously demonstrated the advantages of carrying \"Sour krout\" on board, by taking his crews to the Hawaiian Islands without losing any of his men to scurvy. For this, the British Admiralty awarded him a medal.\n\nThe name \"antiscorbutic\" was used in the eighteenth and nineteenth centuries for foods known to prevent scurvy. These foods included lemons, limes, oranges, sauerkraut, cabbage, malt, and portable soup. In 1928, the Canadian Arctic anthropologist Vilhjalmur Stefansson showed that the Inuit avoid scurvy on a diet of largely raw meat. Later studies on traditional food diets of the Yukon First Nations, Dene, Inuit, and Métis of Northern Canada showed that their daily intake of vitamin C averaged between 52 and 62 mg/day, comparable with the Estimated Average Requirement.\n\nVitamin C was discovered in 1912, isolated in 1928 and synthesized in 1933, making it the first vitamin to be synthesized. Shortly thereafter Tadeus Reichstein succeeded in synthesizing the vitamin in bulk by what is now called the Reichstein process. This made possible the inexpensive mass-production of vitamin C. In 1934 Hoffmann–La Roche trademarked synthetic vitamin C under the brand name Redoxon and began to market it as a dietary supplement.\n\nIn 1907 a laboratory animal model which would help to identify the antiscorbutic factor was discovered by the Norwegian physicians Axel Holst and Theodor Frølich, who when studying shipboard beriberi, fed guinea pigs their test diet of grains and flour and were surprised when scurvy resulted instead of beriberi. By luck, this species did not make its own vitamin C, whereas mice and rats do. In 1912, the Polish biochemist Casimir Funk developed the concept of vitamins. One of these was thought to be the anti-scorbutic factor. In 1928, this was referred to as \"water-soluble C,\" although its chemical structure had not been determined.\n\nFrom 1928 to 1932, Albert Szent-Györgyi and Joseph L. Svirbely's Hungarian team, and Charles Glen King's American team, identified the anti-scorbutic factor. Szent-Györgyi isolated hexuronic acid from animal adrenal glands, and suspected it to be the antiscorbutic factor. In late 1931, Szent-Györgyi gave Svirbely the last of his adrenal-derived hexuronic acid with the suggestion that it might be the anti-scorbutic factor. By the spring of 1932, King's laboratory had proven this, but published the result without giving Szent-Györgyi credit for it. This led to a bitter dispute over priority. In 1933, Walter Norman Haworth chemically identified the vitamin as -hexuronic acid, proving this by synthesis in 1933. Haworth and Szent-Györgyi proposed that L-hexuronic acid be named a-scorbic acid, and chemically -ascorbic acid, in honor of its activity against scurvy. The term's etymology is from Latin, \"a-\" meaning away, or off from, while -scorbic is from Medieval Latin \"scorbuticus\" (pertaining to scurvy), cognate with Old Norse \"skyrbjugr\", French \"scorbut\", Dutch \"scheurbuik\" and Low German \"scharbock\". Partly for this discovery, Szent-Györgyi was awarded the 1937 Nobel Prize in Medicine, and Haworth shared that year's Nobel Prize in Chemistry.\n\nIn 1957, J.J. Burns showed that some mammals are susceptible to scurvy as their liver does not produce the enzyme -gulonolactone oxidase, the last of the chain of four enzymes that synthesize vitamin C. American biochemist Irwin Stone was the first to exploit vitamin C for its food preservative properties. He later developed the theory that humans possess a mutated form of the -gulonolactone oxidase coding gene.\n\nIn 2008, researchers at the University of Montpellier discovered that in humans and other primates the red blood cells have evolved a mechanism to more efficiently utilize the vitamin C present in the body by recycling oxidized -dehydroascorbic acid (DHA) back into ascorbic acid for reuse by the body. The mechanism was not found to be present in mammals that synthesize their own vitamin C.\n\nVitamin C megadosage is a term describing the consumption or injection of vitamin C in doses comparable to or higher than the amounts produced by the livers of mammals which are able to synthesize vitamin C. The theory behind this, although not the actual term, was described in 1970 in an article by Linus Pauling, a famous scientist who had been awarded the Nobel Prize in Chemistry in 1954. Briefly, his position was that for optimal health, humans should be consuming at least 2,300 mg/day to compensate for the inability to synthesize vitamin C. The recommendation also fell into the consumption range for gorillas - a non-synthesizing near-relative to humans. A second argument for high intake is that serum ascorbic acid concentrations increase as intake increases until it plateaus at about 190 to 200 micromoles per liter (µmol/L) once consumption exceeds 1,250 milligrams. As noted, government recommendations are a range of 40 to 110 mg/day and normal plasma is approximately 50 µmol/L, so 'normal' is about 25% of what can be achieved when oral consumption is in the proposed megadose range.\n\nLinus Pauling, famous awardee of the Nobel Prize for Chemistry, popularized the concept of high dose vitamin C as prevention and treatment of the common cold in 1970. A few years later he proposed that vitamin C would prevent cardiovascular disease, and that 10 grams/day, initially (10 days) administered intravenously and thereafter orally, would cure late-stage cancer. Mega-dosing with ascorbic acid has other champions, among them chemist Irwin Stone and the controversial Matthias Rath and Patrick Holford, who both have been accused of making unsubstantiated treatment claims for treating cancer and HIV infection.\n\nThe mega-dosing theory is to a large degree discredited. Modest benefits are demonstrated for the common cold. Benefits are not superior when supplement intakes of more than 1,000 mg/day are compared to intakes between 200 and 1,000 mg/day, and so not limited to the mega-dose range. The theory that large amounts of intravenous ascorbic acid can be used to treat late-stage cancer is - some forty years after Pauling's seminal paper - still considered unproven and still in need of high quality research. However, a lack of conclusive evidence has not stopped individual physicians from prescribing intravenous ascorbic acid to thousands of people with cancer.\n\nIn February 2011, the Swiss Post issued a postage stamp bearing a depiction of a model of a molecule of vitamin C to mark the International Year of Chemistry.\n\n",
                "Vitamin C megadosage\n\nVitamin C megadosage is a term describing the consumption or injection of vitamin C (ascorbate) in doses comparable to the amounts produced by the livers of most other mammals. Such dosages correspond to amounts well beyond the current Recommended Dietary Allowance of 90 mg/day, and often well beyond the Tolerable upper intake level of 2000 mg/day. Oral dosages are usually divided and consumed in portions over the day. Injections of hundreds of grams per day are advocated by some physicians for the treatment of certain conditions, poisonings, or recovery from trauma. People who practice vitamin C megadosage may consume many vitamin C pills throughout each day or dissolve pure vitamin C crystals in water or juice and drink it throughout the day.\n\nHistoric advocates of vitamin C megadosage include Linus Pauling, who won the Nobel Prize in Chemistry in 1954. Pauling argued that, due to a non-functional form of L-gulonolactone oxidase, an enzyme required to make vitamin C that is functional in most other mammalian relatives, humans have developed a number of adaptations to cope with the relative deficiency. These adaptations, he argued, ultimately shortened lifespan but could be reversed or mitigated by supplementing humans with the hypothetical amount of vitamin C that would have been produced in the body if the enzyme were working.\n\nVitamin C megadoses are claimed by alternative medicine advocates including Matthias Rath and Patrick Holford to have preventative and curative effects on diseases such as cancer and AIDS, but the available scientific evidence does not support these claims. Some trials show some effect in combination with other therapies, but this does not imply vitamin C megadoses in themselves have therapeutic effect.\n\nThe World Health Organization recommends a daily intake of 45 mg/day of vitamin C for healthy adults, and 25–30 mg/day in infants. The Vitamin C article provides examples of recommendations from individual countries. None exceed 110 mg/day. Vitamin C is necessary for production of collagen and other biomolecules, and for the prevention of scurvy. Vitamin C is an antioxidant, which has led to its endorsement by some researchers as a complementary therapy for improving quality of life. Since the 1930s, when it first became available in pure form, some physicians have experimented with higher than recommended vitamin C consumption or injection. Certain animal species, including haplorhine primates (which includes humans), members of the Caviidae family of rodents (including guinea pigs and capybaras), most species of bats, many passerine birds, and about 96% of fish (the teleosts), do not synthesize vitamin C internally.\n\nSince its discovery, vitamin C has been considered almost a panacea by some, although this led to suspicions of it being overhyped by others. Vitamin C has been promoted in alternative medicine as a treatment for the common cold, cancer, polio and various other illnesses. The evidence for these claims is mixed. Orthomolecular-based megadose recommendations for vitamin C are based mainly on theoretical speculation and observational studies, such as those published by Fred R. Klenner from the 1940s through the 1970s. There is a strong advocacy movement for such doses of vitamin C, and there is an absence of large scale, formal trials in the 10 to 200+ grams per day range. \n\nThe single repeatable side effect of oral megadose vitamin C is a mild laxative effect if the practitioner attempts to consume too much too quickly. In the United States and Canada, a tolerable upper intake level (UL) was set at 2000 mg/day, referencing this mild laxative effect as the reason for establishing the UL. However, the European Food Safety Authority (EFSA) reviewed the safety question in 2006 and reached the conclusion that there was not sufficient evidence to set a UL for vitamin C. The Japan National Institute of Health and Nutrition reviewed the same question in 2010 and also reached the conclusion that there was not sufficient evidence to set a UL.\n\nAbout 70-90% of vitamin C is adsorbed by the body when taken orally at normal levels (30–180 mg daily). Adsorption is only about 50% for daily doses of 1 g. Oral administration, even of mega doses, cannot raise blood concentration above 0.22 mM.\n\nHumans and other species that do not synthesize vitamin C carry a mutated and ineffective form of the enzyme L-gulonolactone oxidase, the fourth and last step in the ascorbate-producing machinery. In the anthropoids lineage, this mutation likely occurred 40 to 25 million years ago. The three surviving enzymes continue to produce the precursors to vitamin C, but the process is incomplete and the body then disassembles them.\n\nIn the 1960s, the Nobel-Prize-winning chemist Linus Pauling, after contact with Irwin Stone, began actively promoting vitamin C as a means to greatly improve human health and resistance to disease. His book \"How to Live Longer and Feel Better\" was a bestseller and advocated taking more than 10 grams per day orally, thus approaching the amounts released by the liver directly into the circulation in other mammals: an adult goat, a typical example of a vitamin-C-producing animal, will manufacture more than 13,000 mg of vitamin C per day in normal health and much more when stressed.\n\nMatthias Rath is a controversial German physician who worked with and published two articles discussing the possible relationship between lipoprotein and vitamin C with Pauling. He is an active proponent and publicist for high-dose vitamin C. Pauling's and Rath's extended theory states that deaths from scurvy in humans during the ice age, when vitamin C was scarce, selected for individuals who could repair arteries with a layer of cholesterol provided by lipoprotein(a), a lipoprotein found in vitamin C-deficient species.\n\nStone and Pauling believed that the optimum daily requirement of vitamin C is around 2,300 milligrams for a human requiring 2,500 kcal a day.\n\nPauling criticized the established US Recommended Daily Allowance, pointing out that it is based on the known quantities that will prevent acute scurvy but is not necessarily the dosage for optimal health.\n\nAlthough sometimes considered free of toxicity, there are known side effects from vitamin C intake, and it has been suggested that intravenous injections should require \"a medical environment and trained professionals.\"\n\n\nBlood levels of vitamin C remain steady at approximately 200 mg per day. Although vitamin C can be well tolerated at doses well above what government organizations recommend, adverse effects can occur at doses above 3 grams per day. The common 'threshold' side effect of megadoses is diarrhea. Other possible adverse effects include increased oxalate excretion and kidney stones, increased uric-acid excretion, systemic conditioning (\"rebound scurvy\"), preoxidant effects, iron overload, reduced absorption of vitamin B12 and copper, increased oxygen demand, and acid erosion of the teeth with chewing ascorbic-acid tablets. In addition, one case has been noted of a woman who had received a kidney transplant followed by high-dose vitamin C and died soon afterwards as a result of calcium oxalate deposits that destroyed her new kidney. Her doctors concluded that high-dose vitamin C therapy should be avoided in patients with renal failure.\n\nAs discussed previously, vitamin C generally exhibits low toxicity. The (the dose that will kill 50% of a population) is generally accepted to be 11900 milligrams [11.9 grams] per kilogram in rat populations.\n\nPharmaceuticals designed to reduce stomach acid, such as the proton-pump inhibitors (PPIs), are among the most widely sold drugs in the world. One PPI, omeprazole (Prilosec), has been found to lower the bioavailability of vitamin C by 12% after 28 days of treatment, independent of dietary intake. The probable mechanism of vitamin C reduction, intragastric pH elevated into alkalinity, would apply to all other PPI drugs, though not necessarily to doses of PPIs low enough to keep the stomach slightly acidic. In another study, 40 mg/day of omeprazole lowered the fasting gastric vitamin C levels from 3.8 to 0.7 µg/mL.\n\nAspirin may also inhibit the absorption of vitamin C.\n\nThere are regulations in most countries that limit the claims on the treatment of disease that can be placed on food and dietary supplement product labels. For example: Claims of therapeutic effect with respect to the treatment of any medical condition or disease are prohibited by the United States Food and Drug Administration even if substance has gone through well conducted clinical trials with positive outcomes. Claims are limited to Structure:Function phrasing (\"Helps maintain a healthy immune system\") and the following notice is mandatory on food and dietary supplement product labels that make these types of health claims: \"These statements have not been evaluated by the Food and Drug Administration. This product is not intended to diagnose, treat, cure, or prevent any disease.\"\n\nThe use of vitamin C in high doses as a treatment for cancer was promoted by Linus Pauling, based on a 1976 study published with Ewan Cameron which reported intravenous vitamin C significantly increased lifespans of patients with advanced cancer. This trial was criticized by the National Cancer Institute for being designed poorly, and three subsequent trials conducted at the Mayo Clinic could not replicate these results.\n\nMore recently, \"in vitro\" data in animal models suggests intravenous ascorbic acid at high doses may hold some promise in the treatment of cancer; however, this has not been supported in more rigorous clinical trials in humans. Preliminary clinical trials in humans have shown that it is unlikely to be a \"miracle pill\" for cancer and more research is necessary before any definitive conclusions about efficacy can be reached. A 2010 review of 33 years of research on vitamin C to treat cancer stated \"we have to conclude that we still do not know whether Vitamin C has any clinically significant antitumor activity. Nor do we know which histological types of cancers, if any, are susceptible to this agent. Finally, we don't know what the recommended dose of Vitamin C is, if there is indeed such a dose, that can produce an anti-tumor response.\"\n\nThe American Cancer Society has stated, \"Although high doses of vitamin C have been suggested as a cancer treatment, the available evidence from clinical trials has not shown any benefit.\"\n\nOne study performed in 2000 used high intravenous doses of vitamin C (66 mg/kg/hour over 24 hours, for a total dose of around 110 grams) after severe burn injury, but despite being described as promising, it has not been replicated by independent institutions and thus is not a widely accepted treatment. Based on that study, the American Burn Association (ABA) considers high-dose ascorbic acid an option to be considered for adjuvant therapy in addition to the more accepted standard treatments. However, one medical review article noted vitamin C at the doses studied can be toxic, and recommended further validation by future studies before this therapy is clinically used.\n\n\n"
            ],
            "good_message": false,
            "loggedData": [
                {
                    "curr_message": " Scurvy is a disease resulting from a lack of vitamin C. Early symptoms include weakness, feeling tired, and sore arms and legs.",
                    "last_key_pressed": "<<selected>>"
                }
            ],
            "offensive": null,
            "researchTask": true,
            "selectedData": [
                "Scurvy is a disease resulting from a lack of vitamin C. Early symptoms include weakness, feeling tired, and sore arms and legs."
            ],
            "shown_passages": [
                [
                    "Scurvy",
                    [
                        "Scurvy is a disease resulting from a lack of vitamin C. Early symptoms include weakness, feeling tired, and sore arms and legs.",
                        "Without treatment, decreased red blood cells, gum disease, changes to hair, and bleeding from the skin may occur.",
                        "As scurvy worsens there can be poor wound healing, personality changes, and finally death from infection or bleeding.",
                        "Typically, scurvy is caused by a lack of vitamin C in the diet.",
                        "It takes at least a month of little to no vitamin C before symptoms occur.",
                        "In modern times, it occurs most commonly in people with mental disorders, unusual eating habits, alcoholism, and old people who live alone."
                    ]
                ],
                [
                    "Vitamin C",
                    [
                        "Vitamin C, also known as ascorbic acid and -ascorbic acid, is a vitamin found in food and used as a dietary supplement.",
                        "The disease scurvy is prevented and treated with vitamin C-containing foods or dietary supplements.",
                        "Evidence does not support use in the general population for the prevention of the common cold.",
                        "There is, however, some evidence that regular use may shorten the length of colds.",
                        "It is unclear if supplementation affects the risk of cancer, cardiovascular disease, or dementia.",
                        "It may be taken by mouth or by injection.",
                        "Vitamin C is generally well tolerated.",
                        "Large doses may cause gastrointestinal discomfort, headache, trouble sleeping, and flushing of the skin."
                    ]
                ],
                [
                    "Vitamin C megadosage",
                    [
                        "Vitamin C megadosage is a term describing the consumption or injection of vitamin C (ascorbate) in doses comparable to the amounts produced by the livers of most other mammals.",
                        "Such dosages correspond to amounts well beyond the current Recommended Dietary Allowance of 90 mg/day, and often well beyond the Tolerable upper intake level of 2000 mg/day.",
                        "Oral dosages are usually divided and consumed in portions over the day.",
                        "Injections of hundreds of grams per day are advocated by some physicians for the treatment of certain conditions, poisonings, or recovery from trauma.",
                        "People who practice vitamin C megadosage may consume many vitamin C pills throughout each day or dissolve pure vitamin C crystals in water or juice and drink it throughout the day."
                    ]
                ]
            ],
            "speaker": "0_Wizard",
            "suggestionsConfig": "show",
            "suggestionsNumber": 5,
            "suggestionsShown": true,
            "text": " Scurvy is a disease resulting from a lack of vitamin C. Early symptoms include weakness, feeling tired, and sore arms and legs.",
            "textboxFilled": 0,
            "time": 1580233307.3737662,
            "timeTaken": 16.833,
            "totalTasks": 120,
            "turn": 12
        },
        {
            "checked_passage": {
                "no_passages_used": "no_passages_used"
            },
            "checked_sentence": {
                "no_passages_used": "no_passages_used"
            },
            "clicked_passages": {},
            "context": {
                "conversation": [
                    "I'm obsessed with filming. ",
                    "Cinematograpy is awesome!  It is the science or art of motion-picture photography by recording light or other electromagnetic radiation",
                    "How do cinematographers record light? ",
                    "They typically do it electronically by means of an image sensor, or chemically by means of a light-sensitive material such as film stock."
                ],
                "index": 29
            },
            "contextCount": 0,
            "full_passages": [
                "Cinematography\n\nCinematography (also called \"Direction of Photography\") is the science or art of motion-picture photography by recording light or other electromagnetic radiation, either electronically by means of an image sensor, or chemically by means of a light-sensitive material such as film stock.\n\nTypically, a lens is used to repeatedly focus the light reflected from objects into real images on the light-sensitive surface inside a camera during a questioned exposure, creating multiple images. With an electronic image sensor, this produces an electrical charge at each pixel, which is electronically processed and stored in a video file for subsequent display or processing. The result with photographic emulsion is a series of invisible latent images on the film stock, which are later chemically \"developed\" into a visible image. The images on the film stock are played back at a rapid speed and projected onto a screen, creating the illusion of motion.\n\nCinematography finds uses in many fields of science and business as well as for entertainment purposes and mass communication.\n\nThe word \"cinematography\" was created from the Greek words (\"kinema\"), meaning \"movement, motion\" and (\"graphein\") meaning \"to record\", together meaning \"recording motion.\" The word used to refer to the art, process, or job of filming movies, but later its meaning was restricted to \"motion picture photography.\"\n\nIn the 1830s, moving images were produced on revolving drums and disks, with independent invention by Simon von Stampfer (stroboscope) in Austria, Joseph Plateau (phenakistoscope) in Belgium, and William Horner (zoetrope) in Britain.\n\nIn 1845, Francis Ronalds invented the first successful camera able to make continuous recordings of the varying indications of meteorological and geomagnetic instruments over time. The cameras were supplied to numerous observatories around the world and some remained in use until well into the 20th century.\n\nWilliam Lincoln patented a device, in 1867, that showed animated pictures called the \"wheel of life\" or \"zoopraxiscope\". In it, moving drawings or photographs were watched through a slit.\n\nOn 19 June 1873, Eadweard Muybridge successfully photographed a horse named \"Sallie Gardner\" in fast motion using a series of 24 stereoscopic cameras. The cameras were arranged along a track parallel to the horse's, and each camera shutter was controlled by a trip wire triggered by the horse's hooves. They were 21 inches apart to cover the 20 feet taken by the horse stride, taking pictures at one thousandth of a second. At the end of the decade, Muybridge had adapted sequences of his photographs to a zoopraxiscope for short, primitive projected \"movies,\" which were sensations on his lecture tours by 1879 or 1880.\n\nNine years later, in 1882, French scientist Étienne-Jules Marey invented a chronophotographic gun, which was capable of taking 12 consecutive frames a second, recording all the frames of the same picture.\n\nThe late nineteenth to the early twentieth century brought rise to the use of film not only for entertainment purposes but for scientific exploration as well. French biologist and filmmaker Jean Painleve lobbied heavily for the use of film in the scientific field, as the new medium was more efficient in capturing and documenting the behavior, movement, and environment of microorganisms, cells, and bacteria, than the naked eye. The introduction of film into scientific fields allowed for not only the viewing \"new images and objects, such as cells and natural objects, but also the viewing of them in real time\", whereas prior to the invention of moving pictures, scientists and doctors alike had to rely on hand drawn sketches of human anatomy and its microorganisms. This posed a great inconvenience in the science and medical worlds. The development of film and increased usage of cameras allowed doctors and scientists to grasp a better understanding and knowledge of their projects.\n\nThe experimental film \"Roundhay Garden Scene\", filmed by Louis Le Prince on 14 October 1888, in Roundhay, Leeds, England, is the earliest surviving motion picture. This movie was shot on paper film.\n\nW. K. L. Dickson, working under the direction of Thomas Alva Edison, was the first to design a successful apparatus, the Kinetograph, patented in 1891. This camera took a series of instantaneous photographs on standard Eastman Kodak photographic emulsion coated onto a transparent celluloid strip 35 mm wide. The results of this work were first shown in public in 1893, using the viewing apparatus also designed by Dickson, the Kinetoscope. Contained within a large box, only one person at a time looking into it through a peephole could view the movie.\n\nIn the following year, Charles Francis Jenkins and his projector, the Phantoscope, made a successful audience viewing while Louis and Auguste Lumière perfected the Cinématographe, an apparatus that took, printed, and projected film, in Paris in December 1895. The Lumière brothers were the first to present projected, moving, photographic, pictures to a paying audience of more than one person.\n\nIn 1896, movie theaters were open in France (Paris, Lyon, Bordeaux, Nice, Marseille); Italy (Rome, Milan, Naples, Genoa, Venice, Bologna, Forlì); Brussels; and London.\n\nIn 1896, Edison showed his improved Vitascope projector, the first commercially successful projector in the U.S.\n\nCooper Hewitt invented mercury lamps which made it practical to shoot films indoors without sunlight in 1905.\n\nThe first animated cartoon was produced in 1906.\n\nCredits began to appear at the beginning of motion pictures in 1911.\n\nThe Bell and Howell 2709 movie camera invented in 1915 allowed directors to make close-ups without physically moving the camera.\n\nBy the late 1920s, most of the movies produced were sound films.\n\nWide screen formats were first experimented with in the 1950s.\n\nBy the 1970s, most movies were color films. IMAX and other 70mm formats gained popularity. Wide distribution of films became commonplace, setting the ground for \"blockbusters.\"\n\nFilm cinematography dominated the motion picture industry from its inception until the 2010s when digital cinematography became dominant. Film cinematography is still used by some directors, especially in specific applications or out of fondness of the format.\n\nFrom its birth in the 1880s, movies were predominantly monochrome. Contrary to popular belief, monochrome doesn't always mean black and white; it means a movie shot in a single tone or color. Since the cost of tinted film bases was substantially higher, most movies were produced in black and white monochrome. Even with the advent of early color experiments, the greater expense of color meant films were mostly made in black and white until the 1950s, when cheaper color processes were introduced, and in some years the percentage of films shot on color film surpassed 51%. By the 1960s, color became by far the dominant film stock. In the coming decades, the usage of color film greatly increased while monochrome films became scarce.\n\nAfter the advent of motion pictures, a tremendous amount of energy was invested in the production of photography in natural color. The invention of the talking picture further increased the demand for the use of color photography. However, in comparison to other technological advances of the time, the arrival of color photography was a relatively slow process.\n\nEarly movies were not actually color movies since they were shot monochrome and hand-colored or machine-colored afterwards. (Such movies are referred to as \"colored\" and not \"color\".) The earliest such example is the hand-tinted Annabelle Serpentine Dance in 1895 by Edison Manufacturing Company. Machine-based tinting later became popular. Tinting continued until the advent of natural color cinematography in the 1910s. Many black and white movies have been colorized recently using digital tinting. This includes footage shot from both world wars, sporting events and political propaganda.\n\nIn 1902, Edward Raymond Turner produced the first films with a natural color process rather than using colorization techniques. In 1908, kinemacolor was introduced. In the same year, the short film \"A Visit to the Seaside\" became the first natural color movie to be publicly presented.\n\nIn 1917, the earliest version of Technicolor was introduced. Kodachrome was introduced in 1935. Eastmancolor was introduced in 1950 and became the color standard for the rest of the century.\n\nIn the 2010s, color films were largely superseded by color digital cinematography.\n\nIn digital cinematography, the movie is shot on digital medium such as flash storage, as well as distributed through a digital medium such as a hard drive.\n\nBeginning in the late 1980s, Sony began marketing the concept of \"electronic cinematography,\" utilizing its analog Sony HDVS professional video cameras. The effort met with very little success. However, this led to one of the earliest digitally shot feature movies, \"Julia and Julia\", being produced in 1987. In 1998, with the introduction of HDCAM recorders and 1920 × 1080 pixel digital professional video cameras based on CCD technology, the idea, now re-branded as \"digital cinematography,\" began to gain traction in the market.\n\nShot and released in 1998, \"The Last Broadcast\" is believed by some to be the first feature-length video shot and edited entirely on consumer-level digital equipment. In May 1999, George Lucas challenged the supremacy of the movie-making medium of film for the first time by including footage filmed with high-definition digital cameras in \"\". In late 2013, Paramount became the first major studio to distribute movies to theaters in digital format, eliminating 35mm film entirely. Since then the demand of movies to be developed onto digital format rather than 35mm has increased drastically.\n\nAs digital technology improved, movie studios began increasingly shifting towards digital cinematography. Since the 2010s, digital cinematography has become the dominant form of cinematography after largely superseding film cinematography.\n\nNumerous aspects contribute to the art of cinematography, including:\n\nThe first film cameras were fastened directly to the head of a tripod or other support, with only the crudest kind of leveling devices provided, in the manner of the still-camera tripod heads of the period. The earliest film cameras were thus effectively fixed during the shot, and hence the first camera movements were the result of mounting a camera on a moving vehicle. The first known of these was a film shot by a Lumière cameraman from the back platform of a train leaving Jerusalem in 1896, and by 1898, there were a number of films shot from moving trains. Although listed under the general heading of \"panoramas\" in the sales catalogues of the time, those films shot straight forward from in front of a railway engine were usually specifically referred to as \"phantom rides\".\n\nIn 1897, Robert W. Paul had the first real rotating camera head made to put on a tripod, so that he could follow the passing processions of Queen Victoria's Diamond Jubilee in one uninterrupted shot. This device had the camera mounted on a vertical axis that could be rotated by a worm gear driven by turning a crank handle, and Paul put it on general sale the next year. Shots taken using such a \"panning\" head were also referred to as \"panoramas\" in the film catalogues of the first decade of the cinema. This eventually led to the creation of a panoramic photo as well.\n\nThe standard pattern for early film studios was provided by the studio which Georges Méliès had built in 1897. This had a glass roof and three glass walls constructed after the model of large studios for still photography, and it was fitted with thin cotton cloths that could be stretched below the roof to diffuse the direct ray of the sun on sunny days. The soft overall light without real shadows that this arrangement produced, and which also exists naturally on lightly overcast days, was to become the basis for film lighting in film studios for the next decade.\n\nCinematography can begin with digital image sensor or rolls of film. Advancements in film emulsion and grain structure provided a wide range of available film stocks. The selection of a film stock is one of the first decisions made in preparing a typical film production.\n\nAside from the film gauge selection – 8 mm (amateur), 16 mm (semi-professional), 35 mm (professional) and 65 mm (epic photography, rarely used except in special event venues) – the cinematographer has a selection of stocks in reversal (which, when developed, create a positive image) and negative formats along with a wide range of film speeds (varying sensitivity to light) from ISO 50 (slow, least sensitive to light) to 800 (very fast, extremely sensitive to light) and differing response to color (low saturation, high saturation) and contrast (varying levels between pure black (no exposure) and pure white (complete overexposure).\nAdvancements and adjustments to nearly all gauges of film create the \"super\" formats wherein the area of the film used to capture a single frame of an image is expanded, although the physical gauge of the film remains the same. Super 8 mm, Super 16 mm, and Super 35 mm all utilize more of the overall film area for the image than their \"regular\" non-super counterparts. The larger the film gauge, the higher the overall image resolution clarity and technical quality. The techniques used by the film laboratory to process the film stock can also offer a considerable variance in the image produced. By controlling the temperature and varying the duration in which the film is soaked in the development chemicals, and by skipping certain chemical processes (or partially skipping all of them), cinematographers can achieve very different looks from a single film stock in the laboratory. Some techniques that can be used are push processing, bleach bypass, and cross processing.\n\nMost of modern cinema uses digital cinematography and has no film stocks , but the cameras themselves can be adjusted in ways that go far beyond the abilities of one particular film stock. They can provide varying degrees of color sensitivity, image contrast, light sensitivity and so on. One camera can achieve all the various looks of different emulsions. Digital image adjustments such as ISO and contrast are executed by estimating the same adjustments that would take place if actual film were in use, and are thus vulnerable to the camera's sensor designers perceptions of various film stocks and image adjustment parameters.\n\nFilters, such as diffusion filters or color effect filters, are also widely used to enhance mood or dramatic effects. Most photographic filters are made up of two pieces of optical glass glued together with some form of image or light manipulation material between the glass. In the case of color filters, there is often a translucent color medium pressed between two planes of optical glass. Color filters work by blocking out certain color wavelengths of light from reaching the film. With color film, this works very intuitively wherein a blue filter will cut down on the passage of red, orange, and yellow light and create a blue tint on the film. In black-and-white photography, color filters are used somewhat counter intuitively; for instance a yellow filter, which cuts down on blue wavelengths of light, can be used to darken a daylight sky (by eliminating blue light from hitting the film, thus greatly underexposing the mostly blue sky) while not biasing most human flesh tone. Certain cinematographers, such as Christopher Doyle, are well known for their innovative use of filters. Filters can be used in front of the lens or, in some cases, behind the lens for different effects. Christopher Doyle was a pioneer for increased usage of filters in movies. He was highly respected throughout the cinema world.\n\nLenses can be attached to the camera to give a certain look, feel, or effect by focus, color, etc.\n\nAs does the human eye, the camera creates perspective and spatial relations with the rest of the world. However, unlike one's eye, a cinematographer can select different lenses for different purposes. Variation in focal length is one of the chief benefits. The focal length of the lens determines the angle of view and, therefore, the field of view. Cinematographers can choose from a range of wide-angle lenses, \"normal\" lenses and long focus lenses, as well as macro lenses and other special effect lens systems such as borescope lenses. Wide-angle lenses have short focal lengths and make spatial distances more obvious. A person in the distance is shown as much smaller while someone in the front will loom large. On the other hand, long focus lenses reduce such exaggerations, depicting far-off objects as seemingly close together and flattening perspective. The differences between the perspective rendering is actually not due to the focal length by itself, but by the distance between the subjects and the camera. Therefore, the use of different focal lengths in combination with different camera to subject distances creates these different rendering. Changing the focal length only while keeping the same camera position doesn't affect perspective but the camera angle of view only.\n\nA zoom lens allows a camera operator to change his focal length within a shot or quickly between setups for shots. As prime lenses offer greater optical quality and are \"faster\" (larger aperture openings, usable in less light) than zoom lenses, they are often employed in professional cinematography over zoom lenses. Certain scenes or even types of filmmaking, however, may require the use of zooms for speed or ease of use, as well as shots involving a zoom move.\n\nAs in other photography, the control of the exposed image is done in the lens with the control of the diaphragm aperture. For proper selection, the cinematographer needs that all lenses be engraved with T-Stop, not f-stop so that the eventual light loss due to the glass doesn't affect the exposure control when setting it using the usual meters. The choice of the aperture also affects image quality (aberrations) and depth of field.\n\nFocal length and diaphragm aperture affect the depth of field of a scene – that is, how much the background, mid-ground and foreground will be rendered in \"acceptable focus\" (only one exact plane of the image is in precise focus) on the film or video target. Depth of field (not to be confused with depth of focus) is determined by the aperture size and the focal distance. A large or deep depth of field is generated with a very small iris aperture and focusing on a point in the distance, whereas a shallow depth of field will be achieved with a large (open) iris aperture and focusing closer to the lens. Depth of field is also governed by the format size. If one considers the field of view and angle of view, the smaller the image is, the shorter the focal length should be, as to keep the same field of view. Then, the smaller the image is, the more depth of field is obtained, for the same field of view. Therefore, 70mm has less depth of field than 35mm for a given field of view, 16mm more than 35mm, and video cameras even more depth of field than 16mm. As videographers try to emulate the look of 35 mm film with digital cameras, this is one issue of frustration – excessive depth of field with digital cameras and using additional optical devices to reduce that depth of field.\n\nIn \"Citizen Kane\" (1941), cinematographer Gregg Toland and director Orson Welles used tighter apertures to create every detail of the foreground and background of the sets in sharp focus. This practice is known as deep focus. Deep focus became a popular cinematographic device from the 1940s onwards in Hollywood. Today, the trend is for more shallow focus.\n\nTo change the plane of focus from one object or character to another within a shot is commonly known as a \"rack focus\".\n\nThe aspect ratio of an image is the ratio of its width to its height. This can be expressed either as a ratio of 2 integers, such as 4:3, or in a decimal format, such as 1.33:1 or simply 1.33.\n\nDifferent ratios provide different aesthetic effects. Standards for aspect ratio have varied significantly over time.\n\nDuring the silent era, aspect ratios varied widely, from square 1:1, all the way up to the extreme widescreen 4:1 Polyvision. However, from the 1910s, silent motion pictures generally settled on the ratio of 4:3 (1.33). The introduction of sound-on-film briefly narrowed the aspect ratio, to allow room for a sound stripe. In 1932, a new standard was introduced, the Academy ratio of 1.37, by means of thickening the frame line.\n\nFor years, mainstream cinematographers were limited to using the Academy ratio, but in the 1950s, thanks to the popularity of Cinerama, widescreen ratios were introduced in an effort to pull audiences back into the theater and away from their home television sets. These new widescreen formats provided cinematographers a wider frame within which to compose their images.\n\nMany different proprietary photographic systems were invented and utilized in the 1950s to create widescreen movies, but one dominated film: the anamorphic process, which optically squeezes the image to photograph twice the horizontal area to the same size vertical as standard \"spherical\" lenses. The first commonly used anamorphic format was CinemaScope, which used a 2.35 aspect ratio, although it was originally 2.55. CinemaScope was used from 1953 to 1967, but due to technical flaws in the design and its ownership by Fox, several third-party companies, led by Panavision's technical improvements in the 1950s, dominated the anamorphic cine lens market. Changes to SMPTE projection standards altered the projected ratio from 2.35 to 2.39 in 1970, although this did not change anything regarding the photographic anamorphic standards; all changes in respect to the aspect ratio of anamorphic 35 mm photography are specific to camera or projector gate sizes, not the optical system. After the \"widescreen wars\" of the 1950s, the motion-picture industry settled into 1.85 as a standard for theatrical projection in the United States and the United Kingdom. This is a cropped version of 1.37. Europe and Asia opted for 1.66 at first, although 1.85 has largely permeated these markets in recent decades. Certain \"epic\" or adventure movies utilized the anamorphic 2.39.\n\nIn the 1990s, with the advent of high-definition video, television engineers created the 1.78 (16:9) ratio as a mathematical compromise between the theatrical standard of 1.85 and television's 1.33, as it was not practical to produce a traditional CRT television tube with a width of 1.85. Until that point, nothing had ever been originated in 1.78. Today, this is a standard for high-definition video and for widescreen television.\n\nLight is necessary to create an image exposure on a frame of film or on a digital target (CCD, etc.). The art of lighting for cinematography goes far beyond basic exposure, however, into the essence of visual storytelling. Lighting contributes considerably to the emotional response an audience has watching a motion picture. The increased usage of filters can greatly impact the final image and affect the lighting.\n\nCinematography can not only depict a moving subject but can use a camera, which represents the audience's viewpoint or perspective, that moves during the course of filming. This movement plays a considerable role in the emotional language of film images and the audience's emotional reaction to the action. Techniques range from the most basic movements of panning (horizontal shift in viewpoint from a fixed position; like turning your head side-to-side) and tilting (vertical shift in viewpoint from a fixed position; like tipping your head back to look at the sky or down to look at the ground) to dollying (placing the camera on a moving platform to move it closer or farther from the subject), tracking (placing the camera on a moving platform to move it to the left or right), craning (moving the camera in a vertical position; being able to lift it off the ground as well as swing it side-to-side from a fixed base position), and combinations of the above. Early cinematographers often faced problems that were not common to other graphic artists because of the element of motion.\nCameras have been mounted to nearly every imaginable form of transportation.\n\nMost cameras can also be handheld, that is held in the hands of the camera operator who moves from one position to another while filming the action. Personal stabilizing platforms came into being in the late 1970s through the invention of Garrett Brown, which became known as the Steadicam. The Steadicam is a body harness and stabilization arm that connects to the camera, supporting the camera while isolating it from the operator's body movements. After the Steadicam patent expired in the early 1990s, many other companies began manufacturing their concept of the personal camera stabilizer. This invention is much more common throughout the cinematic world today. From feature-length films to the evening news, more and more networks have begun to use a personal camera stabilizer.\n\nThe first special effects in the cinema were created while the film was being shot. These came to be known as \"in-camera\" effects. Later, optical and digital effects were developed so that editors and visual effects artists could more tightly control the process by manipulating the film in post-production.\n\nThe 1896 movie The Execution of Mary Stuart shows an actor dressed as the queen placing her head on the execution block in front of a small group of bystanders in Elizabethan dress. The executioner brings his axe down, and the queen's severed head drops onto the ground. This trick was worked by stopping the camera and replacing the actor with a dummy, then restarting the camera before the axe falls. The two pieces of film were then trimmed and cemented together so that the action appeared continuous when the film was shown. Thus creating an overall illusion and successfully laying the foundation for special affects.\n\nThis film was among those exported to Europe with the first Kinetoscope machines in 1895 and was seen by Georges Méliès, who was putting on magic shows in his Theatre Robert-Houdin in Paris at the time. He took up filmmaking in 1896, and after making imitations of other films from Edison, Lumière, and Robert Paul, he made \"Escamotage d'un dame chez Robert-Houdin (The Vanishing Lady)\". This film shows a woman being made to vanish by using the same stop motion technique as the earlier Edison film. After this, Georges Méliès made many single shot films using this trick over the next couple of years.\n\nThe other basic technique for trick cinematography involves double exposure of the film in the camera, which was first done by George Albert Smith in July 1898 in the UK. Smith's \"The Corsican Brothers\" (1898) was described in the catalogue of the Warwick Trading Company, which took up the distribution of Smith's films in 1900, thus:\n\"One of the twin brothers returns home from shooting in the Corsican mountains, and is visited by the ghost of the other twin. By extremely careful photography the ghost appears *quite transparent*. After indicating that he has been killed by a sword-thrust, and appealing for vengeance, he disappears. A 'vision' then appears showing the fatal duel in the snow. To the Corsican's amazement, the duel and death of his brother are vividly depicted in the vision, and overcome by his feelings, he falls to the floor just as his mother enters the room.\"\nThe ghost effect was done by draping the set in black velvet after the main action had been shot, and then re-exposing the negative with the actor playing the ghost going through the actions at the appropriate point. Likewise, the vision, which appeared within a circular vignette or matte, was similarly superimposed over a black area in the backdrop to the scene, rather than over a part of the set with detail in it, so that nothing appeared through the image, which seemed quite solid. Smith used this technique again in \"Santa Claus\" (1898).\n\nGeorges Méliès first used superimposition on a dark background in \"La Caverne maudite (The Cave of the Demons)\" made a couple of months later in 1898, and elaborated it with multiple superimpositions in the one shot in \"Un Homme de têtes (The Four Troublesome Heads)\". He created further variations in subsequent films.\n\nMotion picture images are presented to an audience at a constant speed. In the theater it is 24 frames per second, in NTSC (US) Television it is 30 frames per second (29.97 to be exact), in PAL (Europe) television it is 25 frames per second. This speed of presentation does not vary.\n\nHowever, by varying the speed at which the image is captured, various effects can be created knowing that the faster or slower recorded image will be played at a constant speed. Giving the cinematographer even more freedom for creativity and expression to be made.\n\nFor instance, time-lapse photography is created by exposing an image at an extremely slow rate. If a cinematographer sets a camera to expose one frame every minute for four hours, and then that footage is projected at 24 frames per second, a four-hour event will take 10 seconds to present, and one can present the events of a whole day (24 hours) in just one minute.\n\nThe inverse of this, if an image is captured at speeds above that at which they will be presented, the effect is to greatly slow down (slow motion) the image. If a cinematographer shoots a person diving into a pool at 96 frames per second, and that image is played back at 24 frames per second, the presentation will take 4 times as long as the actual event. Extreme slow motion, capturing many thousands of frames per second can present things normally invisible to the human eye, such as bullets in flight and shockwaves travelling through media, a potentially powerful cinematographical technique.\n\nIn motion pictures, the manipulation of time and space is a considerable contributing factor to the narrative storytelling tools. Film editing plays a much stronger role in this manipulation, but frame rate selection in the photography of the original action is also a contributing factor to altering time. For example, Charlie Chaplin's \"Modern Times\" was shot at \"silent speed\" (18 fps) but projected at \"sound speed\" (24 fps), which makes the slapstick action appear even more frenetic.\n\nSpeed ramping, or simply \"ramping\", is a process whereby the capture frame rate of the camera changes over time. For example, if in the course of 10 seconds of capture, the capture frame rate is adjusted from 60 frames per second to 24 frames per second, when played back at the standard movie rate of 24 frames per second, a unique time-manipulation effect is achieved. For example, someone pushing a door open and walking out into the street would appear to start off in slow-motion, but in a few seconds later within the same shot, the person would appear to walk in \"realtime\" (normal speed). The opposite speed-ramping is done in \"The Matrix\" when Neo re-enters the Matrix for the first time to see the Oracle. As he comes out of the warehouse \"load-point\", the camera zooms into Neo at normal speed but as it gets closer to Neo's face, time seems to slow down, foreshadowing the manipulation of time itself within the Matrix later in the movie.\n\nG.A. Smith initiated the technique of reverse motion and also improved the quality of self-motivating images. This he did by repeating the action a second time while filming it with an inverted camera and then joining the tail of the second negative to that of the first. The first films using this were \"Tipsy, Topsy, Turvy\" and \"The Awkward Sign Painter\", the latter which showed a sign painter lettering a sign, and then the painting on the sign vanishing under the painter's brush. The earliest surviving example of this technique is Smith's \"The House That Jack Built\", made before September 1901. Here, a small boy is shown knocking down a castle just constructed by a little girl out of children's building blocks. A title then appears, saying \"Reversed\", and the action is repeated in reverse so that the castle re-erects itself under his blows.\n\nCecil Hepworth improved upon this technique by printing the negative of the forwards motion backwards frame by frame, so that in the production of the print the original action was exactly reversed. Hepworth made \"The Bathers\" in 1900, in which bathers who have undressed and jumped into the water appear to spring backwards out of it, and have their clothes magically fly back onto their bodies.\n\nThe use of different camera speeds also appeared around 1900. Robert Paul's \"On a Runaway Motor Car through Piccadilly Circus\" (1899), had the camera turn so slowly that when the film was projected at the usual 16 frames per second, the scenery appeared to be passing at great speed. Cecil Hepworth used the opposite effect in \"The Indian Chief and the Seidlitz powder\" (1901), in which a naïve Red Indian eats a lot of the fizzy stomach medicine, causing his stomach to expand and then he then leaps around balloon-like. This was done by cranking the camera faster than the normal 16 frames per second giving the first \"slow motion\" effect.\n\nIn descending order of seniority, the following staff is involved:\n\n\nIn the film industry, the cinematographer is responsible for the technical aspects of the images (lighting, lens choices, composition, exposure, filtration, film selection), but works closely with the director to ensure that the artistic aesthetics are supporting the director's vision of the story being told. The cinematographers are the heads of the camera, grip and lighting crew on a set, and for this reason, they are often called directors of photography or DPs. The ASC defines cinematography as a creative and interpretive process that culminates in the authorship of an original work of art rather than the simple recording of a physical event. Cinematography is not a subcategory of photography. Rather, photography is but one craft that the cinematographer uses in addition to other physical, organizational, managerial, interpretive. and image-manipulating techniques to effect one coherent process.\nIn British tradition, if the DOP actually operates the camera him/herself they are called the \"cinematographer\". On smaller productions, it is common for one person to perform all these functions alone. The career progression usually involves climbing up the ladder from seconding, firsting, eventually to operating the camera.\n\nDirectors of photography make many creative and interpretive decisions during the course of their work, from pre-production to post-production, all of which affect the overall feel and look of the motion picture. Many of these decisions are similar to what a photographer needs to note when taking a picture: the cinematographer controls the film choice itself (from a range of available stocks with varying sensitivities to light and color), the selection of lens focal lengths, aperture exposure and focus. Cinematography, however, has a temporal aspect (see persistence of vision), unlike still photography, which is purely a single still image. It is also bulkier and more strenuous to deal with movie cameras, and it involves a more complex array of choices. As such a cinematographer often needs to work co-operatively with more people than does a photographer, who could frequently function as a single person. As a result, the cinematographer's job also includes personnel management and logistical organization. Given the in-depth knowledge. a cinematographer requires not only of his or her own craft but also that of other personnel, formal tuition in analogue or digital filmmaking can be advantageous.\n\n\n",
                "Photography\n\nPhotography is the science, art, application and practice of creating durable images by recording light or other electromagnetic radiation, either electronically by means of an image sensor, or chemically by means of a light-sensitive material such as photographic film.\n\nTypically, a lens is used to focus the light reflected or emitted from objects into a real image on the light-sensitive surface inside a camera during a timed exposure. With an electronic image sensor, this produces an electrical charge at each pixel, which is electronically processed and stored in a digital image file for subsequent display or processing. The result with photographic emulsion is an invisible latent image, which is later chemically \"developed\" into a visible image, either negative or positive depending on the purpose of the photographic material and the method of processing. A negative image on film is traditionally used to photographically create a positive image on a paper base, known as a print, either by using an enlarger or by contact printing.\n\nPhotography is employed in many fields of science, manufacturing (e.g., photolithography), and business, as well as its more direct uses for art, film and video production, recreational purposes, hobby, and mass communication.\n\nThe word \"photography\" was created from the Greek roots φωτός (\"phōtos\"), genitive of φῶς (\"phōs\"), \"light\" and γραφή (\"graphé\") \"representation by means of lines\" or \"drawing\", together meaning \"drawing with light\".\n\nSeveral people may have coined the same new term from these roots independently. Hercules Florence, a French painter and inventor living in Campinas, Brazil, used the French form of the word, \"photographie\", in private notes which a Brazilian historian believes were written in 1834. This claim is widely reported but apparently has never been independently confirmed as beyond reasonable doubt.\nThe German newspaper \"Vossische Zeitung\" of 25 February 1839 contained an article entitled \"Photographie\", discussing several priority claims - especially Talbot's - regarding Daguerre's claim of invention. The article is the earliest known occurrence of the word in public print. It was signed \"J.M.\", believed to have been Berlin astronomer Johann von Maedler. \nCredit has traditionally been given to Sir John Herschel both for coining the word and for introducing it to the public. His uses of it in private correspondence prior to 25 February 1839 and at his Royal Society lecture on the subject in London on 14 March 1839 have long been amply documented and accepted as settled facts.\n\nThe inventors Niépce, Talbot and Daguerre seem not to have known or used the word \"photography\", but referred to their processes as \"Heliography\" (Niépce), \"Photogenic Drawing\" / \"Talbotype\" / \"Calotype\" (Talbot) and \"Daguerreotype\" (Daguerre).\n\nPhotography is the result of combining several technical discoveries. Long before the first photographs were made, ancient Han Chinese philosopher Mo Di from the Mohist School of Logic was the first to discover and develop the scientific principles of optics, camera obscura, and pinhole camera. Later Greek mathematicians Aristotle and Euclid also independently described a pinhole camera in the 5th and 4th centuries BCE. In the 6th century CE, Byzantine mathematician Anthemius of Tralles used a type of camera obscura in his experiments. Both the Han Chinese polymath Shen Kuo (1031–95) and Arab physicist Ibn al-Haytham (Alhazen) (965–1040) independently invented the camera obscura and pinhole camera, Albertus Magnus (1193–1280) discovered silver nitrate, and Georg Fabricius (1516–71) discovered silver chloride. Shen Kuo explains the science of camera obscura and optical physics in his scientific work Dream Pool Essays while the techniques described in Ibn al-Haytham's Book of Optics are capable of producing primitive photographs using medieval materials.\n\nDaniele Barbaro described a diaphragm in 1566. Wilhelm Homberg described how light darkened some chemicals (photochemical effect) in 1694. The fiction book \"Giphantie\", published in 1760, by French author Tiphaigne de la Roche, described what can be interpreted as photography.\n\nThe discovery of the camera obscura that provides an image of a scene dates back to ancient China. Leonardo da Vinci mentions natural camera obscura that are formed by dark caves on the edge of a sunlit valley. A hole in the cave wall will act as a pinhole camera and project a laterally reversed, upside down image on a piece of paper. So the birth of photography was primarily concerned with inventing means to capture and keep the image produced by the camera obscura.\n\nRenaissance painters used the camera obscura which, in fact, gives the optical rendering in color that dominates Western Art. The camera obscura literally means \"dark chamber\" in Latin. It is a box with a hole in it which allows light to go through and create an image onto the piece of paper.\n\nAround the year 1800, British inventor Thomas Wedgwood made the first known attempt to capture the image in a camera obscura by means of a light-sensitive substance. He used paper or white leather treated with silver nitrate. Although he succeeded in capturing the shadows of objects placed on the surface in direct sunlight, and even made shadow copies of paintings on glass, it was reported in 1802 that \"the images formed by means of a camera obscura have been found too faint to produce, in any moderate time, an effect upon the nitrate of silver.\" The shadow images eventually darkened all over.\n\nThe first permanent photoetching was an image produced in 1822 by the French inventor Nicéphore Niépce, but it was destroyed in a later attempt to make prints from it. Niépce was successful again in 1825. In 1826 or 1827, he made the \"View from the Window at Le Gras\", the earliest surviving photograph from nature (i.e., of the image of a real-world scene, as formed in a camera obscura by a lens).\n\nBecause Niépce's camera photographs required an extremely long exposure (at least eight hours and probably several days), he sought to greatly improve his bitumen process or replace it with one that was more practical. In partnership with Louis Daguerre, he worked out post-exposure processing methods that produced visually superior results and replaced the bitumen with a more light-sensitive resin, but hours of exposure in the camera were still required. With an eye to eventual commercial exploitation, the partners opted for total secrecy.\n\nNiépce died in 1833 and Daguerre then redirected the experiments toward the light-sensitive silver halides, which Niépce had abandoned many years earlier because of his inability to make the images he captured with them light-fast and permanent. Daguerre's efforts culminated in what would later be named the daguerreotype process. The essential elements—a silver-plated surface sensitized by iodine vapor, developed by mercury vapor, and \"fixed\" with hot saturated salt water—were in place in 1837. The required exposure time was measured in minutes instead of hours. Daguerre took the earliest confirmed photograph of a person in 1838 while capturing a view of a Paris street: unlike the other pedestrian and horse-drawn traffic on the busy boulevard, which appears deserted, one man having his boots polished stood sufficiently still throughout the several-minutes-long exposure to be visible. The existence of Daguerre's process was publicly announced, without details, on 7 January 1839. The news created an international sensation. France soon agreed to pay Daguerre a pension in exchange for the right to present his invention to the world as the gift of France, which occurred when complete working instructions were unveiled on 19 August 1839. In that same year, American photographer Robert Cornelius is credited with taking the earliest surviving photographic self-portrait.\n\nIn Brazil, Hercules Florence had apparently started working out a silver-salt-based paper process in 1832, later naming it \"Photographie\".\n\nMeanwhile, a British inventor, William Fox Talbot, had succeeded in making crude but reasonably light-fast silver images on paper as early as 1834 but had kept his work secret. After reading about Daguerre's invention in January 1839, Talbot published his hitherto secret method and set about improving on it. At first, like other pre-daguerreotype processes, Talbot's paper-based photography typically required hours-long exposures in the camera, but in 1840 he created the calotype process, which used the chemical development of a latent image to greatly reduce the exposure needed and compete with the daguerreotype. In both its original and calotype forms, Talbot's process, unlike Daguerre's, created a translucent negative which could be used to print multiple positive copies; this is the basis of most modern chemical photography up to the present day, as Daguerreotypes could only be replicated by rephotographing them with a camera. Talbot's famous tiny paper negative of the Oriel window in Lacock Abbey, one of a number of camera photographs he made in the summer of 1835, may be the oldest camera negative in existence.\n\nBritish chemist John Herschel made many contributions to the new field. He invented the cyanotype process, later familiar as the \"blueprint\". He was the first to use the terms \"photography\", \"negative\" and \"positive\". He had discovered in 1819 that sodium thiosulphate was a solvent of silver halides, and in 1839 he informed Talbot (and, indirectly, Daguerre) that it could be used to \"fix\" silver-halide-based photographs and make them completely light-fast. He made the first glass negative in late 1839.\nIn the March 1851 issue of \"The Chemist\", Frederick Scott Archer published his wet plate collodion process. It became the most widely used photographic medium until the gelatin dry plate, introduced in the 1870s, eventually replaced it. There are three subsets to the collodion process; the Ambrotype (a positive image on glass), the Ferrotype or Tintype (a positive image on metal) and the glass negative, which was used to make positive prints on albumen or salted paper.\n\nMany advances in photographic glass plates and printing were made during the rest of the 19th century. In 1891, Gabriel Lippmann introduced a process for making natural-color photographs based on the optical phenomenon of the interference of light waves. His scientifically elegant and important but ultimately impractical invention earned him the Nobel Prize in Physics in 1908.\n\nGlass plates were the medium for most original camera photography from the late 1850s until the general introduction of flexible plastic films during the 1890s. Although the convenience of the film greatly popularized amateur photography, early films were somewhat more expensive and of markedly lower optical quality than their glass plate equivalents, and until the late 1910s they were not available in the large formats preferred by most professional photographers, so the new medium did not immediately or completely replace the old. Because of the superior dimensional stability of glass, the use of plates for some scientific applications, such as astrophotography, continued into the 1990s, and in the niche field of laser holography, it has persisted into the 2010s.\n\nHurter and Driffield began pioneering work on the light sensitivity of photographic emulsions in 1876. Their work enabled the first quantitative measure of film speed to be devised.\n\nThe first flexible photographic roll film was marketed by George Eastman in 1885, but this original \"film\" was actually a coating on a paper base. As part of the processing, the image-bearing layer was stripped from the paper and transferred to a hardened gelatin support. The first transparent plastic roll film followed in 1889. It was made from highly flammable nitrocellulose (\"celluloid\"), now usually called \"nitrate film\".\n\nAlthough cellulose acetate or \"safety film\" had been introduced by Kodak in 1908, at first it found only a few special applications as an alternative to the hazardous nitrate film, which had the advantages of being considerably tougher, slightly more transparent, and cheaper. The changeover was not completed for X-ray films until 1933, and although safety film was always used for 16 mm and 8 mm home movies, nitrate film remained standard for theatrical 35 mm motion pictures until it was finally discontinued in 1951.\n\nFilms remained the dominant form of photography until the early 21st century when advances in digital photography drew consumers to digital formats. Although modern photography is dominated by digital users, film continues to be used by enthusiasts and professional photographers. The distinctive \"look\" of film based photographs compared to digital images is likely due to a combination of factors, including: (1) differences in spectral and tonal sensitivity (S-shaped density-to-exposure (H&D curve) with film vs. linear response curve for digital CCD sensors) (2) resolution and (3) continuity of tone.\n\nOriginally, all photography was monochrome, or \"black-and-white\". Even after color film was readily available, black-and-white photography continued to dominate for decades, due to its lower cost and its \"classic\" photographic look. The tones and contrast between light and dark areas define black-and-white photography. It is important to note that monochromatic pictures are not necessarily composed of pure blacks, whites, and intermediate shades of gray but can involve shades of one particular hue depending on the process. The cyanotype process, for example, produces an image composed of blue tones. The albumen print process first used more than years ago, produces brownish tones.\n\nMany photographers continue to produce some monochrome images, sometimes because of the established archival permanence of well-processed silver-halide-based materials. Some full-color digital images are processed using a variety of techniques to create black-and-white results, and some manufacturers produce digital cameras that exclusively shoot monochrome. Monochrome printing or electronic display can be used to salvage certain photographs taken in color which are unsatisfactory in their original form; sometimes when presented as black-and-white or single-color-toned images they are found to be more effective. Although color photography has long predominated, monochrome images are still produced, mostly for artistic reasons. Almost all digital cameras have an option to shoot in monochrome, and almost all image editing software can combine or selectively discard RGB color channels to produce a monochrome image from one shot in color.\n\nColor photography was explored beginning in the 1840s. Early experiments in color required extremely long exposures (hours or days for camera images) and could not \"fix\" the photograph to prevent the color from quickly fading when exposed to white light.\n\nThe first permanent color photograph was taken in 1861 using the three-color-separation principle first published by Scottish physicist James Clerk Maxwell in 1855. The foundation of virtually all practical color processes, Maxwell's idea was to take three separate black-and-white photographs through red, green and blue filters. This provides the photographer with the three basic channels required to recreate a color image. Transparent prints of the images could be projected through similar color filters and superimposed on the projection screen, an additive method of color reproduction. A color print on paper could be produced by superimposing carbon prints of the three images made in their complementary colors, a subtractive method of color reproduction pioneered by Louis Ducos du Hauron in the late 1860s.\n\nRussian photographer Sergei Mikhailovich Prokudin-Gorskii made extensive use of this color separation technique, employing a special camera which successively exposed the three color-filtered images on different parts of an oblong plate. Because his exposures were not simultaneous, unsteady subjects exhibited color \"fringes\" or, if rapidly moving through the scene, appeared as brightly colored ghosts in the resulting projected or printed images.\n\nImplementation of color photography was hindered by the limited sensitivity of early photographic materials, which were mostly sensitive to blue, only slightly sensitive to green, and virtually insensitive to red. The discovery of dye sensitization by photochemist Hermann Vogel in 1873 suddenly made it possible to add sensitivity to green, yellow and even red. Improved color sensitizers and ongoing improvements in the overall sensitivity of emulsions steadily reduced the once-prohibitive long exposure times required for color, bringing it ever closer to commercial viability.\n\nAutochrome, the first commercially successful color process, was introduced by the Lumière brothers in 1907. Autochrome plates incorporated a mosaic color filter layer made of dyed grains of potato starch, which allowed the three color components to be recorded as adjacent microscopic image fragments. After an Autochrome plate was reversal processed to produce a positive transparency, the starch grains served to illuminate each fragment with the correct color and the tiny colored points blended together in the eye, synthesizing the color of the subject by the additive method. Autochrome plates were one of several varieties of additive color screen plates and films marketed between the 1890s and the 1950s.\n\nKodachrome, the first modern \"integral tripack\" (or \"monopack\") color film, was introduced by Kodak in 1935. It captured the three color components in a multi-layer emulsion. One layer was sensitized to record the red-dominated part of the spectrum, another layer recorded only the green part and a third recorded only the blue. Without special film processing, the result would simply be three superimposed black-and-white images, but complementary cyan, magenta, and yellow dye images were created in those layers by adding color couplers during a complex processing procedure.\n\nAgfa's similarly structured Agfacolor Neu was introduced in 1936. Unlike Kodachrome, the color couplers in Agfacolor Neu were incorporated into the emulsion layers during manufacture, which greatly simplified the processing. Currently, available color films still employ a multi-layer emulsion and the same principles, most closely resembling Agfa's product.\n\nInstant color film, used in a special camera which yielded a unique finished color print only a minute or two after the exposure, was introduced by Polaroid in 1963.\n\nColor photography may form images as positive transparencies, which can be used in a slide projector, or as color negatives intended for use in creating positive color enlargements on specially coated paper. The latter is now the most common form of film (non-digital) color photography owing to the introduction of automated photo printing equipment. After a transition period centered around 1995–2005, color film was relegated to a niche market by inexpensive multi-megapixel digital cameras. Film continues to be the preference of some photographers because of its distinctive \"look\".\n\nIn 1981, Sony unveiled the first consumer camera to use a charge-coupled device for imaging, eliminating the need for film: the Sony Mavica. While the Mavica saved images to disk, the images were displayed on television, and the camera was not fully digital. In 1991, Kodak unveiled the DCS 100, the first commercially available digital single lens reflex camera. Although its high cost precluded uses other than photojournalism and professional photography, commercial digital photography was born.\n\nDigital imaging uses an electronic image sensor to record the image as a set of electronic data rather than as chemical changes on film. An important difference between digital and chemical photography is that chemical photography resists photo manipulation because it involves film and photographic paper, while digital imaging is a highly manipulative medium. This difference allows for a degree of image post-processing that is comparatively difficult in film-based photography and permits different communicative potentials and applications.\n\nDigital photography dominates the 21st century. More than 99% of photographs taken around the world are through digital cameras, increasingly through smartphones.\n\nSynthesis photography is part of computer-generated imagery (CGI) where the shooting process is modeled on real photography. The CGI, creating digital copies of real universe, requires a visual representation process of these universes. Synthesis photography is the application of analog and digital photography in digital space. With the characteristics of the real photography but not being constrained by the physical limits of real world, synthesis photography allows to get away from real photography.\n\nA large variety of photographic techniques and media are used in the process of capturing images for photography. These include the camera; stereoscopy; dualphotography; full-spectrum, ultraviolet and infrared media; light field photography; and other imaging techniques.\n\nThe camera is the image-forming device, and a photographic plate, photographic film or a silicon electronic image sensor is the capture medium. The respective recording medium can be the plate or film itself, or a digital magnetic or electronic memory.\n\nPhotographers control the camera and lens to \"expose\" the light recording material to the required amount of light to form a \"latent image\" (on plate or film) or RAW file (in digital cameras) which, after appropriate processing, is converted to a usable image. Digital cameras use an electronic image sensor based on light-sensitive electronics such as charge-coupled device (CCD) or complementary metal-oxide-semiconductor (CMOS) technology. The resulting digital image is stored electronically, but can be reproduced on a paper.\n\nThe camera (or 'camera obscura') is a dark room or chamber from which, as far as possible, all light is excluded except the light that forms the image. It was discovered and used in the 16th century by painters. The subject being photographed, however, must be illuminated. Cameras can range from small to very large, a whole room that is kept dark while the object to be photographed is in another room where it is properly illuminated. This was common for reproduction photography of flat copy when large film negatives were used (see Process camera).\n\nAs soon as photographic materials became \"fast\" (sensitive) enough for taking candid or surreptitious pictures, small \"detective\" cameras were made, some actually disguised as a book or handbag or pocket watch (the \"Ticka\" camera) or even worn hidden behind an Ascot necktie with a tie pin that was really the lens.\n\nThe movie camera is a type of photographic camera which takes a rapid sequence of photographs on recording medium. In contrast to a still camera, which captures a single snapshot at a time, the movie camera takes a series of images, each called a \"frame\". This is accomplished through an intermittent mechanism. The frames are later played back in a movie projector at a specific speed, called the \"frame rate\" (number of frames per second). While viewing, a person's eyes and brain merge the separate pictures together to create the illusion of motion.\n\nPhotographs, both monochrome and color, can be captured and displayed through two side-by-side images that emulate human stereoscopic vision. Stereoscopic photography was the first that captured figures in motion. While known colloquially as \"3-D\" photography, the more accurate term is stereoscopy. Such cameras have long been realized by using film and more recently in digital electronic methods (including cell phone cameras).\n\nDualphotography consists of photographing a scene from both sides of a photographic device at once (e.g. camera for back-to-back dualphotography, or two networked cameras for portal-plane dualphotography). The dualphoto apparatus can be used to simultaneously capture both the subject and the photographer, or both sides of a geographical place at once, thus adding a supplementary narrative layer to that of a single image.\n\nUltraviolet and infrared films have been available for many decades and employed in a variety of photographic avenues since the 1960s. New technological trends in digital photography have opened a new direction in full spectrum photography, where careful filtering choices across the ultraviolet, visible and infrared lead to new artistic visions.\n\nModified digital cameras can detect some ultraviolet, all of the visible and much of the near infrared spectrum, as most digital imaging sensors are sensitive from about 350 nm to 1000 nm. An off-the-shelf digital camera contains an infrared hot mirror filter that blocks most of the infrared and a bit of the ultraviolet that would otherwise be detected by the sensor, narrowing the accepted range from about 400 nm to 700 nm.\n\nReplacing a hot mirror or infrared blocking filter with an infrared pass or a wide spectrally transmitting filter allows the camera to detect the wider spectrum light at greater sensitivity. Without the hot-mirror, the red, green and blue (or cyan, yellow and magenta) colored micro-filters placed over the sensor elements pass varying amounts of ultraviolet (blue window) and infrared (primarily red and somewhat lesser the green and blue micro-filters).\n\nUses of full spectrum photography are for fine art photography, geology, forensics and law enforcement.\n\nDigital methods of image capture and display processing have enabled the new technology of \"light field photography\" (also known as synthetic aperture photography). This process allows focusing at various depths of field to be selected \"after\" the photograph has been captured. As explained by Michael Faraday in 1846, the \"light field\" is understood as 5-dimensional, with each point in 3-D space having attributes of two more angles that define the direction of each ray passing through that point.\n\nThese additional vector attributes can be captured optically through the use of microlenses at each pixel point within the 2-dimensional image sensor. Every pixel of the final image is actually a selection from each sub-array located under each microlens, as identified by a post-image capture focus algorithm.\n\nBesides the camera, other methods of forming images with light are available. For instance, a photocopy or xerography machine forms permanent images but uses the transfer of static electrical charges rather than photographic medium, hence the term electrophotography. Photograms are images produced by the shadows of objects cast on the photographic paper, without the use of a camera. Objects can also be placed directly on the glass of an image scanner to produce digital pictures.\n\nAn amateur photographer is one who practices photography as a hobby/passion and not necessarily for profit. The quality of some amateur work is comparable to that of many professionals and may be highly specialized or eclectic in choice of subjects. Amateur photography is often pre-eminent in photographic subjects which have little prospect of commercial use or reward. Amateur photography grew during the late 19th century due to the popularization of the hand-held camera. Nowadays it has spread widely through social media and is carried out throughout different platforms and equipment, switching to the use of cell phone as a key tool for making photography more accessible to everyone.\n\nCommercial photography is probably best defined as any photography for which the photographer is paid for images rather than works of art. In this light, money could be paid for the subject of the photograph or the photograph itself. Wholesale, retail, and professional uses of photography would fall under this definition. The commercial photographic world could include:\nThe market for photographic services demonstrates the aphorism \"A picture is worth a thousand words\", which has an interesting basis in the history of photography. Magazines and newspapers, companies putting up Web sites, advertising agencies and other groups pay for photography.\n\nMany people take photographs for commercial purposes. Organizations with a budget and a need for photography have several options: they can employ a photographer directly, organize a public competition, or obtain rights to stock photographs. Photo stock can be procured through traditional stock giants, such as Getty Images or Corbis; smaller microstock agencies, such as Fotolia; or web marketplaces, such as Cutcaster.\n\nDuring the 20th century, both fine art photography and documentary photography became accepted by the English-speaking art world and the gallery system. In the United States, a handful of photographers, including Alfred Stieglitz, Edward Steichen, John Szarkowski, F. Holland Day, and Edward Weston, spent their lives advocating for photography as a fine art.\nAt first, fine art photographers tried to imitate painting styles. This movement is called Pictorialism, often using soft focus for a dreamy, 'romantic' look. In reaction to that, Weston, Ansel Adams, and others formed the Group f/64 to advocate 'straight photography', the photograph as a (sharply focused) thing in itself and not an imitation of something else.\n\nThe aesthetics of photography is a matter that continues to be discussed regularly, especially in artistic circles. Many artists argued that photography was the mechanical reproduction of an image. If photography is authentically art, then photography in the context of art would need redefinition, such as determining what component of a photograph makes it beautiful to the viewer. The controversy began with the earliest images \"written with light\"; Nicéphore Niépce, Louis Daguerre, and others among the very earliest photographers were met with acclaim, but some questioned if their work met the definitions and purposes of art.\n\nClive Bell in his classic essay \"Art\" states that only \"significant form\" can distinguish art from what is not art.\n\nOn 7 February 2007, Sotheby's London sold the 2001 photograph \"99 Cent II Diptychon\" for an unprecedented $3,346,456 to an anonymous bidder, making it the most expensive at the time.\n\nConceptual photography turns a concept or idea into a photograph. Even though what is depicted in the photographs are real objects, the subject is strictly abstract.\n\nPhotojournalism is a particular form of photography (the collecting, editing, and presenting of news material for publication or broadcast) that employs images in order to tell a news story. It is now usually understood to refer only to still images, but in some cases the term also refers to video used in broadcast journalism. Photojournalism is distinguished from other close branches of photography (e.g., documentary photography, social documentary photography, street photography or celebrity photography) by complying with a rigid ethical framework which demands that the work be both honest and impartial whilst telling the story in strictly journalistic terms. Photojournalists create pictures that contribute to the news media, and help communities connect with one other. Photojournalists must be well informed and knowledgeable about events happening right outside their door. They deliver news in a creative format that is not only informative, but also entertaining.\n\nThe camera has a long and distinguished history as a means of recording scientific phenomena from the first use by Daguerre and Fox-Talbot, such as astronomical events (eclipses for example), small creatures and plants when the camera was attached to the eyepiece of microscopes (in photomicroscopy) and for macro photography of larger specimens. The camera also proved useful in recording crime scenes and the scenes of accidents, such as the Wootton bridge collapse in 1861. The methods used in analysing photographs for use in legal cases are collectively known as forensic photography. Crime scene photos are taken from three vantage point. The vantage points are overview, mid-range, and close-up.\n\nIn 1845 Francis Ronalds, the Honorary Director of the Kew Observatory, invented the first successful camera to make continuous recordings of meteorological and geomagnetic parameters. Different machines produced 12- or 24- hour photographic traces of the minute-by-minute variations of atmospheric pressure, temperature, humidity, atmospheric electricity, and the three components of geomagnetic forces. The cameras were supplied to numerous observatories around the world and some remained in use until well into the 20th century. Charles Brooke a little later developed similar instruments for the Greenwich Observatory.\n\nScience uses image technology that has derived from the design of the Pin Hole camera. X-Ray machines are similar in design to Pin Hole cameras with high-grade filters and laser radiation.\nPhotography has become ubiquitous in recording events and data in science and engineering, and at crime scenes or accident scenes. The method has been much extended by using other wavelengths, such as infrared photography and ultraviolet photography, as well as spectroscopy. Those methods were first used in the Victorian era and improved much further since that time.\n\nThe first photographed atom was discovered in 2012 by physicists at Griffith University, Australia. They used an electric field to trap an \"Ion\" of the element, Ytterbium. The image was recorded on a CCD, an electronic photographic film.\n\nThere are many ongoing questions about different aspects of photography. In her writing \"On Photography\" (1977), Susan Sontag discusses concerns about the objectivity of photography. This is a highly debated subject within the photographic community. Sontag argues, \"To photograph is to appropriate the thing photographed. It means putting one's self into a certain relation to the world that feels like knowledge, and therefore like power.\" Photographers decide what to take a photo of, what elements to exclude and what angle to frame the photo, and these factors may reflect a particular socio-historical context. Along these lines, it can be argued that photography is a subjective form of representation.\n\nModern photography has raised a number of concerns on its effect on society. In Alfred Hitchcock's \"Rear Window\" (1954), the camera is presented as promoting voyeurism. 'Although the camera is an observation station, the act of photographing is more than passive observing'.\nThe camera doesn't rape or even possess, though it may presume, intrude, trespass, distort, exploit, and, at the farthest reach of metaphor, assassinate – all activities that, unlike the sexual push and shove, can be conducted from a distance, and with some detachment.\nDigital imaging has raised ethical concerns because of the ease of manipulating digital photographs in post-processing. Many photojournalists have declared they will not crop their pictures or are forbidden from combining elements of multiple photos to make \"photomontages\", passing them as \"real\" photographs. Today's technology has made image editing relatively simple for even the novice photographer. However, recent changes of in-camera processing allow digital fingerprinting of photos to detect tampering for purposes of forensic photography.\n\nPhotography is one of the new media forms that changes perception and changes the structure of society. Further unease has been caused around cameras in regards to desensitization. Fears that disturbing or explicit images are widely accessible to children and society at large have been raised. Particularly, photos of war and pornography are causing a stir. Sontag is concerned that \"to photograph is to turn people into objects that can be symbolically possessed.\" Desensitization discussion goes hand in hand with debates about censored images. Sontag writes of her concern that the ability to censor pictures means the photographer has the ability to construct reality.\n\nOne of the practices through which photography constitutes society is tourism. Tourism and photography combine to create a \"tourist gaze\"\nin which local inhabitants are positioned and defined by the camera lens. However, it has also been argued that there exists a \"reverse gaze\" through which indigenous photographees can position the tourist photographer as a shallow consumer of images.\n\nAdditionally, photography has been the topic of many songs in popular culture.\n\nPhotography is both restricted as well as protected by the law in many jurisdictions. Protection of photographs is typically achieved through the granting of copyright or moral rights to the photographer. In the United States, photography is protected as a First Amendment right and anyone is free to photograph anything seen in public spaces as long as it is in plain view. In the UK a recent law (Counter-Terrorism Act 2008) increases the power of the police to prevent people, even press photographers, from taking pictures in public places.\n\n\n\n\n\n\n",
                "Digital camera\n\nA digital camera or digicam is a camera that captures photographs in digital memory. Most cameras produced today are digital, and while there are still dedicated compact cameras on the market, the use of dedicated digital cameras is dwindling, as digital cameras are now incorporated into many devices ranging from mobile devices to vehicles. However, expensive, high-end, high-definition dedicated cameras are still commonly used by professionals.\n\nDigital and movie cameras share an optical system, typically using a lens with a variable diaphragm to focus light onto an image pickup device. The diaphragm and shutter admit the correct amount of light to the imager, just as with film but the image pickup device is electronic rather than chemical. However, unlike film cameras, digital cameras can display images on a screen immediately after being recorded, and store and delete images from memory. Many digital cameras can also record moving videos with sound. Some digital cameras can crop and stitch pictures and perform other elementary image editing.\n\nThe history of the digital camera began with Eugene F. Lally of the Jet Propulsion Laboratory, who was thinking about how to use a mosaic photosensor to capture digital images. His 1961 idea was to take pictures of the planets and stars while travelling through space to give information about the astronauts' position. As with Texas Instruments employee Willis Adcock's filmless camera (US patent 4,057,830) in 1972, the technology had yet to catch up with the concept.\nSteven Sasson as an engineer at Eastman Kodak invented and built the first electronic camera using a charge-coupled device image sensor in 1975. Earlier ones used a camera tube; later ones digitized the signal. Early uses were mainly military and scientific; followed by medical and news applications.\n\nIn 1986, Japanese company Nikon introduced the first digital single-lens reflex (DSLR) camera, the Nikon SVC. In the mid-to-late 1990s, DSLR cameras became common among consumers. By the mid-2000s, DSLR cameras had largely replaced film cameras.\n\nIn 2000, Sharp introduced the world's first digital camera phone, the J-SH04 J-Phone, in Japan. By the mid-2000s, higher-end cell phones had an integrated digital camera. By the beginning of the 2010s, almost all smartphones had an integrated digital camera.\n\nThe two major types of digital image sensor are CCD and CMOS. A CCD sensor has one amplifier for all the pixels, while each pixel in a CMOS active-pixel sensor has its own amplifier. Compared to CCDs, CMOS sensors use less power. Cameras with a small sensor use a back-side-illuminated CMOS (BSI-CMOS) sensor. Overall final image quality is more dependent on the image processing capability of the camera, than on sensor type.\n\nThe resolution of a digital camera is often limited by the image sensor that turns light into discrete signals. The brighter the image at a given point on the sensor, the larger the value that is read for that pixel.\nDepending on the physical structure of the sensor, a color filter array may be used, which requires demosaicing to recreate a full-color image.\nThe number of pixels in the sensor determines the camera's \"pixel count\".\nIn a typical sensor, the pixel count is the product of the number of rows and the number of columns. For example, a 1,000 by 1,000 pixel sensor would have 1,000,000 pixels, or 1 megapixel.\n\nFinal quality of an image depends on all optical transformations in the chain of producing the image. Carl Zeiss points out that the weakest link in an optical chain determines the final image quality. In case of a digital camera, a simplistic way of expressing it is that the lens determines the maximum sharpness of the image while the image sensor determines the maximum resolution. The illustration on the right can be said to compare a lens with very poor sharpness on a camera with high resolution, to a lens with good sharpness on a camera with lower resolution.\n\nSince the first digital backs were introduced, there have been three main methods of capturing the image, each based on the hardware configuration of the sensor and color filters.\n\n\"Single-shot\" capture systems use either one sensor chip with a Bayer filter mosaic, or three separate image sensors (one each for the primary additive colors red, green, and blue) which are exposed to the same image via a beam splitter (see Three-CCD camera).\n\n\"Multi-shot\" exposes the sensor to the image in a sequence of three or more openings of the lens aperture. There are several methods of application of the multi-shot technique. The most common originally was to use a single image sensor with three filters passed in front of the sensor in sequence to obtain the additive color information. Another multiple shot method is called Microscanning. This method uses a single sensor chip with a Bayer filter and physically moved the sensor on the focus plane of the lens to construct a higher resolution image than the native resolution of the chip. A third version combined the two methods without a Bayer filter on the chip.\n\nThe third method is called \"scanning\" because the sensor moves across the focal plane much like the sensor of an image scanner. The \"linear\" or \"tri-linear\" sensors in scanning cameras utilize only a single line of photosensors, or three lines for the three colors. Scanning may be accomplished by moving the sensor (for example, when using color co-site sampling) or by rotating the whole camera. A digital rotating line camera offers images of very high total resolution.\n\nThe choice of method for a given capture is determined largely by the subject matter. It is usually inappropriate to attempt to capture a subject that moves with anything but a single-shot system. However, the higher color fidelity and larger file sizes and resolutions available with multi-shot and scanning backs make them attractive for commercial photographers working with stationary subjects and large-format photographs.\n\nImprovements in single-shot cameras and image file processing at the beginning of the 21st century made single shot cameras almost completely dominant, even in high-end commercial photography.\n\nMost current consumer digital cameras use a Bayer filter mosaic in combination with an optical anti-aliasing filter to reduce the aliasing due to the reduced sampling of the different primary-color images.\nA demosaicing algorithm is used to interpolate color information to create a full array of RGB image data.\n\nCameras that use a beam-splitter single-shot 3CCD approach, three-filter multi-shot approach, color co-site sampling or Foveon X3 sensor do not use anti-aliasing filters, nor demosaicing.\n\nFirmware in the camera, or a software in a raw converter program such as Adobe Camera Raw, interprets the raw data from the sensor to obtain a full color image, because the RGB color model requires three intensity values for each pixel: one each for the red, green, and blue (other color models, when used, also require three or more values per pixel).\nA single sensor element cannot simultaneously record these three intensities, and so a color filter array (CFA) must be used to selectively filter a particular color for each pixel.\n\nThe Bayer filter pattern is a repeating 2x2 mosaic pattern of light filters, with green ones at opposite corners and red and blue in the other two positions. The high proportion of green takes advantage of properties of the human visual system, which determines brightness mostly from green and is far more sensitive to brightness than to hue or saturation. Sometimes a 4-color filter pattern is used, often involving two different hues of green. This provides potentially more accurate color, but requires a slightly more complicated interpolation process.\n\nThe color intensity values not captured for each pixel can be interpolated from the values of adjacent pixels which represent the color being calculated.\n\nCameras with digital image sensors that are smaller than the typical 35mm film size have a smaller field or angle of view when used with a lens of the same focal length. This is because angle of view is a function of both focal length and the sensor or film size used.\n\nThe crop factor is relative to the 35mm film format. If a smaller sensor is used, as in most digicams, the field of view is cropped by the sensor to smaller than the 35mm full-frame format's field of view. This narrowing of the field of view may be described as crop factor, a factor by which a longer focal length lens would be needed to get the same field of view on a 35mm film camera. Full-frame digital SLRs utilize a sensor of the same size as a frame of 35mm film.\n\nCommon values for field of view crop in DSLRs using active pixel sensors include 1.3x for some Canon (APS-H) sensors, 1.5x for Sony APS-C sensors used by Nikon, Pentax and Konica Minolta and for Fujifilm sensors, 1.6 (APS-C) for most Canon sensors, ~1.7x for Sigma's Foveon sensors and 2x for Kodak and Panasonic 4/3-inch sensors currently used by Olympus and Panasonic. Crop factors for non-SLR consumer compact and bridge cameras are larger, frequently 4x or more.\n\nDigital cameras come in a wide range of sizes, prices and capabilities. In addition to general purpose digital cameras, specialized cameras including multispectral imaging equipment and astrographs are used for scientific, military, medical and other special purposes.\n\nCompact cameras are intended to be portable (pocketable) and are particularly suitable for casual \"snapshots\".\n\nMany incorporate a retractable lens assembly that provides optical zoom. In most models, an auto actuating lens cover protects the lens from elements. Most ruggedized or water-resistant models do not retract, and most with superzoom capability do not retract fully.\n\nCompact cameras are usually designed to be easy to use. Almost all include an automatic mode, or \"auto mode\", which automatically makes all camera settings for the user. Some also have manual controls. Compact digital cameras typically contain a small sensor which trades-off picture quality for compactness and simplicity; images can usually only be stored using lossy compression (JPEG). Most have a built-in flash usually of low power, sufficient for nearby subjects. A few high end compact digital cameras have a hotshoe for connecting to an external flash. Live preview is almost always used to frame the photo on an integrated LCD. In addition to being able to take still photographs almost all compact cameras have the ability to record video.\n\nCompacts often have macro capability and zoom lenses, but the zoom range (up to 30x) is generally enough for candid photography but less than is available on bridge cameras (more than 60x), or the interchangeable lenses of DSLR cameras available at a much higher cost. Autofocus systems in compact digital cameras generally are based on a contrast-detection methodology using the image data from the live preview feed of the main imager. Some compact digital cameras use a hybrid autofocus system similar to what is commonly available on DSLRs. Some high end travel compact cameras have 30x optical zoom have full manual control with lens ring, electronic viewfinder, Hybrid Optical Image Stabilization, built-in flash, Full HD 60p, RAW, burst shooting up to 10fps, built-in Wi-Fi with NFC and GPS altogether.\n\nTypically, compact digital cameras incorporate a nearly silent leaf shutter into the lens but play a simulated camera sound for skeuomorphic purposes.\n\nFor low cost and small size, these cameras typically use image sensor formats with a diagonal between 6 and 11 mm, corresponding to a crop factor between 7 and 4. This gives them weaker low-light performance, greater depth of field, generally closer focusing ability, and smaller components than cameras using larger sensors. Some cameras use a larger sensor including, at the high end, a pricey full-frame sensor compact camera, such as Sony Cyber-shot DSC-RX1, but have capability near that of a DSLR.\n\nA variety of additional features are available depending on the model of the camera. Such features include ones such as GPS, compass, barometer and altimeter for above mean sea level or under(water) mean sea level. and some are rugged and waterproof.\n\nStarting in 2011, some compact digital cameras can take 3D still photos. These 3D compact stereo cameras can capture 3D panoramic photos with dual lens or even single lens for play back on a 3D TV.\n\nIn 2013, Sony released two add-on camera models without display, to be used with a smartphone or tablet, controlled by a mobile application via WiFi.\n\nRugged compact cameras typically include protection against submersion, hot and cold conditions, shock and pressure. Terms used to describe such properties include waterproof, freezeproof, heatproof, shockproof and crushproof, respectively. Nearly all major camera manufacturers have at least one product in this category. Some are waterproof to a considerable depth up to 82 feet (27 m); others only 10 feet (3m), but only a few will float. Ruggeds often lack some of the features of ordinary compact camera, but they have video capability and the majority can record sound. Most have image stabilization and built-in flash. Touchscreen LCD and GPS do not work under water.\n\nGoPro and other brands offer action cameras which are rugged, small and can be easily attached to helmet, arm, bicycle, etc. Most have wide angle and fixed focus, and can take still pictures and video, typically with sound.\n\nThe rising popularity of action cameras is in line with many people desiring to share photos or videos in social media. Many competitive manufacturers of action cameras results in many options and lowered, competitive prices, and nowadays, cameras are sold bundled with waterproof housings, accessories, and mountings compatible with the popular GoPro.\n\nThe 360-degree camera can take picture or video 360 degrees using two lenses back-to-back and shooting at the same time. Some of the cameras are Ricoh Theta S, Nikon Keymission 360 and Samsung Gear 360. Nico360 was launched in 2016 and claimed as the world's smallest 360-degree camera with size 46 x 46 x 28 mm (1.8 x 1.8 x 1.1 in) and price less than $200. With virtual reality mode built-in stitching, Wifi, and Bluetooth, live streaming can be done. Due to it also being water resistant, the Nico360 can be used as action camera.\n\nThere are tend that action cameras have capabilities to shoot 360 degrees with at least 4K resolution.\n\nBridge cameras physically resemble DSLRs, and are sometimes called DSLR-shape or DSLR-like. They provide some similar features but, like compacts, they use a fixed lens and a small sensor. Some compact cameras have also PSAM mode. Most use live preview to frame the image. Their usual autofocus is by the same contrast-detect mechanism as compacts, but many bridge cameras have a manual focus mode and some have a separate focus ring for greater control.\n\nBig physical size and small sensor allow superzoom and wide aperture. Bridgcams generally include an image stabilization system to enable longer handheld exposures, sometimes better than DSLR for low light condition.\n\nAs of 2014, bridge cameras come in two principal classes in terms of sensor size, firstly the more traditional 1/2.3\" sensor (as measured by image sensor format) which gives more flexibility in lens design and allows for handholdable zoom from 20 to 24mm (35mm equivalent) wide angle all the way up to over 1000 mm supertele, and secondly a 1\" sensor that allows better image quality particularly in low light (higher ISO) but puts greater constraints on lens design, resulting in zoom lenses that stop at 200mm (constant aperture, e.g. Sony RX10) or 400mm (variable aperture, e.g. Panasonic Lumix FZ1000) equivalent, corresponding to an optical zoom factor of roughly 10 to 15.\n\nSome bridge cameras have a lens thread to attach accessories such as wide-angle or telephoto converters as well as filters such as UV or Circular Polarizing filter and lens hoods. The scene is composed by viewing the display or the electronic viewfinder (EVF). Most have a slightly longer shutter lag than a DSLR. Many of these cameras can store images in a raw format in addition to supporting JPEG. The majority have a built-in flash, but only a few have a hotshoe.\n\nIn bright sun, the quality difference between a good compact camera and a digital SLR is minimal but bridge cameras are more portable, cost less and have a greater zoom ability. Thus a bridge camera may better suit outdoor daytime activities, except when seeking professional-quality photos.\n\nIn late 2008, a new type of camera emerged called mirrorless interchangeable-lens camera (MILC), which uses various sensors and offers lens interchangeability. These are simpler and more compact than DSLRs due to not having a lens reflex system. MILC camera models are available with various sensor sizes including: a small 1/2.3 inch sensor, as is commonly used in bridge cameras such as the original Pentax Q (more recent Pentax Q versions have a slightly larger 1/1.7 inch sensor); a 1-inch sensor; a Micro Four Thirds sensor; an APS-C sensor such as the Sony NEX series, Fujifilm X series, Pentax K-01, and Canon EOS M; and some, such as the Sony α7, use a full frame (35 mm) sensor and even Hasselblad X1D is the first medium format MILC. Some MILC cameras have a separate electronic viewfinder. In other cameras the back display is used as a viewfinder in same way as in compact cameras. Disadvantage of MILC over DSLR is battery energy consume due to high energy consume of electronic viewfinder.\n\nOlympus and Panasonic released many Micro Four Thirds cameras with interchangeable lenses which are fully compatible each other without any adapter, while the others have proprietary mounts. In 2014, Kodak released its first Micro Four Third system camera.\n\n, MILC cameras are available which appeal to both amateurs and professionals.\n\nWhile most digital cameras with interchangeable lenses feature a lens-mount of some kind, there are also a number of modular cameras, where the shutter and sensor are incorporated into the lens module.\n\nThe first such modular camera was the Minolta Dimâge V in 1996, followed by the Minolta Dimâge EX 1500 in 1998 and the Minolta MetaFlash 3D 1500 in 1999. In 2009, Ricoh released the Ricoh GXR modular camera.\n\nAt CES 2013, Sakar International announced the Polaroid iM1836, an 18 MP camera with 1\"-sensor with interchangeable sensor-lens. An adapter for Micro Four Thirds, Nikon and K-mount lenses was planned to ship with the camera.\n\nThere are also a number of add-on camera modules for smartphones called lens-style cameras (lens camera). They contain all components of a digital camera in a module, but lack a viewfinder, display and most of the controls. Instead they can be mounted to a smartphone and use its display and controls. Lens-style cameras include:\n\nDigital single-lens reflex cameras (DSLR) use a reflex mirror that can reflect the light and also can swivel from one position to another position and back to initial position. By default, the reflex mirror is set 45 degree from horizontal, blocks the light to the sensor and reflects light from the lens to penta-mirror/prism at the DSLR camera and after some reflections arrives at the viewfinder. The reflex mirror is pulled out horizontally below the penta-mirror/prism when shutter release is fully pressed, so the viewfinder will be dark and the light/image can directly strike the sensor at the time of exposure (speed setting).\n\nAutofocus is accomplished using sensors in the mirror box. Some DSLRs have a \"live view\" mode that allows framing using the screen with image from the sensor.\n\nThese cameras have much larger sensors than the other types, typically 18 mm to 36 mm on the diagonal (crop factor 2, 1.6, or 1). The larger sensor permits more light to be received by each pixel; this, combined with the relatively large lenses provides superior low-light performance. For the same field of view and the same aperture, a larger sensor gives shallower focus.\nThey use interchangeable lenses for versatility. Usually some lenses are made for digital SLR use only, but recent trend the lenses can also be used in detachable lens video camera with or without adapter.\n\nA DSLT uses a fixed translucent mirror instead of a moving reflex mirror as in DSLR. A translucent mirror or transmissive mirror or semi-transparent mirror is a mirror which reflects the light to two things at the same time. It reflects it along the path to a pentaprism/pentamirror which then goes to an optical view finder (OVF) as is done with a reflex mirror in DSLR cameras. The translucent mirror also sends light along a second path to the sensor. The total amount of light is not changed, just some of the light travels one path and some of it travels the other. The consequences are that DSLT cameras should shoot a half stop differently from DSL. One advantage of using a DSLT camera is the blind moments a DSLR user experiences while the reflecting mirror is moved to send the light to the sensor instead of the viewfinder do not exist for DSLT cameras. Because there is no time at which light is not traveling along both paths, DSLT cameras get the benefit of continuous auto-focus tracking. This is especially beneficial for burst mode shooting in low-light conditions and also for tracking when taking video.\n\nUntil early 2014, only Sony had released DSLT cameras. By March 2014, Sony had released more DSLTs than DSLRs with a relatively complete lenses line-up.\n\nA rangefinder is a device to measure subject distance, with the intent to adjust the focus of a camera's objective lens accordingly (open-loop controller). The rangefinder and lens focusing mechanism may or may not be coupled. In common parlance, the term \"rangefinder camera\" is interpreted very narrowly to denote manual-focus cameras with a visually-read out optical rangefinder based on parallax. Most digital cameras achieve focus through analysis of the image captured by the objective lens and distance estimation, if it is provided at all, is only a byproduct of the focusing process (closed-loop controller).\n\nA line-scan camera traditionally has a single row of pixel sensors, instead of a matrix of them. The lines are continuously fed to a computer that joins them to each other and makes an image. This is most commonly done by connecting the camera output to a frame grabber which resides in a PCI slot of an industrial computer. The frame grabber acts to buffer the image and sometimes provide some processing before delivering to the computer software for processing.\n\nMultiple rows of sensors may be used to make colored images, or to increase sensitivity by TDI (Time delay and integration).\nMany industrial applications require a wide field of view. Traditionally maintaining consistent light over large 2D areas is quite difficult. With a line scan camera all that is necessary is to provide even illumination across the “line” currently being viewed by the camera. This makes possible sharp pictures of objects that pass the camera at high speed.\n\nSuch cameras are also commonly used to make photo finishes, to determine the winner when multiple competitors cross the finishing line at nearly the same time. They can also be used as industrial instruments for analyzing fast processes.\n\nLinescan cameras are also extensively used in imaging from satellites (see push broom scanner). In this case the row of sensors is perpendicular to the direction of satellite motion. Linescan cameras are widely used in scanners. In this case, the camera moves horizontally.\n\nStand alone cameras can be used as remote camera. One kind weighs 2.31 ounces (65.5 g), with a periscope shape, IPx7 water-resistance and dust-resistance rating and can be enhanced to IPx8 by using a cap. They have no viewfinder or LCD. Lens is a 146 degree wide angle or standard lens, with fixed focus. It can have a microphone and speaker, And it can take photos and video. As a remote camera, a phone app using Android or iOS is needed to send live video, change settings, take photos, or use time lapse.\n\nMany devices have a built-in digital camera, including, for example, smartphones, mobile phones, PDAs and laptop computers. Built-in cameras generally store the images in the JPEG file format.\n\nMobile phones incorporating digital cameras were introduced in Japan in 2001 by J-Phone. In 2003 camera phones outsold stand-alone digital cameras, and in 2006 they outsold film and digital stand-alone cameras. Five billion camera phones were sold in five years, and by 2007 more than half of the installed base of all mobile phones were camera phones. Sales of separate cameras peaked in 2008.\n\nSales of traditional digital cameras have declined due to the increasing use of smartphones for casual photography, which also enable easier manipulation and sharing of photos through the use of apps and web-based services. \"Bridge cameras\", in contrast, have held their ground with functionality that most smartphone cameras lack, such as optical zoom and other advanced features. DSLRs have also lost ground to Mirrorless interchangeable-lens camera (MILC)s offering the same sensor size in a smaller camera. A few expensive ones use a full-frame sensor as DSLR professional cameras.\n\nIn response to the convenience and flexibility of smartphone cameras, some manufacturers produced \"smart\" digital cameras that combine features of traditional cameras with those of a smartphone. In 2012, Nikon and Samsung released the Coolpix S800c and Galaxy Camera, the first two digital cameras to run the Android operating system. Since this software platform is used in many smartphones, they can integrate with services (such as e-mail attachments, social networks and photo sharing sites) as smartphones do, and use other Android-compatible software as well.\n\nIn an inversion, some phone makers have introduced smartphones with cameras designed to resemble traditional digital cameras. Nokia released the 808 PureView and Lumia 1020 in 2012 and 2013; the two devices respectively run the Symbian and Windows Phone operating systems, and both include a 41-megapixel camera (along with a camera grip attachment for the latter). Similarly, Samsung introduced the Galaxy S4 Zoom, having a 16-megapixel camera and 10x optical zoom, combining traits from the Galaxy S4 Mini with the Galaxy Camera. Furthermore, Panasonic Lumic DMC-CM1 is an Android KitKat 4.4 smartphone with 20MP, 1\" sensor, the largest sensor for a smartphone ever, with Leica fixed lens equivalent of 28mm at F2.8, can take RAW image and 4K video, has 21mm thickness.\n\nLight-field cameras were introduced in 2013 with one consumer product and several professional ones.\n\nAfter a big dip of sales in 2012, consumer digital camera sales declined again in 2013 by 36 percent. In 2011, compact digital cameras sold 10 million per month. In 2013, sales fell to about 4 million per month. DSLR and MILC sales also declined in 2013 by 10–15% after almost ten years of double digit growth.\nWorldwide unit sales of digital cameras is continuously declining from 148 million in 2011 to 58 million in 2015 and tends to decrease more in the following years.\n\nFilm camera sold got the peak at 36.671 million units in 1997 and digital camera sold began in 1999. In 2008, film camera market was dead and digital camera sold got the peak by 121.463 million units in 2010. In 2002, cell phone with camera has been introduced and in 2003 the cell phone with camera sold 80 million units per year. In 2011 the cell phone with camera sold hundreds of millions per year, when digital camera sold initialized to decline. In 2015, digital camera sold is 35.395 million units or only less than a third of digital camera sold number in a peak and also slightly less than film camera sold number in a peak.\n\nMany digital cameras can connect directly to a computer to transfer data:-\n\n\n\nA common alternative is the use of a card reader which may be capable of reading several types of storage media, as well as high speed transfer of data to the computer. Use of a card reader also avoids draining the camera battery during the download process. An external card reader allows convenient direct access to the images on a collection of storage media. But if only one storage card is in use, moving it back and forth between the camera and the reader can be inconvenient. Many computers have a card reader built in, at least for SD cards.\n\nMany modern cameras support the PictBridge standard, which allows them to send data directly to a PictBridge-capable computer printer without the need for a computer.\n\nWireless connectivity can also provide for printing photos without a cable connection.\nAn \"instant-print camera\", is a digital camera with a built-in printer. This confers a similar functionality as an instant camera which uses instant film to quickly generate a physical photograph. Such non-digital cameras were popularized by Polaroid in 1972.\n\nMany digital cameras include a video output port. Usually sVideo, it sends a standard-definition video signal to a television, allowing the user to show one picture at a time. Buttons or menus on the camera allow the user to select the photo, advance from one to another, or automatically send a \"slide show\" to the TV.\n\nHDMI has been adopted by many high-end digital camera makers, to show photos in their high-resolution quality on an HDTV.\n\nIn January 2008, Silicon Image announced a new technology for sending video from mobile devices to a television in digital form. MHL sends pictures as a video stream, up to 1080p resolution, and is compatible with HDMI.\n\nSome DVD recorders and television sets can read memory cards used in cameras; alternatively several types of flash card readers have TV output capability.\n\nCameras can be equipped with a varying amount of environmental sealing to provide protection against splashing water, moisture (humidity and fog), dust and sand, or complete waterproofness to a certain depth and for a certain duration. The latter is one of the approaches to allow underwater photography, the other approach being the use of waterproof housings. Many waterproof digital cameras are also shockproof and resistant to low temperatures.\n\nMany digital cameras have preset modes for different applications. Within the constraints of correct exposure various parameters can be changed, including exposure, aperture, focusing, light metering, white balance, and equivalent sensitivity. For example, a portrait might use a wider aperture to render the background out of focus, and would seek out and focus on a human face rather than other image content.\n\nMany camera phones and most stand alone digital cameras store image data in flash memory cards or other removable media. Most stand-alone cameras use SD format, while a few use CompactFlash or other types. In January 2012, a faster XQD card format was announced. In early 2014, some high end cameras have two hot-swapable memory slots. Photographers can swap one of the memory card with camera-on. Each memory slot can accept either Compact Flash or SD Card. All new Sony cameras also have two memory slots, one for its Memory Stick and one for SD Card, but not hot-swapable.\n\nA few cameras used other removable storage such as Microdrives (very small hard disk drives), CD single (185 MB), and 3.5\" floppy disks. Other unusual formats include:\nMost manufacturers of digital cameras do not provide drivers and software to allow their cameras to work with Linux or other free software. Still, many cameras use the standard USB storage protocol, and are thus easily usable. Other cameras are supported by the gPhoto project.\n\nThe Joint Photography Experts Group standard (JPEG) is the most common file format for storing image data. Other file types include Tagged Image File Format (TIFF) and various Raw image formats.\n\nMany cameras, especially high-end ones, support a raw image format. A raw image is the unprocessed set of pixel data directly from the camera's sensor, often saved in a proprietary format. Adobe Systems has released the DNG format, a royalty-free raw image format used by at least 10 camera manufacturers.\n\nRaw files initially had to be processed in specialized image editing programs, but over time many mainstream editing programs, such as Google's Picasa, have added support for raw images. Rendering to standard images from raw sensor data allows more flexibility in making major adjustments without losing image quality or retaking the picture.\n\nFormats for movies are AVI, DV, MPEG, MOV (often containing motion JPEG), WMV, and ASF (basically the same as WMV). Recent formats include MP4, which is based on the QuickTime format and uses newer compression algorithms to allow longer recording times in the same space.\n\nOther formats that are used in cameras (but not for pictures) are the Design Rule for Camera Format (DCF), an ISO specification, used in almost all camera since 1998, which defines an internal file structure and naming. Also used is the Digital Print Order Format (DPOF), which dictates what order images are to be printed in and how many copies. The DCF 1998 defines a logical file system with 8.3 filenames and makes the usage of either FAT12, FAT16, FAT32 or exFAT mandatory for its physical layer in order to maximize platform interoperability.\n\nMost cameras include Exif data that provides metadata about the picture. Exif data may include aperture, exposure time, focal length, date and time taken, and location.\n\nDigital cameras have become smaller over time, resulting in an ongoing need to develop a battery small enough to fit in the camera and yet able to power it for a reasonable length of time.\n\nDigital cameras utilize either proprietary or standard consumer batteries. , most cameras use proprietary lithium-ion batteries while some use standard AA batteries or primarily use a proprietary Lithium-ion rechargeable battery pack but have an optional AA battery holder available.\n\nThe most common class of battery used in digital cameras is proprietary battery formats. These are built to a manufacturer's custom specifications. Almost all proprietary batteries are lithium-ion. In addition to being available from the OEM, aftermarket replacement batteries are commonly available for most camera models.\n\nDigital cameras that utilize off-the-shelf batteries are typically designed to be able to use both single-use disposable and rechargeable batteries, but not with both types in use at the same time. The most common off-the-shelf battery size used is AA. CR2, CR-V3 batteries, and AAA batteries are also used in some cameras. The CR2 and CR-V3 batteries are lithium based, intended for a single use. Rechargeable RCR-V3 lithium-ion batteries are also available as an alternative to non-rechargeable CR-V3 batteries.\n\nSome battery grips for DSLRs come with a separate holder to accommodate AA cells as an external power source.\n\nWhen digital cameras became common, many photographers asked whether their film cameras could be converted to digital. The answer was yes and no. For the majority of 35 mm film cameras the answer is no, the reworking and cost would be too great, especially as lenses have been evolving as well as cameras. For most a conversion to digital, to give enough space for the electronics and allow a liquid crystal display to preview, would require removing the back of the camera and replacing it with a custom built digital unit.\n\nMany early professional SLR cameras, such as the Kodak DCS series, were developed from 35 mm film cameras. The technology of the time, however, meant that rather than being digital \"backs\" the bodies of these cameras were mounted on large, bulky digital units, often bigger than the camera portion itself. These were factory built cameras, however, not aftermarket conversions.\n\nA notable exception is the Nikon E2 and Nikon E3, using additional optics to convert the 35mm format to a 2/3 CCD-sensor.\n\nA few 35 mm cameras have had digital camera backs made by their manufacturer, Leica being a notable example. Medium format and large format cameras (those using film stock greater than 35 mm), have a low unit production, and typical digital backs for them cost over $10,000. These cameras also tend to be highly modular, with handgrips, film backs, winders, and lenses available separately to fit various needs.\n\nThe very large sensor these backs use leads to enormous image sizes. For example, Phase One's P45 39 MP image back creates a single TIFF image of size up to 224.6 MB, and even greater pixel counts are available. Medium format digitals such as this are geared more towards studio and portrait photography than their smaller DSLR counterparts; the ISO speed in particular tends to have a maximum of 400, versus 6400 for some DSLR cameras. (Canon EOS-1D Mark IV and Nikon D3S have ISO 12800 plus Hi-3 ISO 102400 with the Canon EOS-1Dx's ISO of 204800)\n\nIn the industrial and high-end professional photography market, some camera systems use modular (removable) image sensors. For example, some medium format SLR cameras, such as the Mamiya 645D series, allow installation of either a digital camera back or a traditional photographic film back.\n\n\nLinear array cameras are also called scan backs.\n\n\nMost earlier digital camera backs used linear array sensors, moving vertically to digitize the image. Many of them only capture grayscale images. The relatively long exposure times, in the range of seconds or even minutes generally limit scan backs to studio applications, where all aspects of the photographic scene are under the photographer's control.\n\nSome other camera backs use CCD arrays similar to typical cameras. These are called single-shot backs.\n\nSince it is much easier to manufacture a high-quality linear CCD array with only thousands of pixels than a CCD matrix with millions, very high resolution linear CCD camera backs were available much earlier than their CCD matrix counterparts. For example, you could buy an (albeit expensive) camera back with over 7,000 pixel horizontal resolution in the mid-1990s. However, , it is still difficult to buy a comparable CCD matrix camera of the same resolution. Rotating line cameras, with about 10,000 color pixels in its sensor line, are able, , to capture about 120,000 lines during one full 360 degree rotation, thereby creating a single digital image of 1,200 Megapixels.\n\nMost modern digital camera backs use CCD or CMOS matrix sensors. The matrix sensor captures the entire image frame at once, instead of incrementing scanning the frame area through the prolonged exposure. For example, Phase One produces a 39 million pixel digital camera back with a 49.1 x 36.8 mm CCD in 2008. This CCD array is a little smaller than a frame of 120 film and much larger than a 35 mm frame (36 x 24 mm). In comparison, consumer digital cameras use arrays ranging from 36 x 24 mm (full frame on high end consumer DSLRs) to 1.28 x 0.96 mm (on camera phones) CMOS sensor.\n\n\n"
            ],
            "good_message": false,
            "loggedData": [],
            "offensive": null,
            "researchTask": true,
            "selectedData": [],
            "shown_passages": [
                [
                    "Cinematography",
                    [
                        "Cinematography (also called \"Direction of Photography\") is the science or art of motion-picture photography by recording light or other electromagnetic radiation, either electronically by means of an image sensor, or chemically by means of a light-sensitive material such as film stock.",
                        "Typically, a lens is used to repeatedly focus the light reflected from objects into real images on the light-sensitive surface inside a camera during a questioned exposure, creating multiple images.",
                        "With an electronic image sensor, this produces an electrical charge at each pixel, which is electronically processed and stored in a video file for subsequent display or processing."
                    ]
                ],
                [
                    "Photography",
                    [
                        "Photography is the science, art, application and practice of creating durable images by recording light or other electromagnetic radiation, either electronically by means of an image sensor, or chemically by means of a light-sensitive material such as photographic film.",
                        "Typically, a lens is used to focus the light reflected or emitted from objects into a real image on the light-sensitive surface inside a camera during a timed exposure.",
                        "With an electronic image sensor, this produces an electrical charge at each pixel, which is electronically processed and stored in a digital image file for subsequent display or processing."
                    ]
                ],
                [
                    "Digital camera",
                    [
                        "A digital camera or digicam is a camera that captures photographs in digital memory.",
                        "Most cameras produced today are digital, and while there are still dedicated compact cameras on the market, the use of dedicated digital cameras is dwindling, as digital cameras are now incorporated into many devices ranging from mobile devices to vehicles.",
                        "However, expensive, high-end, high-definition dedicated cameras are still commonly used by professionals.",
                        "Digital and movie cameras share an optical system, typically using a lens with a variable diaphragm to focus light onto an image pickup device.",
                        "The diaphragm and shutter admit the correct amount of light to the imager, just as with film but the image pickup device is electronic rather than chemical."
                    ]
                ]
            ],
            "speaker": "0_Wizard",
            "suggestionsConfig": "show",
            "suggestionsNumber": 5,
            "suggestionsShown": true,
            "text": "They typically do it electronically by means of an image sensor, or chemically by means of a light-sensitive material such as film stock.",
            "textboxFilled": 0,
            "time": 1580233310.808211,
            "timeTaken": 3.341,
            "totalTasks": 120,
            "turn": 13
        },
        {
            "checked_passage": {
                "no_passages_used": "no_passages_used"
            },
            "checked_sentence": {
                "no_passages_used": "no_passages_used"
            },
            "clicked_passages": {},
            "context": {
                "conversation": [
                    "I'm obsessed with filming. ",
                    "Cinematograpy is awesome!  It is the science or art of motion-picture photography by recording light or other electromagnetic radiation",
                    "How do cinematographers record light? ",
                    "They typically do it electronically by means of an image sensor, or chemically by means of a light-sensitive material such as film stock."
                ],
                "index": 29
            },
            "contextCount": 0,
            "full_passages": [
                "Photography\n\nPhotography is the science, art, application and practice of creating durable images by recording light or other electromagnetic radiation, either electronically by means of an image sensor, or chemically by means of a light-sensitive material such as photographic film.\n\nTypically, a lens is used to focus the light reflected or emitted from objects into a real image on the light-sensitive surface inside a camera during a timed exposure. With an electronic image sensor, this produces an electrical charge at each pixel, which is electronically processed and stored in a digital image file for subsequent display or processing. The result with photographic emulsion is an invisible latent image, which is later chemically \"developed\" into a visible image, either negative or positive depending on the purpose of the photographic material and the method of processing. A negative image on film is traditionally used to photographically create a positive image on a paper base, known as a print, either by using an enlarger or by contact printing.\n\nPhotography is employed in many fields of science, manufacturing (e.g., photolithography), and business, as well as its more direct uses for art, film and video production, recreational purposes, hobby, and mass communication.\n\nThe word \"photography\" was created from the Greek roots φωτός (\"phōtos\"), genitive of φῶς (\"phōs\"), \"light\" and γραφή (\"graphé\") \"representation by means of lines\" or \"drawing\", together meaning \"drawing with light\".\n\nSeveral people may have coined the same new term from these roots independently. Hercules Florence, a French painter and inventor living in Campinas, Brazil, used the French form of the word, \"photographie\", in private notes which a Brazilian historian believes were written in 1834. This claim is widely reported but apparently has never been independently confirmed as beyond reasonable doubt.\nThe German newspaper \"Vossische Zeitung\" of 25 February 1839 contained an article entitled \"Photographie\", discussing several priority claims - especially Talbot's - regarding Daguerre's claim of invention. The article is the earliest known occurrence of the word in public print. It was signed \"J.M.\", believed to have been Berlin astronomer Johann von Maedler. \nCredit has traditionally been given to Sir John Herschel both for coining the word and for introducing it to the public. His uses of it in private correspondence prior to 25 February 1839 and at his Royal Society lecture on the subject in London on 14 March 1839 have long been amply documented and accepted as settled facts.\n\nThe inventors Niépce, Talbot and Daguerre seem not to have known or used the word \"photography\", but referred to their processes as \"Heliography\" (Niépce), \"Photogenic Drawing\" / \"Talbotype\" / \"Calotype\" (Talbot) and \"Daguerreotype\" (Daguerre).\n\nPhotography is the result of combining several technical discoveries. Long before the first photographs were made, ancient Han Chinese philosopher Mo Di from the Mohist School of Logic was the first to discover and develop the scientific principles of optics, camera obscura, and pinhole camera. Later Greek mathematicians Aristotle and Euclid also independently described a pinhole camera in the 5th and 4th centuries BCE. In the 6th century CE, Byzantine mathematician Anthemius of Tralles used a type of camera obscura in his experiments. Both the Han Chinese polymath Shen Kuo (1031–95) and Arab physicist Ibn al-Haytham (Alhazen) (965–1040) independently invented the camera obscura and pinhole camera, Albertus Magnus (1193–1280) discovered silver nitrate, and Georg Fabricius (1516–71) discovered silver chloride. Shen Kuo explains the science of camera obscura and optical physics in his scientific work Dream Pool Essays while the techniques described in Ibn al-Haytham's Book of Optics are capable of producing primitive photographs using medieval materials.\n\nDaniele Barbaro described a diaphragm in 1566. Wilhelm Homberg described how light darkened some chemicals (photochemical effect) in 1694. The fiction book \"Giphantie\", published in 1760, by French author Tiphaigne de la Roche, described what can be interpreted as photography.\n\nThe discovery of the camera obscura that provides an image of a scene dates back to ancient China. Leonardo da Vinci mentions natural camera obscura that are formed by dark caves on the edge of a sunlit valley. A hole in the cave wall will act as a pinhole camera and project a laterally reversed, upside down image on a piece of paper. So the birth of photography was primarily concerned with inventing means to capture and keep the image produced by the camera obscura.\n\nRenaissance painters used the camera obscura which, in fact, gives the optical rendering in color that dominates Western Art. The camera obscura literally means \"dark chamber\" in Latin. It is a box with a hole in it which allows light to go through and create an image onto the piece of paper.\n\nAround the year 1800, British inventor Thomas Wedgwood made the first known attempt to capture the image in a camera obscura by means of a light-sensitive substance. He used paper or white leather treated with silver nitrate. Although he succeeded in capturing the shadows of objects placed on the surface in direct sunlight, and even made shadow copies of paintings on glass, it was reported in 1802 that \"the images formed by means of a camera obscura have been found too faint to produce, in any moderate time, an effect upon the nitrate of silver.\" The shadow images eventually darkened all over.\n\nThe first permanent photoetching was an image produced in 1822 by the French inventor Nicéphore Niépce, but it was destroyed in a later attempt to make prints from it. Niépce was successful again in 1825. In 1826 or 1827, he made the \"View from the Window at Le Gras\", the earliest surviving photograph from nature (i.e., of the image of a real-world scene, as formed in a camera obscura by a lens).\n\nBecause Niépce's camera photographs required an extremely long exposure (at least eight hours and probably several days), he sought to greatly improve his bitumen process or replace it with one that was more practical. In partnership with Louis Daguerre, he worked out post-exposure processing methods that produced visually superior results and replaced the bitumen with a more light-sensitive resin, but hours of exposure in the camera were still required. With an eye to eventual commercial exploitation, the partners opted for total secrecy.\n\nNiépce died in 1833 and Daguerre then redirected the experiments toward the light-sensitive silver halides, which Niépce had abandoned many years earlier because of his inability to make the images he captured with them light-fast and permanent. Daguerre's efforts culminated in what would later be named the daguerreotype process. The essential elements—a silver-plated surface sensitized by iodine vapor, developed by mercury vapor, and \"fixed\" with hot saturated salt water—were in place in 1837. The required exposure time was measured in minutes instead of hours. Daguerre took the earliest confirmed photograph of a person in 1838 while capturing a view of a Paris street: unlike the other pedestrian and horse-drawn traffic on the busy boulevard, which appears deserted, one man having his boots polished stood sufficiently still throughout the several-minutes-long exposure to be visible. The existence of Daguerre's process was publicly announced, without details, on 7 January 1839. The news created an international sensation. France soon agreed to pay Daguerre a pension in exchange for the right to present his invention to the world as the gift of France, which occurred when complete working instructions were unveiled on 19 August 1839. In that same year, American photographer Robert Cornelius is credited with taking the earliest surviving photographic self-portrait.\n\nIn Brazil, Hercules Florence had apparently started working out a silver-salt-based paper process in 1832, later naming it \"Photographie\".\n\nMeanwhile, a British inventor, William Fox Talbot, had succeeded in making crude but reasonably light-fast silver images on paper as early as 1834 but had kept his work secret. After reading about Daguerre's invention in January 1839, Talbot published his hitherto secret method and set about improving on it. At first, like other pre-daguerreotype processes, Talbot's paper-based photography typically required hours-long exposures in the camera, but in 1840 he created the calotype process, which used the chemical development of a latent image to greatly reduce the exposure needed and compete with the daguerreotype. In both its original and calotype forms, Talbot's process, unlike Daguerre's, created a translucent negative which could be used to print multiple positive copies; this is the basis of most modern chemical photography up to the present day, as Daguerreotypes could only be replicated by rephotographing them with a camera. Talbot's famous tiny paper negative of the Oriel window in Lacock Abbey, one of a number of camera photographs he made in the summer of 1835, may be the oldest camera negative in existence.\n\nBritish chemist John Herschel made many contributions to the new field. He invented the cyanotype process, later familiar as the \"blueprint\". He was the first to use the terms \"photography\", \"negative\" and \"positive\". He had discovered in 1819 that sodium thiosulphate was a solvent of silver halides, and in 1839 he informed Talbot (and, indirectly, Daguerre) that it could be used to \"fix\" silver-halide-based photographs and make them completely light-fast. He made the first glass negative in late 1839.\nIn the March 1851 issue of \"The Chemist\", Frederick Scott Archer published his wet plate collodion process. It became the most widely used photographic medium until the gelatin dry plate, introduced in the 1870s, eventually replaced it. There are three subsets to the collodion process; the Ambrotype (a positive image on glass), the Ferrotype or Tintype (a positive image on metal) and the glass negative, which was used to make positive prints on albumen or salted paper.\n\nMany advances in photographic glass plates and printing were made during the rest of the 19th century. In 1891, Gabriel Lippmann introduced a process for making natural-color photographs based on the optical phenomenon of the interference of light waves. His scientifically elegant and important but ultimately impractical invention earned him the Nobel Prize in Physics in 1908.\n\nGlass plates were the medium for most original camera photography from the late 1850s until the general introduction of flexible plastic films during the 1890s. Although the convenience of the film greatly popularized amateur photography, early films were somewhat more expensive and of markedly lower optical quality than their glass plate equivalents, and until the late 1910s they were not available in the large formats preferred by most professional photographers, so the new medium did not immediately or completely replace the old. Because of the superior dimensional stability of glass, the use of plates for some scientific applications, such as astrophotography, continued into the 1990s, and in the niche field of laser holography, it has persisted into the 2010s.\n\nHurter and Driffield began pioneering work on the light sensitivity of photographic emulsions in 1876. Their work enabled the first quantitative measure of film speed to be devised.\n\nThe first flexible photographic roll film was marketed by George Eastman in 1885, but this original \"film\" was actually a coating on a paper base. As part of the processing, the image-bearing layer was stripped from the paper and transferred to a hardened gelatin support. The first transparent plastic roll film followed in 1889. It was made from highly flammable nitrocellulose (\"celluloid\"), now usually called \"nitrate film\".\n\nAlthough cellulose acetate or \"safety film\" had been introduced by Kodak in 1908, at first it found only a few special applications as an alternative to the hazardous nitrate film, which had the advantages of being considerably tougher, slightly more transparent, and cheaper. The changeover was not completed for X-ray films until 1933, and although safety film was always used for 16 mm and 8 mm home movies, nitrate film remained standard for theatrical 35 mm motion pictures until it was finally discontinued in 1951.\n\nFilms remained the dominant form of photography until the early 21st century when advances in digital photography drew consumers to digital formats. Although modern photography is dominated by digital users, film continues to be used by enthusiasts and professional photographers. The distinctive \"look\" of film based photographs compared to digital images is likely due to a combination of factors, including: (1) differences in spectral and tonal sensitivity (S-shaped density-to-exposure (H&D curve) with film vs. linear response curve for digital CCD sensors) (2) resolution and (3) continuity of tone.\n\nOriginally, all photography was monochrome, or \"black-and-white\". Even after color film was readily available, black-and-white photography continued to dominate for decades, due to its lower cost and its \"classic\" photographic look. The tones and contrast between light and dark areas define black-and-white photography. It is important to note that monochromatic pictures are not necessarily composed of pure blacks, whites, and intermediate shades of gray but can involve shades of one particular hue depending on the process. The cyanotype process, for example, produces an image composed of blue tones. The albumen print process first used more than years ago, produces brownish tones.\n\nMany photographers continue to produce some monochrome images, sometimes because of the established archival permanence of well-processed silver-halide-based materials. Some full-color digital images are processed using a variety of techniques to create black-and-white results, and some manufacturers produce digital cameras that exclusively shoot monochrome. Monochrome printing or electronic display can be used to salvage certain photographs taken in color which are unsatisfactory in their original form; sometimes when presented as black-and-white or single-color-toned images they are found to be more effective. Although color photography has long predominated, monochrome images are still produced, mostly for artistic reasons. Almost all digital cameras have an option to shoot in monochrome, and almost all image editing software can combine or selectively discard RGB color channels to produce a monochrome image from one shot in color.\n\nColor photography was explored beginning in the 1840s. Early experiments in color required extremely long exposures (hours or days for camera images) and could not \"fix\" the photograph to prevent the color from quickly fading when exposed to white light.\n\nThe first permanent color photograph was taken in 1861 using the three-color-separation principle first published by Scottish physicist James Clerk Maxwell in 1855. The foundation of virtually all practical color processes, Maxwell's idea was to take three separate black-and-white photographs through red, green and blue filters. This provides the photographer with the three basic channels required to recreate a color image. Transparent prints of the images could be projected through similar color filters and superimposed on the projection screen, an additive method of color reproduction. A color print on paper could be produced by superimposing carbon prints of the three images made in their complementary colors, a subtractive method of color reproduction pioneered by Louis Ducos du Hauron in the late 1860s.\n\nRussian photographer Sergei Mikhailovich Prokudin-Gorskii made extensive use of this color separation technique, employing a special camera which successively exposed the three color-filtered images on different parts of an oblong plate. Because his exposures were not simultaneous, unsteady subjects exhibited color \"fringes\" or, if rapidly moving through the scene, appeared as brightly colored ghosts in the resulting projected or printed images.\n\nImplementation of color photography was hindered by the limited sensitivity of early photographic materials, which were mostly sensitive to blue, only slightly sensitive to green, and virtually insensitive to red. The discovery of dye sensitization by photochemist Hermann Vogel in 1873 suddenly made it possible to add sensitivity to green, yellow and even red. Improved color sensitizers and ongoing improvements in the overall sensitivity of emulsions steadily reduced the once-prohibitive long exposure times required for color, bringing it ever closer to commercial viability.\n\nAutochrome, the first commercially successful color process, was introduced by the Lumière brothers in 1907. Autochrome plates incorporated a mosaic color filter layer made of dyed grains of potato starch, which allowed the three color components to be recorded as adjacent microscopic image fragments. After an Autochrome plate was reversal processed to produce a positive transparency, the starch grains served to illuminate each fragment with the correct color and the tiny colored points blended together in the eye, synthesizing the color of the subject by the additive method. Autochrome plates were one of several varieties of additive color screen plates and films marketed between the 1890s and the 1950s.\n\nKodachrome, the first modern \"integral tripack\" (or \"monopack\") color film, was introduced by Kodak in 1935. It captured the three color components in a multi-layer emulsion. One layer was sensitized to record the red-dominated part of the spectrum, another layer recorded only the green part and a third recorded only the blue. Without special film processing, the result would simply be three superimposed black-and-white images, but complementary cyan, magenta, and yellow dye images were created in those layers by adding color couplers during a complex processing procedure.\n\nAgfa's similarly structured Agfacolor Neu was introduced in 1936. Unlike Kodachrome, the color couplers in Agfacolor Neu were incorporated into the emulsion layers during manufacture, which greatly simplified the processing. Currently, available color films still employ a multi-layer emulsion and the same principles, most closely resembling Agfa's product.\n\nInstant color film, used in a special camera which yielded a unique finished color print only a minute or two after the exposure, was introduced by Polaroid in 1963.\n\nColor photography may form images as positive transparencies, which can be used in a slide projector, or as color negatives intended for use in creating positive color enlargements on specially coated paper. The latter is now the most common form of film (non-digital) color photography owing to the introduction of automated photo printing equipment. After a transition period centered around 1995–2005, color film was relegated to a niche market by inexpensive multi-megapixel digital cameras. Film continues to be the preference of some photographers because of its distinctive \"look\".\n\nIn 1981, Sony unveiled the first consumer camera to use a charge-coupled device for imaging, eliminating the need for film: the Sony Mavica. While the Mavica saved images to disk, the images were displayed on television, and the camera was not fully digital. In 1991, Kodak unveiled the DCS 100, the first commercially available digital single lens reflex camera. Although its high cost precluded uses other than photojournalism and professional photography, commercial digital photography was born.\n\nDigital imaging uses an electronic image sensor to record the image as a set of electronic data rather than as chemical changes on film. An important difference between digital and chemical photography is that chemical photography resists photo manipulation because it involves film and photographic paper, while digital imaging is a highly manipulative medium. This difference allows for a degree of image post-processing that is comparatively difficult in film-based photography and permits different communicative potentials and applications.\n\nDigital photography dominates the 21st century. More than 99% of photographs taken around the world are through digital cameras, increasingly through smartphones.\n\nSynthesis photography is part of computer-generated imagery (CGI) where the shooting process is modeled on real photography. The CGI, creating digital copies of real universe, requires a visual representation process of these universes. Synthesis photography is the application of analog and digital photography in digital space. With the characteristics of the real photography but not being constrained by the physical limits of real world, synthesis photography allows to get away from real photography.\n\nA large variety of photographic techniques and media are used in the process of capturing images for photography. These include the camera; stereoscopy; dualphotography; full-spectrum, ultraviolet and infrared media; light field photography; and other imaging techniques.\n\nThe camera is the image-forming device, and a photographic plate, photographic film or a silicon electronic image sensor is the capture medium. The respective recording medium can be the plate or film itself, or a digital magnetic or electronic memory.\n\nPhotographers control the camera and lens to \"expose\" the light recording material to the required amount of light to form a \"latent image\" (on plate or film) or RAW file (in digital cameras) which, after appropriate processing, is converted to a usable image. Digital cameras use an electronic image sensor based on light-sensitive electronics such as charge-coupled device (CCD) or complementary metal-oxide-semiconductor (CMOS) technology. The resulting digital image is stored electronically, but can be reproduced on a paper.\n\nThe camera (or 'camera obscura') is a dark room or chamber from which, as far as possible, all light is excluded except the light that forms the image. It was discovered and used in the 16th century by painters. The subject being photographed, however, must be illuminated. Cameras can range from small to very large, a whole room that is kept dark while the object to be photographed is in another room where it is properly illuminated. This was common for reproduction photography of flat copy when large film negatives were used (see Process camera).\n\nAs soon as photographic materials became \"fast\" (sensitive) enough for taking candid or surreptitious pictures, small \"detective\" cameras were made, some actually disguised as a book or handbag or pocket watch (the \"Ticka\" camera) or even worn hidden behind an Ascot necktie with a tie pin that was really the lens.\n\nThe movie camera is a type of photographic camera which takes a rapid sequence of photographs on recording medium. In contrast to a still camera, which captures a single snapshot at a time, the movie camera takes a series of images, each called a \"frame\". This is accomplished through an intermittent mechanism. The frames are later played back in a movie projector at a specific speed, called the \"frame rate\" (number of frames per second). While viewing, a person's eyes and brain merge the separate pictures together to create the illusion of motion.\n\nPhotographs, both monochrome and color, can be captured and displayed through two side-by-side images that emulate human stereoscopic vision. Stereoscopic photography was the first that captured figures in motion. While known colloquially as \"3-D\" photography, the more accurate term is stereoscopy. Such cameras have long been realized by using film and more recently in digital electronic methods (including cell phone cameras).\n\nDualphotography consists of photographing a scene from both sides of a photographic device at once (e.g. camera for back-to-back dualphotography, or two networked cameras for portal-plane dualphotography). The dualphoto apparatus can be used to simultaneously capture both the subject and the photographer, or both sides of a geographical place at once, thus adding a supplementary narrative layer to that of a single image.\n\nUltraviolet and infrared films have been available for many decades and employed in a variety of photographic avenues since the 1960s. New technological trends in digital photography have opened a new direction in full spectrum photography, where careful filtering choices across the ultraviolet, visible and infrared lead to new artistic visions.\n\nModified digital cameras can detect some ultraviolet, all of the visible and much of the near infrared spectrum, as most digital imaging sensors are sensitive from about 350 nm to 1000 nm. An off-the-shelf digital camera contains an infrared hot mirror filter that blocks most of the infrared and a bit of the ultraviolet that would otherwise be detected by the sensor, narrowing the accepted range from about 400 nm to 700 nm.\n\nReplacing a hot mirror or infrared blocking filter with an infrared pass or a wide spectrally transmitting filter allows the camera to detect the wider spectrum light at greater sensitivity. Without the hot-mirror, the red, green and blue (or cyan, yellow and magenta) colored micro-filters placed over the sensor elements pass varying amounts of ultraviolet (blue window) and infrared (primarily red and somewhat lesser the green and blue micro-filters).\n\nUses of full spectrum photography are for fine art photography, geology, forensics and law enforcement.\n\nDigital methods of image capture and display processing have enabled the new technology of \"light field photography\" (also known as synthetic aperture photography). This process allows focusing at various depths of field to be selected \"after\" the photograph has been captured. As explained by Michael Faraday in 1846, the \"light field\" is understood as 5-dimensional, with each point in 3-D space having attributes of two more angles that define the direction of each ray passing through that point.\n\nThese additional vector attributes can be captured optically through the use of microlenses at each pixel point within the 2-dimensional image sensor. Every pixel of the final image is actually a selection from each sub-array located under each microlens, as identified by a post-image capture focus algorithm.\n\nBesides the camera, other methods of forming images with light are available. For instance, a photocopy or xerography machine forms permanent images but uses the transfer of static electrical charges rather than photographic medium, hence the term electrophotography. Photograms are images produced by the shadows of objects cast on the photographic paper, without the use of a camera. Objects can also be placed directly on the glass of an image scanner to produce digital pictures.\n\nAn amateur photographer is one who practices photography as a hobby/passion and not necessarily for profit. The quality of some amateur work is comparable to that of many professionals and may be highly specialized or eclectic in choice of subjects. Amateur photography is often pre-eminent in photographic subjects which have little prospect of commercial use or reward. Amateur photography grew during the late 19th century due to the popularization of the hand-held camera. Nowadays it has spread widely through social media and is carried out throughout different platforms and equipment, switching to the use of cell phone as a key tool for making photography more accessible to everyone.\n\nCommercial photography is probably best defined as any photography for which the photographer is paid for images rather than works of art. In this light, money could be paid for the subject of the photograph or the photograph itself. Wholesale, retail, and professional uses of photography would fall under this definition. The commercial photographic world could include:\nThe market for photographic services demonstrates the aphorism \"A picture is worth a thousand words\", which has an interesting basis in the history of photography. Magazines and newspapers, companies putting up Web sites, advertising agencies and other groups pay for photography.\n\nMany people take photographs for commercial purposes. Organizations with a budget and a need for photography have several options: they can employ a photographer directly, organize a public competition, or obtain rights to stock photographs. Photo stock can be procured through traditional stock giants, such as Getty Images or Corbis; smaller microstock agencies, such as Fotolia; or web marketplaces, such as Cutcaster.\n\nDuring the 20th century, both fine art photography and documentary photography became accepted by the English-speaking art world and the gallery system. In the United States, a handful of photographers, including Alfred Stieglitz, Edward Steichen, John Szarkowski, F. Holland Day, and Edward Weston, spent their lives advocating for photography as a fine art.\nAt first, fine art photographers tried to imitate painting styles. This movement is called Pictorialism, often using soft focus for a dreamy, 'romantic' look. In reaction to that, Weston, Ansel Adams, and others formed the Group f/64 to advocate 'straight photography', the photograph as a (sharply focused) thing in itself and not an imitation of something else.\n\nThe aesthetics of photography is a matter that continues to be discussed regularly, especially in artistic circles. Many artists argued that photography was the mechanical reproduction of an image. If photography is authentically art, then photography in the context of art would need redefinition, such as determining what component of a photograph makes it beautiful to the viewer. The controversy began with the earliest images \"written with light\"; Nicéphore Niépce, Louis Daguerre, and others among the very earliest photographers were met with acclaim, but some questioned if their work met the definitions and purposes of art.\n\nClive Bell in his classic essay \"Art\" states that only \"significant form\" can distinguish art from what is not art.\n\nOn 7 February 2007, Sotheby's London sold the 2001 photograph \"99 Cent II Diptychon\" for an unprecedented $3,346,456 to an anonymous bidder, making it the most expensive at the time.\n\nConceptual photography turns a concept or idea into a photograph. Even though what is depicted in the photographs are real objects, the subject is strictly abstract.\n\nPhotojournalism is a particular form of photography (the collecting, editing, and presenting of news material for publication or broadcast) that employs images in order to tell a news story. It is now usually understood to refer only to still images, but in some cases the term also refers to video used in broadcast journalism. Photojournalism is distinguished from other close branches of photography (e.g., documentary photography, social documentary photography, street photography or celebrity photography) by complying with a rigid ethical framework which demands that the work be both honest and impartial whilst telling the story in strictly journalistic terms. Photojournalists create pictures that contribute to the news media, and help communities connect with one other. Photojournalists must be well informed and knowledgeable about events happening right outside their door. They deliver news in a creative format that is not only informative, but also entertaining.\n\nThe camera has a long and distinguished history as a means of recording scientific phenomena from the first use by Daguerre and Fox-Talbot, such as astronomical events (eclipses for example), small creatures and plants when the camera was attached to the eyepiece of microscopes (in photomicroscopy) and for macro photography of larger specimens. The camera also proved useful in recording crime scenes and the scenes of accidents, such as the Wootton bridge collapse in 1861. The methods used in analysing photographs for use in legal cases are collectively known as forensic photography. Crime scene photos are taken from three vantage point. The vantage points are overview, mid-range, and close-up.\n\nIn 1845 Francis Ronalds, the Honorary Director of the Kew Observatory, invented the first successful camera to make continuous recordings of meteorological and geomagnetic parameters. Different machines produced 12- or 24- hour photographic traces of the minute-by-minute variations of atmospheric pressure, temperature, humidity, atmospheric electricity, and the three components of geomagnetic forces. The cameras were supplied to numerous observatories around the world and some remained in use until well into the 20th century. Charles Brooke a little later developed similar instruments for the Greenwich Observatory.\n\nScience uses image technology that has derived from the design of the Pin Hole camera. X-Ray machines are similar in design to Pin Hole cameras with high-grade filters and laser radiation.\nPhotography has become ubiquitous in recording events and data in science and engineering, and at crime scenes or accident scenes. The method has been much extended by using other wavelengths, such as infrared photography and ultraviolet photography, as well as spectroscopy. Those methods were first used in the Victorian era and improved much further since that time.\n\nThe first photographed atom was discovered in 2012 by physicists at Griffith University, Australia. They used an electric field to trap an \"Ion\" of the element, Ytterbium. The image was recorded on a CCD, an electronic photographic film.\n\nThere are many ongoing questions about different aspects of photography. In her writing \"On Photography\" (1977), Susan Sontag discusses concerns about the objectivity of photography. This is a highly debated subject within the photographic community. Sontag argues, \"To photograph is to appropriate the thing photographed. It means putting one's self into a certain relation to the world that feels like knowledge, and therefore like power.\" Photographers decide what to take a photo of, what elements to exclude and what angle to frame the photo, and these factors may reflect a particular socio-historical context. Along these lines, it can be argued that photography is a subjective form of representation.\n\nModern photography has raised a number of concerns on its effect on society. In Alfred Hitchcock's \"Rear Window\" (1954), the camera is presented as promoting voyeurism. 'Although the camera is an observation station, the act of photographing is more than passive observing'.\nThe camera doesn't rape or even possess, though it may presume, intrude, trespass, distort, exploit, and, at the farthest reach of metaphor, assassinate – all activities that, unlike the sexual push and shove, can be conducted from a distance, and with some detachment.\nDigital imaging has raised ethical concerns because of the ease of manipulating digital photographs in post-processing. Many photojournalists have declared they will not crop their pictures or are forbidden from combining elements of multiple photos to make \"photomontages\", passing them as \"real\" photographs. Today's technology has made image editing relatively simple for even the novice photographer. However, recent changes of in-camera processing allow digital fingerprinting of photos to detect tampering for purposes of forensic photography.\n\nPhotography is one of the new media forms that changes perception and changes the structure of society. Further unease has been caused around cameras in regards to desensitization. Fears that disturbing or explicit images are widely accessible to children and society at large have been raised. Particularly, photos of war and pornography are causing a stir. Sontag is concerned that \"to photograph is to turn people into objects that can be symbolically possessed.\" Desensitization discussion goes hand in hand with debates about censored images. Sontag writes of her concern that the ability to censor pictures means the photographer has the ability to construct reality.\n\nOne of the practices through which photography constitutes society is tourism. Tourism and photography combine to create a \"tourist gaze\"\nin which local inhabitants are positioned and defined by the camera lens. However, it has also been argued that there exists a \"reverse gaze\" through which indigenous photographees can position the tourist photographer as a shallow consumer of images.\n\nAdditionally, photography has been the topic of many songs in popular culture.\n\nPhotography is both restricted as well as protected by the law in many jurisdictions. Protection of photographs is typically achieved through the granting of copyright or moral rights to the photographer. In the United States, photography is protected as a First Amendment right and anyone is free to photograph anything seen in public spaces as long as it is in plain view. In the UK a recent law (Counter-Terrorism Act 2008) increases the power of the police to prevent people, even press photographers, from taking pictures in public places.\n\n\n\n\n\n\n",
                "Optics\n\nOptics is the branch of physics which involves the behaviour and properties of light, including its interactions with matter and the construction of instruments that use or detect it. Optics usually describes the behaviour of visible, ultraviolet, and infrared light. Because light is an electromagnetic wave, other forms of electromagnetic radiation such as X-rays, microwaves, and radio waves exhibit similar properties.\n\nMost optical phenomena can be accounted for using the classical electromagnetic description of light. Complete electromagnetic descriptions of light are, however, often difficult to apply in practice. Practical optics is usually done using simplified models. The most common of these, geometric optics, treats light as a collection of rays that travel in straight lines and bend when they pass through or reflect from surfaces. Physical optics is a more comprehensive model of light, which includes wave effects such as diffraction and interference that cannot be accounted for in geometric optics. Historically, the ray-based model of light was developed first, followed by the wave model of light. Progress in electromagnetic theory in the 19th century led to the discovery that light waves were in fact electromagnetic radiation.\n\nSome phenomena depend on the fact that light has both wave-like and particle-like properties. Explanation of these effects requires quantum mechanics. When considering light's particle-like properties, the light is modelled as a collection of particles called \"photons\". Quantum optics deals with the application of quantum mechanics to optical systems.\n\nOptical science is relevant to and studied in many related disciplines including astronomy, various engineering fields, photography, and medicine (particularly ophthalmology and optometry). Practical applications of optics are found in a variety of technologies and everyday objects, including mirrors, lenses, telescopes, microscopes, lasers, and fibre optics.\n\nOptics began with the development of lenses by the ancient Egyptians and Mesopotamians. The earliest known lenses, made from polished crystal, often quartz, date from as early as 700 BC for Assyrian lenses such as the Layard/Nimrud lens. The ancient Romans and Greeks filled glass spheres with water to make lenses. These practical developments were followed by the development of theories of light and vision by ancient Greek and Indian philosophers, and the development of geometrical optics in the Greco-Roman world. The word \"optics\" comes from the ancient Greek word (\"optikē\"), meaning \"appearance, look\".\n\nGreek philosophy on optics broke down into two opposing theories on how vision worked, the \"intromission theory\" and the \"emission theory\". The intro-mission approach saw vision as coming from objects casting off copies of themselves (called eidola) that were captured by the eye. With many propagators including Democritus, Epicurus, Aristotle and their followers, this theory seems to have some contact with modern theories of what vision really is, but it remained only speculation lacking any experimental foundation.\n\nPlato first articulated the emission theory, the idea that visual perception is accomplished by rays emitted by the eyes. He also commented on the parity reversal of mirrors in \"Timaeus\". Some hundred years later, Euclid wrote a treatise entitled \"Optics\" where he linked vision to geometry, creating \"geometrical optics\". He based his work on Plato's emission theory wherein he described the mathematical rules of perspective and described the effects of refraction qualitatively, although he questioned that a beam of light from the eye could instantaneously light up the stars every time someone blinked. Ptolemy, in his treatise \"Optics\", held an extramission-intromission theory of vision: the rays (or flux) from the eye formed a cone, the vertex being within the eye, and the base defining the visual field. The rays were sensitive, and conveyed information back to the observer’s intellect about the distance and orientation of surfaces. He summarised much of Euclid and went on to describe a way to measure the angle of refraction, though he failed to notice the empirical relationship between it and the angle of incidence.\n\nDuring the Middle Ages, Greek ideas about optics were resurrected and extended by writers in the Muslim world. One of the earliest of these was Al-Kindi (c. 801–73) who wrote on the merits of Aristotelian and Euclidean ideas of optics, favouring the emission theory since it could better quantify optical phenomena. In 984, the Persian mathematician Ibn Sahl wrote the treatise \"On burning mirrors and lenses\", correctly describing a law of refraction equivalent to Snell's law. He used this law to compute optimum shapes for lenses and curved mirrors. In the early 11th century, Alhazen (Ibn al-Haytham) wrote the \"Book of Optics\" (\"Kitab al-manazir\") in which he explored reflection and refraction and proposed a new system for explaining vision and light based on observation and experiment. He rejected the \"emission theory\" of Ptolemaic optics with its rays being emitted by the eye, and instead put forward the idea that light reflected in all directions in straight lines from all points of the objects being viewed and then entered the eye, although he was unable to correctly explain how the eye captured the rays. Alhazen's work was largely ignored in the Arabic world but it was anonymously translated into Latin around 1200 A.D. and further summarised and expanded on by the Polish monk Witelo making it a standard text on optics in Europe for the next 400 years.\n\nIn the 13th century in medieval Europe, English bishop Robert Grosseteste wrote on a wide range of scientific topics, and discussed light from four different perspectives: an epistemology of light, a metaphysics or cosmogony of light, an etiology or physics of light, and a theology of light, basing it on the works Aristotle and Platonism. Grosseteste's most famous disciple, Roger Bacon, wrote works citing a wide range of recently translated optical and philosophical works, including those of Alhazen, Aristotle, Avicenna, Averroes, Euclid, al-Kindi, Ptolemy, Tideus, and Constantine the African. Bacon was able to use parts of glass spheres as magnifying glasses to demonstrate that light reflects from objects rather than being released from them.\n\nThe first wearable eyeglasses were invented in Italy around 1286. \nThis was the start of the optical industry of grinding and polishing lenses for these \"spectacles\", first in Venice and Florence in the thirteenth century, and later in the spectacle making centres in both the Netherlands and Germany. Spectacle makers created improved types of lenses for the correction of vision based more on empirical knowledge gained from observing the effects of the lenses rather than using the rudimentary optical theory of the day (theory which for the most part could not even adequately explain how spectacles worked). This practical development, mastery, and experimentation with lenses led directly to the invention of the compound optical microscope around 1595, and the refracting telescope in 1608, both of which appeared in the spectacle making centres in the Netherlands.\n\nIn the early 17th century Johannes Kepler expanded on geometric optics in his writings, covering lenses, reflection by flat and curved mirrors, the principles of pinhole cameras, inverse-square law governing the intensity of light, and the optical explanations of astronomical phenomena such as lunar and solar eclipses and astronomical parallax. He was also able to correctly deduce the role of the retina as the actual organ that recorded images, finally being able to scientifically quantify the effects of different types of lenses that spectacle makers had been observing over the previous 300 years. After the invention of the telescope Kepler set out the theoretical basis on how they worked and described an improved version, known as the \"Keplerian telescope\", using two convex lenses to produce higher magnification.\nOptical theory progressed in the mid-17th century with treatises written by philosopher René Descartes, which explained a variety of optical phenomena including reflection and refraction by assuming that light was emitted by objects which produced it. This differed substantively from the ancient Greek emission theory. In the late 1660s and early 1670s, Isaac Newton expanded Descartes' ideas into a corpuscle theory of light, famously determining that white light was a mix of colours which can be separated into its component parts with a prism. In 1690, Christiaan Huygens proposed a wave theory for light based on suggestions that had been made by Robert Hooke in 1664. Hooke himself publicly criticised Newton's theories of light and the feud between the two lasted until Hooke's death. In 1704, Newton published \"Opticks\" and, at the time, partly because of his success in other areas of physics, he was generally considered to be the victor in the debate over the nature of light.\n\nNewtonian optics was generally accepted until the early 19th century when Thomas Young and Augustin-Jean Fresnel conducted experiments on the interference of light that firmly established light's wave nature. Young's famous double slit experiment showed that light followed the law of superposition, which is a wave-like property not predicted by Newton's corpuscle theory. This work led to a theory of diffraction for light and opened an entire area of study in physical optics. Wave optics was successfully unified with electromagnetic theory by James Clerk Maxwell in the 1860s.\n\nThe next development in optical theory came in 1899 when Max Planck correctly modelled blackbody radiation by assuming that the exchange of energy between light and matter only occurred in discrete amounts he called \"quanta\". In 1905 Albert Einstein published the theory of the photoelectric effect that firmly established the quantization of light itself. In 1913 Niels Bohr showed that atoms could only emit discrete amounts of energy, thus explaining the discrete lines seen in emission and absorption spectra. The understanding of the interaction between light and matter which followed from these developments not only formed the basis of quantum optics but also was crucial for the development of quantum mechanics as a whole. The ultimate culmination, the theory of quantum electrodynamics, explains all optics and electromagnetic processes in general as the result of the exchange of real and virtual photons.\n\nQuantum optics gained practical importance with the inventions of the maser in 1953 and of the laser in 1960. Following the work of Paul Dirac in quantum field theory, George Sudarshan, Roy J. Glauber, and Leonard Mandel applied quantum theory to the electromagnetic field in the 1950s and 1960s to gain a more detailed understanding of photodetection and the statistics of light.\n\nClassical optics is divided into two main branches: geometrical (or ray) optics and physical (or wave) optics. In geometrical optics, light is considered to travel in straight lines, while in physical optics, light is considered as an electromagnetic wave.\n\nGeometrical optics can be viewed as an approximation of physical optics that applies when the wavelength of the light used is much smaller than the size of the optical elements in the system being modelled.\n\n\"Geometrical optics\", or \"ray optics\", describes the propagation of light in terms of \"rays\" which travel in straight lines, and whose paths are governed by the laws of reflection and refraction at interfaces between different media. These laws were discovered empirically as far back as 984 AD and have been used in the design of optical components and instruments from then until the present day. They can be summarised as follows:\n\nWhen a ray of light hits the boundary between two transparent materials, it is divided into a reflected and a refracted ray.\n\nwhere is a constant for any two materials and a given colour of light. If the first material is air or vacuum, is the refractive index of the second material.\n\nThe laws of reflection and refraction can be derived from Fermat's principle which states that \"the path taken between two points by a ray of light is the path that can be traversed in the least time.\"\n\nGeometric optics is often simplified by making the paraxial approximation, or \"small angle approximation\". The mathematical behaviour then becomes linear, allowing optical components and systems to be described by simple matrices. This leads to the techniques of Gaussian optics and \"paraxial ray tracing\", which are used to find basic properties of optical systems, such as approximate image and object positions and magnifications.\n\nReflections can be divided into two types: specular reflection and diffuse reflection. Specular reflection describes the gloss of surfaces such as mirrors, which reflect light in a simple, predictable way. This allows for production of reflected images that can be associated with an actual (real) or extrapolated (virtual) location in space. Diffuse reflection describes non-glossy materials, such as paper or rock. The reflections from these surfaces can only be described statistically, with the exact distribution of the reflected light depending on the microscopic structure of the material. Many diffuse reflectors are described or can be approximated by Lambert's cosine law, which describes surfaces that have equal luminance when viewed from any angle. Glossy surfaces can give both specular and diffuse reflection.\n\nIn specular reflection, the direction of the reflected ray is determined by the angle the incident ray makes with the surface normal, a line perpendicular to the surface at the point where the ray hits. The incident and reflected rays and the normal lie in a single plane, and the angle between the reflected ray and the surface normal is the same as that between the incident ray and the normal. This is known as the Law of Reflection.\n\nFor flat mirrors, the law of reflection implies that images of objects are upright and the same distance behind the mirror as the objects are in front of the mirror. The image size is the same as the object size. The law also implies that mirror images are parity inverted, which we perceive as a left-right inversion. Images formed from reflection in two (or any even number of) mirrors are not parity inverted. Corner reflectors retroreflect light, producing reflected rays that travel back in the direction from which the incident rays came.\n\nMirrors with curved surfaces can be modelled by ray tracing and using the law of reflection at each point on the surface. For mirrors with parabolic surfaces, parallel rays incident on the mirror produce reflected rays that converge at a common focus. Other curved surfaces may also focus light, but with aberrations due to the diverging shape causing the focus to be smeared out in space. In particular, spherical mirrors exhibit spherical aberration. Curved mirrors can form images with magnification greater than or less than one, and the magnification can be negative, indicating that the image is inverted. An upright image formed by reflection in a mirror is always virtual, while an inverted image is real and can be projected onto a screen.\n\nRefraction occurs when light travels through an area of space that has a changing index of refraction; this principle allows for lenses and the focusing of light. The simplest case of refraction occurs when there is an interface between a uniform medium with index of refraction formula_2 and another medium with index of refraction formula_3. In such situations, Snell's Law describes the resulting deflection of the light ray:\n\nwhere formula_5 and formula_6 are the angles between the normal (to the interface) and the incident and refracted waves, respectively.\n\nThe index of refraction of a medium is related to the speed, , of light in that medium by\nwhere is the speed of light in vacuum.\n\nSnell's Law can be used to predict the deflection of light rays as they pass through linear media as long as the indexes of refraction and the geometry of the media are known. For example, the propagation of light through a prism results in the light ray being deflected depending on the shape and orientation of the prism. In most materials, the index of refraction varies with the frequency of the light. Taking this into account, Snell's Law can be used to predict how a prism will disperse light into a spectrum. The discovery of this phenomenon when passing light through a prism is famously attributed to Isaac Newton.\n\nSome media have an index of refraction which varies gradually with position and, thus, light rays in the medium are curved. This effect is responsible for mirages seen on hot days: a change in index of refraction air with height causes light rays to bend, creating the appearance of specular reflections in the distance (as if on the surface of a pool of water). Optical materials with varying index of refraction are called gradient-index (GRIN) materials. Such materials are used to make gradient-index optics.\n\nFor light rays travelling from a material with a high index of refraction to a material with a low index of refraction, Snell's law predicts that there is no formula_6 when formula_5 is large. In this case, no transmission occurs; all the light is reflected. This phenomenon is called total internal reflection and allows for fibre optics technology. As light travels down an optical fibre, it undergoes total internal reflection allowing for essentially no light to be lost over the length of the cable.\n\nA device which produces converging or diverging light rays due to refraction is known as a \"lens\". Lenses are characterized by their focal length: a converging lens has positive focal length, while a diverging lens has negative focal length. Smaller focal length indicates that the lens has a stronger converging or diverging effect. The focal length of a simple lens in air is given by the lensmaker's equation.\n\nRay tracing can be used to show how images are formed by a lens. For a thin lens in air, the location of the image is given by the simple equation\n\nwhere formula_11 is the distance from the object to the lens, formula_12 is the distance from the lens to the image, and formula_13 is the focal length of the lens. In the sign convention used here, the object and image distances are positive if the object and image are on opposite sides of the lens.\nIncoming parallel rays are focused by a converging lens onto a spot one focal length from the lens, on the far side of the lens. This is called the rear focal point of the lens. Rays from an object at finite distance are focused further from the lens than the focal distance; the closer the object is to the lens, the further the image is from the lens.\n\nWith diverging lenses, incoming parallel rays diverge after going through the lens, in such a way that they seem to have originated at a spot one focal length in front of the lens. This is the lens's front focal point. Rays from an object at finite distance are associated with a virtual image that is closer to the lens than the focal point, and on the same side of the lens as the object. The closer the object is to the lens, the closer the virtual image is to the lens. As with mirrors, upright images produced by a single lens are virtual, while inverted images are real.\n\nLenses suffer from aberrations that distort images. \"Monochromatic aberrations\" occur because the geometry of the lens does not perfectly direct rays from each object point to a single point on the image, while chromatic aberration occurs because the index of refraction of the lens varies with the wavelength of the light.\n\nIn physical optics, light is considered to propagate as a wave. This model predicts phenomena such as interference and diffraction, which are not explained by geometric optics. The speed of light waves in air is approximately 3.0×10 m/s (exactly 299,792,458 m/s in vacuum). The wavelength of visible light waves varies between 400 and 700 nm, but the term \"light\" is also often applied to infrared (0.7–300 μm) and ultraviolet radiation (10–400 nm). \nThe wave model can be used to make predictions about how an optical system will behave without requiring an explanation of what is \"waving\" in what medium. Until the middle of the 19th century, most physicists believed in an \"ethereal\" medium in which the light disturbance propagated. The existence of electromagnetic waves was predicted in 1865 by Maxwell's equations. These waves propagate at the speed of light and have varying electric and magnetic fields which are orthogonal to one another, and also to the direction of propagation of the waves. Light waves are now generally treated as electromagnetic waves except when quantum mechanical effects have to be considered.\n\nMany simplified approximations are available for analysing and designing optical systems. Most of these use a single scalar quantity to represent the electric field of the light wave, rather than using a vector model with orthogonal electric and magnetic vectors.\nThe Huygens–Fresnel equation is one such model. This was derived empirically by Fresnel in 1815, based on Huygens' hypothesis that each point on a wavefront generates a secondary spherical wavefront, which Fresnel combined with the principle of superposition of waves. The Kirchhoff diffraction equation, which is derived using Maxwell's equations, puts the Huygens-Fresnel equation on a firmer physical foundation. Examples of the application of Huygens–Fresnel principle can be found in the sections on diffraction and Fraunhofer diffraction.\n\nMore rigorous models, involving the modelling of both electric and magnetic fields of the light wave, are required when dealing with the detailed interaction of light with materials where the interaction depends on their electric and magnetic properties. For instance, the behaviour of a light wave interacting with a metal surface is quite different from what happens when it interacts with a dielectric material. A vector model must also be used to model polarised light.\n\nNumerical modeling techniques such as the finite element method, the boundary element method and the transmission-line matrix method can be used to model the propagation of light in systems which cannot be solved analytically. Such models are computationally demanding and are normally only used to solve small-scale problems that require accuracy beyond that which can be achieved with analytical solutions.\n\nAll of the results from geometrical optics can be recovered using the techniques of Fourier optics which apply many of the same mathematical and analytical techniques used in acoustic engineering and signal processing.\n\nGaussian beam propagation is a simple paraxial physical optics model for the propagation of coherent radiation such as laser beams. This technique partially accounts for diffraction, allowing accurate calculations of the rate at which a laser beam expands with distance, and the minimum size to which the beam can be focused. Gaussian beam propagation thus bridges the gap between geometric and physical optics.\n\nIn the absence of nonlinear effects, the superposition principle can be used to predict the shape of interacting waveforms through the simple addition of the disturbances. This interaction of waves to produce a resulting pattern is generally termed \"interference\" and can result in a variety of outcomes. If two waves of the same wavelength and frequency are \"in phase\", both the wave crests and wave troughs align. This results in constructive interference and an increase in the amplitude of the wave, which for light is associated with a brightening of the waveform in that location. Alternatively, if the two waves of the same wavelength and frequency are out of phase, then the wave crests will align with wave troughs and vice versa. This results in destructive interference and a decrease in the amplitude of the wave, which for light is associated with a dimming of the waveform at that location. See below for an illustration of this effect.\n\nSince the Huygens–Fresnel principle states that every point of a wavefront is associated with the production of a new disturbance, it is possible for a wavefront to interfere with itself constructively or destructively at different locations producing bright and dark fringes in regular and predictable patterns. Interferometry is the science of measuring these patterns, usually as a means of making precise determinations of distances or angular resolutions. The Michelson interferometer was a famous instrument which used interference effects to accurately measure the speed of light.\n\nThe appearance of thin films and coatings is directly affected by interference effects. Antireflective coatings use destructive interference to reduce the reflectivity of the surfaces they coat, and can be used to minimise glare and unwanted reflections. The simplest case is a single layer with thickness one-fourth the wavelength of incident light. The reflected wave from the top of the film and the reflected wave from the film/material interface are then exactly 180° out of phase, causing destructive interference. The waves are only exactly out of phase for one wavelength, which would typically be chosen to be near the centre of the visible spectrum, around 550 nm. More complex designs using multiple layers can achieve low reflectivity over a broad band, or extremely low reflectivity at a single wavelength.\n\nConstructive interference in thin films can create strong reflection of light in a range of wavelengths, which can be narrow or broad depending on the design of the coating. These films are used to make dielectric mirrors, interference filters, heat reflectors, and filters for colour separation in colour television cameras. This interference effect is also what causes the colourful rainbow patterns seen in oil slicks.\n\nDiffraction is the process by which light interference is most commonly observed. The effect was first described in 1665 by Francesco Maria Grimaldi, who also coined the term from the Latin \"diffringere\", 'to break into pieces'. Later that century, Robert Hooke and Isaac Newton also described phenomena now known to be diffraction in Newton's rings while James Gregory recorded his observations of diffraction patterns from bird feathers.\n\nThe first physical optics model of diffraction that relied on the Huygens–Fresnel principle was developed in 1803 by Thomas Young in his interference experiments with the interference patterns of two closely spaced slits. Young showed that his results could only be explained if the two slits acted as two unique sources of waves rather than corpuscles. In 1815 and 1818, Augustin-Jean Fresnel firmly established the mathematics of how wave interference can account for diffraction.\n\nThe simplest physical models of diffraction use equations that describe the angular separation of light and dark fringes due to light of a particular wavelength (λ). In general, the equation takes the form\n\nwhere formula_15 is the separation between two wavefront sources (in the case of Young's experiments, it was two slits), formula_16 is the angular separation between the central fringe and the formula_17th order fringe, where the central maximum is formula_18.\n\nThis equation is modified slightly to take into account a variety of situations such as diffraction through a single gap, diffraction through multiple slits, or diffraction through a diffraction grating that contains a large number of slits at equal spacing. More complicated models of diffraction require working with the mathematics of Fresnel or Fraunhofer diffraction.\n\nX-ray diffraction makes use of the fact that atoms in a crystal have regular spacing at distances that are on the order of one angstrom. To see diffraction patterns, x-rays with similar wavelengths to that spacing are passed through the crystal. Since crystals are three-dimensional objects rather than two-dimensional gratings, the associated diffraction pattern varies in two directions according to Bragg reflection, with the associated bright spots occurring in unique patterns and formula_15 being twice the spacing between atoms.\n\nDiffraction effects limit the ability for an optical detector to optically resolve separate light sources. In general, light that is passing through an aperture will experience diffraction and the best images that can be created (as described in diffraction-limited optics) appear as a central spot with surrounding bright rings, separated by dark nulls; this pattern is known as an Airy pattern, and the central bright lobe as an Airy disk. The size of such a disk is given by\n\nwhere \"θ\" is the angular resolution, \"λ\" is the wavelength of the light, and \"D\" is the diameter of the lens aperture. If the angular separation of the two points is significantly less than the Airy disk angular radius, then the two points cannot be resolved in the image, but if their angular separation is much greater than this, distinct images of the two points are formed and they can therefore be resolved. Rayleigh defined the somewhat arbitrary \"Rayleigh criterion\" that two points whose angular separation is equal to the Airy disk radius (measured to first null, that is, to the first place where no light is seen) can be considered to be resolved. It can be seen that the greater the diameter of the lens or its aperture, the finer the resolution. Interferometry, with its ability to mimic extremely large baseline apertures, allows for the greatest angular resolution possible.\n\nFor astronomical imaging, the atmosphere prevents optimal resolution from being achieved in the visible spectrum due to the atmospheric scattering and dispersion which cause stars to twinkle. Astronomers refer to this effect as the quality of astronomical seeing. Techniques known as adaptive optics have been used to eliminate the atmospheric disruption of images and achieve results that approach the diffraction limit.\n\nRefractive processes take place in the physical optics limit, where the wavelength of light is similar to other distances, as a kind of scattering. The simplest type of scattering is Thomson scattering which occurs when electromagnetic waves are deflected by single particles. In the limit of Thomson scattering, in which the wavelike nature of light is evident, light is dispersed independent of the frequency, in contrast to Compton scattering which is frequency-dependent and strictly a quantum mechanical process, involving the nature of light as particles. In a statistical sense, elastic scattering of light by numerous particles much smaller than the wavelength of the light is a process known as Rayleigh scattering while the similar process for scattering by particles that are similar or larger in wavelength is known as Mie scattering with the Tyndall effect being a commonly observed result. A small proportion of light scattering from atoms or molecules may undergo Raman scattering, wherein the frequency changes due to excitation of the atoms and molecules. Brillouin scattering occurs when the frequency of light changes due to local changes with time and movements of a dense material.\n\nDispersion occurs when different frequencies of light have different phase velocities, due either to material properties (\"material dispersion\") or to the geometry of an optical waveguide (\"waveguide dispersion\"). The most familiar form of dispersion is a decrease in index of refraction with increasing wavelength, which is seen in most transparent materials. This is called \"normal dispersion\". It occurs in all dielectric materials, in wavelength ranges where the material does not absorb light. In wavelength ranges where a medium has significant absorption, the index of refraction can increase with wavelength. This is called \"anomalous dispersion\".\n\nThe separation of colours by a prism is an example of normal dispersion. At the surfaces of the prism, Snell's law predicts that light incident at an angle θ to the normal will be refracted at an angle arcsin(sin (θ) / \"n\"). Thus, blue light, with its higher refractive index, is bent more strongly than red light, resulting in the well-known rainbow pattern.\n\nMaterial dispersion is often characterised by the Abbe number, which gives a simple measure of dispersion based on the index of refraction at three specific wavelengths. Waveguide dispersion is dependent on the propagation constant. Both kinds of dispersion cause changes in the group characteristics of the wave, the features of the wave packet that change with the same frequency as the amplitude of the electromagnetic wave. \"Group velocity dispersion\" manifests as a spreading-out of the signal \"envelope\" of the radiation and can be quantified with a group dispersion delay parameter:\n\nwhere formula_22 is the group velocity. For a uniform medium, the group velocity is\n\nwhere \"n\" is the index of refraction and \"c\" is the speed of light in a vacuum. This gives a simpler form for the dispersion delay parameter:\n\nIf \"D\" is less than zero, the medium is said to have \"positive dispersion\" or normal dispersion. If \"D\" is greater than zero, the medium has \"negative dispersion\". If a light pulse is propagated through a normally dispersive medium, the result is the higher frequency components slow down more than the lower frequency components. The pulse therefore becomes \"positively chirped\", or \"up-chirped\", increasing in frequency with time. This causes the spectrum coming out of a prism to appear with red light the least refracted and blue/violet light the most refracted. Conversely, if a pulse travels through an anomalously (negatively) dispersive medium, high frequency components travel faster than the lower ones, and the pulse becomes \"negatively chirped\", or \"down-chirped\", decreasing in frequency with time.\n\nThe result of group velocity dispersion, whether negative or positive, is ultimately temporal spreading of the pulse. This makes dispersion management extremely important in optical communications systems based on optical fibres, since if dispersion is too high, a group of pulses representing information will each spread in time and merge, making it impossible to extract the signal.\n\nPolarization is a general property of waves that describes the orientation of their oscillations. For transverse waves such as many electromagnetic waves, it describes the orientation of the oscillations in the plane perpendicular to the wave's direction of travel. The oscillations may be oriented in a single direction (linear polarization), or the oscillation direction may rotate as the wave travels (circular or elliptical polarization). Circularly polarised waves can rotate rightward or leftward in the direction of travel, and which of those two rotations is present in a wave is called the wave's chirality.\n\nThe typical way to consider polarization is to keep track of the orientation of the electric field vector as the electromagnetic wave propagates. The electric field vector of a plane wave may be arbitrarily divided into two perpendicular components labeled \"x\" and \"y\" (with z indicating the direction of travel). The shape traced out in the x-y plane by the electric field vector is a Lissajous figure that describes the \"polarization state\". The following figures show some examples of the evolution of the electric field vector (blue), with time (the vertical axes), at a particular point in space, along with its \"x\" and \"y\" components (red/left and green/right), and the path traced by the vector in the plane (purple): The same evolution would occur when looking at the electric field at a particular time while evolving the point in space, along the direction opposite to propagation.\n\nIn the leftmost figure above, the x and y components of the light wave are in phase. In this case, the ratio of their strengths is constant, so the direction of the electric vector (the vector sum of these two components) is constant. Since the tip of the vector traces out a single line in the plane, this special case is called linear polarization. The direction of this line depends on the relative amplitudes of the two components.\n\nIn the middle figure, the two orthogonal components have the same amplitudes and are 90° out of phase. In this case, one component is zero when the other component is at maximum or minimum amplitude. There are two possible phase relationships that satisfy this requirement: the \"x\" component can be 90° ahead of the \"y\" component or it can be 90° behind the \"y\" component. In this special case, the electric vector traces out a circle in the plane, so this polarization is called circular polarization. The rotation direction in the circle depends on which of the two phase relationships exists and corresponds to \"right-hand circular polarization\" and \"left-hand circular polarization\".\n\nIn all other cases, where the two components either do not have the same amplitudes and/or their phase difference is neither zero nor a multiple of 90°, the polarization is called elliptical polarization because the electric vector traces out an ellipse in the plane (the \"polarization ellipse\"). This is shown in the above figure on the right. Detailed mathematics of polarization is done using Jones calculus and is characterised by the Stokes parameters.\n\nMedia that have different indexes of refraction for different polarization modes are called \"birefringent\". Well known manifestations of this effect appear in optical wave plates/retarders (linear modes) and in Faraday rotation/optical rotation (circular modes). If the path length in the birefringent medium is sufficient, plane waves will exit the material with a significantly different propagation direction, due to refraction. For example, this is the case with macroscopic crystals of calcite, which present the viewer with two offset, orthogonally polarised images of whatever is viewed through them. It was this effect that provided the first discovery of polarization, by Erasmus Bartholinus in 1669. In addition, the phase shift, and thus the change in polarization state, is usually frequency dependent, which, in combination with dichroism, often gives rise to bright colours and rainbow-like effects. In mineralogy, such properties, known as pleochroism, are frequently exploited for the purpose of identifying minerals using polarization microscopes. Additionally, many plastics that are not normally birefringent will become so when subject to mechanical stress, a phenomenon which is the basis of photoelasticity. Non-birefringent methods, to rotate the linear polarization of light beams, include the use of prismatic polarization rotators which use total internal reflection in a prism set designed for efficient collinear transmission.\n\nMedia that reduce the amplitude of certain polarization modes are called \"dichroic\", with devices that block nearly all of the radiation in one mode known as \"polarizing filters\" or simply \"polarisers\". Malus' law, which is named after Étienne-Louis Malus, says that when a perfect polariser is placed in a linear polarised beam of light, the intensity, \"I\", of the light that passes through is given by\n\nwhere\n\nA beam of unpolarised light can be thought of as containing a uniform mixture of linear polarizations at all possible angles. Since the average value of formula_26 is 1/2, the transmission coefficient becomes\n\nIn practice, some light is lost in the polariser and the actual transmission of unpolarised light will be somewhat lower than this, around 38% for Polaroid-type polarisers but considerably higher (>49.9%) for some birefringent prism types.\n\nIn addition to birefringence and dichroism in extended media, polarization effects can also occur at the (reflective) interface between two materials of different refractive index. These effects are treated by the Fresnel equations. Part of the wave is transmitted and part is reflected, with the ratio depending on angle of incidence and the angle of refraction. In this way, physical optics recovers Brewster's angle. When light reflects from a thin film on a surface, interference between the reflections from the film's surfaces can produce polarization in the reflected and transmitted light.\n\nMost sources of electromagnetic radiation contain a large number of atoms or molecules that emit light. The orientation of the electric fields produced by these emitters may not be correlated, in which case the light is said to be \"unpolarised\". If there is partial correlation between the emitters, the light is \"partially polarised\". If the polarization is consistent across the spectrum of the source, partially polarised light can be described as a superposition of a completely unpolarised component, and a completely polarised one. One may then describe the light in terms of the degree of polarization, and the parameters of the polarization ellipse.\n\nLight reflected by shiny transparent materials is partly or fully polarised, except when the light is normal (perpendicular) to the surface. It was this effect that allowed the mathematician Étienne-Louis Malus to make the measurements that allowed for his development of the first mathematical models for polarised light. Polarization occurs when light is scattered in the atmosphere. The scattered light produces the brightness and colour in clear skies. This partial polarization of scattered light can be taken advantage of using polarizing filters to darken the sky in photographs. Optical polarization is principally of importance in chemistry due to circular dichroism and optical rotation (\"\"circular birefringence\"\") exhibited by optically active (chiral) molecules.\n\n\"Modern optics\" encompasses the areas of optical science and engineering that became popular in the 20th century. These areas of optical science typically relate to the electromagnetic or quantum properties of light but do include other topics. A major subfield of modern optics, quantum optics, deals with specifically quantum mechanical properties of light. Quantum optics is not just theoretical; some modern devices, such as lasers, have principles of operation that depend on quantum mechanics. Light detectors, such as photomultipliers and channeltrons, respond to individual photons. Electronic image sensors, such as CCDs, exhibit shot noise corresponding to the statistics of individual photon events. Light-emitting diodes and photovoltaic cells, too, cannot be understood without quantum mechanics. In the study of these devices, quantum optics often overlaps with quantum electronics.\n\nSpecialty areas of optics research include the study of how light interacts with specific materials as in crystal optics and metamaterials. Other research focuses on the phenomenology of electromagnetic waves as in singular optics, non-imaging optics, non-linear optics, statistical optics, and radiometry. Additionally, computer engineers have taken an interest in integrated optics, machine vision, and photonic computing as possible components of the \"next generation\" of computers.\n\nToday, the pure science of optics is called optical science or optical physics to distinguish it from applied optical sciences, which are referred to as optical engineering. Prominent subfields of optical engineering include illumination engineering, photonics, and optoelectronics with practical applications like lens design, fabrication and testing of optical components, and image processing. Some of these fields overlap, with nebulous boundaries between the subjects terms that mean slightly different things in different parts of the world and in different areas of industry. A professional community of researchers in nonlinear optics has developed in the last several decades due to advances in laser technology.\n\nA laser is a device that emits light (electromagnetic radiation) through a process called \"stimulated emission\". The term \"laser\" is an acronym for \"Light Amplification by Stimulated Emission of Radiation\". Laser light is usually spatially coherent, which means that the light either is emitted in a narrow, low-divergence beam, or can be converted into one with the help of optical components such as lenses. Because the microwave equivalent of the laser, the \"maser\", was developed first, devices that emit microwave and radio frequencies are usually called \"masers\".\n\nThe first working laser was demonstrated on 16 May 1960 by Theodore Maiman at Hughes Research Laboratories. When first invented, they were called \"a solution looking for a problem\". Since then, lasers have become a multibillion-dollar industry, finding utility in thousands of highly varied applications. The first application of lasers visible in the daily lives of the general population was the supermarket barcode scanner, introduced in 1974. The laserdisc player, introduced in 1978, was the first successful consumer product to include a laser, but the compact disc player was the first laser-equipped device to become truly common in consumers' homes, beginning in 1982. These optical storage devices use a semiconductor laser less than a millimetre wide to scan the surface of the disc for data retrieval. Fibre-optic communication relies on lasers to transmit large amounts of information at the speed of light. Other common applications of lasers include laser printers and laser pointers. Lasers are used in medicine in areas such as bloodless surgery, laser eye surgery, and laser capture microdissection and in military applications such as missile defence systems, electro-optical countermeasures (EOCM), and lidar. Lasers are also used in holograms, bubblegrams, laser light shows, and laser hair removal.\n\nThe Kapitsa–Dirac effect causes beams of particles to diffract as the result of meeting a standing wave of light. Light can be used to position matter using various phenomena (see optical tweezers).\n\nOptics is part of everyday life. The ubiquity of visual systems in biology indicates the central role optics plays as the science of one of the five senses. Many people benefit from eyeglasses or contact lenses, and optics are integral to the functioning of many consumer goods including cameras. Rainbows and mirages are examples of optical phenomena. Optical communication provides the backbone for both the Internet and modern telephony.\n\nThe human eye functions by focusing light onto a layer of photoreceptor cells called the retina, which forms the inner lining of the back of the eye. The focusing is accomplished by a series of transparent media. Light entering the eye passes first through the cornea, which provides much of the eye's optical power. The light then continues through the fluid just behind the cornea—the anterior chamber, then passes through the pupil. The light then passes through the lens, which focuses the light further and allows adjustment of focus. The light then passes through the main body of fluid in the eye—the vitreous humour, and reaches the retina. The cells in the retina line the back of the eye, except for where the optic nerve exits; this results in a blind spot.\n\nThere are two types of photoreceptor cells, rods and cones, which are sensitive to different aspects of light. Rod cells are sensitive to the intensity of light over a wide frequency range, thus are responsible for black-and-white vision. Rod cells are not present on the fovea, the area of the retina responsible for central vision, and are not as responsive as cone cells to spatial and temporal changes in light. There are, however, twenty times more rod cells than cone cells in the retina because the rod cells are present across a wider area. Because of their wider distribution, rods are responsible for peripheral vision.\n\nIn contrast, cone cells are less sensitive to the overall intensity of light, but come in three varieties that are sensitive to different frequency-ranges and thus are used in the perception of colour and photopic vision. Cone cells are highly concentrated in the fovea and have a high visual acuity meaning that they are better at spatial resolution than rod cells. Since cone cells are not as sensitive to dim light as rod cells, most night vision is limited to rod cells. Likewise, since cone cells are in the fovea, central vision (including the vision needed to do most reading, fine detail work such as sewing, or careful examination of objects) is done by cone cells.\n\nCiliary muscles around the lens allow the eye's focus to be adjusted. This process is known as accommodation. The near point and far point define the nearest and farthest distances from the eye at which an object can be brought into sharp focus. For a person with normal vision, the far point is located at infinity. The near point's location depends on how much the muscles can increase the curvature of the lens, and how inflexible the lens has become with age. Optometrists, ophthalmologists, and opticians usually consider an appropriate near point to be closer than normal reading distance—approximately 25 cm.\n\nDefects in vision can be explained using optical principles. As people age, the lens becomes less flexible and the near point recedes from the eye, a condition known as presbyopia. Similarly, people suffering from hyperopia cannot decrease the focal length of their lens enough to allow for nearby objects to be imaged on their retina. Conversely, people who cannot increase the focal length of their lens enough to allow for distant objects to be imaged on the retina suffer from myopia and have a far point that is considerably closer than infinity. A condition known as astigmatism results when the cornea is not spherical but instead is more curved in one direction. This causes horizontally extended objects to be focused on different parts of the retina than vertically extended objects, and results in distorted images.\n\nAll of these conditions can be corrected using corrective lenses. For presbyopia and hyperopia, a converging lens provides the extra curvature necessary to bring the near point closer to the eye while for myopia a diverging lens provides the curvature necessary to send the far point to infinity. Astigmatism is corrected with a cylindrical surface lens that curves more strongly in one direction than in another, compensating for the non-uniformity of the cornea.\n\nThe optical power of corrective lenses is measured in diopters, a value equal to the reciprocal of the focal length measured in metres; with a positive focal length corresponding to a converging lens and a negative focal length corresponding to a diverging lens. For lenses that correct for astigmatism as well, three numbers are given: one for the spherical power, one for the cylindrical power, and one for the angle of orientation of the astigmatism.\n\nOptical illusions (also called visual illusions) are characterized by visually perceived images that differ from objective reality. The information gathered by the eye is processed in the brain to give a percept that differs from the object being imaged. Optical illusions can be the result of a variety of phenomena including physical effects that create images that are different from the objects that make them, the physiological effects on the eyes and brain of excessive stimulation (e.g. brightness, tilt, colour, movement), and cognitive illusions where the eye and brain make unconscious inferences.\n\nCognitive illusions include some which result from the unconscious misapplication of certain optical principles. For example, the Ames room, Hering, Müller-Lyer, Orbison, Ponzo, Sander, and Wundt illusions all rely on the suggestion of the appearance of distance by using converging and diverging lines, in the same way that parallel light rays (or indeed any set of parallel lines) appear to converge at a vanishing point at infinity in two-dimensionally rendered images with artistic perspective. This suggestion is also responsible for the famous moon illusion where the moon, despite having essentially the same angular size, appears much larger near the horizon than it does at zenith. This illusion so confounded Ptolemy that he incorrectly attributed it to atmospheric refraction when he described it in his treatise, \"Optics\".\n\nAnother type of optical illusion exploits broken patterns to trick the mind into perceiving symmetries or asymmetries that are not present. Examples include the café wall, Ehrenstein, Fraser spiral, Poggendorff, and Zöllner illusions. Related, but not strictly illusions, are patterns that occur due to the superimposition of periodic structures. For example, transparent tissues with a grid structure produce shapes known as moiré patterns, while the superimposition of periodic transparent patterns comprising parallel opaque lines or curves produces line moiré patterns.\n\nSingle lenses have a variety of applications including photographic lenses, corrective lenses, and magnifying glasses while single mirrors are used in parabolic reflectors and rear-view mirrors. Combining a number of mirrors, prisms, and lenses produces compound optical instruments which have practical uses. For example, a periscope is simply two plane mirrors aligned to allow for viewing around obstructions. The most famous compound optical instruments in science are the microscope and the telescope which were both invented by the Dutch in the late 16th century.\n\nMicroscopes were first developed with just two lenses: an objective lens and an eyepiece. The objective lens is essentially a magnifying glass and was designed with a very small focal length while the eyepiece generally has a longer focal length. This has the effect of producing magnified images of close objects. Generally, an additional source of illumination is used since magnified images are dimmer due to the conservation of energy and the spreading of light rays over a larger surface area. Modern microscopes, known as \"compound microscopes\" have many lenses in them (typically four) to optimize the functionality and enhance image stability. A slightly different variety of microscope, the comparison microscope, looks at side-by-side images to produce a stereoscopic binocular view that appears three dimensional when used by humans.\n\nThe first telescopes, called \"refracting telescopes\" were also developed with a single objective and eyepiece lens. In contrast to the microscope, the objective lens of the telescope was designed with a large focal length to avoid optical aberrations. The objective focuses an image of a distant object at its focal point which is adjusted to be at the focal point of an eyepiece of a much smaller focal length. The main goal of a telescope is not necessarily magnification, but rather collection of light which is determined by the physical size of the objective lens. Thus, telescopes are normally indicated by the diameters of their objectives rather than by the magnification which can be changed by switching eyepieces. Because the magnification of a telescope is equal to the focal length of the objective divided by the focal length of the eyepiece, smaller focal-length eyepieces cause greater magnification.\n\nSince crafting large lenses is much more difficult than crafting large mirrors, most modern telescopes are \"reflecting telescopes\", that is, telescopes that use a primary mirror rather than an objective lens. The same general optical considerations apply to reflecting telescopes that applied to refracting telescopes, namely, the larger the primary mirror, the more light collected, and the magnification is still equal to the focal length of the primary mirror divided by the focal length of the eyepiece. Professional telescopes generally do not have eyepieces and instead place an instrument (often a charge-coupled device) at the focal point instead.\n\nThe optics of photography involves both lenses and the medium in which the electromagnetic radiation is recorded, whether it be a plate, film, or charge-coupled device. Photographers must consider the reciprocity of the camera and the shot which is summarized by the relation\n\nIn other words, the smaller the aperture (giving greater depth of focus), the less light coming in, so the length of time has to be increased (leading to possible blurriness if motion occurs). An example of the use of the law of reciprocity is the Sunny 16 rule which gives a rough estimate for the settings needed to estimate the proper exposure in daylight.\n\nA camera's aperture is measured by a unitless number called the f-number or f-stop, #, often notated as formula_28, and given by\nwhere formula_13 is the focal length, and formula_31 is the diameter of the entrance pupil. By convention, \"#\" is treated as a single symbol, and specific values of # are written by replacing the number sign with the value. The two ways to increase the f-stop are to either decrease the diameter of the entrance pupil or change to a longer focal length (in the case of a zoom lens, this can be done by simply adjusting the lens). Higher f-numbers also have a larger depth of field due to the lens approaching the limit of a pinhole camera which is able to focus all images perfectly, regardless of distance, but requires very long exposure times.\n\nThe field of view that the lens will provide changes with the focal length of the lens. There are three basic classifications based on the relationship to the diagonal size of the film or sensor size of the camera to the focal length of the lens:\n\n\nModern zoom lenses may have some or all of these attributes.\n\nThe absolute value for the exposure time required depends on how sensitive to light the medium being used is (measured by the film speed, or, for digital media, by the quantum efficiency). Early photography used media that had very low light sensitivity, and so exposure times had to be long even for very bright shots. As technology has improved, so has the sensitivity through film cameras and digital cameras.\n\nOther results from physical and geometrical optics apply to camera optics. For example, the maximum resolution capability of a particular camera set-up is determined by the diffraction limit associated with the pupil size and given, roughly, by the Rayleigh criterion.\n\nThe unique optical properties of the atmosphere cause a wide range of spectacular optical phenomena. The blue colour of the sky is a direct result of Rayleigh scattering which redirects higher frequency (blue) sunlight back into the field of view of the observer. Because blue light is scattered more easily than red light, the sun takes on a reddish hue when it is observed through a thick atmosphere, as during a sunrise or sunset. Additional particulate matter in the sky can scatter different colours at different angles creating colourful glowing skies at dusk and dawn. Scattering off of ice crystals and other particles in the atmosphere are responsible for halos, afterglows, coronas, rays of sunlight, and sun dogs. The variation in these kinds of phenomena is due to different particle sizes and geometries.\n\nMirages are optical phenomena in which light rays are bent due to thermal variations in the refraction index of air, producing displaced or heavily distorted images of distant objects. Other dramatic optical phenomena associated with this include the Novaya Zemlya effect where the sun appears to rise earlier than predicted with a distorted shape. A spectacular form of refraction occurs with a temperature inversion called the Fata Morgana where objects on the horizon or even beyond the horizon, such as islands, cliffs, ships or icebergs, appear elongated and elevated, like \"fairy tale castles\".\n\nRainbows are the result of a combination of internal reflection and dispersive refraction of light in raindrops. A single reflection off the backs of an array of raindrops produces a rainbow with an angular size on the sky that ranges from 40° to 42° with red on the outside. Double rainbows are produced by two internal reflections with angular size of 50.5° to 54° with violet on the outside. Because rainbows are seen with the sun 180° away from the centre of the rainbow, rainbows are more prominent the closer the sun is to the horizon.\n\n\n\n\n\n\n",
                "Cinematography\n\nCinematography (also called \"Direction of Photography\") is the science or art of motion-picture photography by recording light or other electromagnetic radiation, either electronically by means of an image sensor, or chemically by means of a light-sensitive material such as film stock.\n\nTypically, a lens is used to repeatedly focus the light reflected from objects into real images on the light-sensitive surface inside a camera during a questioned exposure, creating multiple images. With an electronic image sensor, this produces an electrical charge at each pixel, which is electronically processed and stored in a video file for subsequent display or processing. The result with photographic emulsion is a series of invisible latent images on the film stock, which are later chemically \"developed\" into a visible image. The images on the film stock are played back at a rapid speed and projected onto a screen, creating the illusion of motion.\n\nCinematography finds uses in many fields of science and business as well as for entertainment purposes and mass communication.\n\nThe word \"cinematography\" was created from the Greek words (\"kinema\"), meaning \"movement, motion\" and (\"graphein\") meaning \"to record\", together meaning \"recording motion.\" The word used to refer to the art, process, or job of filming movies, but later its meaning was restricted to \"motion picture photography.\"\n\nIn the 1830s, moving images were produced on revolving drums and disks, with independent invention by Simon von Stampfer (stroboscope) in Austria, Joseph Plateau (phenakistoscope) in Belgium, and William Horner (zoetrope) in Britain.\n\nIn 1845, Francis Ronalds invented the first successful camera able to make continuous recordings of the varying indications of meteorological and geomagnetic instruments over time. The cameras were supplied to numerous observatories around the world and some remained in use until well into the 20th century.\n\nWilliam Lincoln patented a device, in 1867, that showed animated pictures called the \"wheel of life\" or \"zoopraxiscope\". In it, moving drawings or photographs were watched through a slit.\n\nOn 19 June 1873, Eadweard Muybridge successfully photographed a horse named \"Sallie Gardner\" in fast motion using a series of 24 stereoscopic cameras. The cameras were arranged along a track parallel to the horse's, and each camera shutter was controlled by a trip wire triggered by the horse's hooves. They were 21 inches apart to cover the 20 feet taken by the horse stride, taking pictures at one thousandth of a second. At the end of the decade, Muybridge had adapted sequences of his photographs to a zoopraxiscope for short, primitive projected \"movies,\" which were sensations on his lecture tours by 1879 or 1880.\n\nNine years later, in 1882, French scientist Étienne-Jules Marey invented a chronophotographic gun, which was capable of taking 12 consecutive frames a second, recording all the frames of the same picture.\n\nThe late nineteenth to the early twentieth century brought rise to the use of film not only for entertainment purposes but for scientific exploration as well. French biologist and filmmaker Jean Painleve lobbied heavily for the use of film in the scientific field, as the new medium was more efficient in capturing and documenting the behavior, movement, and environment of microorganisms, cells, and bacteria, than the naked eye. The introduction of film into scientific fields allowed for not only the viewing \"new images and objects, such as cells and natural objects, but also the viewing of them in real time\", whereas prior to the invention of moving pictures, scientists and doctors alike had to rely on hand drawn sketches of human anatomy and its microorganisms. This posed a great inconvenience in the science and medical worlds. The development of film and increased usage of cameras allowed doctors and scientists to grasp a better understanding and knowledge of their projects.\n\nThe experimental film \"Roundhay Garden Scene\", filmed by Louis Le Prince on 14 October 1888, in Roundhay, Leeds, England, is the earliest surviving motion picture. This movie was shot on paper film.\n\nW. K. L. Dickson, working under the direction of Thomas Alva Edison, was the first to design a successful apparatus, the Kinetograph, patented in 1891. This camera took a series of instantaneous photographs on standard Eastman Kodak photographic emulsion coated onto a transparent celluloid strip 35 mm wide. The results of this work were first shown in public in 1893, using the viewing apparatus also designed by Dickson, the Kinetoscope. Contained within a large box, only one person at a time looking into it through a peephole could view the movie.\n\nIn the following year, Charles Francis Jenkins and his projector, the Phantoscope, made a successful audience viewing while Louis and Auguste Lumière perfected the Cinématographe, an apparatus that took, printed, and projected film, in Paris in December 1895. The Lumière brothers were the first to present projected, moving, photographic, pictures to a paying audience of more than one person.\n\nIn 1896, movie theaters were open in France (Paris, Lyon, Bordeaux, Nice, Marseille); Italy (Rome, Milan, Naples, Genoa, Venice, Bologna, Forlì); Brussels; and London.\n\nIn 1896, Edison showed his improved Vitascope projector, the first commercially successful projector in the U.S.\n\nCooper Hewitt invented mercury lamps which made it practical to shoot films indoors without sunlight in 1905.\n\nThe first animated cartoon was produced in 1906.\n\nCredits began to appear at the beginning of motion pictures in 1911.\n\nThe Bell and Howell 2709 movie camera invented in 1915 allowed directors to make close-ups without physically moving the camera.\n\nBy the late 1920s, most of the movies produced were sound films.\n\nWide screen formats were first experimented with in the 1950s.\n\nBy the 1970s, most movies were color films. IMAX and other 70mm formats gained popularity. Wide distribution of films became commonplace, setting the ground for \"blockbusters.\"\n\nFilm cinematography dominated the motion picture industry from its inception until the 2010s when digital cinematography became dominant. Film cinematography is still used by some directors, especially in specific applications or out of fondness of the format.\n\nFrom its birth in the 1880s, movies were predominantly monochrome. Contrary to popular belief, monochrome doesn't always mean black and white; it means a movie shot in a single tone or color. Since the cost of tinted film bases was substantially higher, most movies were produced in black and white monochrome. Even with the advent of early color experiments, the greater expense of color meant films were mostly made in black and white until the 1950s, when cheaper color processes were introduced, and in some years the percentage of films shot on color film surpassed 51%. By the 1960s, color became by far the dominant film stock. In the coming decades, the usage of color film greatly increased while monochrome films became scarce.\n\nAfter the advent of motion pictures, a tremendous amount of energy was invested in the production of photography in natural color. The invention of the talking picture further increased the demand for the use of color photography. However, in comparison to other technological advances of the time, the arrival of color photography was a relatively slow process.\n\nEarly movies were not actually color movies since they were shot monochrome and hand-colored or machine-colored afterwards. (Such movies are referred to as \"colored\" and not \"color\".) The earliest such example is the hand-tinted Annabelle Serpentine Dance in 1895 by Edison Manufacturing Company. Machine-based tinting later became popular. Tinting continued until the advent of natural color cinematography in the 1910s. Many black and white movies have been colorized recently using digital tinting. This includes footage shot from both world wars, sporting events and political propaganda.\n\nIn 1902, Edward Raymond Turner produced the first films with a natural color process rather than using colorization techniques. In 1908, kinemacolor was introduced. In the same year, the short film \"A Visit to the Seaside\" became the first natural color movie to be publicly presented.\n\nIn 1917, the earliest version of Technicolor was introduced. Kodachrome was introduced in 1935. Eastmancolor was introduced in 1950 and became the color standard for the rest of the century.\n\nIn the 2010s, color films were largely superseded by color digital cinematography.\n\nIn digital cinematography, the movie is shot on digital medium such as flash storage, as well as distributed through a digital medium such as a hard drive.\n\nBeginning in the late 1980s, Sony began marketing the concept of \"electronic cinematography,\" utilizing its analog Sony HDVS professional video cameras. The effort met with very little success. However, this led to one of the earliest digitally shot feature movies, \"Julia and Julia\", being produced in 1987. In 1998, with the introduction of HDCAM recorders and 1920 × 1080 pixel digital professional video cameras based on CCD technology, the idea, now re-branded as \"digital cinematography,\" began to gain traction in the market.\n\nShot and released in 1998, \"The Last Broadcast\" is believed by some to be the first feature-length video shot and edited entirely on consumer-level digital equipment. In May 1999, George Lucas challenged the supremacy of the movie-making medium of film for the first time by including footage filmed with high-definition digital cameras in \"\". In late 2013, Paramount became the first major studio to distribute movies to theaters in digital format, eliminating 35mm film entirely. Since then the demand of movies to be developed onto digital format rather than 35mm has increased drastically.\n\nAs digital technology improved, movie studios began increasingly shifting towards digital cinematography. Since the 2010s, digital cinematography has become the dominant form of cinematography after largely superseding film cinematography.\n\nNumerous aspects contribute to the art of cinematography, including:\n\nThe first film cameras were fastened directly to the head of a tripod or other support, with only the crudest kind of leveling devices provided, in the manner of the still-camera tripod heads of the period. The earliest film cameras were thus effectively fixed during the shot, and hence the first camera movements were the result of mounting a camera on a moving vehicle. The first known of these was a film shot by a Lumière cameraman from the back platform of a train leaving Jerusalem in 1896, and by 1898, there were a number of films shot from moving trains. Although listed under the general heading of \"panoramas\" in the sales catalogues of the time, those films shot straight forward from in front of a railway engine were usually specifically referred to as \"phantom rides\".\n\nIn 1897, Robert W. Paul had the first real rotating camera head made to put on a tripod, so that he could follow the passing processions of Queen Victoria's Diamond Jubilee in one uninterrupted shot. This device had the camera mounted on a vertical axis that could be rotated by a worm gear driven by turning a crank handle, and Paul put it on general sale the next year. Shots taken using such a \"panning\" head were also referred to as \"panoramas\" in the film catalogues of the first decade of the cinema. This eventually led to the creation of a panoramic photo as well.\n\nThe standard pattern for early film studios was provided by the studio which Georges Méliès had built in 1897. This had a glass roof and three glass walls constructed after the model of large studios for still photography, and it was fitted with thin cotton cloths that could be stretched below the roof to diffuse the direct ray of the sun on sunny days. The soft overall light without real shadows that this arrangement produced, and which also exists naturally on lightly overcast days, was to become the basis for film lighting in film studios for the next decade.\n\nCinematography can begin with digital image sensor or rolls of film. Advancements in film emulsion and grain structure provided a wide range of available film stocks. The selection of a film stock is one of the first decisions made in preparing a typical film production.\n\nAside from the film gauge selection – 8 mm (amateur), 16 mm (semi-professional), 35 mm (professional) and 65 mm (epic photography, rarely used except in special event venues) – the cinematographer has a selection of stocks in reversal (which, when developed, create a positive image) and negative formats along with a wide range of film speeds (varying sensitivity to light) from ISO 50 (slow, least sensitive to light) to 800 (very fast, extremely sensitive to light) and differing response to color (low saturation, high saturation) and contrast (varying levels between pure black (no exposure) and pure white (complete overexposure).\nAdvancements and adjustments to nearly all gauges of film create the \"super\" formats wherein the area of the film used to capture a single frame of an image is expanded, although the physical gauge of the film remains the same. Super 8 mm, Super 16 mm, and Super 35 mm all utilize more of the overall film area for the image than their \"regular\" non-super counterparts. The larger the film gauge, the higher the overall image resolution clarity and technical quality. The techniques used by the film laboratory to process the film stock can also offer a considerable variance in the image produced. By controlling the temperature and varying the duration in which the film is soaked in the development chemicals, and by skipping certain chemical processes (or partially skipping all of them), cinematographers can achieve very different looks from a single film stock in the laboratory. Some techniques that can be used are push processing, bleach bypass, and cross processing.\n\nMost of modern cinema uses digital cinematography and has no film stocks , but the cameras themselves can be adjusted in ways that go far beyond the abilities of one particular film stock. They can provide varying degrees of color sensitivity, image contrast, light sensitivity and so on. One camera can achieve all the various looks of different emulsions. Digital image adjustments such as ISO and contrast are executed by estimating the same adjustments that would take place if actual film were in use, and are thus vulnerable to the camera's sensor designers perceptions of various film stocks and image adjustment parameters.\n\nFilters, such as diffusion filters or color effect filters, are also widely used to enhance mood or dramatic effects. Most photographic filters are made up of two pieces of optical glass glued together with some form of image or light manipulation material between the glass. In the case of color filters, there is often a translucent color medium pressed between two planes of optical glass. Color filters work by blocking out certain color wavelengths of light from reaching the film. With color film, this works very intuitively wherein a blue filter will cut down on the passage of red, orange, and yellow light and create a blue tint on the film. In black-and-white photography, color filters are used somewhat counter intuitively; for instance a yellow filter, which cuts down on blue wavelengths of light, can be used to darken a daylight sky (by eliminating blue light from hitting the film, thus greatly underexposing the mostly blue sky) while not biasing most human flesh tone. Certain cinematographers, such as Christopher Doyle, are well known for their innovative use of filters. Filters can be used in front of the lens or, in some cases, behind the lens for different effects. Christopher Doyle was a pioneer for increased usage of filters in movies. He was highly respected throughout the cinema world.\n\nLenses can be attached to the camera to give a certain look, feel, or effect by focus, color, etc.\n\nAs does the human eye, the camera creates perspective and spatial relations with the rest of the world. However, unlike one's eye, a cinematographer can select different lenses for different purposes. Variation in focal length is one of the chief benefits. The focal length of the lens determines the angle of view and, therefore, the field of view. Cinematographers can choose from a range of wide-angle lenses, \"normal\" lenses and long focus lenses, as well as macro lenses and other special effect lens systems such as borescope lenses. Wide-angle lenses have short focal lengths and make spatial distances more obvious. A person in the distance is shown as much smaller while someone in the front will loom large. On the other hand, long focus lenses reduce such exaggerations, depicting far-off objects as seemingly close together and flattening perspective. The differences between the perspective rendering is actually not due to the focal length by itself, but by the distance between the subjects and the camera. Therefore, the use of different focal lengths in combination with different camera to subject distances creates these different rendering. Changing the focal length only while keeping the same camera position doesn't affect perspective but the camera angle of view only.\n\nA zoom lens allows a camera operator to change his focal length within a shot or quickly between setups for shots. As prime lenses offer greater optical quality and are \"faster\" (larger aperture openings, usable in less light) than zoom lenses, they are often employed in professional cinematography over zoom lenses. Certain scenes or even types of filmmaking, however, may require the use of zooms for speed or ease of use, as well as shots involving a zoom move.\n\nAs in other photography, the control of the exposed image is done in the lens with the control of the diaphragm aperture. For proper selection, the cinematographer needs that all lenses be engraved with T-Stop, not f-stop so that the eventual light loss due to the glass doesn't affect the exposure control when setting it using the usual meters. The choice of the aperture also affects image quality (aberrations) and depth of field.\n\nFocal length and diaphragm aperture affect the depth of field of a scene – that is, how much the background, mid-ground and foreground will be rendered in \"acceptable focus\" (only one exact plane of the image is in precise focus) on the film or video target. Depth of field (not to be confused with depth of focus) is determined by the aperture size and the focal distance. A large or deep depth of field is generated with a very small iris aperture and focusing on a point in the distance, whereas a shallow depth of field will be achieved with a large (open) iris aperture and focusing closer to the lens. Depth of field is also governed by the format size. If one considers the field of view and angle of view, the smaller the image is, the shorter the focal length should be, as to keep the same field of view. Then, the smaller the image is, the more depth of field is obtained, for the same field of view. Therefore, 70mm has less depth of field than 35mm for a given field of view, 16mm more than 35mm, and video cameras even more depth of field than 16mm. As videographers try to emulate the look of 35 mm film with digital cameras, this is one issue of frustration – excessive depth of field with digital cameras and using additional optical devices to reduce that depth of field.\n\nIn \"Citizen Kane\" (1941), cinematographer Gregg Toland and director Orson Welles used tighter apertures to create every detail of the foreground and background of the sets in sharp focus. This practice is known as deep focus. Deep focus became a popular cinematographic device from the 1940s onwards in Hollywood. Today, the trend is for more shallow focus.\n\nTo change the plane of focus from one object or character to another within a shot is commonly known as a \"rack focus\".\n\nThe aspect ratio of an image is the ratio of its width to its height. This can be expressed either as a ratio of 2 integers, such as 4:3, or in a decimal format, such as 1.33:1 or simply 1.33.\n\nDifferent ratios provide different aesthetic effects. Standards for aspect ratio have varied significantly over time.\n\nDuring the silent era, aspect ratios varied widely, from square 1:1, all the way up to the extreme widescreen 4:1 Polyvision. However, from the 1910s, silent motion pictures generally settled on the ratio of 4:3 (1.33). The introduction of sound-on-film briefly narrowed the aspect ratio, to allow room for a sound stripe. In 1932, a new standard was introduced, the Academy ratio of 1.37, by means of thickening the frame line.\n\nFor years, mainstream cinematographers were limited to using the Academy ratio, but in the 1950s, thanks to the popularity of Cinerama, widescreen ratios were introduced in an effort to pull audiences back into the theater and away from their home television sets. These new widescreen formats provided cinematographers a wider frame within which to compose their images.\n\nMany different proprietary photographic systems were invented and utilized in the 1950s to create widescreen movies, but one dominated film: the anamorphic process, which optically squeezes the image to photograph twice the horizontal area to the same size vertical as standard \"spherical\" lenses. The first commonly used anamorphic format was CinemaScope, which used a 2.35 aspect ratio, although it was originally 2.55. CinemaScope was used from 1953 to 1967, but due to technical flaws in the design and its ownership by Fox, several third-party companies, led by Panavision's technical improvements in the 1950s, dominated the anamorphic cine lens market. Changes to SMPTE projection standards altered the projected ratio from 2.35 to 2.39 in 1970, although this did not change anything regarding the photographic anamorphic standards; all changes in respect to the aspect ratio of anamorphic 35 mm photography are specific to camera or projector gate sizes, not the optical system. After the \"widescreen wars\" of the 1950s, the motion-picture industry settled into 1.85 as a standard for theatrical projection in the United States and the United Kingdom. This is a cropped version of 1.37. Europe and Asia opted for 1.66 at first, although 1.85 has largely permeated these markets in recent decades. Certain \"epic\" or adventure movies utilized the anamorphic 2.39.\n\nIn the 1990s, with the advent of high-definition video, television engineers created the 1.78 (16:9) ratio as a mathematical compromise between the theatrical standard of 1.85 and television's 1.33, as it was not practical to produce a traditional CRT television tube with a width of 1.85. Until that point, nothing had ever been originated in 1.78. Today, this is a standard for high-definition video and for widescreen television.\n\nLight is necessary to create an image exposure on a frame of film or on a digital target (CCD, etc.). The art of lighting for cinematography goes far beyond basic exposure, however, into the essence of visual storytelling. Lighting contributes considerably to the emotional response an audience has watching a motion picture. The increased usage of filters can greatly impact the final image and affect the lighting.\n\nCinematography can not only depict a moving subject but can use a camera, which represents the audience's viewpoint or perspective, that moves during the course of filming. This movement plays a considerable role in the emotional language of film images and the audience's emotional reaction to the action. Techniques range from the most basic movements of panning (horizontal shift in viewpoint from a fixed position; like turning your head side-to-side) and tilting (vertical shift in viewpoint from a fixed position; like tipping your head back to look at the sky or down to look at the ground) to dollying (placing the camera on a moving platform to move it closer or farther from the subject), tracking (placing the camera on a moving platform to move it to the left or right), craning (moving the camera in a vertical position; being able to lift it off the ground as well as swing it side-to-side from a fixed base position), and combinations of the above. Early cinematographers often faced problems that were not common to other graphic artists because of the element of motion.\nCameras have been mounted to nearly every imaginable form of transportation.\n\nMost cameras can also be handheld, that is held in the hands of the camera operator who moves from one position to another while filming the action. Personal stabilizing platforms came into being in the late 1970s through the invention of Garrett Brown, which became known as the Steadicam. The Steadicam is a body harness and stabilization arm that connects to the camera, supporting the camera while isolating it from the operator's body movements. After the Steadicam patent expired in the early 1990s, many other companies began manufacturing their concept of the personal camera stabilizer. This invention is much more common throughout the cinematic world today. From feature-length films to the evening news, more and more networks have begun to use a personal camera stabilizer.\n\nThe first special effects in the cinema were created while the film was being shot. These came to be known as \"in-camera\" effects. Later, optical and digital effects were developed so that editors and visual effects artists could more tightly control the process by manipulating the film in post-production.\n\nThe 1896 movie The Execution of Mary Stuart shows an actor dressed as the queen placing her head on the execution block in front of a small group of bystanders in Elizabethan dress. The executioner brings his axe down, and the queen's severed head drops onto the ground. This trick was worked by stopping the camera and replacing the actor with a dummy, then restarting the camera before the axe falls. The two pieces of film were then trimmed and cemented together so that the action appeared continuous when the film was shown. Thus creating an overall illusion and successfully laying the foundation for special affects.\n\nThis film was among those exported to Europe with the first Kinetoscope machines in 1895 and was seen by Georges Méliès, who was putting on magic shows in his Theatre Robert-Houdin in Paris at the time. He took up filmmaking in 1896, and after making imitations of other films from Edison, Lumière, and Robert Paul, he made \"Escamotage d'un dame chez Robert-Houdin (The Vanishing Lady)\". This film shows a woman being made to vanish by using the same stop motion technique as the earlier Edison film. After this, Georges Méliès made many single shot films using this trick over the next couple of years.\n\nThe other basic technique for trick cinematography involves double exposure of the film in the camera, which was first done by George Albert Smith in July 1898 in the UK. Smith's \"The Corsican Brothers\" (1898) was described in the catalogue of the Warwick Trading Company, which took up the distribution of Smith's films in 1900, thus:\n\"One of the twin brothers returns home from shooting in the Corsican mountains, and is visited by the ghost of the other twin. By extremely careful photography the ghost appears *quite transparent*. After indicating that he has been killed by a sword-thrust, and appealing for vengeance, he disappears. A 'vision' then appears showing the fatal duel in the snow. To the Corsican's amazement, the duel and death of his brother are vividly depicted in the vision, and overcome by his feelings, he falls to the floor just as his mother enters the room.\"\nThe ghost effect was done by draping the set in black velvet after the main action had been shot, and then re-exposing the negative with the actor playing the ghost going through the actions at the appropriate point. Likewise, the vision, which appeared within a circular vignette or matte, was similarly superimposed over a black area in the backdrop to the scene, rather than over a part of the set with detail in it, so that nothing appeared through the image, which seemed quite solid. Smith used this technique again in \"Santa Claus\" (1898).\n\nGeorges Méliès first used superimposition on a dark background in \"La Caverne maudite (The Cave of the Demons)\" made a couple of months later in 1898, and elaborated it with multiple superimpositions in the one shot in \"Un Homme de têtes (The Four Troublesome Heads)\". He created further variations in subsequent films.\n\nMotion picture images are presented to an audience at a constant speed. In the theater it is 24 frames per second, in NTSC (US) Television it is 30 frames per second (29.97 to be exact), in PAL (Europe) television it is 25 frames per second. This speed of presentation does not vary.\n\nHowever, by varying the speed at which the image is captured, various effects can be created knowing that the faster or slower recorded image will be played at a constant speed. Giving the cinematographer even more freedom for creativity and expression to be made.\n\nFor instance, time-lapse photography is created by exposing an image at an extremely slow rate. If a cinematographer sets a camera to expose one frame every minute for four hours, and then that footage is projected at 24 frames per second, a four-hour event will take 10 seconds to present, and one can present the events of a whole day (24 hours) in just one minute.\n\nThe inverse of this, if an image is captured at speeds above that at which they will be presented, the effect is to greatly slow down (slow motion) the image. If a cinematographer shoots a person diving into a pool at 96 frames per second, and that image is played back at 24 frames per second, the presentation will take 4 times as long as the actual event. Extreme slow motion, capturing many thousands of frames per second can present things normally invisible to the human eye, such as bullets in flight and shockwaves travelling through media, a potentially powerful cinematographical technique.\n\nIn motion pictures, the manipulation of time and space is a considerable contributing factor to the narrative storytelling tools. Film editing plays a much stronger role in this manipulation, but frame rate selection in the photography of the original action is also a contributing factor to altering time. For example, Charlie Chaplin's \"Modern Times\" was shot at \"silent speed\" (18 fps) but projected at \"sound speed\" (24 fps), which makes the slapstick action appear even more frenetic.\n\nSpeed ramping, or simply \"ramping\", is a process whereby the capture frame rate of the camera changes over time. For example, if in the course of 10 seconds of capture, the capture frame rate is adjusted from 60 frames per second to 24 frames per second, when played back at the standard movie rate of 24 frames per second, a unique time-manipulation effect is achieved. For example, someone pushing a door open and walking out into the street would appear to start off in slow-motion, but in a few seconds later within the same shot, the person would appear to walk in \"realtime\" (normal speed). The opposite speed-ramping is done in \"The Matrix\" when Neo re-enters the Matrix for the first time to see the Oracle. As he comes out of the warehouse \"load-point\", the camera zooms into Neo at normal speed but as it gets closer to Neo's face, time seems to slow down, foreshadowing the manipulation of time itself within the Matrix later in the movie.\n\nG.A. Smith initiated the technique of reverse motion and also improved the quality of self-motivating images. This he did by repeating the action a second time while filming it with an inverted camera and then joining the tail of the second negative to that of the first. The first films using this were \"Tipsy, Topsy, Turvy\" and \"The Awkward Sign Painter\", the latter which showed a sign painter lettering a sign, and then the painting on the sign vanishing under the painter's brush. The earliest surviving example of this technique is Smith's \"The House That Jack Built\", made before September 1901. Here, a small boy is shown knocking down a castle just constructed by a little girl out of children's building blocks. A title then appears, saying \"Reversed\", and the action is repeated in reverse so that the castle re-erects itself under his blows.\n\nCecil Hepworth improved upon this technique by printing the negative of the forwards motion backwards frame by frame, so that in the production of the print the original action was exactly reversed. Hepworth made \"The Bathers\" in 1900, in which bathers who have undressed and jumped into the water appear to spring backwards out of it, and have their clothes magically fly back onto their bodies.\n\nThe use of different camera speeds also appeared around 1900. Robert Paul's \"On a Runaway Motor Car through Piccadilly Circus\" (1899), had the camera turn so slowly that when the film was projected at the usual 16 frames per second, the scenery appeared to be passing at great speed. Cecil Hepworth used the opposite effect in \"The Indian Chief and the Seidlitz powder\" (1901), in which a naïve Red Indian eats a lot of the fizzy stomach medicine, causing his stomach to expand and then he then leaps around balloon-like. This was done by cranking the camera faster than the normal 16 frames per second giving the first \"slow motion\" effect.\n\nIn descending order of seniority, the following staff is involved:\n\n\nIn the film industry, the cinematographer is responsible for the technical aspects of the images (lighting, lens choices, composition, exposure, filtration, film selection), but works closely with the director to ensure that the artistic aesthetics are supporting the director's vision of the story being told. The cinematographers are the heads of the camera, grip and lighting crew on a set, and for this reason, they are often called directors of photography or DPs. The ASC defines cinematography as a creative and interpretive process that culminates in the authorship of an original work of art rather than the simple recording of a physical event. Cinematography is not a subcategory of photography. Rather, photography is but one craft that the cinematographer uses in addition to other physical, organizational, managerial, interpretive. and image-manipulating techniques to effect one coherent process.\nIn British tradition, if the DOP actually operates the camera him/herself they are called the \"cinematographer\". On smaller productions, it is common for one person to perform all these functions alone. The career progression usually involves climbing up the ladder from seconding, firsting, eventually to operating the camera.\n\nDirectors of photography make many creative and interpretive decisions during the course of their work, from pre-production to post-production, all of which affect the overall feel and look of the motion picture. Many of these decisions are similar to what a photographer needs to note when taking a picture: the cinematographer controls the film choice itself (from a range of available stocks with varying sensitivities to light and color), the selection of lens focal lengths, aperture exposure and focus. Cinematography, however, has a temporal aspect (see persistence of vision), unlike still photography, which is purely a single still image. It is also bulkier and more strenuous to deal with movie cameras, and it involves a more complex array of choices. As such a cinematographer often needs to work co-operatively with more people than does a photographer, who could frequently function as a single person. As a result, the cinematographer's job also includes personnel management and logistical organization. Given the in-depth knowledge. a cinematographer requires not only of his or her own craft but also that of other personnel, formal tuition in analogue or digital filmmaking can be advantageous.\n\n\n"
            ],
            "good_message": false,
            "loggedData": [
                {
                    "curr_message": " Typically, a lens is used to focus the light reflected or emitted from objects into a real image on the light-sensitive surface inside a camera during a timed exposure.",
                    "last_key_pressed": "<<selected>>"
                }
            ],
            "offensive": null,
            "researchTask": true,
            "selectedData": [
                "Typically, a lens is used to focus the light reflected or emitted from objects into a real image on the light-sensitive surface inside a camera during a timed exposure."
            ],
            "shown_passages": [
                [
                    "Photography",
                    [
                        "Photography is the science, art, application and practice of creating durable images by recording light or other electromagnetic radiation, either electronically by means of an image sensor, or chemically by means of a light-sensitive material such as photographic film.",
                        "Typically, a lens is used to focus the light reflected or emitted from objects into a real image on the light-sensitive surface inside a camera during a timed exposure.",
                        "With an electronic image sensor, this produces an electrical charge at each pixel, which is electronically processed and stored in a digital image file for subsequent display or processing."
                    ]
                ],
                [
                    "Optics",
                    [
                        "Optics is the branch of physics which involves the behaviour and properties of light, including its interactions with matter and the construction of instruments that use or detect it.",
                        "Optics usually describes the behaviour of visible, ultraviolet, and infrared light.",
                        "Because light is an electromagnetic wave, other forms of electromagnetic radiation such as X-rays, microwaves, and radio waves exhibit similar properties.",
                        "Most optical phenomena can be accounted for using the classical electromagnetic description of light.",
                        "Complete electromagnetic descriptions of light are, however, often difficult to apply in practice."
                    ]
                ],
                [
                    "Cinematography",
                    [
                        "Cinematography (also called \"Direction of Photography\") is the science or art of motion-picture photography by recording light or other electromagnetic radiation, either electronically by means of an image sensor, or chemically by means of a light-sensitive material such as film stock.",
                        "Typically, a lens is used to repeatedly focus the light reflected from objects into real images on the light-sensitive surface inside a camera during a questioned exposure, creating multiple images.",
                        "With an electronic image sensor, this produces an electrical charge at each pixel, which is electronically processed and stored in a video file for subsequent display or processing."
                    ]
                ]
            ],
            "speaker": "0_Wizard",
            "suggestionsConfig": "show",
            "suggestionsNumber": 5,
            "suggestionsShown": true,
            "text": " Typically, a lens is used to focus the light reflected or emitted from objects into a real image on the light-sensitive surface inside a camera during a timed exposure.",
            "textboxFilled": 0,
            "time": 1580233321.0535243,
            "timeTaken": 10.258,
            "totalTasks": 120,
            "turn": 14
        },
        {
            "checked_passage": {
                "no_passages_used": "no_passages_used"
            },
            "checked_sentence": {
                "no_passages_used": "no_passages_used"
            },
            "clicked_passages": {},
            "context": {
                "conversation": [
                    "i really love animals, do u know about Armadillo?",
                    "I know a little bit. I saw a giant armadillo in a museum. Apparently they eat a lot of ants and termites! "
                ],
                "index": 1
            },
            "contextCount": 0,
            "full_passages": [
                "Giant armadillo\n\nThe giant armadillo (\"Priodontes maximus\"), colloquially tatou, ocarro, tatu-canastra or tatú carreta, is the largest living species of armadillo (although their extinct relatives, the glyptodonts, were much larger). It lives in South America, ranging throughout as far south as northern Argentina. This species is considered vulnerable to extinction.\n\nThe giant armadillo prefers termites and some ants as prey, and often consumes the entire population of a termite mound. It also has been known to prey upon worms, larvae and larger creatures, such as spiders and snakes, and plants.\n\nAt least one zoo park, in Villavicencio, Colombia – \"Los Ocarros\" – is dedicated to this animal.\n\nThe giant armadillo is the largest living species of armadillo, with 11 to 13 hinged bands protecting the body and a further three or four on the neck. Its body is dark brown in color, with a lighter, yellowish band running along the sides, and a pale, yellow-white head. These armadillos have around 80 to 100 teeth, which is more than any other terrestrial mammal. The teeth are all similar in appearance, being reduced premolars and molars, grow constantly throughout life, and lack enamel. They also possess extremely long front claws, including a sickle-shaped third claw, which are proportionately the largest of any living mammal. The tail is covered in small rounded scales and does not have the heavy bony scutes that cover the upper body and top of the head. The animal is almost entirely hairless, with just a few beige colored hairs protruding between the scutes.\n\nGiant armadillos typically weigh around when fully grown, however a specimen has been weighed in the wild and captive specimens have been weighed up to . The typical length of the species is , with the tail adding another .\n\nGiant armadillos are found throughout much of northern South America east of the Andes, except for eastern Brazil and Paraguay. In the south, they reach the northernmost provinces of Argentina, including Salta, Formosa, Chaco, and Santiago del Estero. There are no recognised geographic subspecies. They primarily inhabit open habitats, with cerrado grasslands covering about 25% of their range, but they can also be found in lowland forests.\n\nGiant armadillos are solitary and nocturnal, spending the day in burrows. They also burrow to escape predators, being unable to completely roll into a protective ball. Compared with those of other armadillos, their burrows are unusually large, with entrances averaging wide, and typically opening to the west.\n\nGiant armadillos use their large front claws to dig for prey and rip open termite mounds. The diet is mainly composed of termites, although ants, worms, spiders and other invertebrates are also eaten. Little is currently known about this species' reproductive biology, and no juveniles have ever been discovered in the field. The average sleep time of a captive giant armadillo is said to be 18.1 hours.\n\nArmadillos have not been extensively studied in the wild; therefore, little is known about their natural ecology and behavior. In the only long term study on the species, that started in 2003 in the Peruvian Amazon, dozens of other species of mammals, reptiles and birds were found using the giant armadillos' burrows on the same day, including the rare short-eared dog (\"Atelocynus microtis\"). Because of this, the species is considered a habitat engineer, and the local extinction of \"Priodontes\" may have cascading effects in the mammalian community by impoverishing fossorial habitat.\n\nFemale giant armadillos have two teats and are thought to normally give birth to only a single young per year. Little is known with certainty about their life history, although it is thought that the young are weaned by about seven to eight months of age, and that the mother periodically seals up the entrance to burrows containing younger offspring, presumably to protect them from predators. Although they have never bred in captivity, a wild-born giant armadillo at San Antonio Zoo was estimated to have been around sixteen years old when it died.\n\nHunted throughout its range, a single giant armadillo supplies a great deal of meat, and is the primary source of protein for some indigenous peoples. In addition, live giant armadillos are frequently captured for trade on the black market, and invariably die during transportation or in captivity. Despite this species’ wide range, it is locally rare. This is further exacerbated by habitat loss resulting from deforestation. Current estimates indicate the giant armadillo may have undergone a worrying population decline of 30 to 50 percent over the past three decades. Without intervention, this trend is likely to continue.\n\nThe giant armadillo was classified as vulnerable on the World Conservation Union's Red List in 2002, and is listed under Appendix I (threatened with extinction) of the Convention on the International Trade in Endangered Species of Wild Flora and Fauna.\n\nThe giant armadillo is protected by law in Colombia, Guyana, Brazil, Argentina, Paraguay, Suriname and Peru, and international trade is banned by its listing on Appendix I of the Convention on International Trade in Endangered Species (CITES). However, hunting for food and sale in the black market continues to occur throughout its entire range. Some populations occur within protected reserves, including the Parque das Emas in Brazil, and the Central Suriname Nature Reserve, a massive 1.6-million-hectare site of pristine rainforest managed by Conservation International. Such protection helps to some degree to mitigate the threat of habitat loss, but targeted conservation action is required to prevent the further decline of this species.\n\n",
                "Termite\n\nTermites are eusocial insects that are classified at the taxonomic rank of infraorder Isoptera, or as epifamily Termitoidae within the cockroach order Blattodea. Termites were once classified in a separate order from cockroaches, but recent phylogenetic studies indicate that they evolved from close ancestors of cockroaches during the Jurassic or Triassic. However, the first termites possibly emerged during the Permian or even the Carboniferous. About 3,106 species are currently described, with a few hundred more left to be described. Although these insects are often called \"white ants\", they are not ants.\n\nLike ants and some bees and wasps from the separate order Hymenoptera, termites divide labour among castes consisting of sterile male and female \"workers\" and \"soldiers\". All colonies have fertile males called \"kings\" and one or more fertile females called \"queens\". Termites mostly feed on dead plant material and cellulose, generally in the form of wood, leaf litter, soil, or animal dung. Termites are major detritivores, particularly in the subtropical and tropical regions, and their recycling of wood and plant matter is of considerable ecological importance.\n\nTermites are among the most successful groups of insects on Earth, colonising most landmasses except for Antarctica. Their colonies range in size from a few hundred individuals to enormous societies with several million individuals. Termite queens have the longest lifespan of any insect in the world, with some queens reportedly living up to 30 to 50 years. Unlike ants, which undergo a complete metamorphosis, each individual termite goes through an incomplete metamorphosis that proceeds through egg, nymph, and adult stages. Colonies are described as superorganisms because the termites form part of a self-regulating entity: the colony itself.\n\nTermites are a delicacy in the diet of some human cultures and are used in many traditional medicines. Several hundred species are economically significant as pests that can cause serious damage to buildings, crops, or plantation forests. Some species, such as the West Indian drywood termite (\"Cryptotermes brevis\"), are regarded as invasive species.\n\nThe infraorder name Isoptera is derived from the Greek words \"iso\" (equal) and \"ptera\" (winged), which refers to the nearly equal size of the fore and hind wings. \"Termite\" derives from the Latin and Late Latin word \"termes\" (\"woodworm, white ant\"), altered by the influence of Latin \"terere\" (\"to rub, wear, erode\") from the earlier word \"tarmes\". Termite nests were commonly known as \"terminarium\" or \"termitaria\". In early English, termites were known as \"wood ants\" or \"white ants\". The modern term was first used in 1781.\n\nDNA analysis from 16S rRNA sequences has supported a hypothesis, originally suggested by Cleveland and colleagues in 1934, that these insects are most closely related to wood-eating cockroaches (genus \"Cryptocercus\", the woodroach). This earlier conclusion had been based on the similarity of the symbiotic gut flagellates in the wood-eating cockroaches to those in certain species of termites regarded as living fossils. In the 1960s additional evidence supporting that hypothesis emerged when F. A. McKittrick noted similar morphological characteristics between some termites and \"Cryptocercus\" nymphs. These similarities have led some authors to propose that termites be reclassified as a single family, the Termitidae, within the order Blattodea, which contains cockroaches. Other researchers advocate the more conservative measure of retaining the termites as the Termitoidae, an epifamily within the cockroach order, which preserves the classification of termites at family level and below.\n\nThe oldest unambiguous termite fossils date to the early Cretaceous, but given the diversity of Cretaceous termites and early fossil records showing mutualism between microorganisms and these insects, they likely originated earlier in the Jurassic or Triassic. Further evidence of a Jurassic origin is the assumption that the extinct \"Fruitafossor\" consumed termites, judging from its morphological similarity to modern termite-eating mammals. The oldest termite nest discovered is believed to be from the Upper Cretaceous in West Texas, where the oldest known faecal pellets were also discovered.\n\nClaims that termites emerged earlier have faced controversy. For example, F. M. Weesner indicated that the Mastotermitidae termites may go back to the Late Permian, 251 million years ago, and fossil wings that have a close resemblance to the wings of \"Mastotermes\" of the Mastotermitidae, the most primitive living termite, have been discovered in the Permian layers in Kansas. It is even possible that the first termites emerged during the Carboniferous. Termites are thought to be the descendants of the genus \"Cryptocercus\". The folded wings of the fossil wood roach \"Pycnoblattina\", arranged in a convex pattern between segments 1a and 2a, resemble those seen in \"Mastotermes\", the only living insect with the same pattern. Krishna \"et al.\", though, consider that all of the Paleozoic and Triassic insects tentatively classified as termites are in fact unrelated to termites and should be excluded from the Isoptera. Termites were the first social insects to evolve a caste system, evolving more than 100 million years ago.\n\nTermites have long been accepted to be closely related to cockroaches and mantids, and they are classified in the same superorder (Dictyoptera). Strong evidence suggests termites are highly specialised wood-eating cockroaches. The cockroach genus \"Cryptocercus\" shares the strongest phylogenetical similarity with termites and is considered to be a sister-group to termites. Termites and \"Cryptocercus\" share similar morphological and social features: for example, most cockroaches do not exhibit social characteristics, but \"Cryptocercus\" takes care of its young and exhibits other social behaviour such as trophallaxis and allogrooming. The primitive giant northern termite (\"Mastotermes darwiniensis\") exhibits numerous cockroach-like characteristics that are not shared with other termites, such as laying its eggs in rafts and having anal lobes on the wings. Cryptocercidae and Isoptera are united in the clade Xylophagodea. Although termites are sometimes called \"white ants\", they are actually not ants. Ants belong to the family Formicidae within the order Hymenoptera. The similarity of their social structure to that of termites is attributed to convergent evolution.\nAs of 2013, about 3,106 living and fossil termite species are recognised, classified in 12 families. The infraorder Isoptera is divided into the following clade and family groups, showing the subfamilies in their respective classification:\n\nOrder Blattaria\n\nTermites are found on all continents except Antarctica. The diversity of termite species is low in North America and Europe (10 species known in Europe and 50 in North America), but is high in South America, where over 400 species are known. Of the 3,000 termite species currently classified, 1,000 are found in Africa, where mounds are extremely abundant in certain regions. Approximately 1.1 million active termite mounds can be found in the northern Kruger National Park alone. In Asia, there are 435 species of termites, which are mainly distributed in China. Within China, termite species are restricted to mild tropical and subtropical habitats south of the Yangtze River. In Australia, all ecological groups of termites (dampwood, drywood, subterranean) are endemic to the country, with over 360 classified species.\n\nDue to their soft cuticles, termites do not inhabit cool or cold habitats. There are three ecological groups of termites: dampwood, drywood and subterranean. Dampwood termites are found only in coniferous forests, and drywood termites are found in hardwood forests; subterranean termites live in widely diverse areas. One species in the drywood group is the West Indian drywood termite \"(Cryptotermes brevis)\", which is an invasive species in Australia.\n\nTermites are usually small, measuring between in length. The largest of all extant termites are the queens of the species \"Macrotermes bellicosus\", measuring up to over 10 centimetres (4 in) in length. Another giant termite, the extinct \"Gyatermes styriensis\", flourished in Austria during the Miocene and had a wingspan of and a body length of .\n\nMost worker and soldier termites are completely blind as they do not have a pair of eyes. However, some species, such as \"Hodotermes mossambicus\", have compound eyes which they use for orientation and to distinguish sunlight from moonlight. The alates have eyes along with lateral ocelli. Lateral ocelli, however, are not found in all termites. Like other insects, termites have a small tongue-shaped labrum and a clypeus; the clypeus is divided into a postclypeus and anteclypeus. Termite antennae have a number of functions such as the sensing of touch, taste, odours (including pheromones), heat and vibration. The three basic segments of a termite antenna include a scape, a pedicel (typically shorter than the scape), and the flagellum (all segments beyond the scape and pedicel). The mouth parts contain a maxillae, a labium, and a set of mandibles. The maxillae and labium have palps that help termites sense food and handling.\n\nConsistent with all insects, the anatomy of the termite thorax consists of three segments: the prothorax, the mesothorax and the metathorax. Each segment contains a pair of legs. On alates, the wings are located at the mesothorax and metathorax. The mesothorax and metathorax have well-developed exoskeletal plates; the prothorax has smaller plates.\nTermites have a ten-segmented abdomen with two plates, the tergites and the sternites. The tenth abdominal segment has a pair of short cerci. There are ten tergites, of which nine are wide and one is elongated. The reproductive organs are similar to those in cockroaches but are more simplified. For example, the intromittent organ is not present in male alates, and the sperm is either immotile or aflagellate. However, Mastotermitidae termites have multiflagellate sperm with limited motility. The genitals in females are also simplified. Unlike in other termites, Mastotermitidae females have an ovipositor, a feature strikingly similar to that in female cockroaches.\n\nThe non-reproductive castes of termites are wingless and rely exclusively on their six legs for locomotion. The alates fly only for a brief amount of time, so they also rely on their legs. The appearance of the legs is similar in each caste, but the soldiers have larger and heavier legs. The structure of the legs is consistent with other insects: the parts of a leg include a coxa, trochanter, femur, tibia and the tarsus. The number of tibial spurs on an individual's leg varies. Some species of termite have an arolium, located between the claws, which is present in species that climb on smooth surfaces but is absent in most termites.\n\nUnlike in ants, the hind-wings and fore-wings are of equal length. Most of the time, the alates are poor flyers; their technique is to launch themselves in the air and fly in a random direction. Studies show that in comparison to larger termites, smaller termites cannot fly long distances. When a termite is in flight, its wings remain at a right angle, and when the termite is at rest, its wings remain parallel to the body.\n\nWorker termites undertake the most labour within the colony, being responsible for foraging, food storage, and brood and nest maintenance. Workers are tasked with the digestion of cellulose in food and are thus the most likely caste to be found in infested wood. The process of worker termites feeding other nestmates is known as trophallaxis. Trophallaxis is an effective nutritional tactic to convert and recycle nitrogenous components. It frees the parents from feeding all but the first generation of offspring, allowing for the group to grow much larger and ensuring that the necessary gut symbionts are transferred from one generation to another. Some termite species do not have a true worker caste, instead relying on nymphs that perform the same work without differentiating as a separate caste.\n\nThe soldier caste has anatomical and behavioural specialisations, and their sole purpose is to defend the colony. Many soldiers have large heads with highly modified powerful jaws so enlarged they cannot feed themselves. Instead, like juveniles, they are fed by workers. Fontanelles, simple holes in the forehead that exude defensive secretions, are a feature of the family Rhinotermitidae. Many species are readily identified using the characteristics of the soldiers' larger and darker head and large mandibles. Among certain termites, soldiers may use their globular (phragmotic) heads to block their narrow tunnels. Different sorts of soldiers include minor and major soldiers, and nasutes, which have a horn-like nozzle frontal projection (a nasus). These unique soldiers are able to spray noxious, sticky secretions containing diterpenes at their enemies. Nitrogen fixation plays an important role in nasute nutrition.\n\nThe reproductive caste of a mature colony includes a fertile female and male, known as the queen and king. The queen of the colony is responsible for egg production for the colony. Unlike in ants, the king mates with her for life. In some species, the abdomen of the queen swells up dramatically to increase fecundity, a characteristic known as physogastrism. Depending on the species, the queen starts producing reproductive winged alates at a certain time of the year, and huge swarms emerge from the colony when nuptial flight begins. These swarms attract a wide variety of predators.\n\nTermites are often compared with the social Hymenoptera (ants and various species of bees and wasps), but their differing evolutionary origins result in major differences in life cycle. In the eusocial Hymenoptera, the workers are exclusively female, males (drones) are haploid and develop from unfertilised eggs, while females (both workers and the queen) are diploid and develop from fertilised eggs. In contrast, worker termites, which constitute the majority in a colony, are diploid individuals of both sexes and develop from fertilised eggs. Depending on species, male and female workers may have different roles in a termite colony.\n\nThe life cycle of a termite begins with an egg, but is different from that of a bee or ant in that it goes through a developmental process called incomplete metamorphosis, with egg, nymph and adult stages. Nymphs resemble small adults, and go through a series of moults as they grow. In some species, eggs go through four moulting stages and nymphs go through three. Nymphs first moult into workers, and then some workers go through further moulting and become soldiers or alates; workers become alates only by moulting into alate nymphs.\n\nThe development of nymphs into adults can take months; the time period depends on food availability, temperature, and the general population of the colony. Since nymphs are unable to feed themselves, workers must feed them, but workers also take part in the social life of the colony and have certain other tasks to accomplish such as foraging, building or maintaining the nest or tending to the queen. Pheromones regulate the caste system in termite colonies, preventing all but a very few of the termites from becoming fertile queens.\n\nTermite alates only leave the colony when a nuptial flight takes place. Alate males and females pair up together and then land in search of a suitable place for a colony. A termite king and queen do not mate until they find such a spot. When they do, they excavate a chamber big enough for both, close up the entrance and proceed to mate. After mating, the pair never go outside and spend the rest of their lives in the nest. Nuptial flight time varies in each species. For example, alates in certain species emerge during the day in summer while others emerge during the winter. The nuptial flight may also begin at dusk, when the alates swarm around areas with lots of lights. The time when nuptial flight begins depends on the environmental conditions, the time of day, moisture, wind speed and precipitation. The number of termites in a colony also varies, with the larger species typically having 100–1,000 individuals. However, some termite colonies, including those with large individuals, can number in the millions.\n\nThe queen only lays 10–20 eggs in the very early stages of the colony, but lays as many as 1,000 a day when the colony is several years old. At maturity, a primary queen has a great capacity to lay eggs. In some species, the mature queen has a greatly distended abdomen and may produce 40,000 eggs a day. The two mature ovaries may have some 2,000 ovarioles each. The abdomen increases the queen's body length to several times more than before mating and reduces her ability to move freely; attendant workers provide assistance.\n\nThe king grows only slightly larger after initial mating and continues to mate with the queen for life (a termite queen can live between 30  to 50 years); this is very different from ant colonies, in which a queen mates once with the male(s) and stores the gametes for life, as the male ants die shortly after mating. If a queen is absent, a termite king produces pheromones which encourage the development of replacement termite queens. As the queen and king are monogamous, sperm competition does not occur.\n\nTermites going through incomplete metamorphosis on the path to becoming alates form a subcaste in certain species of termite, functioning as potential supplementary reproductives. These supplementary reproductives only mature into primary reproductives upon the death of a king or queen, or when the primary reproductives are separated from the colony. Supplementaries have the ability to replace a dead primary reproductive, and there may also be more than a single supplementary within a colony. Some queens have the ability to switch from sexual reproduction to asexual reproduction. Studies show that while termite queens mate with the king to produce colony workers, the queens reproduce their replacements (neotenic queens) parthenogenetically.\n\nTermites are detritivores, consuming dead plants at any level of decomposition. They also play a vital role in the ecosystem by recycling waste material such as dead wood, faeces and plants. Many species eat cellulose, having a specialised midgut that breaks down the fibre. Termites are considered to be a major source (11%) of atmospheric methane, one of the prime greenhouse gases, produced from the breakdown of cellulose. Termites rely primarily upon symbiotic protozoa (metamonads) and other microbes such as flagellate protists in their guts to digest the cellulose for them, allowing them to absorb the end products for their own use. Gut protozoa, such as \"Trichonympha\", in turn, rely on symbiotic bacteria embedded on their surfaces to produce some of the necessary digestive enzymes. Most higher termites, especially in the family Termitidae, can produce their own cellulase enzymes, but they rely primarily upon the bacteria. The flagellates have been lost in Termitidae. Scientists' understanding of the relationship between the termite digestive tract and the microbial endosymbionts is still rudimentary; what is true in all termite species, however, is that the workers feed the other members of the colony with substances derived from the digestion of plant material, either from the mouth or anus. Judging from closely related bacterial species, it is strongly presumed that the termites' and cockroach's gut microbiota derives from their dictyopteran ancestors.\n\nCertain species such as \"Gnathamitermes tubiformans\" have seasonal food habits. For example, they may preferentially consume Red three-awn (\"Aristida longiseta\") during the summer, Buffalograss (\"Buchloe dactyloides\") from May to August, and blue grama \"Bouteloua gracilis\" during spring, summer and autumn. Colonies of \"G. tubiformans\" consume less food in spring than they do during autumn when their feeding activity is high.\n\nVarious woods differ in their susceptibility to termite attack; the differences are attributed to such factors as moisture content, hardness, and resin and lignin content. In one study, the drywood termite \"Cryptotermes brevis\" strongly preferred poplar and maple woods to other woods that were generally rejected by the termite colony. These preferences may in part have represented conditioned or learned behaviour.\n\nSome species of termite practice fungiculture. They maintain a \"garden\" of specialised fungi of genus \"Termitomyces\", which are nourished by the excrement of the insects. When the fungi are eaten, their spores pass undamaged through the intestines of the termites to complete the cycle by germinating in the fresh faecal pellets. Molecular evidence suggests that the family Macrotermitinae developed agriculture about 31 million years ago. It is assumed that more than 90 percent of dry wood in the semiarid savannah ecosystems of Africa and Asia are reprocessed by these termites. Originally living in the rainforest, fungus farming allowed them to colonise the African savannah and other new environments, eventually expanding into Asia.\n\nDepending on their feeding habits, termites are placed into two groups: the lower termites and higher termites. The lower termites predominately feed on wood. As wood is difficult to digest, termites prefer to consume fungus-infected wood because it is easier to digest and the fungi are high in protein. Meanwhile, the higher termites consume a wide variety of materials, including faeces, humus, grass, leaves and roots. The gut in the lower termites contains many species of bacteria along with protozoa, while the higher termites only have a few species of bacteria with no protozoa.\n\nTermites are consumed by a wide variety of predators. One termite species alone, \"Hodotermes mossambicus\", was found in the stomach contents of 65 birds and 19 mammals. Ants, arthropods, reptiles, and amphibians such as bees, centipedes, cockroaches, crickets, dragonflies, frogs, lizards, scorpions, spiders, and toads consume these insects, while 2 spiders in the family Ammoxenidae are specialist termite predators. Other predators include aardvarks, aardwolves, anteaters, bats, bears, bilbies, many birds, echidnas, foxes, galagos, numbats, mice and pangolins. The aardwolf is an insectivorous mammal that primarily feeds on termites; it locates its food by sound and also by detecting the scent secreted by the soldiers; a single aardwolf is capable of consuming thousands of termites in a single night by using its long, sticky tongue. Sloth bears break open mounds to consume the nestmates, while chimpanzees have developed tools to \"fish\" termites from their nest. Wear pattern analysis of bone tools used by the early hominin \"Paranthropus robustus\" suggests that they used these tools to dig into termite mounds.\nAmong all predators, ants are the greatest enemy to termites. Some ant genera are specialist predators of termites. For example, \"Megaponera\" is a strictly termite-eating (termitophagous) genus that perform raiding activities, some lasting several hours. \"Paltothyreus tarsatus\" is another termite-raiding species, with each individual stacking as many termites as possible in its mandibles before returning home, all the while recruiting additional nestmates to the raiding site through chemical trails. The Malaysian basicerotine ants \"Eurhopalothrix heliscata\" uses a different strategy of termite hunting by pressing themselves into tight spaces, as they hunt through rotting wood housing termite colonies. Once inside, the ants seize their prey by using their short but sharp mandibles. \"Tetramorium uelense\" is a specialised predator species that feeds on small termites. A scout recruits 10–30 workers to an area where termites are present, killing them by immobilising them with their stinger. \"Centromyrmex\" and \"Iridomyrmex\" colonies sometimes nest in termite mounds, and so the termites are preyed on by these ants. No evidence for any kind of relationship (other than a predatory one) is known. Other ants, including \"Acanthostichus\", \"Camponotus\", \"Crematogaster\", \"Cylindromyrmex\", \"Leptogenys\", \"Odontomachus\", \"Ophthalmopone\", \"Pachycondyla\", \"Rhytidoponera\", \"Solenopsis\" and \"Wasmannia\", also prey on termites. In contrast to all these ant species, and despite their enormous diversity of prey, \"Dorylus ants\" rarely consume termites.\n\nAnts are not the only invertebrates that perform raids. Many sphecoid wasps and several species including \"Polybia Lepeletier\" and \"Angiopolybia Araujo\" are known to raid termite mounds during the termites' nuptial flight.\n\nTermites are less likely to be attacked by parasites than bees, wasps and ants, as they are usually well protected in their mounds. Nevertheless, termites are infected by a variety of parasites. Some of these include dipteran flies, \"Pyemotes\" mites, and a large number of nematode parasites. Most nematode parasites are in the order Rhabditida; others are in the genus \"Mermis\", \"Diplogaster aerivora\" and \"Harteria gallinarum\". Under imminent threat of an attack by parasites, a colony may migrate to a new location. Fungi pathogens such as \"Aspergillus nomius\" and \"Metarhizium anisopliae\" are, however, major threats to a termite colony as they are not host-specific and may infect large portions of the colony; transmission usually occurs via direct physical contact. \"M. anispliae\" is known to weaken the termite immune system. Infection with \"A. nomius\" only occurs when a colony is under great stress.\n\nTermites are infected by viruses including Entomopoxvirinae and the Nuclear Polyhedrosis Virus.\n\nBecause the worker and soldier castes lack wings and thus never fly, and the reproductives use their wings for just a brief amount of time, termites predominantly rely upon their legs to move about.\n\nForaging behaviour depends on the type of termite. For example, certain species feed on the wood structures they inhabit, and others harvest food that is near the nest. Most workers are rarely found out in the open, and do not forage unprotected; they rely on sheeting and runways to protect them from predators. Subterranean termites construct tunnels and galleries to look for food, and workers who manage to find food sources recruit additional nestmates by depositing a phagostimulant pheromone that attracts workers. Foraging workers use semiochemicals to communicate with each other, and workers who begin to forage outside of their nest release trail pheromones from their sternal glands. In one species, \"Nasutitermes costalis\", there are three phases in a foraging expedition: first, soldiers scout an area. When they find a food source, they communicate to other soldiers and a small force of workers starts to emerge. In the second phase, workers appear in large numbers at the site. The third phase is marked by a decrease in the number of soldiers present and an increase in the number of workers. Isolated termite workers may engage in Lévy flight behaviour as an optimised strategy for finding their nestmates or foraging for food.\n\nCompetition between two colonies always results in agonistic behaviour towards each other, resulting in fights. These fights can cause mortality on both sides and, in some cases, the gain or loss of territory. \"Cemetery pits\" may be present, where the bodies of dead termites are buried.\n\nStudies show that when termites encounter each other in foraging areas, some of the termites deliberately block passages to prevent other termites from entering. Dead termites from other colonies found in exploratory tunnels leads to the isolation of the area and thus the need to construct new tunnels. Conflict between two competitors does not always occur. For example, though they might block each other's passages, colonies of \"Macrotermes bellicosus\" and \"Macrotermes subhyalinus\" are not always aggressive towards each other. Suicide cramming is known in \"Coptotermes formosanus\". Since \"C. formosanus\" colonies may get into physical conflict, some termites squeeze tightly into foraging tunnels and die, successfully blocking the tunnel and ending all agonistic activities.\n\nAmong the reproductive caste, neotenic queens may compete with each other to become the dominant queen when there are no primary reproductives. This struggle among the queens leads to the elimination of all but a single queen, which, with the king, takes over the colony.\n\nAnts and termites may compete with each other for nesting space. In particular, ants that prey on termites usually have a negative impact on arboreal nesting species.\n\nMost termites are blind, so communication primarily occurs through chemical, mechanical and pheromonal cues. These methods of communication are used in a variety of activities, including foraging, locating reproductives, construction of nests, recognition of nestmates, nuptial flight, locating and fighting enemies, and defending the nests. The most common way of communicating is through antennation. A number of pheromones are known, including contact pheromones (which are transmitted when workers are engaged in trophallaxis or grooming) and alarm, trail and sex pheromones. The alarm pheromone and other defensive chemicals are secreted from the frontal gland. Trail pheromones are secreted from the sternal gland, and sex pheromones derive from two glandular sources: the sternal and tergal glands. When termites go out to look for food, they forage in columns along the ground through vegetation. A trail can be identified by the faecal deposits or runways that are covered by objects. Workers leave pheromones on these trails, which are detected by other nestmates through olfactory receptors. Termites can also communicate through mechanical cues, vibrations, and physical contact. These signals are frequently used for alarm communication or for evaluating a food source.\n\nWhen termites construct their nests, they use predominantly indirect communication. No single termite would be in charge of any particular construction project. Individual termites react rather than think, but at a group level, they exhibit a sort of collective cognition. Specific structures or other objects such as pellets of soil or pillars cause termites to start building. The termite adds these objects onto existing structures, and such behaviour encourages building behaviour in other workers. The result is a self-organised process whereby the information that directs termite activity results from changes in the environment rather than from direct contact among individuals.\n\nTermites can distinguish nestmates and non-nestmates through chemical communication and gut symbionts: chemicals consisting of hydrocarbons released from the cuticle allow the recognition of alien termite species. Each colony has its own distinct odour. This odour is a result of genetic and environmental factors such as the termites' diet and the composition of the bacteria within the termites' intestines.\n\nTermites rely on alarm communication to defend a colony. Alarm pheromones can be released when the nest has been breached or is being attacked by enemies or potential pathogens. Termites always avoid nestmates infected with \"Metarhizium anisopliae\" spores, through vibrational signals released by infected nestmates. Other methods of defence include intense jerking and secretion of fluids from the frontal gland and defecating faeces containing alarm pheromones.\n\nIn some species, some soldiers block tunnels to prevent their enemies from entering the nest, and they may deliberately rupture themselves as an act of defence. In cases where the intrusion is coming from a breach that is larger than the soldier's head, defence requires a special formations where soldiers form a phalanx-like formation around the breach and bite at intruders. If an invasion carried out by \"Megaponera analis\" is successful, an entire colony may be destroyed, although this scenario is rare.\n\nTo termites, any breach of their tunnels or nests is a cause for alarm. When termites detect a potential breach, the soldiers usually bang their heads, apparently to attract other soldiers for defence and to recruit additional workers to repair any breach. Additionally, an alarmed termite bumps into other termites which causes them to be alarmed and to leave pheromone trails to the disturbed area, which is also a way to recruit extra workers.\nThe pantropical subfamily Nasutitermitinae has a specialised caste of soldiers, known as nasutes, that have the ability to exude noxious liquids through a horn-like frontal projection that they use for defence. Nasutes have lost their mandibles through the course of evolution and must be fed by workers. A wide variety of monoterpene hydrocarbon solvents have been identified in the liquids that nasutes secrete.\n\nSoldiers of the species \"Globitermes sulphureus\" commit suicide by autothysis – rupturing a large gland just beneath the surface of their cuticles. The thick, yellow fluid in the gland becomes very sticky on contact with the air, entangling ants or other insects which are trying to invade the nest. Another termite, \"Neocapriterme taracua\", also engages in suicidal defence. Workers physically unable to use their mandibles while in a fight form a pouch full of chemicals, then deliberately rupture themselves, releasing toxic chemicals that paralyse and kill their enemies. The soldiers of the neotropical termite family Serritermitidae have a defence strategy which involves front gland autothysis, with the body rupturing between the head and abdomen. When soldiers guarding nest entrances are attacked by intruders, they engage in autothysis, creating a block that denies entry to any attacker.\n\nWorkers use several different strategies to deal with their dead, including burying, cannibalism, and avoiding a corpse altogether. To avoid pathogens, termites occasionally engage in necrophoresis, in which a nestmate carries away a corpse from the colony to dispose of it elsewhere. Which strategy is used depends on the nature of the corpse a worker is dealing with (i.e. the age of the carcass).\n\nA species of fungus is known to mimic termite eggs, successfully avoiding its natural predators. These small brown balls, known as \"termite balls\", rarely kill the eggs, and in some cases the workers tend to them. This fungus mimics these eggs by producing a cellulose-digesting enzyme known as glucosidases. A unique mimicking behaviour exists between various species of \"Trichopsenius\" beetles and certain termite species within \"Reticulitermes\". The beetles share the same cuticle hydrocarbons as the termites and even biosynthesize them. This chemical mimicry allows the beetles to integrate themselves within the termite colonies. The developed appendages on the physogastric abdomen of \"Austrospirachtha mimetes\" allows the beetle to mimic a termite worker.\n\nSome species of ant are known to capture termites to use as a fresh food source later on, rather than killing them. For example, \"Formica nigra\" captures termites, and those who try to escape are immediately seized and driven underground. Certain species of ants in the subfamily Ponerinae conduct these raids although other ant species go in alone to steal the eggs or nymphs. Ants such as \"Megaponera analis\" attack the outside the mounds and Dorylinae ants attack underground. Despite this, some termites and ants can coexist peacefully. Some species of termite, including \"Nasutitermes corniger\", form associations with certain ant species to keep away predatory ant species. The earliest known association between \"Azteca\" ants and \"Nasutitermes\" termites date back to the Oligocene to Miocene period.\n54 species of ants are known to inhabit \"Nasutitermes\" mounds, both occupied and abandoned ones. One reason many ants live in \"Nasutitermes\" mounds is due to the termites' frequent occurrence in their geographical range; another is to protect themselves from floods. \"Iridomyrmex\" also inhabits termite mounds although no evidence for any kind of relationship (other than a predatory one) is known. In rare cases, certain species of termites live inside active ant colonies. Some invertebrate organisms such as beetles, caterpillars, flies and millipedes are termitophiles and dwell inside termite colonies (they are unable to survive independently). As a result, certain beetles and flies have evolved with their hosts. They have developed a gland that secrete a substance that attracts the workers by licking them. Mounds may also provide shelter and warmth to birds, lizards, snakes and scorpions.\n\nTermites are known to carry pollen and regularly visit flowers, so are regarded as potential pollinators for a number of flowering plants. One flower in particular, \"Rhizanthella gardneri\", is regularly pollinated by foraging workers, and it is perhaps the only Orchidaceae flower in the world to be pollinated by termites.\n\nMany plants have developed effective defences against termites. However, seedlings are vulnerable to termite attacks and need additional protection, as their defence mechanisms only develop when they have passed the seedling stage. Defence is typically achieved by secreting antifeedant chemicals into the woody cell walls. This reduces the ability of termites to efficiently digest the cellulose. A commercial product, \"Blockaid\", has been developed in Australia that uses a range of plant extracts to create a paint-on nontoxic termite barrier for buildings. An extract of a species of Australian figwort, \"Eremophila\", has been shown to repel termites; tests have shown that termites are strongly repelled by the toxic material to the extent that they will starve rather than consume the food. When kept close to the extract, they become disoriented and eventually die.\n\nTermite populations can be substantially impacted by environmental changes including those caused by human intervention. A Brazilian study investigated the termite assemblages of three sites of Caatinga under different levels of anthropogenic disturbance in the semi-arid region of northeastern Brazil were sampled using 65 x 2 m transects. A total of 26 species of termites were present in the three sites, and 196 encounters were recorded in the transects. The termite assemblages were considerably different among sites, with a conspicuous reduction in both diversity and abundance with increased disturbance, related to the reduction of tree density and soil cover, and with the intensity of trampling by cattle and goats. The wood-feeders were the most severely affected feeding group.\n\nA termite nest can be considered as being composed of two parts, the inanimate and the animate. The animate is all of the termites living inside the colony, and the inanimate part is the structure itself, which is constructed by the termites. Nests can be broadly separated into three main categories: subterranean (completely below ground), epigeal (protruding above the soil surface), and arboreal (built above ground, but always connected to the ground via shelter tubes). Epigeal nests (mounds) protrude from the earth with ground contact and are made out of earth and mud. A nest has many functions such as providing a protected living space and providing shelter against predators. Most termites construct underground colonies rather than multifunctional nests and mounds. Primitive termites of today nest in wooden structures such as logs, stumps and the dead parts of trees, as did termites millions of years ago.\n\nTo build their nests, termites primarily use faeces, which have many desirable properties as a construction material. Other building materials include partly digested plant material, used in carton nests (arboreal nests built from faecal elements and wood), and soil, used in subterranean nest and mound construction. Not all nests are visible, as many nests in tropical forests are located underground. Species in the subfamily Apicotermitinae are good examples of subterranean nest builders, as they only dwell inside tunnels. Other termites live in wood, and tunnels are constructed as they feed on the wood. Nests and mounds protect the termites' soft bodies against desiccation, light, pathogens and parasites, as well as providing a fortification against predators. Nests made out of carton are particularly weak, and so the inhabitants use counter-attack strategies against invading predators.\n\nArboreal carton nests of mangrove swamp-dwelling \"Nasutitermes\" are enriched in lignin and depleted in cellulose and xylans. This change is caused by bacterial decay in the gut of the termites: they use their faeces as a carton building material. Arboreal termites nests can account for as much as 2% of above ground carbon storage in Puerto Rican mangrove swamps. These \"Nasutitermes\" nests are mainly composed of partially biodegraded wood material from the stems and branches of mangrove trees, namely, \"Rhizophora mangle\" (red mangrove), \"Avicennia germinans\" (black mangrove) and \"Laguncularia racemose\" (white mangrove).\n\nSome species build complex nests called polycalic nests; this habitat is called polycalism. Polycalic species of termites form multiple nests, or calies, connected by subterranean chambers. The termite genera \"Apicotermes\" and \"Trinervitermes\" are known to have polycalic species. Polycalic nests appear to be less frequent in mound-building species although polycalic arboreal nests have been observed in a few species of \"Nasutitermes\".\n\nNests are considered mounds if they protrude from the earth's surface. A mound provides termites the same protection as a nest but is stronger. Mounds located in areas with torrential and continuous rainfall are at risk of mound erosion due to their clay-rich construction. Those made from carton can provide protection from the rain, and in fact can withstand high precipitation. Certain areas in mounds are used as strong points in case of a breach. For example, \"Cubitermes\" colonies build narrow tunnels used as strong points, as the diameter of the tunnels is small enough for soldiers to block. A highly protected chamber, known as the \"queens cell\", houses the queen and king and is used as a last line of defence.\n\nSpecies in the genus \"Macrotermes\" arguably build the most complex structures in the insect world, constructing enormous mounds. These mounds are among the largest in the world, reaching a height of 8 to 9 metres (26 to 29 feet), and consist of chimneys, pinnacles and ridges. Another termite species, \"Amitermes meridionalis\", can build nests 3 to 4 metres (9 to 13 feet) high and 2.5 metres (8 feet) wide. The tallest mound ever recorded was 12.8 metres (42 ft) long found in the Democratic Republic of the Congo.\n\nThe sculptured mounds sometimes have elaborate and distinctive forms, such as those of the compass termite (\"Amitermes meridionalis\" and \"A. laurensis\"), which builds tall, wedge-shaped mounds with the long axis oriented approximately north–south, which gives them their common name. This orientation has been experimentally shown to assist thermoregulation. The north-south orientation causes the internal temperature of a mound to increase rapidly during the morning while avoiding overheating from the midday sun. The temperature then remains at a plateau for the rest of the day until the evening.\n\nTermites construct shelter tubes, also known as earthen tubes or mud tubes, that start from the ground. These shelter tubes can be found on walls and other structures. Constructed by termites during the night, a time of higher humidity, these tubes provide protection to termites from potential predators, especially ants. Shelter tubes also provide high humidity and darkness and allow workers to collect food sources that cannot be accessed in any other way. These passageways are made from soil and faeces and are normally brown in colour. The size of these shelter tubes depends on the amount of food sources that are available. They range from less than 1 cm to several cm in width, but may extend dozens of metres in length.\n\nOwing to their wood-eating habits, many termite species can do great damage to unprotected buildings and other wooden structures. Their habit of remaining concealed often results in their presence being undetected until the timbers are severely damaged, leaving a thin layer of a wall that protects them from the environment. Of the 3,106 species known, only 183 species cause damage; 83 species cause significant damage to wooden structures. In North America, nine subterranean species are pests; in Australia, 16 species have an economic impact; in the Indian subcontinent 26 species are considered pests, and in tropical Africa, 24. In Central America and the West Indies, there are 17 pest species. Among the termite genera, \"Coptotermes\" has the highest number of pest species of any genus, with 28 species known to cause damage. Less than 10% of drywood termites are pests, but they infect wooden structures and furniture in tropical, subtropical and other regions. Dampwood termites only attack lumber material exposed to rainfall or soil.\n\nDrywood termites thrive in warm climates, and human activities can enable them to invade homes since they can be transported through contaminated goods, containers and ships. Colonies of termites have been seen thriving in warm buildings located in cold regions. Some termites are considered invasive species. \"Cryptotermes brevis\", the most widely introduced invasive termite species in the world, has been introduced to all the islands in the West Indies and to Australia.\nIn addition to causing damage to buildings, termites can also damage food crops. Termites may attack trees whose resistance to damage is low but generally ignore fast-growing plants. Most attacks occur at harvest time; crops and trees are attacked during the dry season.\n\nThe damage caused by termites costs the southwestern United States approximately $1.5 billion each year in wood structure damage, but the true cost of damage worldwide cannot be determined. Drywood termites are responsible for a large proportion of the damage caused by termites.\n\nTo better control the population of termites, various methods have been developed to track termite movements. One early method involved distributing termite bait laced with immunoglobulin G (IgG) marker proteins from rabbits or chickens. Termites collected from the field could be tested for the rabbit-IgG markers using a rabbit-IgG-specific assay. More recently developed, less expensive alternatives include tracking the termites using egg white, cow milk, or soy milk proteins, which can be sprayed on termites in the field. Termites bearing these proteins can be traced using a protein-specific ELISA test.\n\n43 termite species are used as food by humans or are fed to livestock. These insects are particularly important in less developed countries where malnutrition is common, as the protein from termites can help improve the human diet. Termites are consumed in many regions globally, but this practice has only become popular in developed nations in recent years.\n\nTermites are consumed by people in many different cultures around the world. In Africa, the alates are an important factor in the diets of native populations. Tribes have different ways of collecting or cultivating insects; sometimes tribes collect soldiers from several species. Though harder to acquire, queens are regarded as a delicacy. Termite alates are high in nutrition with adequate levels of fat and protein. They are regarded as pleasant in taste, having a nut-like flavour after they are cooked.\n\nAlates are collected when the rainy season begins. During a nuptial flight, they are typically seen around lights to which they are attracted, and so nets are set up on lamps and captured alates are later collected. The wings are removed through a technique that is similar to winnowing. The best result comes when they are lightly roasted on a hot plate or fried until crisp. Oil is not required as their bodies usually contain sufficient amounts of oil. Termites are typically eaten when livestock is lean and tribal crops have not yet developed or produced any food, or if food stocks from a previous growing season are limited.\n\nIn addition to Africa, termites are consumed in local or tribal areas in Asia and North and South America. In Australia, Indigenous Australians are aware that termites are edible but do not consume them even in times of scarcity; there are few explanations as to why. Termite mounds are the main sources of soil consumption (geophagy) in many countries including Kenya, Tanzania, Zambia, Zimbabwe and South Africa. Researchers have suggested that termites are suitable candidates for human consumption and space agriculture, as they are high in protein and can be used to convert inedible waste to consumable products for humans.\n\nTermites can be major agricultural pests, particularly in East Africa and North Asia, where crop losses can be severe (3–100% in crop loss in Africa). Counterbalancing this is the greatly improved water infiltration where termite tunnels in the soil allow rainwater to soak in deeply, which helps reduce runoff and consequent soil erosion through bioturbation. In South America, cultivated plants such as eucalyptus, upland rice and sugarcane can be severely damaged by termite infestations, with attacks on leaves, roots and woody tissue. Termites can also attack other plants, including cassava, coffee, cotton, fruit trees, maize, peanuts, soybeans and vegetables. Mounds can disrupt farming activities, making it difficult for farmers to operate farming machinery; however, despite farmers' dislike of the mounds, it is often the case that no net loss of production occurs. Termites can be beneficial to agriculture, such as by boosting crop yields and enriching the soil. Termites and ants can re-colonise untilled land that contains crop stubble, which colonies use for nourishment when they establish their nests. The presence of nests in fields enables larger amounts of rainwater to soak into the ground and increases the amount of nitrogen in the soil, both essential for the growth of crops.\n\nThe termite gut has inspired various research efforts aimed at replacing fossil fuels with cleaner, renewable energy sources. Termites are efficient bioreactors, capable of producing two litres of hydrogen from a single sheet of paper. Approximately 200 species of microbes live inside the termite hindgut, releasing the hydrogen that was trapped inside wood and plants that they digest. Through the action of unidentified enzymes in the termite gut, lignocellulose polymers are broken down into sugars and are transformed into hydrogen. The bacteria within the gut turns the sugar and hydrogen into cellulose acetate, an acetate ester of cellulose on which termites rely for energy. Community DNA sequencing of the microbes in the termite hindgut has been employed to provide a better understanding of the metabolic pathway. Genetic engineering may enable hydrogen to be generated in bioreactors from woody biomass.\n\nThe development of autonomous robots capable of constructing intricate structures without human assistance has been inspired by the complex mounds that termites build. These robots work independently and can move by themselves on a tracked grid, capable of climbing and lifting up bricks. Such robots may be useful for future projects on Mars, or for building levees to prevent flooding.\n\nTermites use sophisticated means to control the temperatures of their mounds. As discussed above, the shape and orientation of the mounds of the Australian compass termite stabilises their internal temperatures during the day. As the towers heat up, the solar chimney effect (stack effect) creates an updraft of air within the mound. Wind blowing across the tops of the towers enhances the circulation of air through the mounds, which also include side vents in their construction. The solar chimney effect has been in use for centuries in the Middle East and Near East for passive cooling, as well as in Europe by the Romans. It is only relatively recently, however, that climate responsive construction techniques have become incorporated into modern architecture. Especially in Africa, the stack effect has become a popular means to achieve natural ventilation and passive cooling in modern buildings.\n\nThe Eastgate Centre is a shopping centre and office block in central Harare, Zimbabwe, whose architect, Mick Pearce, used passive cooling inspired by that used by the local termites. It was the first major building exploiting termite-inspired cooling techniques to attract international attention. Other such buildings include the Learning Resource Center at the Catholic University of Eastern Africa and the Council House 2 building in Melbourne, Australia.\n\nFew zoos hold termites, due to the difficulty in keeping them captive and to the reluctance of authorities to permit potential pests. One of the few that do, the Zoo Basel in Switzerland, has two thriving \"Macrotermes bellicosus\" populations – resulting in an event very rare in captivity: the mass migrations of young flying termites. This happened in September 2008, when thousands of male termites left their mound each night, died, and covered the floors and water pits of the house holding their exhibit.\n\nAfrican tribes in several countries have termites as totems, and for this reason tribe members are forbidden to eat the reproductive alates. Termites are widely used in traditional popular medicine; they are used as treatments for diseases and other conditions such as asthma, bronchitis, hoarseness, influenza, sinusitis, tonsillitis and whooping cough. In Nigeria, \"Macrotermes nigeriensis\" is used for spiritual protection and to treat wounds and sick pregnant women. In Southeast Asia, termites are used in ritual practices. In Malaysia, Singapore and Thailand, termite mounds are commonly worshiped among the populace. Abandoned mounds are viewed as structures created by spirits, believing a local guardian dwells within the mound; this is known as Keramat and Datok Kong. In urban areas, local residents construct red-painted shrines over mounds that have been abandoned, where they pray for good health, protection and luck.\n\n\n\n",
                "Nine-banded armadillo\n\nThe nine-banded armadillo (\"Dasypus novemcinctus\"), or the nine-banded, long-nosed armadillo, is a medium-sized mammal found in North, Central, and South America, making it the most widespread of the armadillos. Its ancestors originated in South America, and remained there until the formation of the Isthmus of Panama allowed them to enter North America as part of the Great American Interchange.\nThe nine-banded armadillo is a solitary, mainly nocturnal animal, found in many kinds of habitats, from mature and secondary rainforests to grassland and dry scrub. It is an insectivore, feeding chiefly on ants, termites, and other small invertebrates. The armadillo can jump straight in the air if sufficiently frightened, making it a particular danger on roads. It is the state small mammal of Texas.\n\nThe nine-banded armadillo evolved in a warm, rainy environment, and is still most commonly found in regions resembling its ancestral home. As a very adaptable animal, though, it can also be found in scrublands, open prairies, and tropical rainforests. It cannot thrive in particularly cold or dry environments, as its large surface area, which is not well insulated by fat, makes it especially susceptible to heat and water loss.\n\nThe nine-banded armadillo has been rapidly expanding its range both north and east within the United States, where it is the only regularly occurring species of armadillo. The armadillo crossed the Rio Grande from Mexico in the late 19th century, and was introduced in Florida at about the same time by humans. By 1995, the species had become well established in Texas, Oklahoma, Louisiana, Arkansas, Mississippi, Alabama, and Florida, and had been sighted as far afield as Kansas, Missouri, Tennessee, Georgia and South Carolina. A decade later, the armadillo had become established in all of those areas and continued its migration, being sighted as far north as southern Nebraska, southern Illinois, and southern Indiana.\nThe primary cause of this rapid expansion is explained simply by the species having few natural predators within the United States, little desire on the part of Americans to hunt or eat the armadillo, and the animals' high reproductive rate. The northern expansion of the armadillo is expected to continue until the species reaches as far north as Ohio, Pennsylvania, New Jersey and Connecticut, and all points southward on the East Coast of the United States. Further northward and westward expansion will probably be limited by the armadillo's poor tolerance of harsh winters, due to its lack of insulating fat and its inability to hibernate.\nAs of 2009, newspaper reports indicated the nine-banded armadillo seems to have expanded its range northward as far as Omaha, Nebraska in the west, and Kentucky Dam and Evansville, Indiana, in the east. In 1995, armadillos were only seen in the southern tip of South Carolina, and within two to three years, they had swept across most of the state. In late 2009, North Carolina began considering the establishment of a hunting season for armadillo, following reports that the species has been moving into the southern reaches of the state (roughly between the areas of Charlotte and Wilmington).\nOutside the United States, the nine-banded armadillo ranges southward through Central and South America into northern Argentina and Uruguay, where it is still expanding its range.\n\nNine-banded armadillos are generally insectivores. They forage for meals by thrusting their snouts into loose soil and leaf litter and frantically digging in erratic patterns, stopping occasionally to dig up grubs, beetles (perhaps the main portion of this species' prey selection), ants, termites, and worms, which their sensitive noses can detect through of soil. They then lap up the insects with their sticky tongues. Nine-banded armadillos have been observed to roll about on ant hills to dislodge and consume the resident ants. They supplement their diets with amphibians and small reptiles, especially in more wintery months when such prey tends to be more sluggish, and occasionally bird eggs and baby mammals. Carrion is also eaten, although perhaps the species is most attracted to the maggots borne by carcasses rather than the meat itself. Less than 10% of the diet of this species is composed by nonanimal matter, though fungi, tubers, fruits, and seeds are occasionally eaten.\n\nNine-banded armadillos generally weigh from , though the largest specimens can scale up to . They are one of the largest species of armadillos. Head and body length is , which combines with the tail, for a total length of . They stand tall at the top of the shell. The outer shell is composed of ossified dermal scutes covered by nonoverlapping, keratinized epidermal scales, which are connected by flexible bands of skin. This armor covers the back, sides, head, tail, and outside surfaces of the legs. The underside of the body and the inner surfaces of the legs have no armored protection. Instead, they are covered by tough skin and a layer of coarse hair. The vertebrae attach to the carapace.\nThe claws on the middle toes of the forefeet are elongated for digging, though not to the same degree as those of the much larger giant armadillo of South America.\nTheir low metabolic rate and poor thermoregulation make them best suited for semitropical environments.\nUnlike the South American three-banded armadillos, the nine-banded armadillo cannot roll itself into a ball. It is, however, capable of floating across rivers by inflating its intestines, or by sinking and running across riverbeds. The second is possible due to its ability to hold its breath for up to six minutes, an adaptation originally developed for allowing the animal to keep its snout submerged in soil for extended periods while foraging. Although nine is the typical number of bands on the nine-banded armadillo, the actual number varies by geographic range.\nArmadillos possess the teeth typical of all sloths and anteaters. The teeth are all small, peg-like molars with open roots and no enamel. Incisors do form in the embryos, but quickly degenerate and are usually absent by birth.\n\nNine-banded armadillos are solitary, largely nocturnal animals that come out to forage around dusk. They are extensive burrowers, with a single animal sometimes maintaining up to 12 burrows on its range. These burrows are roughly wide, deep, and long. Armadillos mark their territory with urine, feces, and excretions from scent glands found on the eyelids, nose, and feet. Males hold breeding territories and may become aggressive in order to keep other males out of their home range to increase chances of pairing with a female. Territorial disputes are settled by kicking and chasing. When they are not foraging, armadillos shuffle along fairly slowly, stopping occasionally to sniff the air for signs of danger.\n\nIf alarmed, nine-banded armadillos can flee with surprising speed. Occasionally, a large predator may be able to ambush the armadillo before it can clear a distance, and breach the hard carapace with a well-placed bite or swipe. If the fleeing escape fails, the armadillo may quickly dig a shallow trench and lodge itself inside. Predators are rarely able to dislodge the animal once it has burrowed itself, and abandon their prey when they cannot breach the armadillo’s armor or grasp its tapered tail. Due to their softer carapaces, juvenile armadillos are more likely to fall victim to natural predation and their cautious behavior generally reflects this. Young nine-banded armadillos tend to forage earlier in the day and are more wary of the approach of an unknown animal (including humans) than are adults. Their known natural predators include cougars (perhaps the leading predator), maned wolves, coyotes, black bears, red wolves, jaguars, alligators, bobcats, and large raptors. By far the leading predator of nine-banded armadillos today is humans, as armadillos are locally harvested for their meat and shells and many thousands fall victim to auto accidents every year.\n\nMating takes place during a two-to-three month long mating season, which occurs from July–August in the Northern Hemisphere and November–January in the Southern Hemisphere. A single egg is fertilized, but implantation is delayed for three to four months to ensure the young will not be born during an unfavorable time. Once the zygote does implant in the uterus, a gestation period of four months occurs, during which the zygote splits into four identical embryos, each of which develops its own placenta, so blood and nutrients are not mixed between them. They are born in March and weigh 3 oz (85 g). After birth, the quadruplets remain in the burrow, living off the mother’s milk for about three months. They then begin to forage with the mother, eventually leaving after six months to a year.\n\nNine-banded armadillos reach sexual maturity at the age of one year, and reproduce every year for the rest of their 12–to-15 year lifespans. A single female can produce up to 56 young over the course of her life. This high reproductive rate is a major cause of the species’ rapid expansion.\n\nThe foraging of nine-banded armadillo can cause mild damage to the root systems of certain plants. Skunks, cotton rats, burrowing owls, pine snakes, and rattlesnakes can be found living in abandoned armadillo burrows. Occasionally, the armadillo may threaten the endangered gopher tortoise by aggressively displacing them from their burrows and claiming the burrows for themselves. Studies have shown the fan-tailed warbler habitually follows armadillos to feed on insects and other invertebrates displaced by them.\n\nThey are typically hunted for their meat, which is said to taste like pork, but are more frequently killed as a result of their tendency to steal the eggs of poultry and game birds. This has caused certain populations of the nine-banded armadillo to become threatened, although the species as a whole is under no immediate threat.\nThey are also valuable for use in medical research, as they are among the few mammals other than humans susceptible to leprosy.\nIn Texas, nine-banded armadillos are raised to participate in armadillo racing, a small-scale, but well-established sport in which the animals scurry down a 40-foot track.\n\nDuring the Great Depression, the species was hunted for its meat in East Texas, where it was known as poor man’s pork, or the \"Hoover hog\" by those who considered President Herbert Hoover to be responsible for the Depression. Earlier, German settlers in Texas would often refer to the armadillo as \"Panzerschwein\" (\"armored pig\"). In 1995, the nine-banded armadillo was, with some resistance, made the state small mammal of Texas, where it is considered a pest and is often seen dead on the roadside. They first forayed into Texas across the Rio Grande from Mexico in the 19th century, eventually spreading across the southeast United States.\n\nNorth American subspecies exhibit reduced genetic variability compared with the subspecies of South America, indicating the armadillos of North America are descended from a relatively small number of individuals that migrated from south of the Rio Grande.\n\n\n"
            ],
            "good_message": false,
            "loggedData": [],
            "offensive": null,
            "researchTask": true,
            "selectedData": [],
            "shown_passages": [
                [
                    "Giant armadillo",
                    [
                        "The giant armadillo (\"Priodontes maximus\"), colloquially tatou, ocarro, tatu-canastra or tatú carreta, is the largest living species of armadillo (although their extinct relatives, the glyptodonts, were much larger).",
                        "It lives in South America, ranging throughout as far south as northern Argentina.",
                        "This species is considered vulnerable to extinction.",
                        "The giant armadillo prefers termites and some ants as prey, and often consumes the entire population of a termite mound.",
                        "It also has been known to prey upon worms, larvae and larger creatures, such as spiders and snakes, and plants.",
                        "At least one zoo park, in Villavicencio, Colombia – \"Los Ocarros\" – is dedicated to this animal."
                    ]
                ],
                [
                    "Termite",
                    [
                        "Termites are eusocial insects that are classified at the taxonomic rank of infraorder Isoptera, or as epifamily Termitoidae within the cockroach order Blattodea.",
                        "Termites were once classified in a separate order from cockroaches, but recent phylogenetic studies indicate that they evolved from close ancestors of cockroaches during the Jurassic or Triassic.",
                        "However, the first termites possibly emerged during the Permian or even the Carboniferous.",
                        "About 3,106 species are currently described, with a few hundred more left to be described.",
                        "Although these insects are often called \"white ants\", they are not ants."
                    ]
                ],
                [
                    "Nine-banded armadillo",
                    [
                        "The nine-banded armadillo (\"Dasypus novemcinctus\"), or the nine-banded, long-nosed armadillo, is a medium-sized mammal found in North, Central, and South America, making it the most widespread of the armadillos.",
                        "Its ancestors originated in South America, and remained there until the formation of the Isthmus of Panama allowed them to enter North America as part of the Great American Interchange.",
                        "The nine-banded armadillo is a solitary, mainly nocturnal animal, found in many kinds of habitats, from mature and secondary rainforests to grassland and dry scrub.",
                        "It is an insectivore, feeding chiefly on ants, termites, and other small invertebrates."
                    ]
                ]
            ],
            "speaker": "0_Wizard",
            "suggestionsConfig": "show",
            "suggestionsNumber": 5,
            "suggestionsShown": true,
            "text": "I know a little bit. I saw a giant armadillo in a museum. Apparently they eat a lot of ants and termites! ",
            "textboxFilled": 0,
            "time": 1580233324.4904575,
            "timeTaken": 3.391,
            "totalTasks": 120,
            "turn": 15
        },
        {
            "checked_passage": {
                "no_passages_used": "no_passages_used"
            },
            "checked_sentence": {
                "no_passages_used": "no_passages_used"
            },
            "clicked_passages": {},
            "context": {
                "conversation": [
                    "i really love animals, do u know about Armadillo?",
                    "I know a little bit. I saw a giant armadillo in a museum. Apparently they eat a lot of ants and termites! "
                ],
                "index": 1
            },
            "contextCount": 0,
            "full_passages": [
                "Nine-banded armadillo\n\nThe nine-banded armadillo (\"Dasypus novemcinctus\"), or the nine-banded, long-nosed armadillo, is a medium-sized mammal found in North, Central, and South America, making it the most widespread of the armadillos. Its ancestors originated in South America, and remained there until the formation of the Isthmus of Panama allowed them to enter North America as part of the Great American Interchange.\nThe nine-banded armadillo is a solitary, mainly nocturnal animal, found in many kinds of habitats, from mature and secondary rainforests to grassland and dry scrub. It is an insectivore, feeding chiefly on ants, termites, and other small invertebrates. The armadillo can jump straight in the air if sufficiently frightened, making it a particular danger on roads. It is the state small mammal of Texas.\n\nThe nine-banded armadillo evolved in a warm, rainy environment, and is still most commonly found in regions resembling its ancestral home. As a very adaptable animal, though, it can also be found in scrublands, open prairies, and tropical rainforests. It cannot thrive in particularly cold or dry environments, as its large surface area, which is not well insulated by fat, makes it especially susceptible to heat and water loss.\n\nThe nine-banded armadillo has been rapidly expanding its range both north and east within the United States, where it is the only regularly occurring species of armadillo. The armadillo crossed the Rio Grande from Mexico in the late 19th century, and was introduced in Florida at about the same time by humans. By 1995, the species had become well established in Texas, Oklahoma, Louisiana, Arkansas, Mississippi, Alabama, and Florida, and had been sighted as far afield as Kansas, Missouri, Tennessee, Georgia and South Carolina. A decade later, the armadillo had become established in all of those areas and continued its migration, being sighted as far north as southern Nebraska, southern Illinois, and southern Indiana.\nThe primary cause of this rapid expansion is explained simply by the species having few natural predators within the United States, little desire on the part of Americans to hunt or eat the armadillo, and the animals' high reproductive rate. The northern expansion of the armadillo is expected to continue until the species reaches as far north as Ohio, Pennsylvania, New Jersey and Connecticut, and all points southward on the East Coast of the United States. Further northward and westward expansion will probably be limited by the armadillo's poor tolerance of harsh winters, due to its lack of insulating fat and its inability to hibernate.\nAs of 2009, newspaper reports indicated the nine-banded armadillo seems to have expanded its range northward as far as Omaha, Nebraska in the west, and Kentucky Dam and Evansville, Indiana, in the east. In 1995, armadillos were only seen in the southern tip of South Carolina, and within two to three years, they had swept across most of the state. In late 2009, North Carolina began considering the establishment of a hunting season for armadillo, following reports that the species has been moving into the southern reaches of the state (roughly between the areas of Charlotte and Wilmington).\nOutside the United States, the nine-banded armadillo ranges southward through Central and South America into northern Argentina and Uruguay, where it is still expanding its range.\n\nNine-banded armadillos are generally insectivores. They forage for meals by thrusting their snouts into loose soil and leaf litter and frantically digging in erratic patterns, stopping occasionally to dig up grubs, beetles (perhaps the main portion of this species' prey selection), ants, termites, and worms, which their sensitive noses can detect through of soil. They then lap up the insects with their sticky tongues. Nine-banded armadillos have been observed to roll about on ant hills to dislodge and consume the resident ants. They supplement their diets with amphibians and small reptiles, especially in more wintery months when such prey tends to be more sluggish, and occasionally bird eggs and baby mammals. Carrion is also eaten, although perhaps the species is most attracted to the maggots borne by carcasses rather than the meat itself. Less than 10% of the diet of this species is composed by nonanimal matter, though fungi, tubers, fruits, and seeds are occasionally eaten.\n\nNine-banded armadillos generally weigh from , though the largest specimens can scale up to . They are one of the largest species of armadillos. Head and body length is , which combines with the tail, for a total length of . They stand tall at the top of the shell. The outer shell is composed of ossified dermal scutes covered by nonoverlapping, keratinized epidermal scales, which are connected by flexible bands of skin. This armor covers the back, sides, head, tail, and outside surfaces of the legs. The underside of the body and the inner surfaces of the legs have no armored protection. Instead, they are covered by tough skin and a layer of coarse hair. The vertebrae attach to the carapace.\nThe claws on the middle toes of the forefeet are elongated for digging, though not to the same degree as those of the much larger giant armadillo of South America.\nTheir low metabolic rate and poor thermoregulation make them best suited for semitropical environments.\nUnlike the South American three-banded armadillos, the nine-banded armadillo cannot roll itself into a ball. It is, however, capable of floating across rivers by inflating its intestines, or by sinking and running across riverbeds. The second is possible due to its ability to hold its breath for up to six minutes, an adaptation originally developed for allowing the animal to keep its snout submerged in soil for extended periods while foraging. Although nine is the typical number of bands on the nine-banded armadillo, the actual number varies by geographic range.\nArmadillos possess the teeth typical of all sloths and anteaters. The teeth are all small, peg-like molars with open roots and no enamel. Incisors do form in the embryos, but quickly degenerate and are usually absent by birth.\n\nNine-banded armadillos are solitary, largely nocturnal animals that come out to forage around dusk. They are extensive burrowers, with a single animal sometimes maintaining up to 12 burrows on its range. These burrows are roughly wide, deep, and long. Armadillos mark their territory with urine, feces, and excretions from scent glands found on the eyelids, nose, and feet. Males hold breeding territories and may become aggressive in order to keep other males out of their home range to increase chances of pairing with a female. Territorial disputes are settled by kicking and chasing. When they are not foraging, armadillos shuffle along fairly slowly, stopping occasionally to sniff the air for signs of danger.\n\nIf alarmed, nine-banded armadillos can flee with surprising speed. Occasionally, a large predator may be able to ambush the armadillo before it can clear a distance, and breach the hard carapace with a well-placed bite or swipe. If the fleeing escape fails, the armadillo may quickly dig a shallow trench and lodge itself inside. Predators are rarely able to dislodge the animal once it has burrowed itself, and abandon their prey when they cannot breach the armadillo’s armor or grasp its tapered tail. Due to their softer carapaces, juvenile armadillos are more likely to fall victim to natural predation and their cautious behavior generally reflects this. Young nine-banded armadillos tend to forage earlier in the day and are more wary of the approach of an unknown animal (including humans) than are adults. Their known natural predators include cougars (perhaps the leading predator), maned wolves, coyotes, black bears, red wolves, jaguars, alligators, bobcats, and large raptors. By far the leading predator of nine-banded armadillos today is humans, as armadillos are locally harvested for their meat and shells and many thousands fall victim to auto accidents every year.\n\nMating takes place during a two-to-three month long mating season, which occurs from July–August in the Northern Hemisphere and November–January in the Southern Hemisphere. A single egg is fertilized, but implantation is delayed for three to four months to ensure the young will not be born during an unfavorable time. Once the zygote does implant in the uterus, a gestation period of four months occurs, during which the zygote splits into four identical embryos, each of which develops its own placenta, so blood and nutrients are not mixed between them. They are born in March and weigh 3 oz (85 g). After birth, the quadruplets remain in the burrow, living off the mother’s milk for about three months. They then begin to forage with the mother, eventually leaving after six months to a year.\n\nNine-banded armadillos reach sexual maturity at the age of one year, and reproduce every year for the rest of their 12–to-15 year lifespans. A single female can produce up to 56 young over the course of her life. This high reproductive rate is a major cause of the species’ rapid expansion.\n\nThe foraging of nine-banded armadillo can cause mild damage to the root systems of certain plants. Skunks, cotton rats, burrowing owls, pine snakes, and rattlesnakes can be found living in abandoned armadillo burrows. Occasionally, the armadillo may threaten the endangered gopher tortoise by aggressively displacing them from their burrows and claiming the burrows for themselves. Studies have shown the fan-tailed warbler habitually follows armadillos to feed on insects and other invertebrates displaced by them.\n\nThey are typically hunted for their meat, which is said to taste like pork, but are more frequently killed as a result of their tendency to steal the eggs of poultry and game birds. This has caused certain populations of the nine-banded armadillo to become threatened, although the species as a whole is under no immediate threat.\nThey are also valuable for use in medical research, as they are among the few mammals other than humans susceptible to leprosy.\nIn Texas, nine-banded armadillos are raised to participate in armadillo racing, a small-scale, but well-established sport in which the animals scurry down a 40-foot track.\n\nDuring the Great Depression, the species was hunted for its meat in East Texas, where it was known as poor man’s pork, or the \"Hoover hog\" by those who considered President Herbert Hoover to be responsible for the Depression. Earlier, German settlers in Texas would often refer to the armadillo as \"Panzerschwein\" (\"armored pig\"). In 1995, the nine-banded armadillo was, with some resistance, made the state small mammal of Texas, where it is considered a pest and is often seen dead on the roadside. They first forayed into Texas across the Rio Grande from Mexico in the 19th century, eventually spreading across the southeast United States.\n\nNorth American subspecies exhibit reduced genetic variability compared with the subspecies of South America, indicating the armadillos of North America are descended from a relatively small number of individuals that migrated from south of the Rio Grande.\n\n\n",
                "Dasypus\n\nDasypus is the only extant genus in the family Dasypodidae. Its species are known as long-nosed or naked-tailed armadillos. They are largely found in South, Central, and North America. \"Dasypus\" are solitary mammals that are primarily nocturnal to avoid temperature extremes and predation. They exist in numerous habitats ranging from brush to grassland areas and are mainly insectivorous.\n\nThe most common and widespread of the \"Dasypus\" species is the nine-banded armadillo (\"Dasypus novemcintus\"), which is commonly used in the study of leprosy due to its unique ability to contract the disease.\n\n\"Dasypus\" are primitive mammals known for their hard armor like shell, called a carapace. Their ossified dermal plates compose a series of six to eleven movable bands covered by leathery keratinous skin, which surrounds and protects the body. A thin epidermal layer separates each of the armor bands, and joints in the shell allow for flexibility. The face, neck, and underside lack a bony covering and are instead covered with small bunches of hair.\n\n\"Dasypus\" species are grey or brown in color and possess long and sharp claws for scavenging and digging burrows. Although they have a very diverse range, armadillos are typically found near bodies of water, and their burrows are often dug into stream banks, tree stumps, or rock or brush piles.\n\nWhen threatened, armadillos run to the nearest burrow or crevice and tightly wedge themselves inside with their back alongside the wall. If no such burrow or shelter is available, armadillos curl up in order to protect their vulnerable underside. \"Dasypus\" species are unable to roll into a complete ball like the Brazilian three-banded armadillo due their excessive number of dermal plates.\n\nBecause they lack significant hair covering, armadillos are particularly sensitive to climate and are therefore most active during summer nights and winter days. Due to their low fat storage, they spend most of their activity foraging for food, which primarily consists of insects, small reptiles and amphibians, and plants. Armadillos have a small, flattened skull with a long lower jaw and narrow snout. They do have small, rudimentary teeth, but lack incisors, canines, and enamel. Their tongue is particularly long and sticky and is used to forage for ants and termites. They have very poor eyesight and instead rely on their keen sense of smell and enhanced hearing to locate buried insects and detect predators.\n\nArmadillos are fully capable of climbing, swimming, and jumping. \"Dasypus\" have a unique ability to build up an oxygen debt and hold their breath for up to six minutes. This allows them to cross streams and ponds underwater by simply walking or running along the bottom. If the water body is too large for this, \"Dasypus\" can instead gulp in air, inflating their stomach and intestine and increasing buoyancy. This allows them to float and more easily swim across the water.\n\n\"Dasypus\" are non-territorial, have large progeny, have few predators, and are capable of living in various environments, thus accounting for their large distribution. They are, though, limited by a lack of sufficient insects as a food source and their low metabolic rate, which prevents them from living in cold climates. \"Dasypus\" originated from South America but has expanded and diversified across numerous countries. The existence of human developments and construction has generally increased the armadillo's ability to expand by facilitating the crossing of previous obstacles. As of 2011, within the United States, they have not yet migrated south due to the lack of rainfall or water availability.\n\nThere are currently eight known \"Dasypus\" species:\n\nAn additional \"Dasypus\" species that is of medium size with noticeably shorter ears and tail is speculated to exist in Paraguay.\n\nArmadillos are most often found in shady forest and brush areas in temperate regions. They thrive in high rainfall habitats most likely due to better soil conditions for burrowing and a higher abundance of food. They are also known to inhabit various other environments ranging from grassland to swamp areas and are able to adapt to numerous regions as long as adequate food and water are available. It has been noted that armadillo species are extremely fond of water and will not only use streams for feeding and drinking but also for mud baths.\n\nMost \"Dasypus\" species are sensitive to temperature due to poor insulation. Because of this, they currently are not found in regions with temperatures below -2 degrees Celsius or 24 annual freeze-over days. They are, however, able to withstand short cold periods by remaining in their burrows.\n\nArmadillo's burrow systems may be up to ten meters in length and two meter deep and are complex systems with a conjoined central den. Armadillos are known to have as many as twelve burrow sites and multiple entrances for each. They often have a primary burrow for nesting and additional shallow burrows within their territory as food traps. In certain coastal prairies, armadillos dig additional burrows for flood protection. Armadillos have been documented to occasionally share burrows with other animals such as rabbits or skunks. However, they very rarely share their burrow with another armadillo except during the mating season. One such incidence of adult armadillos sharing burrows is during extreme cold weather, in which sharing may enhance thermoregulation.\n\n\"Dasypus\" are typically non-aggressive, solitary animals. They are, however, known to occasionally show slight aggressive behavior during the mating season or while a female is nursing. Such behavior often includes kicking or chasing and does not cause substantial injury. Armadillos are more likely to respond to threats by freezing, jumping into the air, or sprinting away.\n\nArmadillos use olfaction as their main perception for foraging. The nine-banded armadillos are capable of smelling food as much as 20 cm below the ground surface. Once a food item is detected, it digs a small hole using its forefeet. Armadillos are also known to stand on their hind legs using their tail to brace themselves and sniff the air to either locate food or orient themselves.\n\nBecause of armadillos’ low body temperature, scavenging habits, and damp living environments they are susceptible to certain infections and parasites. Some of these include the bacterium \"Mycobacterium leprae\", which is the causative of leprosy, the organism \"Trypanosoma cruzi\", which is the causative of Chagas’ disease, and the fungus \"Paracoccidiodies brasiliensis\", which is the causative of mycosis in humans. Despite these predispositions, however, armadillos are still considered less prone to get parasites than other common small mammals such as skunks, opossums, and raccoons. No diagnosis of rabies within \"Dasypus\" species in Florida has been recorded yet.\n\nIn some locations in Florida, \"Dasypus\" have more recently been noted to raid and destroy sea turtle nests, specifically those belonging to the endangered leatherback (\"Dermochelys coriacea\"), loggerhead (\"Caretta caretta\"), and the green (\"Chelonia mydas\"). This accounts for 95% of nest raids in the area and may present a large invasive problem. In order to protect the endangered sea turtles, National Wildlife Refuge staff and the USDA Wildlife Services have actively trapped and removed armadillos from nesting locations. However, though the growth of armadillos in Florida may be contributing to sea turtle invasion, they are also serving as an important primary food source to maintain the endangered Florida panther (\"F. concolor coryi\").\n\nArmadillos have a life expectancy of 7 to 20 years. Juveniles lack fully developed and strengthened armor and are much more susceptible to predation, thereby having a much higher mortality rate than adults. Adult armadillos, however, have shown a significant increase in physical damage compared to juveniles. Since juvenile mortality rates are much higher, this most likely indicates an increased ability of adults to escape from predators. Strangely, various physical damages caused to armadillos do not appear to have any significant consequences in breeding or other physical functions. In captivity, armadillos have a much higher life expectancy, with one recorded \"D. novemcinctus\" species living 23 years.\n\nOne of the largest causes of death of armadillos within North America are highway accidents. This is most likely due to their common response of jumping into the air when startled which causes a direct collision with a passing automobile. Armadillos are also killed by dogs or coyotes as well as hunted by humans as a food source. Despite hunting, predation, and highway accidents, the IUCN lists the majority of \"Dasypus\" species as a least concern endangered animal due to its very large distribution, living tolerance, and large progeny and population. A few species are considered at risk due to habitat loss.\n\nMost \"Dasypus\" are opportunistic feeders. They are insectivores to omnivores but are also known to eat small vertebrates. A study conducted on the nine-banded armadillo’s stomach content concluded that their diet consists of approximately 7% plant matter and 93% animal matter. Plants include fruit, seeds, mushroom and fungi and animal matter includes beetles, snails, ants, worms, reptiles, and amphibians. They also occasionally eat small mammals, bird eggs, and carrion. However, it is believed that carrion is more readily eaten for the maggots and fly pupae within. Armadillos swallow their food with small soil particles and usually avoid chewing altogether.\n\nArmadillos of the genus \"Dasypus\" often pair during the breeding season, whereas they are usually solitary animals. While pairing, the male and female intermittently interact while foraging. These interactions include tail wagging, dorsal touching, sniffing, and tail lifting of the female.\n\nFemales have an external clitoris and a urogenital sinus, which acts as both a urethra and vagina. Males are slightly larger than females in size and have testes that descend into the pelvis and a prominent penis. They lack a scrotum. In order to copulate, the female has to lie on her back due to the high amount of bony armor and the ventrally located genitalia.\n\nAfter conception, there is a fourteen-week period before the blastocyst is actually implanted. The blastocyst is fully developed and remains healthy through oxygen and nutrients received from uterine secretions during this time. Gestation is about five months long, and the implantation delay allows the armadillos to give birth at a more opportune time during the spring.\n\nMembers of \"Dasypus\" are unique among mammals in possessing the reproductive trait of monozygotic polyembryony, meaning their offspring are genetically identical due to the division of a single fertilized egg into four matching embryos. This development of identical quadruplets has been utilized as a tool for genetic research. It is possible that the monozygotic polyembryony was an adaptation to accommodate for the female’s inability to carry more than one egg during the preimplantation stage. Delaying the implantation further has no effect on the number of offspring produced.\n\nThe armadillo young are fully developed at birth. Their eyes are already open, and they are capable of walking after a few hours. However, the skin takes a few weeks to harden. The baby armadillos nurse for two months and by month three or four, they are completely independent. Young armadillos have been noted to occasionally share burrows with siblings during their first summer and fall. Armadillos born in the spring are able to breed during the very next season the following summer.\n\nLeprosy is a chronic infectious disease caused by the bacteria \"Mycobacterium leprae\". \"M. leprae\" is unculturable on artificial media, and only after years of research was the ability to culture the bacteria on the footpads of mice discovered. However, the development of the bacteria and study was still very limited until the successful infection of lepromatous leprosy in the nine-banded armadillo (\"Dasypus novemcinctus\") by Kirchheimer and Storrs in 1971. Soon after, Convit and Pinardi incurred a second successful inoculation of \"M. leprae\" into \"Dasypus sabanicola\". The armadillo became the only known animal other than primates to regularly develop leprosy and has since largely advanced the disease study through use of in vivo propagation of \"M. leprae\". \"Dasypus\" was also an ideal model due to the ability to replicate experiments on their genetically identical siblings. Despite the discovery of additional \"Dasypus\" species capable of infection (\"D. septemcinctus\" and \"D. pilosus\"), the nine-banded armadillo remains a favored animal model due to its availability and ideal body temperatures for bacterial hosting. \"D. sabanicola\" is also continually used in research due to its adaptability to the lab environment and ease of handling. The nine-banded armadillo’s enhanced ability to grow \"M. leprae\" has led to suggestions that armadillo species are more susceptible to the disease due to their generally lower body temperatures.\n\nWhile temperature enhances susceptibility, the actual infection source and mode of transmission are very poorly understood. This is primarily due to the bacteria’s slow multiplication rate and long incubation period, making specific infection period identification difficult. The incubation period itself may range from ten months to four years in the nine-banded armadillo, compared to three to six years in humans. The long life of armadillos is particularly useful in the study of chronic effects of leprosy as well as the propagation of \"M. leprae\" outside of humans. The armadillo model has been useful for biochemical, immunological, and vaccine research.\n\nThough the majority of nine-banded armadillos contract leprosy, about 15% of the species have been found to be resistant. The resistant specimens are used as a study model in order to develop a possible genetic linkage.\n",
                "Six-banded armadillo\n\nThe six-banded armadillo (\"Euphractus sexcinctus\"), also known as the yellow armadillo, is an armadillo found in South America. The sole extant member of its genus, it was first described by Swedish zoologist Carl Linnaeus in 1758. The six-banded armadillo is typically between in head-and-body length, and weighs . The carapace (hard shell on the back) is pale yellow to reddish brown, marked by scales of equal length, and scantily covered by buff to white bristle-like hairs. The forefeet have five distinct toes, each with moderately developed claws.\n\nSix-banded armadillos are efficient diggers and form burrows to live in and search for prey. The armadillo is alert and primarily solitary. An omnivore, it feeds on insects, ants, carrion, and plant material. Due to their poor eyesight, armadillos rely on their sense of smell to detect prey and predators. Births take place throughout the year; gestation is 60 to 64 days long, after which a litter of one to three is born. Weaning occurs at one month, and juveniles mature by nine months. The six-banded armadillo inhabits savannas, primary and secondary forests, \"cerrado\"s, shrublands, and deciduous forests. Fairly common, its range spans from Brazil and southern Suriname in the northeast through Bolivia, Paraguay, and Uruguay into northern Argentina in the southeast. The International Union for the Conservation of Nature and Natural Resources (IUCN) classifies it as least concern, and there are no major threats to its survival.\n\nThe six-banded armadillo is the sole member of the genus \"Euphractus\" and is placed in the family Chlamyphoridae. It was first described by Swedish zoologist Carl Linnaeus as \"Dasypus sexcinctus\" in 1758. The genera \"Chaetophractus\" (hairy armadillos) and \"Zaedyus\" (pichi) have at times been included in \"Euphractus\", though karyotypical, immunological and morphological analyses oppose this. Fossil \"Euphractus\" excavated in Buenos Aires (Argentina), Lagoa Santa, Minas Gerais (Brazil) and Tarija (Bolivia) date back to the Pleistocene.\n\nThe following five subspecies are recognized:\n\nA 2006 morphological study of the phylogeny of armadillos showed that \"Chaetophractus\", \"Chlamyphorus\", \"Euphractus\" and \"Zaedyus\" form a monophyletic clade. The cladogram below (based only on the extant species) is based on this study.\n\nHowever, a mitochondrial DNA investigation has concluded that Chlamyphorinae (fairy armadillos) is the sister group of Tolypeutinae (giant, three-banded and naked-tailed armadillos), with Euphractinae (hairy, six-banded and pichi armadillos) having diverged earlier.\n\nThe six-banded armadillo differs from others in the subfamily Euphractinae, which also contains the pichi and hairy armadillos, in having a narrow head and six to seven movable bands on the carapace (the hard shell on the back). Other names for this armadillo are ' and ' (in Portuguese), and 'yellow armadillo'.\n\nThe six-banded armadillo is the largest in Euphractinae, which also contains the pichi and hairy armadillos; in fact, it is the third largest armadillo after the giant armadillo and the greater long-nosed armadillo. This armadillo is typically between in head-and-body length, and weighs . The carapace is pale yellow to reddish brown (though not a dark shade of brown or black), marked by scales of equal length, and scantily covered by buff to white bristle-like hairs – unlike the hairy armadillos, that are covered by dense hairs. The shell narrows to 70 to 80 percent of its original width towards the top of the head, which is covered by plates arranged in a definite pattern. The forefeet have five distinct toes, each with moderately developed claws, of which the third is the longest.\n\nLike the other euphractines and the pink fairy armadillo, the six-banded armadillo has a tympanic bulla; the ears are long. There are 9 pairs of teeth on the upper jaw and 10 pairs on the lower jaw; the teeth are large and strong and are assisted by strong muscles for chewing. A row of scutes, each wide, extends along the back of the neck. The tail, long, is covered by two to four bands of plates on the underside. Some of these plates have holes for scent gland secretions, a feature seen in no other armadillo except a few big hairy armadillos.\n\nDifferent studies have recorded different activity patterns for the six-banded armadillo – some consider it to be diurnal (active mainly during the day), while others show it is nocturnal (active mainly at night). It is an alert animal; unlike other armadillos, it flees on sensing danger and bites if handled. Primarily solitary, six-banded armadillos will congregate only to feed on carrions. A 1983 study in eastern Brazil calculated the mean home range size as . An efficient digger, this armadillo can dig U-shaped burrows with a single opening, typically in dry areas; the burrows may or may not be permanent shelters. These burrows can go deep into the ground and help in foraging. A study of burrows dug by the giant, six-banded, southern naked-tailed and greater naked-tailed armadillos showed that all burrows were similar in the slopes of the burrow and the surrounding soil, and the direction of the entrance; the location preferred for them and time spent in them, however, differed. Burrows could be easily differentiated by their dimensions; burrows of six-banded armadillos had a mean height of and were wide at the opening, and narrowed down to with a height of to into the burrow. Generally, burrows become wide enough to allow the armadillo to turn around as the depth increases. Unlike the moles, that throw the soil to a side while digging, the six-banded armadillo digs with its forefeet and throws the soil behind with its hindfeet. Armadillos defecate outside their burrows.\n\nThe six-banded armadillo is an omnivore that feeds on carrion, small invertebrates, insects, ants, fruits (typically from bromeliads), palm nuts and tubers. A 2004 study classified it as a \"carnivore-omnivore\". In a study in a Brazilian ranch, plant material was found to predominate in the diet. Captive individuals have been observed preying upon large rats. Due to their poor eyesight, armadillos rely on their sense of smell to detect prey and predators. To kill the prey, the armadillo stands on it, grabs it using its teeth and tears it into pieces. Six-banded armadillos can store subcutaneous fat to support themselves at times when food is scarce; this fat can increase the weight to .\n\nBreeding behaviour has been observed in captivity. Births take place throughout the year. After a gestational period of 60 to 64 days, a litter of one to three is born. Each newborn weighs , and has a hairless and soft carapace; it can give out soft clicks. The pregnant female builds a nest before giving birth; if disturbed, the mother can react aggressively and shift her offspring. The eyes, closed at birth, open at 22 to 25 days. Weaning occurs at one month and the juveniles mature by nine months. One of the armadillos lived for nearly 18 years.\n\nThe six-banded armadillo inhabits savannas, primary and secondary forests, \"cerrado\"s, shrublands and deciduous forests. It can adapt to a variety of habitats; it can even occur on agricultural lands and has been recorded at above the sea level. A study in southeastern Brazil estimated the population density at 0.14 individuals per hectare. The same study showed that the six-banded armadillo often displaces and is displaced by the sympatric southern naked-tail armadillo; this was considered to be helpful in their coexistence. The six-banded armadillo has a wide distribution in South America, from Brazil and southern Suriname in the northeast through Bolivia, Paraguay and Uruguay into northern Argentina in the southeast. Its presence in Peru is doubted.\n\nThe IUCN classifies the six-banded armadillo as least concern, due to its wide distribution, good degree of tolerance and presumably large populations. Moreover, it occurs in several protected areas. Though there are no major threats to its survival, six-banded armadillo populations north of the Amazon River might be declining due to few patches of savannas, human settlement and industrial expansion. Moreover, these armadillos are reportedly hunted for medicinal purposes, though their meat is believed to have an unpalatable taste; in some areas in its range, people detest its meat due to the belief that the animal feeds on \"rotting human corpses\". A 2011 study compared the six- and nine-banded armadillos in Paraíba (Brazil); nine-banded armadillo meat tasted better according to the majority of the locals. Consumption of six-banded armadillo meat was not considered safe by the locals for people with certain health problems, such as injuries and hepatitis, a taboo that had stemmed probably from concerns over the armadillo's diet. Six-banded armadillos often raid cropfields to feed on tender plants; farmers are known to use traps aided by corn baits for these animals to curb the menace.\n\n"
            ],
            "good_message": false,
            "loggedData": [
                {
                    "curr_message": " The nine-banded armadillo (\"Dasypus novemcinctus\"), or the nine-banded, long-nosed armadillo, is a medium-sized mammal found in North, Central, and South America, making it the most widespread of the armadillos.",
                    "last_key_pressed": "<<selected>>"
                }
            ],
            "offensive": null,
            "researchTask": true,
            "selectedData": [
                "The nine-banded armadillo (\"Dasypus novemcinctus\"), or the nine-banded, long-nosed armadillo, is a medium-sized mammal found in North, Central, and South America, making it the most widespread of the armadillos."
            ],
            "shown_passages": [
                [
                    "Nine-banded armadillo",
                    [
                        "The nine-banded armadillo (\"Dasypus novemcinctus\"), or the nine-banded, long-nosed armadillo, is a medium-sized mammal found in North, Central, and South America, making it the most widespread of the armadillos.",
                        "Its ancestors originated in South America, and remained there until the formation of the Isthmus of Panama allowed them to enter North America as part of the Great American Interchange.",
                        "The nine-banded armadillo is a solitary, mainly nocturnal animal, found in many kinds of habitats, from mature and secondary rainforests to grassland and dry scrub.",
                        "It is an insectivore, feeding chiefly on ants, termites, and other small invertebrates."
                    ]
                ],
                [
                    "Dasypus",
                    [
                        "Dasypus is the only extant genus in the family Dasypodidae.",
                        "Its species are known as long-nosed or naked-tailed armadillos.",
                        "They are largely found in South, Central, and North America.",
                        "\"Dasypus\" are solitary mammals that are primarily nocturnal to avoid temperature extremes and predation.",
                        "They exist in numerous habitats ranging from brush to grassland areas and are mainly insectivorous.",
                        "The most common and widespread of the \"Dasypus\" species is the nine-banded armadillo (\"Dasypus novemcintus\"), which is commonly used in the study of leprosy due to its unique ability to contract the disease.",
                        "\"Dasypus\" are primitive mammals known for their hard armor like shell, called a carapace."
                    ]
                ],
                [
                    "Six-banded armadillo",
                    [
                        "The six-banded armadillo (\"Euphractus sexcinctus\"), also known as the yellow armadillo, is an armadillo found in South America.",
                        "The sole extant member of its genus, it was first described by Swedish zoologist Carl Linnaeus in 1758.",
                        "The six-banded armadillo is typically between in head-and-body length, and weighs .",
                        "The carapace (hard shell on the back) is pale yellow to reddish brown, marked by scales of equal length, and scantily covered by buff to white bristle-like hairs.",
                        "The forefeet have five distinct toes, each with moderately developed claws.",
                        "Six-banded armadillos are efficient diggers and form burrows to live in and search for prey."
                    ]
                ]
            ],
            "speaker": "0_Wizard",
            "suggestionsConfig": "show",
            "suggestionsNumber": 5,
            "suggestionsShown": true,
            "text": " The nine-banded armadillo (\"Dasypus novemcinctus\"), or the nine-banded, long-nosed armadillo, is a medium-sized mammal found in North, Central, and South America, making it the most widespread of the armadillos.",
            "textboxFilled": 0,
            "time": 1580233349.2533097,
            "timeTaken": 24.918,
            "totalTasks": 120,
            "turn": 16
        },
        {
            "checked_passage": {
                "no_passages_used": "no_passages_used"
            },
            "checked_sentence": {
                "no_passages_used": "no_passages_used"
            },
            "clicked_passages": {},
            "context": {
                "conversation": [
                    "There are many types of glasses, what kind do you want to chat about?",
                    "I would like to chat about reading glasses which are used for nearsightedness.",
                    "That sounds like a lovely type of glass. I actually am nearsighted myself. do you know who invented glasses?",
                    "I have no idea who invented reading glasses but I do know that those of us who wear these type glasses sometimes wear them around our necks attached to a cord to prevent loss.",
                    "That reminds me so much of the librarian I knew when I was little. She did that! What else do you know about glasses?",
                    "I also know that sunglasses offer protection against UV light and also offer better vision in bright light. Sunglasses can actu.ally prevent damage to the eyes."
                ],
                "index": 7
            },
            "contextCount": 0,
            "full_passages": [
                "Sunglasses\n\nSunglasses or sun glasses are a form of protective eyewear designed primarily to prevent bright sunlight and high-energy visible light from damaging or discomforting the eyes. They can sometimes also function as a visual aid, as variously termed spectacles or glasses exist, featuring lenses that are colored, polarized or darkened. In the early 20th century, they were also known as sun cheaters (cheaters being an American slang term for glasses).\n\nThe American Optometric Association recommends sunglasses whenever a person is in the sun to protect the eyes from ultraviolet radiation (UV) and blue light, which can cause several serious eye problems. Its usage is mandatory immediately after some surgical procedures, such as LASIK, and recommended for a certain time period in dusty areas, when leaving the house and in front of a TV screen or computer monitor after LASEK. Sunglasses have long been associated with celebrities and film actors primarily from a desire to mask their identity. Since the 1940s, sunglasses have been popular as a fashion accessory, especially on the beach.\n\nIn prehistoric and historic time, Inuit peoples wore flattened walrus ivory \"glasses\", looking through narrow slits to block harmful reflected rays of the sun.\n\nIt is said that the Roman emperor Nero liked to watch gladiator fights with emeralds. These, however, appear to have worked rather like mirrors.\nSunglasses made from flat panes of smoky quartz, which offered no corrective powers but did protect the eyes from glare, were used in China in the 12th century or possibly earlier. Ancient documents describe the use of such crystal sunglasses by judges in ancient Chinese courts to conceal their facial expressions while questioning witnesses.\n\nJames Ayscough began experimenting with tinted lenses in spectacles in the mid-18th century, around 1752. These were not \"sunglasses\" as that term is now used; Ayscough believed that blue- or green-tinted glass could correct for specific vision impairments. Protection from the Sun's rays was not a concern for him. One of the earliest surviving depictions of a person wearing sunglasses is of the scientist Antoine Lavoisier in 1772. Yellow/amber and brown-tinted spectacles were also a commonly prescribed item for people with syphilis in the 19th and early 20th centuries because sensitivity to light was one of the symptoms of the disease.\n\nIn the early 1920s, the use of sunglasses started to become more widespread, especially among movie stars. It is commonly believed that this was to avoid recognition by fans, but an alternative reason sometimes given is that they often had red eyes from the powerful arc lamps that were needed due to the extremely slow speed film stocks used. The stereotype persisted long after improvements in film quality and the introduction of ultraviolet filters had eliminated this problem. Inexpensive mass-produced sunglasses made from celluloid were first produced by Sam Foster in 1929. Foster found a ready market on the beaches of Atlantic City, New Jersey, where he began selling sunglasses under the name Foster Grant from a Woolworth on the Boardwalk. By 1938, \"Life\" magazine wrote of how sunglasses were a \"new fad for wear on city streets ... a favorite affectation of thousands of women all over the U.S.\" It stated that 20 million sunglasses were sold in the United States in 1937, but estimated that only about 25% of American wearers needed them to protect their eyes. Polarized sunglasses first became available in 1936, when Edwin H. Land began experimenting with making lenses with his patented Polaroid filter.\n\nAt present, Xiamen, China, is the world's largest producer of sunglasses, with its port exporting 120 million pairs each year.\n\nSunglasses can improve visual comfort and visual clarity by protecting the eye from glare.\n\nVarious types of disposable sunglasses are dispensed to patients after receiving mydriatic eye drops during eye examinations.\n\nThe lenses of polarized sunglasses reduce glare reflected at some angles off shiny non-metallic surfaces, such as water. They allow wearers to see into water when only surface glare would otherwise be seen, and eliminate glare from a road surface when driving into the sun.\n\nSunglasses offer protection against excessive exposure to light, including its visible and invisible components.\n\nThe most widespread protection is against ultraviolet radiation, which can cause short-term and long-term ocular problems such as photokeratitis, snow blindness, cataracts, pterygium, and various forms of eye cancer. Medical experts advise the public on the importance of wearing sunglasses to protect the eyes from UV; for adequate protection, experts recommend sunglasses that reflect or filter out 99% or more of UVA and UVB light, with wavelengths up to 400 nm. Sunglasses that meet this requirement are often labeled as \"UV400\". This is slightly more protection than the widely used standard of the European Union (see below), which requires that 95% of the radiation up to only 380 nm must be reflected or filtered out. Sunglasses are not sufficient to protect the eyes against permanent harm from looking directly at the Sun, even during a solar eclipse. Special eyewear known as solar viewers are required for direct viewing of the sun. This type of eyewear can filter out UV radiation harmful to the eyes.\n\nMore recently, high-energy visible light (HEV) has been implicated as a cause of age-related macular degeneration; before, debates had already existed as to whether \"blue blocking\" or amber tinted lenses may have a protective effect. Some manufacturers already design glasses to block blue light; the insurance company Suva, which covers most Swiss employees, asked eye experts around Charlotte Remé (ETH Zürich) to develop norms for blue blocking, leading to a recommended minimum of 95% of the blue light. Sunglasses are especially important for children, as their ocular lenses are thought to transmit far more HEV light than adults (lenses \"yellow\" with age).\n\nThere has been some speculation that sunglasses actually promote skin cancer. This is due to the eyes being tricked into producing less melanocyte-stimulating hormone in the body.\n\nThe only way to assess the protection of sunglasses is to have the lenses measured, either by the manufacturer or by a properly equipped optician. Several standards for sunglasses (see below) allow a general classification of the UV protection (but not the blue light protection), and manufacturers often indicate simply that the sunglasses meet the requirements of a specific standard rather than publish the exact figures.\n\nThe only \"visible\" quality test for sunglasses is their fit. The lenses should fit close enough to the face that only very little \"stray light\" can reach the eye from their sides, or from above or below, but not so close that the eyelashes smear the lenses. To protect against \"stray light\" from the sides, the lenses should fit close enough to the temples and/or merge into broad temple arms or leather blinders.\n\nIt is not possible to \"see\" the protection that sunglasses offer. Dark lenses do not automatically filter out more harmful UV radiation and blue light than light lenses. Inadequate dark lenses are even more harmful than inadequate light lenses (or wearing no sunglasses at all) because they provoke the pupil to open wider. As a result, more unfiltered radiation enters the eye. Depending on the manufacturing technology, sufficiently protective lenses can block much or little light, resulting in dark or light lenses. The lens color is not a guarantee either. Lenses of various colors can offer sufficient (or insufficient) UV protection. Regarding blue light, the color gives at least a first indication: Blue blocking lenses are commonly yellow or brown, whereas blue or gray lenses cannot offer the necessary blue light protection. However, not every yellow or brown lens blocks sufficient blue light. In rare cases, lenses can filter out too much blue light (i.e., 100%), which affects color vision and can be dangerous in traffic when colored signals are not properly recognized.\nhttps://commons.wikimedia.org/wiki/File%3APolariod_Sunglass.JPG\nHigh prices cannot guarantee sufficient protection as no correlation between high prices and increased UV protection has been demonstrated. A 1995 study reported that \"Expensive brands and polarizing sunglasses do not guarantee optimal UVA protection.\" The Australian Competition and Consumer Commission has also reported that \"[c]onsumers cannot rely on price as an indicator of quality\". One survey even found that a $6.95 pair of generic glasses offered slightly better protection than expensive Salvatore Ferragamo shades.\n\nWhile non-tinted glasses are very rarely worn without the practical purpose of correcting eyesight or protecting one's eyes, sunglasses have become popular for several further reasons, and are sometimes worn even indoors or at night.\n\nSunglasses can be worn to hide one's eyes. They can make eye contact impossible, which can be intimidating to those not wearing sunglasses; the avoided eye contact can also demonstrate the wearer's detachment, which is considered desirable (or \"cool\") in some circles. Eye contact can be avoided even more effectively by using mirrored sunglasses. Sunglasses can also be used to hide emotions; this can range from hiding blinking to hiding weeping and its resulting red eyes. In all cases, hiding one's eyes has implications for nonverbal communication; this is useful in poker, and many professional poker players wear heavily tinted glasses indoors while playing, so that it is more difficult for opponents to read tells which involve eye movement and thus gain an advantage.\n\nFashion trends can be another reason for wearing sunglasses, particularly designer sunglasses from high-end fashion brands. Sunglasses of particular shapes may be in vogue as a fashion accessory. The relevance of sunglasses within the fashion industry has included prominent fashion editors' reviews of annual trends in sunglasses as well as runway fashion shows featuring sunglasses as a primary or secondary component of a look. Fashion trends can also draw on the \"cool\" image of sunglasses and association with a particular lifestyle, especially the close connection between sunglasses and beach life. In some cases, this connection serves as the core concept behind an entire brand.\n\nPeople may also wear sunglasses to hide an abnormal appearance of their eyes. This can be true for people with severe visual impairment, such as the blind, who may wear sunglasses to avoid making others uncomfortable. The assumption is that it may be more comfortable for another person not to see the hidden eyes rather than see abnormal eyes or eyes which seem to look in the wrong direction. People may also wear sunglasses to hide dilated or contracted pupils, bloodshot eyes due to drug use, chronic dark circles or crow's feet, recent physical abuse (such as a black eye), exophthalmos (bulging eyes), a cataract, or eyes which jerk uncontrollably (nystagmus).\n\nLawbreakers have been known to wear sunglasses during or after committing a crime as an aid to hiding their identities.\n\nThe international standard for sunglasses is ISO 12312, which was published in 2013. Part 1 specifies the physical and optical characteristics of glasses, including a range of UV protection levels. Part 2 specifies the test methods used to validate conformance with Part 1.\nAustralia introduced the world's first national standards for sunglasses in 1971. They were updated and expanded in 1990 to \"AS 1067.1-1990 Sunglasses and fashion spectacles\" (incl. Part 1 Safety Requirements and Part 2 Performance Requirements), and replaced in 2003 by AS/NZS 1067:2003 Sunglasses and fashion spectacles. This aligned the Australian standard to the European standard opening the European market to Australian-made sunglasses. The Australian Standard AS-NZS 1067 defines standards for sunglasses with respect both to UVA (wavelengths between 315 nm and 400 nm) and UVB transmittance. The five ratings for transmittance (filter) under this standard are based on the amount of absorbed light, 0 to 4, with \"0\" providing some protection from UV radiation and sunglare, and \"4\" indicating a high level of protection, but not to be worn when driving.\n\nThe European standard EN 1836:2005 has four transmittance ratings: \"0\" for insufficient UV protection, \"2\" for sufficient UHV protection, \"6\" for good UHV protection and \"7\" for \"full\" UHVV protection, meaning that no more than 5% of the 380 nm rays are transmitted. Products which fulfill the standard receive a CE mark. There is no european rating for transmittance protection for radiation of up to 400 nm (\"UV400\"), as required in other countries (incl. the United States) and recommended by experts. The current european standard, EN 1836:2005, was preceded by the older standards EN 166:1995 (Personal eye protection - Specifications), EN167: 1995 (Personal eye protection – Optical test methods), and EN168: 1995 (Personal eye protection – Non-optical test methods), which in 2002 were republished as a revised standard under the name of EN 1836:1997 (which included two amendments). In addition to filtering, the standard also lists requirements for minimum robustness, labeling, materials (non-toxic for skin contact and not combustible) and lack of protrusions (to avoid harm when wearing them). Categories for the European standard, which are required to be marked on the frame:\n\nSunglasses sold in the United States are regulated by the Food and Drug Administration and are required to conform to safety standards. The U.S. standard is ANSI Z80.3-2001, which includes three transmittance categories. According to this standard, the lens should have a UVB (280 to 315 nm) transmittance of no more than one per cent and a UVA (315 to 380 nm) transmittance of no more than 0.3 times the visual light transmittance. The ANSI Z87.1-2003 standard includes requirements for basic impact and high impact protection. In the basic impact test, a 1 in (2.54 cm) steel ball is dropped on the lens from a height of 50 in (127 cm). In the high velocity test, a 1/4 in (6.35 mm) steel ball is shot at the lens at 150 ft/s (45.72 m/s). To pass both tests, no part of the lens may touch the eye.\n\nWhen driving a vehicle, particularly at high speed, dazzling glare caused by a low sun, or by lights reflecting off snow, puddles, other vehicles, or even the front of the vehicle, can be lethal. Sunglasses can protect against glare when driving. Two criteria must be met: vision must be clear, and the glasses must let sufficient light get to the eyes for the driving conditions. General-purpose sunglasses may be too dark, or otherwise unsuitable for driving.\n\nThe Automobile Association and the Federation of Manufacturing Opticians have produced guidance for selection of sunglasses for driving. Variable tint or photochromic lenses increase their optical density when exposed to UV light, reverting to their clear state when the UV brightness decreases. Car windscreens filter out UV light, slowing and limiting the reaction of the lenses and making them unsuitable for driving as they could become too dark or too light for the conditions. Some manufacturers produce special photochromic lenses that adapt to the varying light conditions when driving.\n\nLenses of fixed tint are graded according to the optical density of the tint; in the UK sunglasses must be labelled and show the filter category number. Lenses with light transmission less than 75% are unsuitable for night driving, and lenses with light transmission less than 8% (category 4) are unsuitable for driving at any time; they should by UK law be labelled 'Not suitable for driving and road use'. Yellow tinted lenses are also not recommended for night use. Due to the light levels within the car, filter category 2 lenses which transmit between 18% and 43% of light are recommended for daytime driving. Polarised lenses normally have a fixed tint, and can reduce reflected glare more than non-polarised lenses of the same density, particularly on wet roads.\n\nGraduated lenses, with the bottom part lighter than the top, can make it easier to see the controls within the car. All sunglasses should be marked as meeting the standard for the region where sold. An anti-reflection coating is recommended, and a hard coating to protect the lenses from scratches. Sunglasses with deep side arms can block side, or peripheral, vision and are not recommended for driving.\n\nEven though some of these glasses are proven good enough for driving at night, it is strongly recommended not to do so, due to the changes in a wide variety of light intensities, especially while using yellow tinted protection glasses. The main purpose of these glasses are to protect the wearer from dust and smog particles entering into the eyes while driving at high speeds.\n\nMany of the criteria for sunglasses worn when piloting an aircraft are similar to those for land vehicles. Protection against UV radiation is more important, as its intensity increases with altitude. Polarised glasses are undesirable as aircraft windscreens are often polarised, intentionally or unintentionally, showing Moiré patterns on looking through the windscreen; and some LCDs used by instruments emit polarised light, and can dim or disappear when the pilot turns to look at them.\n\nLike corrective glasses, sunglasses have to meet special requirements when worn for sports. They need shatterproof and impact-resistant lenses; a strap or other fixing is typically used to keep glasses in place during sporting activities, and they have a nose cushion.\n\nFor water sports, so-called water sunglasses (also: surf goggles or water eyewear) are specially adapted for use in turbulent water, such as the surf or whitewater. In addition to the features for sports glasses, water sunglasses can have increased buoyancy to stop them from sinking should they come off, and they can have a vent or other method to eliminate fogging. These sunglasses are used in water sports such as surfing, windsurfing, kiteboarding, wakeboarding, kayaking, jet skiing, bodyboarding, and water skiing.\n\nMountain climbing or traveling across glaciers or snowfields requires above-average eye protection, because sunlight (including ultraviolet radiation) is more intense in higher altitudes, and snow and ice reflect additional light. Popular glasses for this use are a type called glacier glasses or glacier goggles. They typically have very dark round lenses and leather blinders at the sides, which protect the eyes by blocking the Sun's rays around the edges of the lenses.\n\nSpecial shaded visors were once allowed in American football; Jim McMahon, quarterback for the Chicago Bears and San Diego Chargers, famously used a sun visor during his professional football career due to a childhood eye injury and almost always wears dark sunglasses when not wearing a football helmet. Darkened visors now require a doctor's prescription at most levels of the game, mainly because concussion protocol requires officials to look a player in the eye, something made difficult by tinted visors.\n\n Special protection is required for space travel because the sunlight is far more intense and harmful than on Earth, where it is always filtered through the atmosphere. Sun protection is needed against much higher UV radiation and even against harmful infrared radiation, both within and outside the spacecraft. Within the spacecraft, astronauts wear sunglasses with darker lenses and a thin protective gold coating. During space walks, the visor of the astronauts' helmets, which also has a thin gold coating for extra protection, functions as strong sunglasses. The frames of sunglasses and corrective glasses used in space must meet special requirements. They must be flexible and durable, and must fit firmly in zero-gravity. Reliable fit is particularly important when wearing corrective glasses underneath tight helmets and in space suits: once inside the spacesuit, slipped glasses cannot be touched to push them back into place, sometimes for up to ten hours. Frames and glasses must be designed so that small pieces of the glasses such as screws and glass particles cannot become dislodged, then float and be inhaled. 90% of astronauts wear glasses in space, even if they do not require corrective glasses on Earth, because zero-gravity and pressure changes temporarily affect their vision.\n\nThe first sunglasses used in a Moon landing were the original pilot sunglasses produced by American Optical. In 1969 they were used aboard the \"Eagle\", the lunar landing module of Apollo 11, the first manned mission to land on the Moon. NASA research primarily by scientists James B. Stephens and Charles G. Miller at the Jet Propulsion Laboratory (JPL) resulted in special lenses that protected against the light in space and during laser and welding work. The lenses used colored dyes and small particles of zinc oxide, which absorbs ultraviolet light and is also used in sunscreen lotions. The research was later broadened to further terrestrial applications, \"e.g.\", deserts, mountains, and fluorescent-lighted offices, and the technology was commercially marketed by a U.S. company. Since 2002 NASA uses the frame of the designer model \"Titan Minimal Art\" of the Austrian company Silhouette, combined with specially dark lenses developed jointly by the company and \"the\" NASA optometrist Keith Manuel. The frame is very light at 1.8 grams, and does not have screws or hinges that could detach.\n\n The color of the lens can vary depending on style, fashion, and purpose, but for general use, red, grey, green, or brown are recommended to avoid or minimize color distortion, which could affect safety when, for instance, driving a car or a school bus.\nWith the introduction of office computing, ergonomists may recommend mildly tinted glasses for use by display operators, in order to increase contrast.\n\nWhile some blue blocking sunglasses (see above) are produced as regular sunglasses for exposure to bright sunlight, others—especially for macular degeneration patients—do not block light or other colors in order to function well in regular daylight and even dim sunlight. The latter allow the passage of enough light so normal evening activities can continue, while blocking the light that prevents production of the hormone melatonin. Blue-blocking tinted glasses, i.e. amber or yellow, are sometimes recommended to treat insomnia; they are worn in artificial lighting after dark, to reestablish the circadian rhythm.\n\nSome models have polarized lenses, made of Polaroid polarized plastic sheeting, to reduce glare caused by light reflected from non-metallic surfaces such as water (see Brewster's angle for how this works) as well as by polarized diffuse sky radiation (skylight). This can be especially useful to see beneath the surface of the water when fishing.\n\nA mirrored coating can be applied to the lens. This mirrored coating deflects some of the light when it hits the lens so that it is not transmitted through the lens, making it useful in bright conditions; however, it does not necessarily reflect UV radiation as well. Mirrored coatings can be made any color by the manufacturer for styling and fashion purposes. The color of the mirrored surface is irrelevant to the color of the lens. For example, a gray lens can have a blue mirror coating, and a brown lens can have a silver coating. Sunglasses of this type are sometimes called mirrorshades. A mirror coating does not get hot in sunlight and it prevents scattering of rays in the lens bulk.\n\nSunglass lenses are made of either glass, plastic, or SR-91. Plastic lenses are typically made from acrylic, polycarbonate, CR-39 or polyurethane. Glass lenses have the best optical clarity and scratch resistance, but are heavier than plastic lenses. They can also shatter or break on impact. Plastic lenses are lighter and shatter-resistant, but are more prone to scratching. Polycarbonate plastic lenses are the lightest, and are also almost shatterproof, making them good for impact protection. CR-39 is the most common plastic lens, due to low weight, high scratch resistance, and low transparency for ultraviolet and infrared radiation. SR-91 is a proprietary material that was introduced by Kaenon Polarized in 2001. Kaenon's lens formulation was the first non-polycarbonate material to pass the high-mass impact ANSI Z.87.1 testing. Additionally, it was the first to combine this passing score with the highest marks for lens clarity. Jerry Garcia's sunglasses had a polykrypton-C type of lens which was 'cutting edge' in 1995.\n\nAny of the above features, color, polarization, gradation, mirroring, and materials, can be combined into the lens for a pair of sunglasses. Gradient glasses are darker at the top of the lens where the sky is viewed and transparent at the bottom. Corrective lenses or glasses can be manufactured with either tinting or darkened to serve as sunglasses. An alternative is to use the corrective glasses with a secondary lenses such as oversize sunglasses that fit over the regular glasses, clip-on lens that are placed in front of the glasses, and flip-up glasses which feature a dark lens that can be flipped up when not in use (see below). Photochromic lenses gradually darken when exposed to ultraviolet light.\n\n Frames are generally made of plastic, nylon, a metal or a metal alloy. Nylon frames are usually used in sports because they are lightweight and flexible. They are able to bend slightly and return to their original shape instead of breaking when pressure is applied to them. This flex can also help the glasses grip better on the wearer's face. Metal frames are usually more rigid than nylon frames, thus they can be more easily damaged when the wearer participates in sport activities, but this is not to say that they cannot be used for such activities. Because metal frames are more rigid, some models have spring loaded hinges to help them grip the wearer's face better. The end of the resting hook and the bridge over the nose can be textured or have rubber or plastic material to improve hold. The ends of the resting hook are usually curved so that they wrap around the ear; however, some models have straight resting hooks. Oakley, for example, has straight resting hooks on all their glasses, preferring to call them \"earstems\".\n\nIn recent years, manufacturers have started to use various types of woods to make frames for sunglasses. Materials such as bamboo, ebony, rosewood, pear wood, walnut and zebrawood, are used making them non-toxic and nearly allergy free. The construction of a wooden frame involves laser-cutting from planks of wood. Already cut and ground to a uniform size, a buffing wheel is used to sand and buff every piece separately before they are assembled. The laser-cutouts of wood are then glued together by hand (mostly), layer on layer, to produce wooden frames. Some brands have experimented with recycled wood from objects like skateboards, whiskey barrels and baseball bats. Shwood, for example have experimented with these materials, they have even used recycled newspaper to manufacture frames.\n\nTheir final look can vary according to the color, type and finishing. With wooden sunglasses, various shades of brown, beige, burgundy or black are most common. Wooden sunglasses come in various designs and shapes. However, these sunglasses are usually more expensive than the conventional plastic, acetate or metal frames and require more care. They have been famously worn by the likes of Beyoncé, Snoop Dogg and Machine Gun Kelly.\n\nFrames can be made to hold the lenses in several different ways. There are three common styles: full frame, half frame, and frameless. Full frame glasses have the frame go all around the lenses. Half frames go around only half the lens; typically the frames attach to the top of the lenses and on the side near the top. Frameless glasses have no frame around the lenses and the ear stems are attached directly to the lenses. There are two styles of frameless glasses: those that have a piece of frame material connecting the two lenses, and those that are a single lens with ear stems on each side.\n\nSome sports-optimized sunglasses have interchangeable lens options. Lenses can be easily removed and swapped for a different lens, usually of a different color. The purpose is to allow the wearer to easily change lenses when light conditions or activities change. The reasons are that the cost of a set of lenses is less than the cost of a separate pair of glasses, and carrying extra lenses is less bulky than carrying multiple pairs of glasses. It also allows easy replacement of a set of lenses if they are damaged. The most common type of sunglasses with interchangeable lenses has a single lens or shield that covers both eyes. Styles that use two lenses also exist, but are less common.\n\nNose bridges provide support between the lens and the face. They also prevent pressure marks caused by the weight of the lens or frame on the cheeks. People with large noses may need a low nose bridge on their sunglasses. People with medium noses may need a low or medium nose bridge. People with small noses may need sunglasses with high nose bridges to allow clearance.\n\nThe following types are not all mutually exclusive; glasses may be in Aviator style with mirrored lenses, for example.\n\nAviator sunglasses feature oversize teardrop-shaped lenses and a thin metal frame. The design was introduced in 1936 by Bausch & Lomb for issue to U.S. military aviators. As a fashion statement, \"aviator\" sunglasses are often made in mirrored, colored, and wrap-around styles.\n\nThe model first gained popularity in the 1940s when Douglas MacArthur was seen sporting a pair at the Pacific Theatre. However, it was in the late 1960s when the frames became widely used with the rise of the hippie counterculture, which preferred large metallic sunglasses. The brand became an icon of the 1970s, worn by Paul McCartney and Freddie Mercury among others, and was also used as prescription eyeglasses. Aviators' association with disco culture led to a decline in their popularity by 1980. The model saw more limited use throughout the 1980s and 1990s, aided by a 1982 product placement deal, featured most notably in \"Top Gun\" and \"Cobra\", with both films causing a 40% rise in 1986. Aviators became popular again around 2000, as the hippie movement experienced a brief revival, and was prominently featured in the MTV show \"Jackass\".\n\nBased on the eyeglass design of the same name, browline glasses have hard plastic or horn-rimmed arms and upper portions joined to a wire lower frame. A traditional, conservative style based on mid-20th century design, browlines were adapted into sunglasses form in the 1980s and rapidly became one of the most popular styles; it has ebbed and sprung in popularity in the decades that have followed.\n\nOversized sunglasses, which were fashionable in the 1980s, are now often used for humorous purposes. They usually come in bright colors with colored lenses and can be purchased cheaply.\n\nThe singer Elton John sometimes wore oversized sunglasses on stage in the mid-1970s as part of his Captain Fantastic act.\n\nSince the late 2000s, moderately oversized sunglasses have become a fashion trend. There are many variations, such as the \"Onassis\", discussed below, and Dior white sunglasses.\n\nOnassis glasses or \"Jackie O's\" are very large sunglasses worn by women. This style of sunglasses is said to mimic the kind most famously worn by Jacqueline Kennedy Onassis in the 1960s. The glasses continue to be popular with women, and celebrities may use them, ostensibly to hide from paparazzi.\n\nOversized sunglasses, because of their larger frames and lenses, are useful for individuals who are trying to minimize the apparent size or arch of their nose. Oversized sunglasses also offer more protection from sunburn due to the larger areas of skin they cover, although sunblock should still be used.\n\nShutter shades were invented in the late 1940s, became a fad in the early 1980s and have experienced a revival in the early-to-mid 2010s. Instead of tinted lenses, they decrease sun exposure by means of a set of parallel, horizontal shutters (like a small window shutter). Analogous to Inuit goggles (see above), the principle is not to filter light, but to decrease the amount of sun rays falling into the wearer's eyes. To provide UV protection, shutter shades sometimes use lenses in addition to the shutters; if not, they provide very insufficient protection against ultraviolet radiation and blue light.\n\n\"Teashades\" (sometimes also called \"John Lennon glasses\", \"Round Metal\", or, occasionally, \"Granny Glasses\") were a type of psychedelic art wire-rim sunglasses that were often worn, usually for purely aesthetic reasons, by members of the 1960s counterculture. Pop icons such as Mick Jagger, Roger Daltrey, John Lennon, Jerry Garcia, Boy George, Liam Gallagher, Suggs, Ozzy Osbourne, Duckie (Jon Cryer) in \"Pretty in Pink\" and Jodie Foster's character in the film \"Taxi Driver\" all wore teashades. The original teashade design was made up of medium-sized, perfectly round lenses, supported by pads on the bridge of the nose and a thin wire frame. When teashades became popular in the late 1960s, they were often elaborated: Lenses were elaborately colored, mirrored, and produced in excessively large sizes, and with the wire earpieces exaggerated. A uniquely colored or darkened glass lens was usually preferred. Modern versions tend to have plastic lenses, as do many other sunglasses. Teashades are hard to find in shops today; however, they can still be found at many costume Web sites and in some countries.\n\nThe term has now fallen into disuse, although references can still be found in literature of the time. \"Teashades\" was also used to describe glasses worn to hide the effects of recreational drugs such as marijuana (conjunctival injection) or heroin (pupillary constriction) or just bloodshot eyes.\n\nThe Ray-Ban Wayfarer is a (mostly) plastic-framed design for sunglasses produced by the Ray-Ban company. Introduced in 1952, the trapezoidal lenses are wider at the top than the bottom (inspired by the Browline eyeglasses popular at the time), and were famously worn by James Dean, Roy Orbison, Elvis Presley, Bob Marley, The Beatles and other actors and singers. The original frames were black; frames in many different colors were later introduced. There is often a silver piece on the corners as well. Since the early 1980s, makers have also developed variants of the model, most notably the Clubmaster model, introduced in 1982, essentially Browlines made of plastic.\n\nThese were mostly popular in the late 1950s and during the 1960s (being linked to the rock-and-roll/blues and Mod cultures), before plastic glasses were displaced by metallic rims popular among the counter-culture. In the late 1970s, the rise of New wave music, New Romanticism and the popularity of The Blues Brothers aside from 50s and 60s nostalgia and the anti-disco backlash later on brought the model out of near-retirement, becoming the most sold model between 1980 and 1999 aided by a lucrative 1982 product placement deal, which put it on many movies and TV shows such as \"The Breakfast Club\" and \"Moonlighting\". 1980s nostalgia and the influence of the hipster subculture and the television series \"Mad Men\" boosted Wayfarers once again after a slump in the 1990s and 2000s, also aided by a 2000 redesign (\"New Wayfarer\"), surpassing Aviators since 2012.\n\nWrap-arounds are a style of sunglasses characterized by being strongly curved, to wrap around the face. They may have a single curved semi-circular lens that covers both eyes and much of the same area of the face covered by protective goggles, usually with a minimal plastic frame and single piece of plastic serving as a nosepiece. Glasses described as wraparound may alternatively have two lenses, but again with a strongly curved frame.\n\nThese were first made in the 1960s as variants of the Aviator model, used by Yoko Ono and Clint Eastwood in the \"Dirty Harry\" films. The modern variant surged in the mid-1980s, heavily based on the then-popular Wayfarer, but adapting it to a more futuristic look. As a backlash against 80s fashion occurred in the 1990s, wraparounds became one of the favorite frames of the decade.\n\nClip-on glasses are a form of tinted glasses that can be clipped on to eyeglasses for protection from the Sun. An alternative are flip-up glasses.\n\nGradient lenses go from a darker shade at the top to a lighter one at the bottom, so there will be more protection from sunlight the higher one looks through the lens, but the lower one looks through the lens, the less protection is offered. An advantage is that one can wear them indoors without fear of tripping over something and also allowing the user to see. Wearing sunglasses to nightclubs has become common in recent times, where the gradient lens comes in handy. Gradient lenses may also be advantageous for activities such as flying airplanes and driving automobiles, as they allow the operator a clear view of the instrument panel, low in his line of sight and usually hidden in shadow, while still reducing glare from the view out the windscreen. The Independent (London), has also referred to these style of sunglasses as the Murphy Lens.\n\nDouble gradient lenses are dark at the top, light in the middle and dark at the bottom.\n\nGradients should not be confused with bifocals and progressive lenses.\n\nFlip-up sunglasses add the benefits of sunglasses to corrective eyeglasses, allowing the wearer to flip up the tinted lenses for indoor use. An alternative are clip-on glasses.\n\nMirrored lenses have a metallic, partially reflective coating on the outer surface combined with a tinted glass lens. Mirrored lenses of different colors can expand the range of fashion styles.\n\nThere are various words referring to eyepieces with darkened lenses:\n\n\n\n",
                "Glasses\n\nGlasses, also known as eyeglasses or spectacles, are devices consisting of glass or hard plastic lenses mounted in a frame that holds them in front of a person's eyes, typically using a bridge over the nose and arms which rest over the ears. Glasses are typically used for vision correction, such as with reading glasses and glasses used for nearsightedness. Safety glasses provide eye protection against flying debris for construction workers or lab technicians; these glasses may have protection for the sides of the eyes as well as in the lenses. Some types of safety glasses are used to protect against visible and near-visible light or radiation. Glasses are worn for eye protection in some sports, such as squash. Glasses wearers may use a strap to prevent the glasses from falling off during movement or sports. Wearers of glasses that are used only part of the time may have the glasses attached to a cord that goes around their neck, to prevent the loss of the glasses.\n\nSunglasses allow better vision in bright daylight, and may protect one's eyes against damage from high levels of ultraviolet light. Typical sunglasses are darkened for protection against bright light or glare; some specialized glasses are clear in dark or indoor conditions, but turn into sunglasses when in bright light. Most sunglasses do not have corrective power in the lenses; however, special prescription sunglasses can be ordered. Specialized glasses may be used for viewing specific visual information (such as stereoscopy) or 3D glasses for viewing three-dimensional movies. Sometimes glasses with no corrective power in the lenses are worn simply for aesthetic or fashion purposes. Even with glasses used for vision correction, a wide range of designs are available for fashion purposes, using plastic, wire, and other materials.\n\nPeople are more likely to need glasses the older they get with 93% of people between the age of 65-75 wearing corrective lenses.\nGlasses come in many styles. They can be marked or found by their primary function, but also appear in combinations such as prescription sunglasses or safety glasses with enhanced magnification.\n\nCorrective lenses are used to correct refractive errors by bending the light entering the eye in order to alleviate the effects of conditions such as nearsightedness (myopia), farsightedness (hypermetropia) or astigmatism. The ability of one's eyes to accommodate their focus to near and distant focus alters over time. Also, few people have eyes that show exactly equal refractive characteristics; one may need a \"stronger\", (i.e. more refracting), lens than the other. A common condition in people over forty years old is presbyopia, which is caused by the eye's crystalline lens losing elasticity, progressively reducing the ability of the lens to accommodate (i.e. to focus on objects close to the eye). Corrective lenses, to bring the image back into focus on the retina, are made to conform to the prescription of an ophthalmologist or optometrist. A lensmeter can be used to verify the specifications of an existing pair of glasses. Corrective eyeglasses can significantly improve the life quality of the wearer. Not only do they enhance the wearer's visual experience, but can also reduce problems that result from eye strain, such as headaches or squinting.\n\nPinhole glasses are a type of corrective glasses that do not use a lens. Pinhole glasses do not actually refract the light or change focal length. Instead, they create a diffraction limited system, which has an increased depth of field, similar to using a small aperture in photography. This form of correction has many limitations that prevent it from gaining popularity in everyday use. Pinhole glasses can be made in a DIY fashion by making small holes in a piece of card which is then held in front of the eyes with a strap or cardboard arms.\n\nThe most common type of corrective lens is \"single vision\", which has a uniform refractive index. For people with presbyopia and hyperopia, bifocal and trifocal glasses provide two or three different refractive indices, respectively, and progressive lenses have a continuous gradient. Reading glasses provide a separate set of glasses for focusing on close-by objects. Reading glasses are available without prescription from drugstores, and offer a cheap, practical solution, though these have two simple lenses of equal power, so will not correct refraction problems like astigmatism or refractive or prismatic variations between the left and right eye. For total correction of the individual's sight, glasses complying to a recent ophthalmic prescription are required. Adjustable-focus eyeglasses might be used to replace bifocals or trifocals, or might be used to produce cheaper single-vision glasses (since they don't have to be custom-manufactured for every person).\n\nSafety glasses are worn to protect the eyes in different situations. They are made with break-proof plastic lenses to protect the eye from flying debris or other matter. Construction workers, factory workers, machinists and lab technicians are often required to wear safety glasses to shield the eyes from flying debris or hazardous splatters such as blood or chemicals. As of 2017, dentists and surgeons in Canada and other countries are required to wear safety glasses to protect against infection from patients' blood or other body fluids. There are also safety glasses for welding, which are styled like wraparound sunglasses, but with much darker lenses, for use in welding where a full sized welding helmet is inconvenient or uncomfortable. These are often called \"flash goggles\", because they provide protection from welding flash. Nylon frames are usually used for protection eyewear for sports because of their lightweight and flexible properties. Unlike most regular glasses, safety glasses often include protection beside the eyes as well as in front of the eyes.\n\nSunglasses provide improved comfort and protection against bright light and often against ultraviolet (UV) light. Photochromic lenses, which are photosensitive, darken when struck by UV light. The dark tint of the lenses in a pair of sunglasses blocks the transmission of light through the lens.\n\nLight polarization is an added feature that can be applied to sunglass lenses. Polarization filters are positioned to remove horizontally polarized rays of light, which eliminates glare from horizontal surfaces (allowing wearers to see into water when reflected light would otherwise overwhelm the scene). Polarized sunglasses may present some difficulties for pilots since reflections from water and other structures often used to gauge altitude may be removed. Liquid-crystal displays often emit polarized light making them sometimes difficult to view with polarized sunglasses. Sunglasses may be worn just for aesthetic purposes, or simply to hide the eyes. Examples of sunglasses that were popular for these reasons include teashades and mirrorshades. Many blind people wear nearly opaque glasses to hide their eyes for cosmetic reasons.\n\nSunglasses may also have corrective lenses, which requires a prescription. Clip-on sunglasses or sunglass clips can be attached to another pair of glasses. Some wrap-around sunglasses are large enough to be worn over top of another pair of glasses. Otherwise, many people opt to wear contact lenses to correct their vision so that standard sunglasses can be used.\n\nThe illusion of three dimensions on a two dimensional surface can be created by providing each eye with different visual information. 3D glasses create the illusion of three dimensions by filtering a signal containing information for both eyes. The signal, often light reflected off a movie screen or emitted from an electronic display, is filtered so that each eye receives a slightly different image. The filters only work for the type of signal they were designed for.\n\nAnaglyph 3D glasses have a different colored filter for each eye, typically red and blue or red and green. A polarized 3D system on the other hand uses polarized filters. Polarized 3D glasses allow for color 3D, while the red-blue lenses produce an image with distorted coloration. An active shutter 3D system uses electronic shutters. Head-mounted displays can filter the signal electronically and then transmit light directly into the viewers eyes.\n\nAnaglyph and polarized glasses are distributed to audiences at 3D movies. Polarized and active shutter glasses are used with many home theaters. Head-mounted displays are used by a single person, but the input signal can be shared between multiple units.\n\nGlasses can also provide magnification that is useful for people with vision impairments or specific occupational demands. An example would be \"bioptics\" or \"bioptic telescopes\" which have small telescopes mounted on, in, or behind their regular lenses. Newer designs use smaller lightweight telescopes, which can be embedded into the corrective glass and improve aesthetic appearance (mini telescopic spectacles). They may take the form of self-contained glasses that resemble goggles or binoculars, or may be attached to existing glasses.\n\nYellow tinted glasses are a type of glasses with a minor yellow tint. They perform minor color correction, on top of reducing headaches due to lack of blinking. They may also be considered minor corrective unprescribed glasses. Depending on the company, these computer or gaming glasses can also filter out high energy blue and ultra-violet light from LCD screens, fluorescent lighting, and other sources of light. This allows for reduced eye-strain. These glasses can be ordered as standard or prescription lenses that fit into standard optical frames. Due to the ultra-violet light blocking nature of these lenses, they also help users sleep at night along with reducing age-related macular degeneration.\n\nAnti-glare protection glasses, or anti-reflective glasses, can reduce the reflection of light that enters our eyes. The lenses are given an anti-glare coating to prevent reflections of light under different lighting conditions. By reducing the amount of glare on your eyes, vision can be improved. The anti-glare also applies to the outer glass, thus allowing for better eye contact.\n\nThe ophthalmic frame is the part of a pair of glasses which is designed to hold the lenses in proper position.\nOphthalmic frames come in a variety of styles, sizes, materials, shapes, and colors.\n\n\n\n\n\n\nCorrective lenses can be produced in many different shapes from a circular lens called a lens blank. Lens blanks are cut to fit the shape of the frame that will hold them. Frame styles vary and fashion trends change over time, resulting in a multitude of lens shapes. For lower power lenses, there are few restrictions which allows for many trendy and fashionable shapes. Higher power lenses can cause distortion of peripheral vision and may become thick and heavy if a large lens shape is used. However, if the lens becomes too small, the field of view can be drastically reduced. \n\nBifocal, trifocal, and progressive lenses generally require a taller lens shape to leave room for the different segments while preserving an adequate field of view through each segment. Frames with rounded edges are the most efficient for correcting myopic prescriptions, with perfectly round frames being the most efficient. Before the advent of eyeglasses as a fashion item, when frames were constructed with only functionality in mind, virtually all eyeglasses were either round, oval, or curved octagons. It was not until glasses began to be seen as an accessory that different shapes were introduced to be more aesthetically pleasing than functional.\n\nScattered evidence exists for use of vision aid devices in Greek and Roman times, most prominently the use of an emerald by emperor Nero as mentioned by Pliny the Elder.\n\nThe use of a convex lens to form an enlarged/magnified image was most likely described in Ptolemy's \"Optics\" (which however only survives in a poor Arabic translation). Ptolemy's description of lenses was commented upon and improved by Ibn Sahl (10th century) and most notably by Alhazen (\"Book of Optics\", ca. 1021). Latin translations of Ptolemy's \"Optics\" and of Alhazen became available in Europe in the 12th century, coinciding with the development of \"reading stones\".\n\nRobert Grosseteste's treatise \"De iride\" (\"On the Rainbow\"), written between 1220 and 1235, mentions using optics to \"read the smallest letters at incredible distances\". A few years later in 1262, Roger Bacon is also known to have written on the magnifying properties of lenses.\nThe development of the first eyeglasses took place in Northern Italy in the second half of the 13th century. \n\nIndependently of the development of optical lenses, some cultures developed \"sunglasses\" for eye protection, without any corrective properties.\nThus, flat panes of smoky quartz, were used in 12th-century China. Similarly, the Inuit have used snow goggles for eye protection.\n\nThe first eyeglasses were made in Northern Italy, most likely in Pisa, by about 1290:\nIn a sermon delivered on 23 February 1306, the Dominican friar Giordano da Pisa (ca. 1255–1311) wrote \"It is not yet twenty years since there was found the art of making eyeglasses, which make for good vision... And it is so short a time that this new art, never before extant, was discovered. ... I saw the one who first discovered and practiced it, and I talked to him.\" \n\nGiordano's colleague Friar Alessandro della Spina of Pisa (d. 1313) was soon making eyeglasses. The \"Ancient Chronicle of the Dominican Monastery of St. Catherine in Pisa\" records: \"Eyeglasses, having first been made by someone else, who was unwilling to share them, he [Spina] made them and shared them with everyone with a cheerful and willing heart.\" By 1301, there were guild regulations in Venice governing the sale of eyeglasses.\nThe earliest pictorial evidence for the use of eyeglasses is Tommaso da Modena's 1352 portrait of the cardinal Hugh de Provence reading in a scriptorium. Another early example would be a depiction of eyeglasses found north of the Alps in an altarpiece of the church of Bad Wildungen, Germany, in 1403. These early glasses had convex lenses that could correct both hyperopia (farsightedness), and the presbyopia that commonly develops as a symptom of aging. It was not until 1604 that Johannes Kepler published the first correct explanation as to why convex and concave lenses could correct presbyopia and myopia.\n\nEarly frames for glasses consisted of two magnifying glasses riveted together by the handles so that they could grip the nose. These are referred to as \"rivet spectacles\". The earliest surviving examples were found under the floorboards at Kloster Wienhausen, a convent near Celle in Germany; they have been dated to \"circa\" 1400.\n\nClaims that Salvino degli Armati of Florence invented eyeglasses have been exposed as hoaxes. \n\nMarco Polo is sometimes claimed to have encountered eyeglasses during his travels in China in the 13th century, however, no such statement appears in his accounts. Indeed, the earliest mentions of eyeglasses in China occur in the 15th century and those Chinese sources state that eyeglasses were imported.\n\nIt is also sometimes claimed that glasses were first invented in India. In 1907 Professor Berthold Laufer stated in his history of glasses that \"\"the opinion that spectacles originated in India is of the greatest probability and that spectacles must have been known in India earlier than in Europe\"\". However, Joseph Needham showed that the mention of glasses in the manuscript Laufer used to justify the prior invention of them in Asia did not exist in older versions of that manuscript, and the reference to them in later versions was added during the Ming dynasty. In an 1971 article in the British Journal of Ophthalmology it was argued that it: \"\"...is therefore most likely that the use of lenses reached Europe via the Arabs, as did Hindu mathematics and the ophthalmological works of the ancient Hindu surgeon Susruta\"\", but all dates given are well after the existence of eyeglasses in Italy was established, and there had been significant shipments of eye glasses from Italy to the Middle East, with one shipment as large as 24,000 glasses.\n\nThe American scientist Benjamin Franklin, who suffered from both myopia and presbyopia, invented bifocals. Serious historians have from time to time produced evidence to suggest that others may have preceded him in the invention; however, a correspondence between George Whatley and John Fenno, editor of \"The Gazette of the United States\", suggested that Franklin had indeed invented bifocals, and perhaps 50 years earlier than had been originally thought. The first lenses for correcting astigmatism were designed by the British astronomer George Airy in 1825.\n\nOver time, the construction of frames for glasses also evolved. Early eyepieces were designed to be either held in place by hand or by exerting pressure on the nose (\"pince-nez\"). Girolamo Savonarola suggested that eyepieces could be held in place by a ribbon passed over the wearer's head, this in turn secured by the weight of a hat. The modern style of glasses, held by temples passing over the ears, was developed some time before 1727, possibly by the British optician Edward Scarlett. These designs were not immediately successful, however, and various styles with attached handles such as \"scissors-glasses\" and lorgnettes were also fashionable from the second half of the 18th century and into the early 19th century.\n\nIn the early 20th century, Moritz von Rohr and Zeiss (with the assistance of H. Boegehold and A. Sonnefeld), developed the Zeiss Punktal spherical point-focus lenses that dominated the eyeglass lens field for many years. In 2008, Joshua Silver designed eyewear with adjustable corrective glasses. They work by silicone liquid, a syringe, and a pressure mechanism.\n\nDespite the increasing popularity of contact lenses and laser corrective eye surgery, glasses remain very common, as their technology has improved. For instance, it is now possible to purchase frames made of special memory metal alloys that return to their correct shape after being bent. Other frames have spring-loaded hinges. Either of these designs offers dramatically better ability to withstand the stresses of daily wear and the occasional accident. Modern frames are also often made from strong, light-weight materials such as titanium alloys, which were not available in earlier times.\n\nIn the 1930s, \"spectacles\" were described as \"medical appliances.\" Wearing spectacles was sometimes considered socially humiliating. \nIn the 1970s, fashionable glasses started to become available through manufacturers, and the government also recognized the demand for stylized eyewear.\n\nGraham Pullin describes how devices for disability, like glasses, have traditionally been designed to camouflage against the skin and restore ability without being visible. In the past, design for disability has \"been less about projecting a positive image as about trying not to project an image at all.\" Pullin uses the example of spectacles, traditionally categorized as a medical device for \"patients\", and outlines how they are now described as eyewear: a fashionable accessory. Much like other fashion designs and accessories, eyewear is created by designers, has reputable labels, and comes in collections, by season and designer. It is becoming more common for consumers purchase eyewear with clear, non-prescription lenses, illustrating that glasses are no longer a social stigma, but a fashionable accessory that \"frames your face.\"\n\nSome organizations like Lions Clubs International, Unite For Sight, ReSpectacle, and New Eyes for the Needy provide a way to donate glasses and sunglasses. Unite For Sight has redistributed more than 200,000 pairs.\n\nMany people require glasses for the reasons listed above. There are many shapes, colors, and materials that can be used when designing frames and lenses that can be utilized in various combinations. Oftentimes, the selection of a frame is made based on how it will affect the appearance of the wearer. Some people with good natural eyesight like to wear eyeglasses as a style accessory.\n\nFor most of their history, eyeglasses were seen as unfashionable, and carried several potentially negative connotations: wearing glasses caused individuals to be stigmatized and stereotyped as pious clergymen (as those in religious vocation were the most likely to be literate and therefore the most likely to need reading glasses), elderly, or physically weak and passive. The stigma began to fall away in the early 1900s when the popular Theodore Roosevelt was regularly photographed wearing eyeglasses, and in the 1910s when popular comedian Harold Lloyd began wearing a pair of horn-rimmed glasses as the \"Glasses\" character in his films.\nSince, eyeglasses have become an acceptable fashion item and often act as a key component in individuals' personal image. Musicians Buddy Holly and John Lennon became synonymous with the styles of eye-glasses they wore to the point that thick, black horn-rimmed glasses are often called \"Buddy Holly glasses\" and perfectly round metal eyeglass frames called \"John Lennon (or Harry Potter) Glasses.\" British comedic actor Eric Sykes was known in the United Kingdom for wearing thick, square, horn-rimmed glasses, which were in fact a sophisticated hearing aid that alleviated his deafness by allowing him to \"hear\" vibrations. Some celebrities have become so associated with their eyeglasses that they continued to wear them even after taking alternate measures against vision problems: United States Senator Barry Goldwater and comedian Drew Carey continued to wear non-prescription glasses after being fitted for contacts and getting laser eye surgery, respectively.\n\nOther celebrities have used glasses to differentiate themselves from the characters they play, such as Anne Kirkbride, who wore oversized, 1980s-style round horn-rimmed glasses as Deirdre Barlow in the soap opera \"Coronation Street\", and Masaharu Morimoto, who wears glasses to separate his professional persona as a chef from his stage persona as Iron Chef Japanese. In 2012 some NBA players wear lensless glasses with thick plastic frames like horn-rimmed glasses during post-game interviews, geek chic that draws comparisons to Steve Urkel.\n\nIn superhero fiction, eyeglasses have become a standard component of various heroes' disguises (as masks), allowing them to adopt a nondescript demeanor when they are not in their superhero persona: Superman is well known for wearing 1950s style horn-rimmed glasses as Clark Kent, while Wonder Woman wears either round, Harold Lloyd style glasses or 1970s style bug-eye glasses as Diana Prince. An example of the halo effect is seen in the stereotype that those who wear glasses are intelligent.\n\nIn the 20th century, eyeglasses came to be considered a component of fashion; as such, various different styles have come in and out of popularity. Most are still in regular use, albeit with varying degrees of frequency.\n\n\n",
                "Ultraviolet\n\nUltraviolet (UV) is an electromagnetic radiation with a wavelength from 10 nm to 400 nm, shorter than that of visible light but longer than X-rays. UV radiation constitutes about 10% of the total light output of the Sun, and is thus present in sunlight. It is also produced by electric arcs and specialized lights, such as mercury-vapor lamps, tanning lamps, and black lights. Although it's not considered an ionizing radiation because its photons lack the energy to ionize atoms, long-wavelength ultraviolet radiation can cause chemical reactions and causes many substances to glow or fluoresce. Consequently, the chemical and biological effects of UV are greater than simple heating effects, and many practical applications of UV radiation derive from its interactions with organic molecules.\n\nSuntan, freckling and sunburn are familiar effects of over-exposure, along with higher risk of skin cancer. Living things on dry land would be severely damaged by ultraviolet radiation from the Sun if most of it were not filtered out by the Earth's atmosphere. More-energetic, shorter-wavelength \"extreme\" UV below 121 nm ionizes air so strongly that it is absorbed before it reaches the ground. \nUltraviolet is also responsible for the formation of bone-strengthening vitamin D in most land vertebrates, including humans. The UV spectrum thus has effects both beneficial and harmful to human health.\n\nUltraviolet rays are invisible to most humans, although insects, birds, and some mammals can see near-UV.\n\nUltraviolet rays are invisible to most humans: the lens in a human eye ordinarily filters out UVB frequencies or higher, and humans lack color receptor adaptations for ultraviolet rays. Under some conditions, children and young adults can see ultraviolet down to wavelengths of about 310 nm, and people with aphakia (missing lens) or replacement lens can also see some UV wavelengths. Near-UV radiation is visible to insects, some mammals, and birds. Small birds have a fourth color receptor for ultraviolet rays; this gives birds \"true\" UV vision.\n\n\"Ultraviolet\" means \"beyond violet\" (from Latin \"ultra\", \"beyond\"), violet being the color of the highest frequencies of visible light. Ultraviolet has a higher frequency than violet light.\n\nUV radiation was discovered in 1801 when the German physicist Johann Wilhelm Ritter observed that invisible rays just beyond the violet end of the visible spectrum darkened silver chloride-soaked paper more quickly than violet light itself. He called them \"oxidizing rays\" to emphasize chemical reactivity and to distinguish them from \"heat rays\", discovered the previous year at the other end of the visible spectrum. The simpler term \"chemical rays\" was adopted shortly thereafter, and it remained popular throughout the 19th century, although there were those who held that these were an entirely different sort of radiation from light (notably John William Draper, who named them \"tithonic rays\"). The terms chemical rays and heat rays were eventually dropped in favor of ultraviolet and infrared radiation, respectively.\nIn 1878 the sterilizing effect of short-wavelength light by killing bacteria was discovered. By 1903 it was known the most effective wavelengths were around 250 nm. In 1960, the effect of ultraviolet radiation on DNA was established.\n\nThe discovery of the ultraviolet radiation with wavelengths below 200 nm, named \"vacuum ultraviolet\" because it is strongly absorbed by the air, was made in 1893 by the German physicist Victor Schumann.\n\nThe electromagnetic spectrum of ultraviolet radiation (UVR), defined most broadly as 10–400 nanometers, can be subdivided into a number of ranges recommended by the ISO standard ISO-21348:\n\nA variety of solid-state and vacuum devices have been explored for use in different parts of the UV spectrum. Many approaches seek to adapt visible light-sensing devices, but these can suffer from unwanted response to visible light and various instabilities. Ultraviolet can be detected by suitable photodiodes and photocathodes, which can be tailored to be sensitive to different parts of the UV spectrum. Sensitive ultraviolet photomultipliers are available. Spectrometers and radiometers are made for measurement of UV radiation. Silicon detectors are used across the spectrum.\n\nPeople cannot perceive UV directly, since the lens of the human eye blocks most radiation in the wavelength range of 300–400 nm; shorter wavelengths are blocked by the cornea. Nevertheless, the photoreceptors of the retina are sensitive to near-UV, and people lacking a lens (a condition known as aphakia) perceive near-UV as whitish-blue or whitish-violet.\n\nVacuum UV, or VUV, wavelengths (shorter than 200 nm) are strongly absorbed by molecular oxygen in the air, though the longer wavelengths of about 150–200 nm can propagate through nitrogen. Scientific instruments can therefore utilize this spectral range by operating in an oxygen-free atmosphere (commonly pure nitrogen), without the need for costly vacuum chambers. Significant examples include 193 nm photolithography equipment (for semiconductor manufacturing) and circular dichroism spectrometers.\n\nTechnology for VUV instrumentation was largely driven by solar astronomy for many decades. While optics can be used to remove unwanted visible light that contaminates the VUV, in general, detectors can be limited by their response to non-VUV radiation, and the development of \"solar-blind\" devices has been an important area of research. Wide-gap solid-state devices or vacuum devices with high-cutoff photocathodes can be attractive compared to silicon diodes.\n\nExtreme UV (EUV or sometimes XUV) is characterized by a transition in the physics of interaction with matter. Wavelengths longer than about 30 nm interact mainly with the outer valence electrons of atoms, while wavelengths shorter than that interact mainly with inner-shell electrons and nuclei. The long end of the EUV spectrum is set by a prominent He spectral line at 30.4 nm. EUV is strongly absorbed by most known materials, but it is possible to synthesize multilayer optics that reflect up to about 50 percent of EUV radiation at normal incidence. This technology was pioneered by the NIXT and MSSTA sounding rockets in the 1990s, and it has been used to make telescopes for solar imaging. See also the Extreme Ultraviolet Explorer (EUVE) satellite.\n\nVery hot objects emit UV radiation (see black-body radiation). The Sun emits ultraviolet radiation at all wavelengths, including the extreme ultraviolet where it crosses into X-rays at 10 nm. Extremely hot stars emit proportionally more UV radiation than the Sun. Sunlight in space at the top of Earth's atmosphere (see solar constant) is composed of about 50% infrared light, 40% visible light, and 10% ultraviolet light, for a total intensity of about 1400 W/m in vacuum.\n\nHowever, at ground level sunlight is 44% visible light, 3% ultraviolet (with the Sun at its zenith), and the remainder infrared. Thus, the atmosphere blocks about 77% of the Sun's UV, almost entirely in the shorter UV wavelengths, when the Sun is highest in the sky (zenith). Of the ultraviolet radiation that reaches the Earth's surface, more than 95% is the longer wavelengths of UVA, with the small remainder UVB. There is essentially no UVC. The fraction of UVB which remains in UV radiation after passing through the atmosphere is heavily dependent on cloud cover and atmospheric conditions. Thick clouds block UVB effectively, but in \"partly cloudy\" days, patches of blue sky showing between clouds are also sources of (scattered) UVA and UVB, which are produced by Rayleigh scattering in the same way as the visible blue light from those parts of the sky. UV-B also plays a major role in plant development as it affects most of the plant hormones.\n\nThe shorter bands of UVC, as well as even more-energetic UV radiation produced by the Sun, are absorbed by oxygen and generate the ozone in the ozone layer when single oxygen atoms produced by UV photolysis of dioxygen react with more dioxygen. The ozone layer is especially important in blocking most UVB and the remaining part of UVC not already blocked by ordinary oxygen in air.\n\nUltraviolet absorbers are molecules used in organic materials (polymers, paints, etc.) to absorb UV radiation to reduce the UV degradation (photo-oxidation) of a material. The absorbers can themselves degrade over time, so monitoring of absorber levels in weathered materials is necessary.\n\nIn sunscreen, ingredients that absorb UVA/UVB rays, such as avobenzone, oxybenzone and octyl methoxycinnamate, are organic chemical absorbers or \"blockers\". They are contrasted with inorganic absorbers/\"blockers\" of UV radiation such as titanium dioxide and zinc oxide.\n\nFor clothing, the Ultraviolet Protection Factor (UPF) represents the ratio of sunburn-causing UV without and with the protection of the fabric, similar to SPF (Sun Protection Factor) ratings for sunscreen. Standard summer fabrics have UPF of approximately 6, which means that about 20% of UV will pass through.\n\nSuspended nanoparticles in stained glass prevent UV rays from causing chemical reactions that change image colors. A set of stained glass color reference chips is planned to be used to calibrate the color cameras for the 2019 ESA Mars rover mission, since they will remain unfaded by the high level of UV present at the surface of Mars.\n\nCommon soda lime glass is partially transparent to UVA but is opaque to shorter wavelengths, whereas fused quartz glass, depending on quality, can be transparent even to vacuum UV wavelengths. Ordinary window glass passes about 90% of the light above 350 nm, but blocks over 90% of the light below 300 nm.\n\nWood's glass is a nickel-bearing form of glass with a deep blue-purple color that blocks most visible light and passes ultraviolet.\n\nA \"black light\" lamp emits long-wave UVA radiation and little visible light. Fluorescent black light lamps work similarly to other fluorescent lamps, but use a phosphor on the inner tube surface which emits UVA radiation instead of visible light. Some lamps use a deep-bluish-purple Wood's glass optical filter that blocks almost all visible light with wavelengths longer than 400 nanometres. Others use plain glass instead of the more expensive Wood's glass, so they appear light-blue to the eye when operating. A black light may also be formed, very inefficiently, by using a layer of Wood's glass in the envelope for an incandescent bulb. Though cheaper than fluorescent UV lamps, only 0.1% of the input power is emitted as usable ultraviolet radiation. Mercury-vapor black lights in ratings up to 1 kW with UV-emitting phosphor and an envelope of Wood's glass are used for theatrical and concert displays. Black lights are used in applications in which extraneous visible light must be minimized; mainly to observe \"fluorescence\", the colored glow that many substances give off when exposed to UV light. UVA/UVB emitting bulbs are also sold for other special purposes, such as tanning lamps and reptile-keeping.\n\nA shortwave UV lamp can be made using a fluorescent lamp tube with no phosphor coating. These lamps emit ultraviolet light with two peaks in the UVC band at 253.7 nm and 185 nm due to the mercury within the lamp, as well as some visible light. From 85% to 90% of the UV produced by these lamps is at 253.7 nm, whereas only 5–10% is at 185 nm. The fused quartz glass tube passes the 253 nm radiation but blocks the 185 nm wavelength. Such tubes have two or three times the UVC power of a regular fluorescent lamp tube. These low-pressure lamps have a typical efficiency of approximately 30–40%, meaning that for every 100 watts of electricity consumed by the lamp, they will produce approximately 30–40 watts of total UV output. These \"germicidal\" lamps are used extensively for disinfection of surfaces in laboratories and food-processing industries, and for disinfecting water supplies.\n\nHalogen Lamps with fused quartz envelopes are used as inexpensive UV light sources in the near UV range, from 400 to 300 nm, in some scientific instruments.\n\nSpecialized UV gas-discharge lamps containing different gases produce UV radiation at particular spectral lines for scientific purposes. Argon and deuterium arc lamps are often used as stable sources, either windowless or with various windows such as magnesium fluoride. These are often the emitting sources in UV spectroscopy equipment for chemical analysis.\n\nOther UV sources with more continuous emission spectra include xenon arc lamps (commonly used as sunlight simulators), deuterium arc lamps, mercury-xenon arc lamps, and metal-halide arc lamps.\n\nThe excimer lamp, a UV source developed within the last two decades, is seeing increasing use in scientific fields. It has the advantages of high-intensity, high efficiency, and operation at a variety of wavelength bands into the vacuum ultraviolet.\n\nLight-emitting diodes (LEDs) can be manufactured to emit radiation in the ultraviolet range. LED efficiency at 365 nm is about 5–8%, whereas efficiency at 395 nm is closer to 20%, and power outputs at these longer UV wavelengths are also better. Such LED arrays are beginning to be used for UV curing applications, and are already successful in digital print applications and inert UV curing environments. Power densities approaching 3 W/cm (30 kW/m) are now possible, and this, coupled with recent developments by photoinitiator and resin formulators, makes the expansion of LED-cured UV materials likely.\n\nUVC LEDs are beginning to be used in disinfection and as line sources to replace deuterium lamps in liquid chromatography instruments.\n\nGas lasers, laser diodes and solid-state lasers can be manufactured to emit ultraviolet rays, and lasers are available which cover the entire UV range. The nitrogen gas laser uses electronic excitation of nitrogen molecules to emit a beam that is mostly UV. The strongest ultraviolet lines are at 337.1 nm and 357.6.6 nm, wavelength. Another type of high power gas laser is the excimer laser. They are widely used lasers emitting in ultraviolet and vacuum ultraviolet wavelength ranges. Presently, UV argon-fluoride (ArF) excimer lasers operating at 193 nm are routinely used in integrated circuit production by photolithography. The current wavelength limit of production of coherent UV is about 126 nm, characteristic of the Ar* excimer laser.\n\nDirect UV-emitting laser diodes are available at 375 nm. UV diode lasers have been demonstrated using Ce:LiSAF crystals (cerium-doped lithium strontium aluminum fluoride), a process developed in the 1990s at Lawrence Livermore National Laboratory. Wavelengths shorter than 325 nm are commercially generated in diode-pumped solid-state lasers. Ultraviolet lasers can also be made by applying frequency conversion to lower-frequency lasers.\n\nUltraviolet lasers have applications in industry (laser engraving), medicine (dermatology, and keratectomy), chemistry (MALDI), free air secure communications, computing (optical storage) and manufacture of integrated circuits.\n\nThe vacuum ultraviolet (VUV) band (100–200 nm) can be generated by non-linear 4 wave mixing in gases by sum or difference frequency mixing of 2 or more longer wavelength lasers. The generation is generally done in gasses (e.g. krypton, hydrogen which are two-photon resonant near 193 nm) or metal vapors (e.g. magnesium). By making one of the lasers tunable, the VUV can be tuned. If one of the lasers is resonant with a transition in the gas or vapor then the VUV production is intensified. However, resonances also generate wavelength dispersion, and thus the phase matching can limit the tunable range of the 4 wave mixing. Difference frequency mixing (lambda1 + lambda2 − lambda3) has an advantage over sum frequency mixing because the phase matching can provide greater tuning. In particular, difference frequency mixing two photons of an ArF (193 nm) excimer laser with a tunable visible or near IR laser in hydrogen or krypton provides resonantly enhanced tunable VUV covering from 100 nm to 200 nm. Practically, the lack of suitable gas/vapor cell window materials above the lithium fluoride cut-off wavelength limit the tuning range to longer than about 110 nm. Tunable VUV wavelengths down to 75 nm was achieved using window-free configurations.\nLasers have been used to indirectly generate non-coherent extreme UV (EUV) radiation at 13.5 nm for extreme ultraviolet lithography. The EUV is not emitted by the laser, but rather by electron transitions in an extremely hot tin or xenon plasma, which is excited by an excimer laser. This technique does not require a synchrotron, yet can produce UV at the edge of the X-ray spectrum. Synchrotron light sources can also produce all wavelengths of UV, including those at the boundary of the UV and X-ray spectra at 10 nm.\n\nThe impact of ultraviolet radiation on human health has implications for the risks and benefits of sun exposure and is also implicated in issues such as fluorescent lamps and health. Getting too much sun exposure can be harmful, but in moderation is beneficial.\n\nUV light causes the body to produce vitamin D, which is essential for life. The human body needs some UV radiation in order for one to maintain adequate vitamin D levels; however, the harmful effects typically outweigh the benefits.\n\nExposure to ultraviolet radiation from the sun is a source of vitamin D. One minimal erythemal dose of sunlight UV radiation provides the equivalent of about 20,000 IU of vitamin D2, taken as an oral supplement. If an adult's arms and legs are exposed to a half minimal erythemal UV radiation, it is the same as taking 3,000 IU of vitamin D3 through an oral supplement. This exposure of 10–15 minutes, on a frequency of two to three times per week will cause the adult's skin to produce enough vitamin D. It is not necessary to expose the face to the UV, as facial skin provides little vitamin D3. Individuals whose metabolism makes taking oral vitamin D ineffective are able, through exposure to an ultraviolet lamp that emits UV-B radiation, to achieve a 25 (OH) D blood level.\n\nThree benefits of UV exposure are production of vitamin D, improvement in mood, and increased energy.\n\nUVB induces production of vitamin D in the skin at rates of up to 1,000 IUs per minute. This vitamin helps to regulate calcium metabolism (vital for the nervous system and bone health), immunity, cell proliferation, insulin secretion, and blood pressure. In third-world countries, foods fortified with vitamin D are \"practically nonexistent.\" Most people in the world depend on the sun to get vitamin D.\n\nThere are not many foods that naturally have vitamin D. Examples are cod liver oil and oily fish. If people cannot get sunlight, then they will need 1,000 IU of vitamin D per day to stay healthy. A person would have to eat oily fish three or four times per week in order to get enough vitamin D from that food source alone.\n\nPeople with higher levels of vitamin D tend to have lower rates of diabetes, heart disease, and stroke and tend to have lower blood pressure. However, it has been found that vitamin D supplementation does not improve cardiovascular health or metabolism, so the link with vitamin D must be in part indirect. People who get more sun are generally healthier, and also have higher vitamin D levels. It has been found that ultraviolet radiation (even UVA) produces nitric oxide (NO) in the skin, and nitric oxide can lower blood pressure. High blood pressure increases the risk of stroke and heart disease. Although long-term exposure to ultraviolet contributes to non-melanoma skin cancers that are rarely fatal, it has been found in a Danish study that those who get these cancers were less likely to die during the study, and were much less likely to have a heart attack, than those who did not have these cancers.\n\nPeople in certain situations, such as people with intellectual disabilities and neurodevelopmental disorders who stay inside most of the time have low vitamin D levels. Getting enough vitamin D can help stave off \"autoimmune diseases, cardiovascular disease, many types of cancer, dementia, types 1 and 2 diabetes mellitus, and respiratory tract infections.\"\n\nFetuses and children who do not get enough vitamin D can suffer from \"growth retardation and skeletal deformities.\"\n\nUV rays also treat certain skin conditions. Modern phototherapy has been used to successfully treat psoriasis, eczema, jaundice, vitiligo, atopic dermatitis, and localized scleroderma.\n\nWorldwide, one billion people suffer from hypertension. In the U.S., half of the 146 million hypertensive patients don't have their blood pressure under control. In hypertension patients who suffer from vitamin D deficiency, UVB radiation (but not UVA) lowered blood pressure.\n\nModern pharmaceutical therapy has resulted in an overall reduction in hypertension, particularly in countries with high GDP per capita. A review of blood pressure statistics before these pharmaceuticals were available shows a coherent correlation between high blood pressure and higher latitude. Seasons of the year also impact high blood pressure; BP is lower in the summer months in high latitudes than it is in the winter, when there is less sunlight. Individuals with more sun exposure synthesize more active vitamin D (1,25 di-hydroxy cholecalciferol) from diet or ultraviolet radiation exposure. A combination of lower ultraviolet radiation with insufficient vitamin D in a diet leads to vitamin D deficiency. Individuals whose vitamin D ranks in the lowest quartile have double the all-cause mortality of those who rank in the highest quartile. They are also more likely to suffer from cardiovascular disease, hypertension and organ cancer.\n\nMedical trials have demonstrated that vitamin D supplements do not prevent or treat hypertension or cardiovascular disease, although they can help in skeletal metabolism. Epidemiological and observational studies show indications that exposure to ultraviolet radiation, particularly sunlight, might reduce all-cause mortality and can help reduce cardiovascular disease and hypertension. One hundred years of scientific data has demonstrated that the effect of ultraviolet radiation on human skin is carcinogenic. There is a lack of evidence that this carcinogenic effect, like risks such as smoking or alcohol, is responsible for higher mortality. There are significant archives of studies demonstrating that ultraviolet radiation from sunlight provides measurable health benefits, independent of vitamin D.\n\nVitamin D promotes the creation of serotonin. The production of serotonin is in direct proportion to the degree of bright sunlight the body receives. Conversely, serotonin levels decrease when sunlight is at its lowest levels, as in autumn and winter.\n\nChanges in serotonin levels affect how humans act relative to mood and behavior. Measured serotonin is much higher among those who die in summer, rather than winter.\n\nSerotonin is a monoamine neurotransmitter that is thought to provide sensations of happiness, well being and serenity to human beings.\n\nIt is thought that serotonin affects a plethora of human bodily functions from anxiety and mood to bowel function to bone density to sexuality. Its importance in human activity continues to be a source of much scientific examination and experimentation.\n\nThe amount of the brown pigment melanin in the skin increases after exposure to UV radiation at moderate levels depending on skin type; this is commonly known as a sun tan. Melanin is an excellent photoprotectant that absorbs both UVB and UVA radiation and dissipates the energy as harmless heat, protecting the skin against both direct and indirect DNA damage.\n\n\"There is no doubt that a little sunlight is good for you! But 5 to 15 minutes of casual sun exposure of hands, face and arms two to three times a week during the summer months is sufficient to keep your vitamin D levels high.\" – World Health Organization\n\nIn humans, excessive exposure to UV radiation can result in acute and chronic harmful effects on the eye's dioptric system and retina. The risk is elevated at high altitudes and people living in high latitude countries where snow covers the ground right into early summer and sun positions even at zenith are low, are particularly at risk. Skin, the circadian and immune systems can also be affected.\n\nThe differential effects of various wavelengths of light on the human cornea and skin are sometimes called the \"erythemal action spectrum.\". The action spectrum shows that UVA does not cause immediate reaction, but rather UV begins to cause photokeratitis and skin redness (with Caucasians more sensitive) at wavelengths starting near the beginning of the UVB band at 315 nm, and rapidly increasing to 300 nm. The skin and eyes are most sensitive to damage by UV at 265–275 nm, which is in the lower UVC band. At still shorter wavelengths of UV, damage continues to happen, but the overt effects are not as great with so little penetrating the atmosphere. The WHO-standard ultraviolet index is a widely publicized measurement of total strength of UV wavelengths that cause sunburn on human skin, by weighting UV exposure for action spectrum effects at a given time and location. This standard shows that most sunburn happens due to UV at wavelengths near the boundary of the UVA and UVB bands. Bioolympics discover UV reaction index to detect the leak of UV light.\n\nOverexposure to UVB radiation not only can cause sunburn but also some forms of skin cancer. However, the degree of redness and eye irritation (which are largely not caused by UVA) do not predict the long-term effects of UV, although they do mirror the direct damage of DNA by ultraviolet.\n\nAll bands of UV radiation damage collagen fibers and accelerate aging of the skin. Both UVA and UVB destroy vitamin A in skin, which may cause further damage.\n\nUVB radiation can cause direct DNA damage. This cancer connection is one reason for concern about ozone depletion and the ozone hole.\n\nThe most deadly form of skin cancer, malignant melanoma, is mostly caused by DNA damage independent from UVA radiation. This can be seen from the absence of a direct UV signature mutation in 92% of all melanoma. Occasional overexposure and sunburn are probably greater risk factors for melanoma than long-term moderate exposure. UVC is the highest-energy, most-dangerous type of ultraviolet radiation, and causes adverse effects that can variously be mutagenic or carcinogenic.\n\nIn the past, UVA was considered not harmful or less harmful than UVB, but today it is known to contribute to skin cancer via indirect DNA damage (free radicals such as reactive oxygen species). UVA can generate highly reactive chemical intermediates, such as hydroxyl and oxygen radicals, which in turn can damage DNA. The DNA damage caused indirectly to skin by UVA consists mostly of single-strand breaks in DNA, while the damage caused by UVB includes direct formation of thymine dimers or other pyrimidine dimers and double-strand DNA breakage. UVA is immunosuppressive for the entire body (accounting for a large part of the immunosuppressive effects of sunlight exposure), and is mutagenic for basal cell keratinocytes in skin.\n\nUVB photons can cause direct DNA damage. UVB radiation excites DNA molecules in skin cells, causing aberrant covalent bonds to form between adjacent pyrimidine bases, producing a dimer. Most UV-induced pyrimidine dimers in DNA are removed by the process known as nucleotide excision repair that employs about 30 different proteins. Those pyrimidine dimers that escape this repair process can induce a form of programmed cell death (apoptosis) or can cause DNA replication errors leading to mutation.\n\nAs a defense against UV radiation, the amount of the brown pigment melanin in the skin increases when exposed to moderate (depending on skin type) levels of radiation; this is commonly known as a sun tan. The purpose of melanin is to absorb UV radiation and dissipate the energy as harmless heat, blocking the UV from damaging skin tissue. UVA gives a quick tan that lasts for days by oxidizing melanin that was already present and triggers the release of the melanin from melanocytes. UVB yields a tan that takes roughly 2 days to develop because it stimulates the body to produce more melanin.\n\nSunscreen prevents the direct DNA damage which causes sunburn. Most of these products contain an SPF rating to show how well they block UVB rays. The SPF rating, however, offers no data about UVA protection.\n\nSome sunscreen lotions now include compounds such as titanium dioxide which helps protect against UVA rays. Other UVA blocking compounds found in sunscreen include zinc oxide and avobenzone.\n\nMedical organizations recommend that patients protect themselves from UV radiation by using sunscreen. Five sunscreen ingredients have been shown to protect mice against skin tumors. However, some sunscreen chemicals produce potentially harmful substances if they are illuminated while in contact with living cells. The amount of sunscreen that penetrates into the lower layers of the skin may be large enough to cause damage.\n\nSunscreen reduces the direct DNA damage that causes sunburn, by blocking UVB, and the usual SPF rating indicates how effectively this radiation is blocked. SPF is, therefore, also called UVB-PF, for \"UVB protection factor\". This rating, however, offers no data about important protection against UVA, which does not primarily cause sunburn but is still harmful, since it causes indirect DNA damage and is also considered carcinogenic. Several studies suggest that the absence of UVA filters may be the cause of the higher incidence of melanoma found in sunscreen users compared to non-users.\n\nThe photochemical properties of melanin make it an excellent photoprotectant. However, sunscreen chemicals cannot dissipate the energy of the excited state as efficiently as melanin and therefore, if sunscreen ingredients penetrate into the lower layers of the skin, the amount of reactive oxygen species may be increased. The amount of sunscreen that penetrates through the stratum corneum may or may not be large enough to cause damage.\n\nIn an experiment by Hanson et al. that was published in 2006, the amount of harmful reactive oxygen species (ROS) was measured in untreated and in sunscreen treated skin. In the first 20 minutes, the film of sunscreen had a protective effect and the number of ROS species was smaller. After 60 minutes, however, the amount of absorbed sunscreen was so high that the amount of ROS was higher in the sunscreen-treated skin than in the untreated skin. The study indicates that sunscreen must be reapplied within 2 hours in order to prevent UV light from penetrating to sunscreen-infused live skin cells.\n\nUltraviolet radiation can aggravate several skin conditions and diseases, including:\n\nThe eye is most sensitive to damage by UV in the lower UVC band at 265–275 nm. Radiation of this wavelength is almost absent from sunlight but is found in welder's arc lights and other artificial sources. Exposure to these can cause \"welder's flash\" or \"arc eye\" (photokeratitis) and can lead to cataracts, pterygium and pinguecula formation. To a lesser extent, UVB in sunlight from 310–280 nm also causes photokeratitis (\"snow blindness\"), and the cornea, the lens, and the retina can be damaged.\n\nProtective eyewear is beneficial to those exposed to ultraviolet radiation. Since light can reach the eyes from the sides, full-coverage eye protection is usually warranted if there is an increased risk of exposure, as in high-altitude mountaineering. Mountaineers are exposed to higher-than-ordinary levels of UV radiation, both because there is less atmospheric filtering and because of reflection from snow and ice.\nOrdinary, untreated eyeglasses give some protection. Most plastic lenses give more protection than glass lenses, because, as noted above, glass is transparent to UVA and the common acrylic plastic used for lenses is less so. Some plastic lens materials, such as polycarbonate, inherently block most UV.\n\nUV degradation is one form of polymer degradation that affects plastics exposed to sunlight. The problem appears as discoloration or fading, cracking, loss of strength or disintegration. The effects of attack increase with exposure time and sunlight intensity. The addition of UV absorbers inhibits the effect.\n\nSensitive polymers include thermoplastics and speciality fibers like aramids. UV absorption leads to chain degradation and loss of strength at sensitive points in the chain structure. Aramid rope must be shielded with a sheath of thermoplastic if it is to retain its strength.\nMany pigments and dyes absorb UV and change colour, so paintings and textiles may need extra protection both from sunlight and fluorescent bulbs, two common sources of UV radiation. Window glass absorbs some harmful UV, but valuable artifacts need extra shielding. Many museums place black curtains over watercolour paintings and ancient textiles, for example. Since watercolours can have very low pigment levels, they need extra protection from UV. Various forms of picture framing glass, including acrylics (plexiglass), laminates, and coatings, offer different degrees of UV (and visible light) protection.\n\nBecause of its ability to cause chemical reactions and excite fluorescence in materials, ultraviolet radiation has a number of applications. The following table gives some uses of specific wavelength bands in the UV spectrum\n\nPhotographic film responds to ultraviolet radiation but the glass lenses of cameras usually block radiation shorter than 350 nm. Slightly yellow UV-blocking filters are often used for outdoor photography to prevent unwanted bluing and overexposure by UV rays. For photography in the near UV, special filters may be used. Photography with wavelengths shorter than 350 nm requires special quartz lenses which do not absorb the radiation.\nDigital cameras sensors may have internal filters that block UV to improve color rendition accuracy. Sometimes these internal filters can be removed, or they may be absent, and an external visible-light filter prepares the camera for near-UV photography. A few cameras are designed for use in the UV.\n\nPhotography by reflected ultraviolet radiation is useful for medical, scientific, and forensic investigations, in applications as widespread as detecting bruising of skin, alterations of documents, or restoration work on paintings. Photography of the fluorescence produced by ultraviolet illumination uses visible wavelengths of light.\nIn ultraviolet astronomy, measurements are used to discern the chemical composition of the interstellar medium, and the temperature and composition of stars. Because the ozone layer blocks many UV frequencies from reaching telescopes on the surface of the Earth, most UV observations are made from space.\n\nCorona discharge on electrical apparatus can be detected by its ultraviolet emissions. Corona causes degradation of electrical insulation and emission of ozone and nitrogen oxide.\n\nEPROMs (Erasable Programmable Read-Only Memory) are erased by exposure to UV radiation. These modules have a transparent (quartz) window on the top of the chip that allows the UV radiation in.\n\nColorless fluorescent dyes that emit blue light under UV are added as optical brighteners to paper and fabrics. The blue light emitted by these agents counteracts yellow tints that may be present and causes the colors and whites to appear whiter or more brightly colored.\n\nUV fluorescent dyes that glow in the primary colors are used in paints, papers, and textiles either to enhance color under daylight illumination or to provide special effects when lit with UV lamps. Blacklight paints that contain dyes that glow under UV are used in a number of art and esthetic applications.\nTo help prevent counterfeiting of currency, or forgery of important documents such as driver's licenses and passports, the paper may include a UV watermark or fluorescent multicolor fibers that are visible under ultraviolet light. Postage stamps are tagged with a phosphor that glows under UV rays to permit automatic detection of the stamp and facing of the letter.\n\nUV fluorescent dyes are used in many applications (for example, biochemistry and forensics). Some brands of pepper spray will leave an invisible chemical (UV dye) that is not easily washed off on a pepper-sprayed attacker, which would help police identify the attacker later.\n\nIn some types of nondestructive testing UV stimulates fluorescent dyes to highlight defects in a broad range of materials. These dyes may be carried into surface-breaking defects by capillary action (liquid penetrant inspection) or they may be bound to ferrite particles caught in magnetic leakage fields in ferrous materials (magnetic particle inspection).\n\nUV is an investigative tool at the crime scene helpful in locating and identifying bodily fluids such as semen, blood, and saliva. For example, ejaculated fluids or saliva can be detected by high-power UV sources, irrespective of the structure or colour of the surface the fluid is deposited upon.\nUV-Vis microspectroscopy is also used to analyze trace evidence, such as textile fibers and paint chips, as well as questioned documents.\n\nOther applications include the authentication of various collectibles and art, and detecting counterfeit currency. Even materials not specially marked with UV sensitive dyes may have distinctive fluorescence under UV exposure or may fluoresce differently under short-wave versus long-wave ultraviolet.\n\nUsing multi-spectral imaging it is possible to read illegible papyrus, such as the burned papyri of the Villa of the Papyri or of Oxyrhynchus, or the Archimedes palimpsest. The technique involves taking pictures of the illegible document using different filters in the infrared or ultraviolet range, finely tuned to capture certain wavelengths of light. Thus, the optimum spectral portion can be found for distinguishing ink from paper on the papyrus surface.\n\nSimple NUV sources can be used to highlight faded iron-based ink on vellum.\n\nUltraviolet aids in the detection of organic material deposits that remain on surfaces where periodic cleaning and sanitizing may not have been properly accomplished. It is used in the hotel industry, manufacturing, and other industries where levels of cleanliness or contamination are inspected.\n\nPerennial news feature for many television news organizations involves an investigative reporter's using a similar device to reveal unsanitary conditions in hotels, public toilets, hand rails, and such.\n\nUV/VIS spectroscopy is widely used as a technique in chemistry to analyze chemical structure, the most notable one being conjugated systems. UV radiation is often used to excite a given sample where the fluorescent emission is measured with a spectrofluorometer. In biological research, UV radiation is used for quantification of nucleic acids or proteins.\nUltraviolet lamps are also used in analyzing minerals and gems.\n\nIn pollution control applications, ultraviolet analyzers are used to detect emissions of nitrogen oxides, sulfur compounds, mercury, and ammonia, for example in the flue gas of fossil-fired power plants. Ultraviolet radiation can detect thin sheens of spilled oil on water, either by the high reflectivity of oil films at UV wavelengths, fluorescence of compounds in oil or by absorbing of UV created by Raman scattering in water.\n\nIn general, ultraviolet detectors use either a solid-state device, such as one based on silicon carbide or aluminium nitride, or a gas-filled tube as the sensing element. UV detectors that are sensitive to UV in any part of the spectrum respond to irradiation by sunlight and artificial light. A burning hydrogen flame, for instance, radiates strongly in the 185- to 260-nanometer range and only very weakly in the IR region, whereas a coal fire emits very weakly in the UV band yet very strongly at IR wavelengths; thus, a fire detector that operates using both UV and IR detectors is more reliable than one with a UV detector alone. Virtually all fires emit some radiation in the UVC band, whereas the Sun's radiation at this band is absorbed by the Earth's atmosphere. The result is that the UV detector is \"solar blind\", meaning it will not cause an alarm in response to radiation from the Sun, so it can easily be used both indoors and outdoors.\n\nUV detectors are sensitive to most fires, including hydrocarbons, metals, sulfur, hydrogen, hydrazine, and ammonia. Arc welding, electrical arcs, lightning, X-rays used in nondestructive metal testing equipment (though this is highly unlikely), and radioactive materials can produce levels that will activate a UV detection system. The presence of UV-absorbing gases and vapors will attenuate the UV radiation from a fire, adversely affecting the ability of the detector to detect flames. Likewise, the presence of an oil mist in the air or an oil film on the detector window will have the same effect.\n\nUltraviolet radiation is used for very fine resolution photolithography, a procedure wherein a chemical called a photoresist is exposed to UV radiation that has passed through a mask. The exposure causes chemical reactions to occur in the photoresist. After removal of unwanted photoresist, a pattern determined by the mask remains on the sample. Steps may then be taken to \"etch\" away, deposit on or otherwise modify areas of the sample where no photoresist remains.\n\nPhotolithography is used in the manufacture of semiconductors, integrated circuit components, and printed circuit boards. Photolithography processes used to fabricate electronic integrated circuits presently use 193 nm UV and are experimentally using 13.5 nm UV for extreme ultraviolet lithography.\n\nElectronic components that require clear transparency for light to exit or enter (photovoltaic panels and sensors) can be potted using acrylic resins that are cured using UV energy. The advantages are low VOC emissions and rapid curing.\nCertain inks, coatings, and adhesives are formulated with photoinitiators and resins. When exposed to UV light, polymerization occurs, and so the adhesives harden or cure, usually within a few seconds. Applications include glass and plastic bonding, optical fiber coatings, the coating of flooring, UV coating and paper finishes in offset printing, dental fillings, and decorative fingernail \"gels\".\n\nUV sources for UV curing applications include UV lamps, UV LEDs, and Excimer flash lamps. Fast processes such as flexo or offset printing require high-intensity light focused via reflectors onto a moving substrate and medium so high-pressure Hg (mercury) or Fe (iron, doped)-based bulbs are used, energized with electric arcs or microwaves. Lower-power fluorescent lamps and LEDs can be used for static applications. Small high-pressure lamps can have light focused and transmitted to the work area via liquid-filled or fiber-optic light guides.\n\nThe impact of UV on polymers is used for modification of the (roughness and hydrophobicity) of polymer surfaces. For example, a poly(methyl methacrylate) surface can be smoothed by vacuum ultraviolet.\n\nUV radiation is useful in preparing low-surface-energy polymers for adhesives. Polymers exposed to UV will oxidize, thus raising the surface energy of the polymer. Once the surface energy of the polymer has been raised, the bond between the adhesive and the polymer is stronger.\n\nUsing a catalytic chemical reaction from titanium dioxide and UVC exposure, oxidation of organic matter converts pathogens, pollens, and mold spores into harmless inert byproducts. The cleansing mechanism of UV is a photochemical process. Contaminants in the indoor environment are almost entirely organic carbon-based compounds, which break down when exposed to high-intensity UV at 240 to 280 nm. Short-wave ultraviolet radiation can destroy DNA in living microorganisms. UVC's effectiveness is directly related to intensity and exposure time.\n\nUV has also been shown to reduce gaseous contaminants such as carbon monoxide and VOCs. UV lamps radiating at 184 and 254 nm can remove low concentrations of hydrocarbons and carbon monoxide if the air is recycled between the room and the lamp chamber. This arrangement prevents the introduction of ozone into the treated air. Likewise, air may be treated by passing by a single UV source operating at 184 nm and passed over iron pentaoxide to remove the ozone produced by the UV lamp.\n\nUltraviolet lamps are used to sterilize workspaces and tools used in biology laboratories and medical facilities. Commercially available low-pressure mercury-vapor lamps emit about 86% of their radiation at 254 nanometers (nm), with 265 nm being the peak germicidal effectiveness curve. UV at these germicidal wavelengths damage a microorganism's DNA so that it cannot reproduce, making it harmless, (even though the organism may not be killed). Since microorganisms can be shielded from ultraviolet rays in small cracks and other shaded areas, these lamps are used only as a supplement to other sterilization techniques.\n\nUV-C LEDs are relatively new to the commercial market and are gaining in popularity. Due to their monochromatic nature (± 5 nm) these LEDs can target a specific wavelength needed for disinfection. This is especially important knowing that pathogens vary in their sensitivity to specific UV wavelengths. LEDs are mercury free, instant on/off, and have unlimited cycling throughout the day.\n\nDisinfection using UV radiation is commonly used in wastewater treatment applications and is finding an increased usage in municipal drinking water treatment. Many bottlers of spring water use UV disinfection equipment to sterilize their water. Solar water disinfection has been researched for cheaply treating contaminated water using natural sunlight. The UV-A irradiation and increased water temperature kill organisms in the water.\n\nUltraviolet radiation is used in several food processes to kill unwanted microorganisms. UV can be used to pasteurize fruit juices by flowing the juice over a high-intensity ultraviolet source. The effectiveness of such a process depends on the UV absorbance of the juice.\n\nPulsed light (PL) is a technique of killing microorganisms on surfaces using pulses of an intense broad spectrum, rich in UV-C between 200 and 280 nm. Pulsed light works with xenon flash lamps that can produce flashes several times per second. Disinfection robots use pulsed UV\n\nSome animals, including birds, reptiles, and insects such as bees, can see near-ultraviolet wavelengths. Many fruits, flowers, and seeds stand out more strongly from the background in ultraviolet wavelengths as compared to human color vision. Scorpions glow or take on a yellow to green color under UV illumination, thus assisting in the control of these arachnids. Many birds have patterns in their plumage that are invisible at usual wavelengths but observable in ultraviolet, and the urine and other secretions of some animals, including dogs, cats, and human beings, are much easier to spot with ultraviolet. Urine trails of rodents can be detected by pest control technicians for proper treatment of infested dwellings.\n\nButterflies use ultraviolet as a communication system for sex recognition and mating behavior. For example, in the \"Colias eurytheme\" butterfly, males rely on visual cues to locate and identify females. Instead of using chemical stimuli to find mates, males are attracted to the ultraviolet-reflecting color of female hind wings.\n\nMany insects use the ultraviolet wavelength emissions from celestial objects as references for flight navigation. A local ultraviolet emitter will normally disrupt the navigation process and will eventually attract the flying insect.\nThe green fluorescent protein (GFP) is often used in genetics as a marker. Many substances, such as proteins, have significant light absorption bands in the ultraviolet that are of interest in biochemistry and related fields. UV-capable spectrophotometers are common in such laboratories.\n\nUltraviolet traps called bug zappers are used to eliminate various small flying insects. They are attracted to the UV and are killed using an electric shock, or trapped once they come into contact with the device. Different designs of ultraviolet radiation traps are also used by entomologists for collecting nocturnal insects during faunistic survey studies.\n\nUltraviolet radiation is helpful in the treatment of skin conditions such as psoriasis and vitiligo. Exposure to UVA, while the skin is hyper-photosensitive, by taking psoralens is an effective treatment for psoriasis. Due to the potential of psoralens to cause damage to the liver, PUVA therapy may be used only a limited number of times over a patient's lifetime.\n\nUVB phototherapy does not require additional medications or topical preparations for the therapeutic benefit; only the exposure is needed. However, phototherapy can be effective when used in conjunction with certain topical treatments such as anthralin, coal tar, and vitamin A and D derivatives, or systemic treatments such as methotrexate and Soriatane.\n\nReptiles need UVB for biosynthesis of vitamin D, and other metabolic processes. Specifically cholecalciferol (vitamin D3), which is needed to for basic cellular / neural functioning as well as the utilization calcium for bone and egg production. The UVA wavelength is also visible to many reptiles and might play a signifiant role in their ability survive in the wild as well as visual communication between individuals. Therefore, in a typical reptile enclosure, a fluorescent UV a/b source (at the proper strength / spectrum for the species), must be available for many captive species to survive. Simple supplementation with cholecalciferol (Vitamin D3) will not be enough as there's a complete biosynthetic pathway that is \"leapfrogged\" (risks of possible overdoses), the intermediate molecules and metabolites also place important functions in the animals health. Natural sunlight in the right levels is always going to be superior to artificial sources, but this might be possible for keepers in different parts of the world.\n\nIt is a known problem that high levels of output of the UVa part of the spectrum can both cause cellular and DNA damage to sensitive parts of their bodies - especially the eyes were blindness is the result from an improper UVa/b source use and placement photokeratitis. For many keepers there must also be a provision for an adequate heat source this has resulted in the marketing of heat and light \"combination\" products. Keepers should be careful of these \"combination' light/ heat and UVa/b generators, they typically emit high levels of UVa with lower levels of UVb that are set and difficult to control so that animals can have their needs met. A better strategy is to use individual sources of these elements and so they can be placed and controlled by the keepers for the max benefit of the animals.\n\nThe evolution of early reproductive proteins and enzymes is attributed in modern models of evolutionary theory to ultraviolet radiation. UVB causes thymine base pairs next to each other in genetic sequences to bond together into thymine dimers, a disruption in the strand that reproductive enzymes cannot copy. This leads to frameshifting during genetic replication and protein synthesis, usually killing the cell. Before formation of the UV-blocking ozone layer, when early prokaryotes approached the surface of the ocean, they almost invariably died out. The few that survived had developed enzymes that monitored the genetic material and removed thymine dimers by nucleotide excision repair enzymes. Many enzymes and proteins involved in modern mitosis and meiosis are similar to repair enzymes, and are believed to be evolved modifications of the enzymes originally used to overcome DNA damages caused by UV.\n\n\n"
            ],
            "good_message": false,
            "loggedData": [],
            "offensive": null,
            "researchTask": true,
            "selectedData": [],
            "shown_passages": [
                [
                    "Sunglasses",
                    [
                        "Sunglasses or sun glasses are a form of protective eyewear designed primarily to prevent bright sunlight and high-energy visible light from damaging or discomforting the eyes.",
                        "They can sometimes also function as a visual aid, as variously termed spectacles or glasses exist, featuring lenses that are colored, polarized or darkened.",
                        "In the early 20th century, they were also known as sun cheaters (cheaters being an American slang term for glasses).",
                        "The American Optometric Association recommends sunglasses whenever a person is in the sun to protect the eyes from ultraviolet radiation (UV) and blue light, which can cause several serious eye problems."
                    ]
                ],
                [
                    "Glasses",
                    [
                        "Glasses, also known as eyeglasses or spectacles, are devices consisting of glass or hard plastic lenses mounted in a frame that holds them in front of a person's eyes, typically using a bridge over the nose and arms which rest over the ears.",
                        "Glasses are typically used for vision correction, such as with reading glasses and glasses used for nearsightedness.",
                        "Safety glasses provide eye protection against flying debris for construction workers or lab technicians; these glasses may have protection for the sides of the eyes as well as in the lenses.",
                        "Some types of safety glasses are used to protect against visible and near-visible light or radiation."
                    ]
                ],
                [
                    "Ultraviolet",
                    [
                        "Ultraviolet (UV) is an electromagnetic radiation with a wavelength from 10 nm to 400 nm, shorter than that of visible light but longer than X-rays.",
                        "UV radiation constitutes about 10% of the total light output of the Sun, and is thus present in sunlight.",
                        "It is also produced by electric arcs and specialized lights, such as mercury-vapor lamps, tanning lamps, and black lights.",
                        "Although it's not considered an ionizing radiation because its photons lack the energy to ionize atoms, long-wavelength ultraviolet radiation can cause chemical reactions and causes many substances to glow or fluoresce.",
                        "Consequently, the chemical and biological effects of UV are greater than simple heating effects, and many practical applications of UV radiation derive from its interactions with organic molecules."
                    ]
                ]
            ],
            "speaker": "0_Wizard",
            "suggestionsConfig": "show",
            "suggestionsNumber": 5,
            "suggestionsShown": true,
            "text": "I also know that sunglasses offer protection against UV light and also offer better vision in bright light. Sunglasses can actu.ally prevent damage to the eyes.",
            "textboxFilled": 0,
            "time": 1580233352.8021567,
            "timeTaken": 3.41,
            "totalTasks": 120,
            "turn": 17
        },
        {
            "checked_passage": {
                "no_passages_used": "no_passages_used"
            },
            "checked_sentence": {
                "no_passages_used": "no_passages_used"
            },
            "clicked_passages": {},
            "context": {
                "conversation": [
                    "There are many types of glasses, what kind do you want to chat about?",
                    "I would like to chat about reading glasses which are used for nearsightedness.",
                    "That sounds like a lovely type of glass. I actually am nearsighted myself. do you know who invented glasses?",
                    "I have no idea who invented reading glasses but I do know that those of us who wear these type glasses sometimes wear them around our necks attached to a cord to prevent loss.",
                    "That reminds me so much of the librarian I knew when I was little. She did that! What else do you know about glasses?",
                    "I also know that sunglasses offer protection against UV light and also offer better vision in bright light. Sunglasses can actu.ally prevent damage to the eyes."
                ],
                "index": 7
            },
            "contextCount": 0,
            "full_passages": [
                "Glasses\n\nGlasses, also known as eyeglasses or spectacles, are devices consisting of glass or hard plastic lenses mounted in a frame that holds them in front of a person's eyes, typically using a bridge over the nose and arms which rest over the ears. Glasses are typically used for vision correction, such as with reading glasses and glasses used for nearsightedness. Safety glasses provide eye protection against flying debris for construction workers or lab technicians; these glasses may have protection for the sides of the eyes as well as in the lenses. Some types of safety glasses are used to protect against visible and near-visible light or radiation. Glasses are worn for eye protection in some sports, such as squash. Glasses wearers may use a strap to prevent the glasses from falling off during movement or sports. Wearers of glasses that are used only part of the time may have the glasses attached to a cord that goes around their neck, to prevent the loss of the glasses.\n\nSunglasses allow better vision in bright daylight, and may protect one's eyes against damage from high levels of ultraviolet light. Typical sunglasses are darkened for protection against bright light or glare; some specialized glasses are clear in dark or indoor conditions, but turn into sunglasses when in bright light. Most sunglasses do not have corrective power in the lenses; however, special prescription sunglasses can be ordered. Specialized glasses may be used for viewing specific visual information (such as stereoscopy) or 3D glasses for viewing three-dimensional movies. Sometimes glasses with no corrective power in the lenses are worn simply for aesthetic or fashion purposes. Even with glasses used for vision correction, a wide range of designs are available for fashion purposes, using plastic, wire, and other materials.\n\nPeople are more likely to need glasses the older they get with 93% of people between the age of 65-75 wearing corrective lenses.\nGlasses come in many styles. They can be marked or found by their primary function, but also appear in combinations such as prescription sunglasses or safety glasses with enhanced magnification.\n\nCorrective lenses are used to correct refractive errors by bending the light entering the eye in order to alleviate the effects of conditions such as nearsightedness (myopia), farsightedness (hypermetropia) or astigmatism. The ability of one's eyes to accommodate their focus to near and distant focus alters over time. Also, few people have eyes that show exactly equal refractive characteristics; one may need a \"stronger\", (i.e. more refracting), lens than the other. A common condition in people over forty years old is presbyopia, which is caused by the eye's crystalline lens losing elasticity, progressively reducing the ability of the lens to accommodate (i.e. to focus on objects close to the eye). Corrective lenses, to bring the image back into focus on the retina, are made to conform to the prescription of an ophthalmologist or optometrist. A lensmeter can be used to verify the specifications of an existing pair of glasses. Corrective eyeglasses can significantly improve the life quality of the wearer. Not only do they enhance the wearer's visual experience, but can also reduce problems that result from eye strain, such as headaches or squinting.\n\nPinhole glasses are a type of corrective glasses that do not use a lens. Pinhole glasses do not actually refract the light or change focal length. Instead, they create a diffraction limited system, which has an increased depth of field, similar to using a small aperture in photography. This form of correction has many limitations that prevent it from gaining popularity in everyday use. Pinhole glasses can be made in a DIY fashion by making small holes in a piece of card which is then held in front of the eyes with a strap or cardboard arms.\n\nThe most common type of corrective lens is \"single vision\", which has a uniform refractive index. For people with presbyopia and hyperopia, bifocal and trifocal glasses provide two or three different refractive indices, respectively, and progressive lenses have a continuous gradient. Reading glasses provide a separate set of glasses for focusing on close-by objects. Reading glasses are available without prescription from drugstores, and offer a cheap, practical solution, though these have two simple lenses of equal power, so will not correct refraction problems like astigmatism or refractive or prismatic variations between the left and right eye. For total correction of the individual's sight, glasses complying to a recent ophthalmic prescription are required. Adjustable-focus eyeglasses might be used to replace bifocals or trifocals, or might be used to produce cheaper single-vision glasses (since they don't have to be custom-manufactured for every person).\n\nSafety glasses are worn to protect the eyes in different situations. They are made with break-proof plastic lenses to protect the eye from flying debris or other matter. Construction workers, factory workers, machinists and lab technicians are often required to wear safety glasses to shield the eyes from flying debris or hazardous splatters such as blood or chemicals. As of 2017, dentists and surgeons in Canada and other countries are required to wear safety glasses to protect against infection from patients' blood or other body fluids. There are also safety glasses for welding, which are styled like wraparound sunglasses, but with much darker lenses, for use in welding where a full sized welding helmet is inconvenient or uncomfortable. These are often called \"flash goggles\", because they provide protection from welding flash. Nylon frames are usually used for protection eyewear for sports because of their lightweight and flexible properties. Unlike most regular glasses, safety glasses often include protection beside the eyes as well as in front of the eyes.\n\nSunglasses provide improved comfort and protection against bright light and often against ultraviolet (UV) light. Photochromic lenses, which are photosensitive, darken when struck by UV light. The dark tint of the lenses in a pair of sunglasses blocks the transmission of light through the lens.\n\nLight polarization is an added feature that can be applied to sunglass lenses. Polarization filters are positioned to remove horizontally polarized rays of light, which eliminates glare from horizontal surfaces (allowing wearers to see into water when reflected light would otherwise overwhelm the scene). Polarized sunglasses may present some difficulties for pilots since reflections from water and other structures often used to gauge altitude may be removed. Liquid-crystal displays often emit polarized light making them sometimes difficult to view with polarized sunglasses. Sunglasses may be worn just for aesthetic purposes, or simply to hide the eyes. Examples of sunglasses that were popular for these reasons include teashades and mirrorshades. Many blind people wear nearly opaque glasses to hide their eyes for cosmetic reasons.\n\nSunglasses may also have corrective lenses, which requires a prescription. Clip-on sunglasses or sunglass clips can be attached to another pair of glasses. Some wrap-around sunglasses are large enough to be worn over top of another pair of glasses. Otherwise, many people opt to wear contact lenses to correct their vision so that standard sunglasses can be used.\n\nThe illusion of three dimensions on a two dimensional surface can be created by providing each eye with different visual information. 3D glasses create the illusion of three dimensions by filtering a signal containing information for both eyes. The signal, often light reflected off a movie screen or emitted from an electronic display, is filtered so that each eye receives a slightly different image. The filters only work for the type of signal they were designed for.\n\nAnaglyph 3D glasses have a different colored filter for each eye, typically red and blue or red and green. A polarized 3D system on the other hand uses polarized filters. Polarized 3D glasses allow for color 3D, while the red-blue lenses produce an image with distorted coloration. An active shutter 3D system uses electronic shutters. Head-mounted displays can filter the signal electronically and then transmit light directly into the viewers eyes.\n\nAnaglyph and polarized glasses are distributed to audiences at 3D movies. Polarized and active shutter glasses are used with many home theaters. Head-mounted displays are used by a single person, but the input signal can be shared between multiple units.\n\nGlasses can also provide magnification that is useful for people with vision impairments or specific occupational demands. An example would be \"bioptics\" or \"bioptic telescopes\" which have small telescopes mounted on, in, or behind their regular lenses. Newer designs use smaller lightweight telescopes, which can be embedded into the corrective glass and improve aesthetic appearance (mini telescopic spectacles). They may take the form of self-contained glasses that resemble goggles or binoculars, or may be attached to existing glasses.\n\nYellow tinted glasses are a type of glasses with a minor yellow tint. They perform minor color correction, on top of reducing headaches due to lack of blinking. They may also be considered minor corrective unprescribed glasses. Depending on the company, these computer or gaming glasses can also filter out high energy blue and ultra-violet light from LCD screens, fluorescent lighting, and other sources of light. This allows for reduced eye-strain. These glasses can be ordered as standard or prescription lenses that fit into standard optical frames. Due to the ultra-violet light blocking nature of these lenses, they also help users sleep at night along with reducing age-related macular degeneration.\n\nAnti-glare protection glasses, or anti-reflective glasses, can reduce the reflection of light that enters our eyes. The lenses are given an anti-glare coating to prevent reflections of light under different lighting conditions. By reducing the amount of glare on your eyes, vision can be improved. The anti-glare also applies to the outer glass, thus allowing for better eye contact.\n\nThe ophthalmic frame is the part of a pair of glasses which is designed to hold the lenses in proper position.\nOphthalmic frames come in a variety of styles, sizes, materials, shapes, and colors.\n\n\n\n\n\n\nCorrective lenses can be produced in many different shapes from a circular lens called a lens blank. Lens blanks are cut to fit the shape of the frame that will hold them. Frame styles vary and fashion trends change over time, resulting in a multitude of lens shapes. For lower power lenses, there are few restrictions which allows for many trendy and fashionable shapes. Higher power lenses can cause distortion of peripheral vision and may become thick and heavy if a large lens shape is used. However, if the lens becomes too small, the field of view can be drastically reduced. \n\nBifocal, trifocal, and progressive lenses generally require a taller lens shape to leave room for the different segments while preserving an adequate field of view through each segment. Frames with rounded edges are the most efficient for correcting myopic prescriptions, with perfectly round frames being the most efficient. Before the advent of eyeglasses as a fashion item, when frames were constructed with only functionality in mind, virtually all eyeglasses were either round, oval, or curved octagons. It was not until glasses began to be seen as an accessory that different shapes were introduced to be more aesthetically pleasing than functional.\n\nScattered evidence exists for use of vision aid devices in Greek and Roman times, most prominently the use of an emerald by emperor Nero as mentioned by Pliny the Elder.\n\nThe use of a convex lens to form an enlarged/magnified image was most likely described in Ptolemy's \"Optics\" (which however only survives in a poor Arabic translation). Ptolemy's description of lenses was commented upon and improved by Ibn Sahl (10th century) and most notably by Alhazen (\"Book of Optics\", ca. 1021). Latin translations of Ptolemy's \"Optics\" and of Alhazen became available in Europe in the 12th century, coinciding with the development of \"reading stones\".\n\nRobert Grosseteste's treatise \"De iride\" (\"On the Rainbow\"), written between 1220 and 1235, mentions using optics to \"read the smallest letters at incredible distances\". A few years later in 1262, Roger Bacon is also known to have written on the magnifying properties of lenses.\nThe development of the first eyeglasses took place in Northern Italy in the second half of the 13th century. \n\nIndependently of the development of optical lenses, some cultures developed \"sunglasses\" for eye protection, without any corrective properties.\nThus, flat panes of smoky quartz, were used in 12th-century China. Similarly, the Inuit have used snow goggles for eye protection.\n\nThe first eyeglasses were made in Northern Italy, most likely in Pisa, by about 1290:\nIn a sermon delivered on 23 February 1306, the Dominican friar Giordano da Pisa (ca. 1255–1311) wrote \"It is not yet twenty years since there was found the art of making eyeglasses, which make for good vision... And it is so short a time that this new art, never before extant, was discovered. ... I saw the one who first discovered and practiced it, and I talked to him.\" \n\nGiordano's colleague Friar Alessandro della Spina of Pisa (d. 1313) was soon making eyeglasses. The \"Ancient Chronicle of the Dominican Monastery of St. Catherine in Pisa\" records: \"Eyeglasses, having first been made by someone else, who was unwilling to share them, he [Spina] made them and shared them with everyone with a cheerful and willing heart.\" By 1301, there were guild regulations in Venice governing the sale of eyeglasses.\nThe earliest pictorial evidence for the use of eyeglasses is Tommaso da Modena's 1352 portrait of the cardinal Hugh de Provence reading in a scriptorium. Another early example would be a depiction of eyeglasses found north of the Alps in an altarpiece of the church of Bad Wildungen, Germany, in 1403. These early glasses had convex lenses that could correct both hyperopia (farsightedness), and the presbyopia that commonly develops as a symptom of aging. It was not until 1604 that Johannes Kepler published the first correct explanation as to why convex and concave lenses could correct presbyopia and myopia.\n\nEarly frames for glasses consisted of two magnifying glasses riveted together by the handles so that they could grip the nose. These are referred to as \"rivet spectacles\". The earliest surviving examples were found under the floorboards at Kloster Wienhausen, a convent near Celle in Germany; they have been dated to \"circa\" 1400.\n\nClaims that Salvino degli Armati of Florence invented eyeglasses have been exposed as hoaxes. \n\nMarco Polo is sometimes claimed to have encountered eyeglasses during his travels in China in the 13th century, however, no such statement appears in his accounts. Indeed, the earliest mentions of eyeglasses in China occur in the 15th century and those Chinese sources state that eyeglasses were imported.\n\nIt is also sometimes claimed that glasses were first invented in India. In 1907 Professor Berthold Laufer stated in his history of glasses that \"\"the opinion that spectacles originated in India is of the greatest probability and that spectacles must have been known in India earlier than in Europe\"\". However, Joseph Needham showed that the mention of glasses in the manuscript Laufer used to justify the prior invention of them in Asia did not exist in older versions of that manuscript, and the reference to them in later versions was added during the Ming dynasty. In an 1971 article in the British Journal of Ophthalmology it was argued that it: \"\"...is therefore most likely that the use of lenses reached Europe via the Arabs, as did Hindu mathematics and the ophthalmological works of the ancient Hindu surgeon Susruta\"\", but all dates given are well after the existence of eyeglasses in Italy was established, and there had been significant shipments of eye glasses from Italy to the Middle East, with one shipment as large as 24,000 glasses.\n\nThe American scientist Benjamin Franklin, who suffered from both myopia and presbyopia, invented bifocals. Serious historians have from time to time produced evidence to suggest that others may have preceded him in the invention; however, a correspondence between George Whatley and John Fenno, editor of \"The Gazette of the United States\", suggested that Franklin had indeed invented bifocals, and perhaps 50 years earlier than had been originally thought. The first lenses for correcting astigmatism were designed by the British astronomer George Airy in 1825.\n\nOver time, the construction of frames for glasses also evolved. Early eyepieces were designed to be either held in place by hand or by exerting pressure on the nose (\"pince-nez\"). Girolamo Savonarola suggested that eyepieces could be held in place by a ribbon passed over the wearer's head, this in turn secured by the weight of a hat. The modern style of glasses, held by temples passing over the ears, was developed some time before 1727, possibly by the British optician Edward Scarlett. These designs were not immediately successful, however, and various styles with attached handles such as \"scissors-glasses\" and lorgnettes were also fashionable from the second half of the 18th century and into the early 19th century.\n\nIn the early 20th century, Moritz von Rohr and Zeiss (with the assistance of H. Boegehold and A. Sonnefeld), developed the Zeiss Punktal spherical point-focus lenses that dominated the eyeglass lens field for many years. In 2008, Joshua Silver designed eyewear with adjustable corrective glasses. They work by silicone liquid, a syringe, and a pressure mechanism.\n\nDespite the increasing popularity of contact lenses and laser corrective eye surgery, glasses remain very common, as their technology has improved. For instance, it is now possible to purchase frames made of special memory metal alloys that return to their correct shape after being bent. Other frames have spring-loaded hinges. Either of these designs offers dramatically better ability to withstand the stresses of daily wear and the occasional accident. Modern frames are also often made from strong, light-weight materials such as titanium alloys, which were not available in earlier times.\n\nIn the 1930s, \"spectacles\" were described as \"medical appliances.\" Wearing spectacles was sometimes considered socially humiliating. \nIn the 1970s, fashionable glasses started to become available through manufacturers, and the government also recognized the demand for stylized eyewear.\n\nGraham Pullin describes how devices for disability, like glasses, have traditionally been designed to camouflage against the skin and restore ability without being visible. In the past, design for disability has \"been less about projecting a positive image as about trying not to project an image at all.\" Pullin uses the example of spectacles, traditionally categorized as a medical device for \"patients\", and outlines how they are now described as eyewear: a fashionable accessory. Much like other fashion designs and accessories, eyewear is created by designers, has reputable labels, and comes in collections, by season and designer. It is becoming more common for consumers purchase eyewear with clear, non-prescription lenses, illustrating that glasses are no longer a social stigma, but a fashionable accessory that \"frames your face.\"\n\nSome organizations like Lions Clubs International, Unite For Sight, ReSpectacle, and New Eyes for the Needy provide a way to donate glasses and sunglasses. Unite For Sight has redistributed more than 200,000 pairs.\n\nMany people require glasses for the reasons listed above. There are many shapes, colors, and materials that can be used when designing frames and lenses that can be utilized in various combinations. Oftentimes, the selection of a frame is made based on how it will affect the appearance of the wearer. Some people with good natural eyesight like to wear eyeglasses as a style accessory.\n\nFor most of their history, eyeglasses were seen as unfashionable, and carried several potentially negative connotations: wearing glasses caused individuals to be stigmatized and stereotyped as pious clergymen (as those in religious vocation were the most likely to be literate and therefore the most likely to need reading glasses), elderly, or physically weak and passive. The stigma began to fall away in the early 1900s when the popular Theodore Roosevelt was regularly photographed wearing eyeglasses, and in the 1910s when popular comedian Harold Lloyd began wearing a pair of horn-rimmed glasses as the \"Glasses\" character in his films.\nSince, eyeglasses have become an acceptable fashion item and often act as a key component in individuals' personal image. Musicians Buddy Holly and John Lennon became synonymous with the styles of eye-glasses they wore to the point that thick, black horn-rimmed glasses are often called \"Buddy Holly glasses\" and perfectly round metal eyeglass frames called \"John Lennon (or Harry Potter) Glasses.\" British comedic actor Eric Sykes was known in the United Kingdom for wearing thick, square, horn-rimmed glasses, which were in fact a sophisticated hearing aid that alleviated his deafness by allowing him to \"hear\" vibrations. Some celebrities have become so associated with their eyeglasses that they continued to wear them even after taking alternate measures against vision problems: United States Senator Barry Goldwater and comedian Drew Carey continued to wear non-prescription glasses after being fitted for contacts and getting laser eye surgery, respectively.\n\nOther celebrities have used glasses to differentiate themselves from the characters they play, such as Anne Kirkbride, who wore oversized, 1980s-style round horn-rimmed glasses as Deirdre Barlow in the soap opera \"Coronation Street\", and Masaharu Morimoto, who wears glasses to separate his professional persona as a chef from his stage persona as Iron Chef Japanese. In 2012 some NBA players wear lensless glasses with thick plastic frames like horn-rimmed glasses during post-game interviews, geek chic that draws comparisons to Steve Urkel.\n\nIn superhero fiction, eyeglasses have become a standard component of various heroes' disguises (as masks), allowing them to adopt a nondescript demeanor when they are not in their superhero persona: Superman is well known for wearing 1950s style horn-rimmed glasses as Clark Kent, while Wonder Woman wears either round, Harold Lloyd style glasses or 1970s style bug-eye glasses as Diana Prince. An example of the halo effect is seen in the stereotype that those who wear glasses are intelligent.\n\nIn the 20th century, eyeglasses came to be considered a component of fashion; as such, various different styles have come in and out of popularity. Most are still in regular use, albeit with varying degrees of frequency.\n\n\n",
                "Corrective lens\n\nA corrective lens is a lens typically worn in front of the eye to improve vision. The most common use is to treat refractive errors: myopia, hypermetropia, astigmatism, and presbyopia. Glasses or \"spectacles\" are worn on the face a short distance in front of the eye. Contact lenses are worn directly on the surface of the eye. Intraocular lenses are surgically implanted most commonly after cataract removal, but can be used for purely refractive purposes. \n\nCorrective lenses are typically prescribed by an ophthalmologist or an optometrist. The prescription consists of all the specifications necessary to make the lens. Prescriptions typically include the power specifications of each lens (for each eye). Strengths are generally prescribed in quarter-diopter steps (0.25 D) because most people cannot generally distinguish between smaller increments (e.g., eighth-diopter steps / 0.125 D). The use of improper corrective lenses may not be helpful and can even exacerbate binocular vision disorders. Eyecare professionals (optometrists and ophthalmologists) are trained to determine the specific corrective lenses that will provide the clearest, most comfortable and most efficient vision, avoiding double vision and maximizing binocularity. \n\nEvery corrective lens prescription includes a spherical correction in diopters. Convergent powers are positive (e.g., +4.00 D) and condense light to correct for farsightedness (hyperopia) or allow the patient to read more comfortably (see presbyopia and binocular vision disorders). Divergent powers are negative (e.g., −3.75 D) and spread out light to correct for nearsightedness (myopia). If neither convergence nor divergence is required in the prescription, \"plano\" is used to denote a refractive power of zero.\n\nThe term \"sphere\" comes from the geometry of lenses. Lenses derive their power from curved surfaces. A spherical lens has the same curvature in every direction perpendicular to the optical axis. Spherical lenses are adequate correction when a person has no astigmatism. To correct for astigmatism, the \"cylinder\" and \"axis\" components specify how a particular lens is different from a lens composed of purely spherical surfaces.\n\nPatients with astigmatism need a toric lens to see clearly. The geometry of a toric lens focuses light differently in different meridians. A meridian in this case is a plane that is perpendicular to the optical axis. For example, a toric lens, when rotated correctly, could focus an object to the image of a horizontal line at one focal distance while focusing a vertical line to a separate focal distance.\n\nThe power of a toric lens can be specified by describing how the cylinder (the meridian that is most different from the spherical power) differs from the spherical power. Power evenly transitions between the two powers in moving from the meridian with the most convergence to the meridian with the least convergence. For regular toric lenses, these powers are perpendicular to each other and their location relative to vertical and horizontal are specified by the axis component.\n\nThere are two different conventions for indicating the amount of cylinder: \"plus cylinder notation\" and \"minus cylinder notation\". In the former, the cylinder power is a number of diopters more convergent than the sphere power. That means the spherical power describes the most divergent meridian and the cylindrical component describes the most convergent. In the minus cylinder notation, the cylinder power is a number of diopters more divergent than the sphere component. In this convention, the sphere power describes the most convergent meridian and the cylinder component describes the most divergent. Europe typically follows the plus cylinder convention while in the US the minus cylinder notation is used by optometrists and the plus cylinder notation is used by ophthalmologists. Minus cylinder notation is also more common in Asia, although either style may be encountered there. There is no difference in these forms of notation and it is easy to convert between them:\nFor example, a lens with a vertical power of -3.75 and a horizontal power of -2.25 could be specified as either -2.25 -1.50 x 180 or -3.75 +1.50 x 090.\n\nThe axis defines the location of the sphere and cylinder powers. The name \"axis\" comes from the concept of generating a cylinder by rotating a line around an axis. The curve of that cylinder is 90° from that axis of rotation. When dealing with toric lenses, the axis defines the orientation of the steepest and flattest curvatures relative to horizontal and vertcal. The \"3 o'clock\" position is defined as zero, and the 90th meridian is a vertical line. A horizontal line passes through both zero and the 180th meridians. By convention, a horizontal axis is recorded as 180.\n\nIn a regular toric lens, the flattest and steepest curvatures are separated by 90°. As a result, the axis of the cylinder is also the meridian with the same power as the recorded sphere power. The cylinder power, as defined above is the power that is most different from the sphere power. Because they are defined relative to each other, it is important to know if the lens is being described in minus cylinder notation, where the sphere power is the most convergent / least divergent power. When using plus cylinder notation, the opposite is true.\n\nIf the lens is spherical (there is no cylinder component) then there is no need for an axis. A prescription like this is written with D.S. (diopters sphere) after the sphere power (e.g., −3.00 D.S.). This verifies that the prescription is truly spherical rather than the cylinder power being omitted in error.\n\n\nA prescription of \"−1.00 +0.25 x 180\" describes a lens that has a horizontal power of −1.00 D and a vertical power of −0.75.\n\nReady-made single-vision reading glasses go by many names, including over-the-counter glasses, ready readers, cheaters, magnifiers, non-prescription readers, or generic readers. They are designed to lessen the focusing burden of near work, such as reading. They are typically sold in retail locations such as pharmacies and grocery stores, but are also available in book stores and clothing retailers. They are available in common reading prescriptions with strengths ranging from +0.75 to +3.50 diopters. While these \"magnifiers\" do indeed make the image of the viewed object bigger, their main advantage comes from focusing the image, not magnification. \n\nThese glasses are not tailored to a person's individual needs. A difference in refractive error between the eyes or presence of astigmatism will not be accounted for. People with little to no need for correction in the distance may find off-the-shelf glasses work quite well for seeing better during near vision tasks. But if the person has a significant need for distance correction, it is less likely that the over the counter glasses will be perfectly effective. Although such glasses are generally considered safe, an individual prescription, as determined by an ophthalmologist or optometrist and made by a qualified optician, usually results in better visual correction and fewer headaches and visual discomfort. Another criticism of over the counter glasses is that they may alleviate symptoms, causing a person to forgo the other benefits of routine vision exams, such as early diagnosis of chronic disease.\n\nAlthough lenses are normally prescribed by optometrists or ophthalmologists, there is evidence from developing countries that allowing people to select lenses for themselves produces good results in the majority of cases and is less than a tenth of the cost of prescription lenses. \n\nSingle vision lenses correct for only one distance. If they correct for far distance, the person must accommodate to see clearly up close. If the person cannot accommodate, they may need a separate correction for near distances, or else use a multifocal lens (see below).\n\nReading glasses are single vision lenses designed for near work, and include over the counter glasses. They come in two main styles: full frames, in which the entire lens is made in the reading prescription, and half-eyes, style glasses that sit lower down on the nose. Full frame readers must be removed to see distance clearly, while the distance can be clearly viewed over the top of half-eye readers.\n\nA bifocal is a lens with two sections, separated by a line (see image to the right). Generally, the upper part of the lens is used for distance vision, while the lower segment is used for near vision. The area of the lens that caters to near vision is called the add segment. There are many different shapes, sizes, and positions for the add segment that are selected for functional differences as well as the visual demands of the patient. Bifocals allow people with presbyopia to see clearly at distance and near without having to remove the glasses, which would be required with single vision correction.\n\nTrifocal lenses are similar to bifocals, except that the two focal areas are separated by a third area (with intermediate focus correction) in the middle. This segment corrects the wearer's vision for intermediate distances roughly at arms' length, \"e.g.\" computer distance. This lens type has two segment lines, dividing the three different correcting segments.\n\nProgressive addition or varifocal lenses provide a smooth transition from distance correction to near correction, eliminating segment lines and allowing clear vision at all distances, including intermediate (roughly arms' length). The lack of any abrupt change in power and the uniform appearance of the lens gives rise to the name \"no-line bifocal\".\n\nMultifocal contact lenses (e.g. bifocals or progressives) are comparable to spectacles with bifocals or progressive lenses because they have multiple focal points. Multifocal contact lenses are typically designed for constant viewing through the center of the lens, but some designs do incorporate a shift in lens position to view through the reading power (similar to bifocal glasses).\n\nThe power or focal length of adjustable or variable focus can be changed to suit the needs of the wearer. A typical application of such a lens is to refocus the correction allowing clear vision at any distance. Unlike with bifocals, near-vision correction is achieved over the entire field of view, in any direction. Switching between distance and near vision is accomplished by re-adjusting the lens, instead of by tilting and/or rotating the head. The need for constant adjustment when the person's attention switches to an object at a different distance is a design challenge of such a lens. Manual adjustment is more cumbersome than bifocals or similar lenses. Automated systems require electronic systems, power supplies, and sensors that increase the cost, size, and weight of the correction.\n\nA corrective lens with a power of zero is called a plano lens. These lenses are used when one or both eyes do not require correction of a refractive error. Some people with good natural eyesight like to wear eyeglasses as a style accessory, or want to change the appearance of their eyes using novelty contact lenses.\n\nAlthough corrective lenses can be produced in many different profiles, the most common is ophthalmic or convex-concave. In an ophthalmic lens, both the front and back surface have a positive radius, resulting in a positive / convergent front surface and a negative / divergent back surface. The difference in curvature between the front and rear surface leads to the corrective power of the lens. In hyperopia a convergent lens is needed, therefore the convergent front surface overpowers the divergent back surface. For myopia the opposite is true: the divergent back surface is greater in magnitude than the convergent front surface. To correct for presbyopia, the lens, or section of the lens, must be more convergent or less divergent than the person's distance lens.\n\nThe base curve (usually determined from the profile of the front surface of an ophthalmic lens) can be changed to result in the best optic and cosmetic characteristics across the entire surface of the lens. Optometrists may choose to specify a particular base curve when prescribing a corrective lens for either of these reasons. A multitude of mathematical formulas and professional clinical experience has allowed optometrists and lens designers to determine standard base curves that are ideal for most people. As a result, the front surface curve is more standardized and the characteristics that generate a person's unique prescription are typically derived from the geometry of the back surface of the lens.\n\nBifocals and trifocals result in a more complex lens profile, compounding multiple surfaces. The main lens is composed of a typical ophthalmic lens. Thus the base curve defines the front surface of the main part of the lens while the back surface geometry is changed to achieve the desired distance power. The \"bifocal\" is a third spherical segment, called an \"add segment\", found on the front surface of the lens. Steeper and more convergent than the base curve, the add segment combines with the back surface to deliver the person's near correction. Early manufacturing techniques fused a separate lens to the front surface, but modern processes cut all the geometry into a single piece of lens material. There are many locations, profiles, and sizes of add segments, typically referred to as segment type. Some \"seg type\" examples include Flat top, Kryptok, Orthogon, Tillyer Executive, and Ultex A. Trifocals contain two add segments to achieve a lens that corrects the person's vision for three distinct distances.\n\nThe optical center of the add segment may be placed on the lens surface or may hang off into empty space near the lens surface. Although the surface profile of a bifocal segment is spherical, it is often trimmed to have straight edges so that it is contained within a small region of the overall lens surface.\n\nThe progressive addition lens (PAL, also commonly called a no-line or varifocal lens) eliminates the line in bi/tri-focals and is very complex in its profile. PALs are a continuously variable parametric surface that begins using one spherical surface base curve and ends at another, with the radius of curvature continuously varying as the transition is made from one surface to the other. This shift in curvature results in different powers being delivered from different locations on the lens.\nVertex distance is the space between the front of the eye and the back surface of the lens. In glasses with powers beyond ±4.00D, the vertex distance can affect the effective power of the glasses. A shorter vertex distance can expand the field of view, but if the vertex distance is too small, the eyelashes will come into contact with the back of the lens, smudging the lens and causing annoyance for the patient. A skilled frame stylist will help the patient select a good balance of fashionable frame size with good vertex distance in order to achieve ideal aesthetics and field of view. The average vertex distance in a pair of glasses is 12-14mm. A contact lens is placed directly on the eye and thus has a vertex distance of zero.\n\nIn the UK and the US, the refractive index is generally specified with respect to the yellow He-d Fraunhofer line, commonly abbreviated as n. Lens materials are classified by their refractive index, as follows:\n\n\nThis is a general classification. Indexes of n values that are ≥ 1.60 can be, often for marketing purposes, referred to as high-index. Likewise, Trivex and other borderline normal/mid-index materials, may be referred to as mid-index.\n\n\n\nOf all of the properties of a particular lens material, the one that most closely relates to its optical performance is its dispersion, which is specified by the Abbe number. Lower Abbe numbers result in the presence of chromatic aberration (i.e., color fringes above/below or to the left/right of a high contrast object), especially in larger lens sizes and stronger prescriptions (beyond ±4.00D). Generally, lower Abbe numbers are a property of mid and higher index lenses that cannot be avoided, regardless of the material used. The Abbe number for a material at a particular refractive index formulation is usually specified as its Abbe value.\n\nIn practice, a change from 30 to 32 Abbe will not have a practically noticeable benefit, but a change from 30 to 47 could be beneficial for users with strong prescriptions that move their eyes and look \"off-axis\" of optical center of the lens. Note that some users do not sense color fringing directly but will just describe \"off-axis blurriness\".\nAbbe values even as high as that of (V≤45) produce chromatic aberrations which can be perceptible to a user in lenses larger than 40 mm in diameter and especially in strengths that are in excess of ±4D. At ±8D even glass (V≤58) produces chromatic aberration that can be noticed by a user. Chromatic aberration is independent of the lens being of spherical, aspheric, or atoric design.\n\nThe eye's Abbe number is independent of the importance of the corrective lens's Abbe, since the human eye:\nIn contrast, the eye moves to look through various parts of a corrective lens as it shifts its gaze, some of which can be as much as several centimeters off of the optical center. Thus, despite the eye's dispersive properties, the corrective lens's dispersion cannot be dismissed. People who are sensitive to the effects of chromatic aberrations, or who have stronger prescriptions, or who often look off the lens’s optical center, or who prefer larger corrective lens sizes may be impacted by chromatic aberration. To minimize chromatic aberration:\n\nPower error is the change in the optical power of a lens as the eye looks through various points on the area of the lens. Generally, it is least present at the optic center and gets progressively worse as one looks towards the edges of the lens. The actual amount of power error is highly dependent on the strength of the prescription as well as whether a best spherical form of lens or an optically optimal aspherical form was used in the manufacture of the lens. Generally, best spherical form lenses attempt to keep the ocular curve between four and seven diopters.\n\nAs the eye shifts its gaze from looking through the optical center of the corrective lens, the lens-induced astigmatism value increases. In a spherical lens, especially one with a strong correction whose base curve is not in the best spherical form, such increases can significantly impact the clarity of vision in the periphery.\n\nAs corrective power increases, even optimally designed lenses will have distortion that can be noticed by a user. This particularly affects individuals that use the off-axis areas of their lenses for visually demanding tasks. For individuals sensitive to lens errors, the best way to eliminate lens induced aberrations is to use contact lenses. Contacts eliminate all these aberrations since the lens then moves with the eye.\n\nBarring contacts, a good lens designer doesn’t have many parameters which can be traded off to improve vision. Index has little effect on error. Note that, although chromatic aberration is often perceived as \"blurry vision\" in the lens periphery and gives the impression of power error, this is actually due to color shifting. Chromatic aberration can be improved by using a material with improved ABBE. The best way to combat lens induced power error is to limit the choice of corrective lens to one that is in the best spherical form. A lens designer determines the best-form spherical curve using the Oswalt curve on the Tscherning ellipse. This design gives the best achievable optical quality and least sensitivity to lens fitting. A flatter base-curve is sometimes selected for cosmetic reasons. Aspheric or atoric design can reduce errors induced by using a suboptimal flatter base-curve. They cannot surpass the optical quality of a spherical best-form lens, but can reduce the error induced by using a flatter than optimal base curve. The improvement due to flattening is most evident for strong farsighted lenses. High myopes (-6D) may see a slight cosmetic benefit with larger lenses. Mild prescriptions will have no perceptible benefit (-2D). Even at high prescriptions some high myope prescriptions with small lenses may not see any difference, since some aspheric lenses have a spherically designed center area for improved vision and fit.\n\nIn practice, labs tend to produce pre-finished and finished lenses in groups of narrow power ranges to reduce inventory. Lens powers that fall into the range of the prescriptions of each group share a constant base curve. For example, corrections from -4.00D to -4.50D may be grouped and forced to share the same base curve characteristics, but the spherical form is only best for a -4.25D prescription. In this case the error will be imperceptible to the human eye. However, some manufacturers may further cost-reduce inventory and group over a larger range which will result in perceptible error for some users in the range who also use the off-axis area of their lens. Additionally some manufacturers may verge toward a slightly flatter curve. Although if only a slight bias toward plano is introduced it may be negligible cosmetically and optically. These optical degradations due to base-curve grouping also apply to aspherics since their shapes are intentionally flattened and then asphericized to minimize error for the average base curve in the grouping.\n\nNote that the greatest cosmetic improvement on lens thickness (and weight) is had from choosing a frame which holds physically small lenses. The smallest of the popular adult lens sizes available in retail outlets is about across. There are a few adult sizes of , and although they are quite rare, can reduce lens weight to about half of the 50 mm versions. The curves on the front and back of a lens are ideally formed with the specific radius of a sphere. This radius is set by the lens designer based on the prescription and cosmetic consideration. Selecting a smaller lens will mean less of this sphere surface is represented by the lens surface, meaning the lens will have a thinner edge (myopia) or center (hyperopia). A thinner edge reduces light entering into the edge, reducing an additional source of internal reflections.\n\nExtremely thick lenses for myopia can be beveled to reduce flaring out of the very thick edge. Thick myopic lenses are not usually mounted in wire frames, because the thin wire contrasts against the thick lens, to make its thickness much more obvious to others.\n\nIndex can improve the lens thinness, but at a point no more improvement will be realized. For example, if an index and lens size is selected with center to edge thickness difference of 1 mm then changing index can only improve thickness by a fraction of this. This is also true with aspheric design lenses.\n\nThe lens's minimum thickness can also be varied. The FDA ball drop test (5/8\" 0.56 ounce steel ball dropped from 50 inches) effectively sets the minimum thickness of materials. Glass or CR-39 requires 2.0 mm, but some newer materials only require 1.5 mm or even 1.0 mm minimum thickness.\n\nMaterial density typically increases as lens thickness is reduced by increasing index. There is also a minimum lens thickness required to support the lens shape. These factors results in a thinner lens which is not lighter than the original. There are lens materials with lower density at higher index which can result in a truly lighter lens. These materials can be found in a material property table. Reducing frame lens size will give the most noticeable improvement in weight for a given material. Ways to reduce the weight and thickness of corrective lenses, in approximate order of importance are these:\n\nIt is not always possible to follow the above points, because of the rarity of such frames, and the need for more pleasing appearance. However, these are the main factors to consider if ever it should become necessary and possible to do so.\n\nEyeglasses for a high-diopter nearsighted or farsighted person cause a visible distortion of his or her face as seen by other people, in the apparent size of the eyes and facial features visible through the eyeglasses.\n\nEither situation can result in social stigma due to some facial distortions. This can result in low self-esteem of the eyeglass wearer and lead to difficulty in making friends and developing relationships.\n\nPeople with very high-power corrective lenses can benefit socially from contact lenses because these distortions are minimized and their facial appearance to others is normal. Aspheric/atoric eyeglass design can also reduce minification and magnification of the eye for observers at some angles.\n\nSchott B270 is an optical glass used in precision optics. It is NOT an ophthalmic glass. Schott ophthalmic glass types are S-1 and S-3. The issue here is incorrect numbers for UVA and UVB transmission, as well as other related product type issues.\n\nGlass lenses have become less common owing to the danger of shattering and their relatively high weight compared to CR-39 plastic lenses. They still remain in use for specialised circumstances, for example in extremely high prescriptions (currently, glass lenses can be manufactured up to a refractive index of 1.9) and in certain occupations where the hard surface of glass offers more protection from sparks or shards of material. If the highest Abbe value is desired, the only choices for common lens optical material are optical crown glass and CR-39.\n\nHigher-quality optical-grade glass materials exist (e.g. Borosilicate crown glasses such as BK7 (n=1.51680 / V=64.17 / D=2.51 g/cm³), which is commonly used in telescopes and binoculars, and fluorite crown glasses such as Schott N-FK51A (n=1.48656 / V=84.47 / D=3.675 g/cm³), which is 16.2 times the price of a comparable amount of BK7, and are commonly used in high-end camera lenses). However, one would be very hard pressed to find a laboratory that would be willing to acquire or shape custom eyeglass lenses, considering that the order would most likely consist of just two different lenses, out of these materials. Generally, V values above 60 are of dubious value, except in combinations of extreme prescriptions, large lens sizes, a high wearer sensitivity to dispersion, and occupations that involve work with high contrast elements (e.g. reading dark print on very bright white paper, construction involving contrast of building elements against a cloudy white sky, a workplace with recessed can or other concentrated small area lighting, etc.).\n\nFor CR-39:\n\nPlastic lenses are currently the most commonly prescribed lens, owing to their relative safety, low cost, ease of production, and high optical quality. The main drawbacks of many types of plastic lenses are the ease by which a lens can be scratched, and the limitations and costs of producing higher-index lenses. CR-39 lenses are an exception in that they have inherent scratch resistance.\n\n\nTrivex was developed in 2001 by PPG Industries for the military as transparent armor. With Hoya Corporation and Younger Optics PPG announced the availability of Trivex for the optical industry in 2001. Trivex is a urethane based pre-polymer. PPG named the material Trivex because of its three main performance properties, superior optics, ultra light weight, and extreme strength.\n\nTrivex is a relative newcomer that possesses the UV-blocking properties and shatter resistance of polycarbonate while at the same time offering far superior optical quality (i.e., higher Abbe value) and a slightly lower density. Its lower refractive index of 1.532 vs. polycarbonate's 1.586 may result in slightly thicker lenses depending upon the prescription. Along with polycarbonate and the various high-index plastics, Trivex is a lab favorite for use in rimless frames, owing to the ease with which it can be drilled and its resistance to cracking around the drill holes. One other advantage that Trivex has over polycarbonate is that it can be tinted.\n\n\nPolycarbonate is lighter weight than normal plastic. It blocks UV rays, is shatter resistant and is used in sports glasses and glasses for children and teenagers. Because polycarbonate is soft and will scratch easily, scratch resistant coating is typically applied after shaping and polishing the lens. Standard polycarbonate with an Abbe value of 30 is one of the worst materials optically, if chromatic aberration intolerance is of concern. Along with Trivex and the high-index plastics, polycarbonate is an excellent choice for rimless eyeglasses. Similar to the high-index plastics, polycarbonate has a very low Abbe value, which may be bothersome to individuals sensitive to chromatic aberrations.\n\n\nHigh-index plastics allow for thinner lenses. The lenses may not be lighter, however, due to the increase in density vs. mid- and normal index materials. A disadvantage is that high-index plastic lenses suffer from a much higher level of chromatic aberrations, which can be seen from their lower Abbe value. Aside from thinness of the lens, another advantage of high-index plastics is their strength and shatter resistance, although not as shatter resistant as polycarbonate. This makes them particularly suitable for rimless eyeglasses.\n\nThese high-refractive-index plastics are typically thiourethanes, with the sulfur atoms in the polymer being responsible for the high refractive index. The sulfur content can be up to 60 percent by weight for an n=1.74 material.\n\nReflected light calculated using Fresnel reflection equation for normal waves against air on two interfaces. This is reflection without an AR coating.\n\nIndices of refraction for a range of materials can be found in the list of refractive indices.\n\nAnti-reflective coatings help to make the eye behind the lens more visible. They also help lessen back reflections of the white of the eye as well as bright objects behind the eyeglasses wearer (e.g. windows, lamps). Such reduction of back reflections increases the apparent contrast of surroundings. At night, anti-reflective coatings help to reduce headlight glare from oncoming cars, street lamps and heavily lit or neon signs.\n\nOne problem with anti-reflective coatings is that historically they have been very easy to scratch. Newer coatings, such as Crizal Alizé UV with its 5.0 rating and Hoya's Super HiVision with its 10.9 rating on the COLTS Bayer Abrasion Test (glass averages 12–14), try to address this problem by combining scratch resistance with the anti-reflective coating. They also offer a measure of dirt and smudge resistance, due to their hydrophobic properties (110° water drop contact angle for Super HiVision and 116° for Crizal Alizé UV).\n\nA UV coating is used to reduce the transmission of light in the ultraviolet spectrum. UV-B radiation increases the likelihood of cataracts, while long-term exposure to UV-A radiation can damage the retina. DNA damage from UV light is cumulative and irreversible. Some materials such as Trivex and Polycarbonate, naturally block most UV light; they have UV-cutoff wavelengths just outside the visible range, and do not benefit from the application of a UV coating. Many modern anti-reflective coatings also block UV.\n\nResists damage to lens surfaces from minor scratches.\n\nLens manufacturers claim that aspheric lenses improve vision over traditional spheric lenses. This statement could be misleading to individuals who do not know that the lenses are being implicitly compared to \"a spheric flattened away from best-form for cosmetic reasons\". This qualification is necessary since best-form spherics are always better than aspherics for an ophthalmic lens application. Aspherics are only used for corrective lenses when, in order to achieve a flatter lens for cosmetic reasons, the lens design deviates from best-form sphere; this results in degradation of the visual correction, degradation which can, in some part, be compensated for by an aspheric design. The same is true for atoric and bi-aspheric.\n\nWhile it is true that aspheric lenses are used in cameras and binoculars, it would be wrong to assume that this means aspherics/atorics result in better optics for eyewear. Cameras and telescopes use multiple lens elements and have different design criteria. Spectacles are made of only one ophthalmic lens, and the best-form spheric lens has been shown to give the best vision. In cases where best-form is not used, such as cosmetic flattening, thinning or wrap-around sunglasses, an aspheric design can reduce the amount of induced optical distortions.\n\nIt is worth noting that aspheric lenses are a broad category. A lens is made of two curved surfaces, and an aspheric lens is a lens where one or both of those surfaces is not spherical. Further research and development is being conducted to determine whether the mathematical and theoretical benefits of aspheric lenses can be implemented in practice in a way that results in better vision correction.\n\nOptical terms are used to describe error in the eye's lens and the corrective lens. This can cause confusion, since \"astigmatism\" or \"ABBE\" has drastically different impact on vision depending on which lens has the error.\n\nAstigmatism of the eye: People prescribed a sphere and a cylinder prescription have astigmatism of the eye and can be given a toric lens to correct it.\n\nAstigmatism of the corrective lens: This phenomenon is called lens-induced oblique astigmatism error (OAE) or power error and is induced when the eye looks through the ophthalmic lens at a point oblique to the optical center (OC). This may become especially evident beyond -6D.\n\nExample: A patient with astigmatism (or no astigmatism) of the eye and a high prescription may notice astigmatism of the lens (OAE) when looking through the corner of their glasses.\n\nIn ophthalmic terminology, \"aspheric lens\" specifically refers to a subclass of aspheric lens. Designs that feature \"flatter\" curves trade optical quality for cosmetic appearance. By using a non-spheric lens shape, an aspheric lens attempts to correct the error induced by flattening the lens. Typically, the design focuses on reducing the error (OAE) across the horizontal and vertical lens axis edges. This is of primarily benefit to farsighted individuals, whose lenses have a thick center.\n\nAn atoric lens design refers to a lens with more complex aspheric lens design. An atoric lens design can address error over more corners of the lens, not just the horizontal and vertical axis.\n\nA toric lens is designed to compensate for the astigmatism of a patient's eye. Even though this lens is technically \"aspheric\", the terms \"aspheric\" and \"atoric\" are reserved for lenses which correct errors induced by cosmetic lens flattening.\n\nIn the U.S., laws at the federal and state level govern the provision and effective dates of prescriptions for contact lenses and eyeglasses. Federal law requires that eyeglass and contact lens prescriptions be given to every consumer, and that the prescriptions be for a minimum of one year. (FTC Section 456.2 “Separation of examination and dispensing” was reviewed in 2004: FTC 2004 review of section 456.2)).\n\nState laws vary. For example, California law also requires prescriptions to be provided to clients whether demanded or not. Eyeglass prescriptions must be for a minimum of 2 years, and contact prescriptions must be for a minimum of 1 year.\n\n",
                "Presbyopia\n\nPresbyopia is a condition associated with aging of the eye that results in progressively worsening ability to focus clearly on close objects. Symptoms include difficulty reading small print, having to hold reading material farther away, headaches, and eyestrain. Different people will have different degrees of problems. Other types of refractive errors may exist at the same time as presbyopia.\nPresbyopia is a natural part of the aging process. It is due to hardening of the lens of the eye causing the eye to focus light behind rather than on the retina when looking at close objects. It is a type of refractive error along with nearsightedness, farsightedness, and astigmatism. Diagnosis is by an eye examination.\nTreatment is typically with eye glasses. The eyeglasses used have higher focusing power in the lower portion of the lens. Off the shelf reading glasses may be sufficient for some.\nPeople over 35 are at risk for developing presbyopia and all people become affected to some degree. The condition was mentioned as early as the writings of Aristotle in the 4th century BC. Glass lenses first came into use for the problem in the late 13th century.\n\nThe first symptoms most people notice are difficulty reading fine print, particularly in low light conditions, eyestrain when reading for long periods, blurring of near objects or temporarily blurred vision when changing the viewing distance. Many extreme presbyopes complain that their arms have become \"too short\" to hold reading material at a comfortable distance.\n\nPresbyopia, like other focal imperfections, becomes less noticeable in bright sunlight when the pupil becomes smaller. As with any lens, increasing the focal ratio of the lens increases depth of field by reducing the level of blur of out-of-focus objects (compare the effect of aperture on depth of field in photography).\n\nThe onset of correction for presbyopia varies among those with certain professions and those with miotic pupils. In particular, farmers and homemakers seek correction later, whereas service workers and construction workers seek eyesight correction earlier. Scuba divers with interest in underwater photography may notice presbyopic changes while diving before they recognize the symptoms in their normal routines due to the near focus in low light conditions.\n\nMany people with near-sightedness can read comfortably without eyeglasses or contact lenses even after age forty. However, their myopia does not disappear and the long-distance visual challenges remain. Myopes considering refractive surgery are advised that surgically correcting their nearsightedness may be a disadvantage after age forty, when the eyes become presbyopic and lose their ability to accommodate or change focus, because they will then need to use glasses for reading. Myopes with astigmatism find near vision better, though not perfect, without glasses or contact lenses when presbyopia sets in, but the more astigmatism, the poorer the uncorrected near vision.\n\nA surgical technique offered is to create a \"reading eye\" and a \"distance vision eye,\" a technique commonly used in contact lens practice, known as monovision. Monovision can be created with contact lenses, so candidates for this procedure can determine if they are prepared to have their corneas reshaped by surgery to cause this effect permanently.\n\nThe cause of presbyopia is lens hardening by decreasing levels of -crystallin, a process which may be sped up by higher temperatures.\n\nIn optics, the closest point at which an object can be brought into focus by the eye is called the eye's near point. A standard near point distance of 25 cm is typically assumed in the design of optical instruments, and in characterizing optical devices such as magnifying glasses.\n\nThere is some confusion in articles over how the focusing mechanism of the eye works. In the classic book, \"Eye and Brain\" by Gregory, for example, the lens is said to be suspended by a membrane, the 'zonula', which holds it under tension. The tension is released, by contraction of the ciliary muscle, to allow the lens to become more round, for close vision. This implies the ciliary muscle, which is outside the zonula, must be circumferential, contracting like a sphincter, to slacken the tension of the zonula pulling outwards on the lens. This is consistent with the fact that our eyes seem to be in the 'relaxed' state when focusing at infinity, and also explains why no amount of effort seems to enable a myopic person to see farther away.\n\nThe ability to focus on near objects declines throughout life, from an accommodation of about 20 dioptres (ability to focus at 50 mm away) in a child, to 10 dioptres at age 25 (100 mm), and levels off at 0.5 to 1 dioptre at age 60 (ability to focus down to 1–2 meters only). The expected, maximum, and minimum amplitudes of accommodation in diopters (D) for a corrected patient of a given age can be estimated using Hofstetter's formulas: expected amplitude (D) = 18.5 - 0.3 × (age in years), maximum amplitude (D) = 25 - 0.4 × (age in years), minimum amplitude (D) = 15 - 0.25 × (age in years).\n\nIn the visual system, images captured by the eye are translated into electric signals that are transmitted to the brain where they are interpreted. As such, in order to overcome presbyopia, two main components of the visual system can be addressed: the optical system of the eye and the visual processing of the brain.\n\n\nCorrective lenses provide a range of vision correction, some as high as +4.0 diopter. Some with presbyopia choose varifocal or bifocal lenses to eliminate the need for a separate pair of reading glasses; specialized preparations of varifocals or bifocals usually require the services of an optometrist. Some newer bifocal or varifocal spectacle lenses attempt to correct both near and far vision with the same lens.\n\nContact lenses can also be used to correct the focusing loss that comes along with presbyopia. Multifocal contact lenses can be used to correct vision for both the near and the far. Some people choose contact lenses to correct one eye for near and one eye for far with a method called monovision.\n\nNew surgical procedures may also provide solutions for those who do not want to wear glasses or contacts, including the implantation of accommodative intraocular lenses. IntraCOR has been approved in Europe for treatment of both eyes (turning both corneas into multifocal lenses and so dispensing with the need for reading glasses).\n\nAnother treatment option for the correction of presbyopia in patients with emmetropia, as well as in patients with myopia, hyperopia and astigmatism is laser blended vision. This procedure uses laser refractive surgery to correct the dominant eye mainly for distance vision and the nondominant eye mainly for near vision, while the depth of field (i.e. the range of distances at which the image is in focus) of each eye is increased. As a result of the increased depth of field, the brain merges the two images, creating a blend zone, i.e. a zone which is in focus for both eyes. This allows the patient to see near, intermediate and far without glasses. Some literature also suggests the benefits achieved include the brain learning to adapt, assimilating two images, one of which is out of focus. Over time, many patients report they are unaware one eye is out of focus.\n\nSurgically implanted corneal inlays are another treatment option for presbyopia. Corneal inlays typically are implanted in the nondominant eye to minimize impact to binocular uncorrected distance vision. They seek to improve near vision in one of three ways: changing the central refractive index, increasing the depth of focus through the use of a pinhole, and reshaping the central cornea.\n\nThe term is from Greek πρέσβυς \"presbys\" meaning \"old man\" and ὤψ \"ōps\" meaning \"sight\" ( ὠπός \"ōpos\"))\n\n"
            ],
            "good_message": false,
            "loggedData": [
                {
                    "curr_message": " Glasses are typically used for vision correction, such as with reading glasses and glasses used for nearsightedness.",
                    "last_key_pressed": "<<selected>>"
                }
            ],
            "offensive": null,
            "researchTask": true,
            "selectedData": [
                "Glasses are typically used for vision correction, such as with reading glasses and glasses used for nearsightedness."
            ],
            "shown_passages": [
                [
                    "Glasses",
                    [
                        "Glasses, also known as eyeglasses or spectacles, are devices consisting of glass or hard plastic lenses mounted in a frame that holds them in front of a person's eyes, typically using a bridge over the nose and arms which rest over the ears.",
                        "Glasses are typically used for vision correction, such as with reading glasses and glasses used for nearsightedness.",
                        "Safety glasses provide eye protection against flying debris for construction workers or lab technicians; these glasses may have protection for the sides of the eyes as well as in the lenses.",
                        "Some types of safety glasses are used to protect against visible and near-visible light or radiation."
                    ]
                ],
                [
                    "Corrective lens",
                    [
                        "A corrective lens is a lens typically worn in front of the eye to improve vision.",
                        "The most common use is to treat refractive errors: myopia, hypermetropia, astigmatism, and presbyopia.",
                        "Glasses or \"spectacles\" are worn on the face a short distance in front of the eye.",
                        "Contact lenses are worn directly on the surface of the eye.",
                        "Intraocular lenses are surgically implanted most commonly after cataract removal, but can be used for purely refractive purposes.",
                        "Corrective lenses are typically prescribed by an ophthalmologist or an optometrist.",
                        "The prescription consists of all the specifications necessary to make the lens."
                    ]
                ],
                [
                    "Presbyopia",
                    [
                        "Presbyopia is a condition associated with aging of the eye that results in progressively worsening ability to focus clearly on close objects.",
                        "Symptoms include difficulty reading small print, having to hold reading material farther away, headaches, and eyestrain.",
                        "Different people will have different degrees of problems.",
                        "Other types of refractive errors may exist at the same time as presbyopia.",
                        "Presbyopia is a natural part of the aging process.",
                        "It is due to hardening of the lens of the eye causing the eye to focus light behind rather than on the retina when looking at close objects.",
                        "It is a type of refractive error along with nearsightedness, farsightedness, and astigmatism."
                    ]
                ]
            ],
            "speaker": "0_Wizard",
            "suggestionsConfig": "show",
            "suggestionsNumber": 5,
            "suggestionsShown": true,
            "text": " Glasses are typically used for vision correction, such as with reading glasses and glasses used for nearsightedness.",
            "textboxFilled": 0,
            "time": 1580233379.8249803,
            "timeTaken": 27.069,
            "totalTasks": 120,
            "turn": 18
        },
        {
            "checked_passage": {
                "no_passages_used": "no_passages_used"
            },
            "checked_sentence": {
                "no_passages_used": "no_passages_used"
            },
            "clicked_passages": {},
            "context": {
                "conversation": [
                    "I have a horse named Beauty. Do you love horses?",
                    "I don't, but I've rode the subspecies of Equus ferus before. What kind?",
                    "I'm not sure what kind it is. My dad got it for me. I don't know much about them. What is Equus ferus?",
                    " To be specific, it is an odd-toed ungulate mammal part of the taxonomic family Equidae"
                ],
                "index": 27
            },
            "contextCount": 0,
            "full_passages": [
                "Horse\n\nThe horse (\"Equus ferus caballus\") is one of two extant subspecies of \"Equus ferus\". It is an odd-toed ungulate mammal belonging to the taxonomic family Equidae. The horse has evolved over the past 45 to 55 million years from a small multi-toed creature, \"Eohippus\", into the large, single-toed animal of today. Humans began to domesticate horses around 4000 BC, and their domestication is believed to have been widespread by 3000 BC. Horses in the subspecies \"caballus\" are domesticated, although some domesticated populations live in the wild as feral horses. These feral populations are not true wild horses, as this term is used to describe horses that have never been domesticated, such as the endangered Przewalski's horse, a separate subspecies, and the only remaining true wild horse. There is an extensive, specialized vocabulary used to describe equine-related concepts, covering everything from anatomy to life stages, size, colors, markings, breeds, locomotion, and behavior.\n\nHorses' anatomy enables them to make use of speed to escape predators and they have a well-developed sense of balance and a strong fight-or-flight response. Related to this need to flee from predators in the wild is an unusual trait: horses are able to sleep both standing up and lying down, with younger horse tending to sleep significantly more than adults. Female horses, called mares, carry their young for approximately 11 months, and a young horse, called a foal, can stand and run shortly following birth. Most domesticated horses begin training under saddle or in harness between the ages of two and four. They reach full adult development by age five, and have an average lifespan of between 25 and 30 years.\n\nHorse breeds are loosely divided into three categories based on general temperament: spirited \"hot bloods\" with speed and endurance; \"cold bloods\", such as draft horses and some ponies, suitable for slow, heavy work; and \"warmbloods\", developed from crosses between hot bloods and cold bloods, often focusing on creating breeds for specific riding purposes, particularly in Europe. There are more than 300 breeds of horse in the world today, developed for many different uses.\n\nHorses and humans interact in a wide variety of sport competitions and non-competitive recreational pursuits, as well as in working activities such as police work, agriculture, entertainment, and therapy. Horses were historically used in warfare, from which a wide variety of riding and driving techniques developed, using many different styles of equipment and methods of control. Many products are derived from horses, including meat, milk, hide, hair, bone, and pharmaceuticals extracted from the urine of pregnant mares. Humans provide domesticated horses with food, water and shelter, as well as attention from specialists such as veterinarians and farriers.\n\nSpecific terms and specialized language are used to describe equine anatomy, different life stages, colors and breeds.\n\nDepending on breed, management and environment, the modern domestic horse has a life expectancy of 25 to 30 years. Uncommonly, a few animals live into their 40s and, occasionally, beyond. The oldest verifiable record was \"Old Billy\", a 19th-century horse that lived to the age of 62. In modern times, Sugar Puff, who had been listed in \"Guinness World Records\" as the world's oldest living pony, died in 2007 at age 56.\n\nRegardless of a horse or pony's actual birth date, for most competition purposes a year is added to its age each January 1 of each year in the Northern Hemisphere and each August 1 in the Southern Hemisphere. The exception is in endurance riding, where the minimum age to compete is based on the animal's actual calendar age.\n\nThe following terminology is used to describe horses of various ages:\n\n\nIn horse racing, these definitions may differ: For example, in the British Isles, Thoroughbred horse racing defines colts and fillies as less than five years old. However, Australian Thoroughbred racing defines colts and fillies as less than four years old.\n\nThe height of horses is usually measured at the highest point of the withers, where the neck meets the back. This point is used because it is a stable point of the anatomy, unlike the head or neck, which move up and down in relation to the body of the horse.\n\nIn English-speaking countries, the height of horses is often stated in units of hands and inches: one hand is equal to . The height is expressed as the number of full hands, followed by a point, then the number of additional inches, and ending with the abbreviation \"h\" or \"hh\" (for \"hands high\"). Thus, a horse described as \"15.2 h\" is 15 hands plus 2 inches, for a total of in height.\nThe size of horses varies by breed, but also is influenced by nutrition. Light riding horses usually range in height from and can weigh from . Larger riding horses usually start at about and often are as tall as , weighing from . Heavy or draft horses are usually at least high and can be as tall as high. They can weigh from about .\n\nThe largest horse in recorded history was probably a Shire horse named Mammoth, who was born in 1848. He stood high and his peak weight was estimated at . The current record holder for the world's smallest horse is Thumbelina, a fully mature miniature horse affected by dwarfism. She is tall and weighs .\n\nPonies are taxonomically the same animals as horses. The distinction between a horse and pony is commonly drawn on the basis of height, especially for competition purposes. However, height alone is not dispositive; the difference between horses and ponies may also include aspects of phenotype, including conformation and temperament.\n\nThe traditional standard for height of a horse or a pony at maturity is . An animal 14.2 h or over is usually considered to be a horse and one less than 14.2 h a pony, but there are many exceptions to the traditional standard. In Australia, ponies are considered to be those under . For competition in the Western division of the United States Equestrian Federation, the cutoff is . The International Federation for Equestrian Sports, the world governing body for horse sport, uses metric measurements and defines a pony as being any horse measuring less than at the withers without shoes, which is just over 14.2 h, and , or just over 14.2½ h, with shoes.\n\nHeight is not the sole criterion for distinguishing horses from ponies. Breed registries for horses that typically produce individuals both under and over 14.2 h consider all animals of that breed to be horses regardless of their height. Conversely, some pony breeds may have features in common with horses, and individual animals may occasionally mature at over 14.2 h, but are still considered to be ponies.\n\nPonies often exhibit thicker manes, tails, and overall coat. They also have proportionally shorter legs, wider barrels, heavier bone, shorter and thicker necks, and short heads with broad foreheads. They may have calmer temperaments than horses and also a high level of intelligence that may or may not be used to cooperate with human handlers. Small size, by itself, is not an exclusive determinant. For example, the Shetland pony which averages , is considered a pony. Conversely, breeds such as the Falabella and other miniature horses, which can be no taller than , are classified by their registries as very small horses, not ponies.\n\nHorses have 64 chromosomes. The horse genome was sequenced in 2007. It contains 2.7 billion DNA base pairs, which is larger than the dog genome, but smaller than the human genome or the bovine genome. The map is available to researchers.\n\nHorses exhibit a diverse array of coat colors and distinctive markings, described by a specialized vocabulary. Often, a horse is classified first by its coat color, before breed or sex. Horses of the same color may be distinguished from one another by white markings, which, along with various spotting patterns, are inherited separately from coat color.\n\nMany genes that create horse coat colors and patterns have been identified. Current genetic tests can identify at least 13 different alleles influencing coat color, and research continues to discover new genes linked to specific traits. The basic coat colors of chestnut and black are determined by the gene controlled by the Melanocortin 1 receptor, also known as the \"extension gene\" or \"red factor,\" as its recessive form is \"red\" (chestnut) and its dominant form is black. Additional genes control suppression of black color to point coloration that results in a bay, spotting patterns such as pinto or leopard, dilution genes such as palomino or dun, as well as graying, and all the other factors that create the many possible coat colors found in horses.\n\nHorses that have a white coat color are often mislabeled; a horse that looks \"white\" is usually a middle-aged or older gray. Grays are born a darker shade, get lighter as they age, but usually keep black skin underneath their white hair coat (with the exception of pink skin under white markings). The only horses properly called white are born with a predominantly white hair coat and pink skin, a fairly rare occurrence. Different and unrelated genetic factors can produce white coat colors in horses, including several different alleles of dominant white and the sabino-1 gene. However, there are no \"albino\" horses, defined as having both pink skin and red eyes.\n\nGestation lasts approximately 340 days, with an average range 320–370 days, and usually results in one foal; twins are rare. Horses are a precocial species, and foals are capable of standing and running within a short time following birth. Foals are usually born in the spring. The estrous cycle of a mare occurs roughly every 19–22 days and occurs from early spring into autumn. Most mares enter an \"anestrus\" period during the winter and thus do not cycle in this period. Foals are generally weaned from their mothers between four and six months of age.\n\nHorses, particularly colts, sometimes are physically capable of reproduction at about 18 months, but domesticated horses are rarely allowed to breed before the age of three, especially females. Horses four years old are considered mature, although the skeleton normally continues to develop until the age of six; maturation also depends on the horse's size, breed, sex, and quality of care. Larger horses have larger bones; therefore, not only do the bones take longer to form bone tissue, but the epiphyseal plates are larger and take longer to convert from cartilage to bone. These plates convert after the other parts of the bones, and are crucial to development.\n\nDepending on maturity, breed, and work expected, horses are usually put under saddle and trained to be ridden between the ages of two and four. Although Thoroughbred race horses are put on the track as young as the age of two in some countries, horses specifically bred for sports such as dressage are generally not put under saddle until they are three or four years old, because their bones and muscles are not solidly developed. For endurance riding competition, horses are not deemed mature enough to compete until they are a full 60 calendar months (five years) old.\n\nThe horse skeleton averages 205 bones. A significant difference between the horse skeleton and that of a human is the lack of a collarbone—the horse's forelimbs are attached to the spinal column by a powerful set of muscles, tendons, and ligaments that attach the shoulder blade to the torso. The horse's legs and hooves are also unique structures. Their leg bones are proportioned differently from those of a human. For example, the body part that is called a horse's \"knee\" is actually made up of the carpal bones that correspond to the human wrist. Similarly, the hock contains bones equivalent to those in the human ankle and heel. The lower leg bones of a horse correspond to the bones of the human hand or foot, and the fetlock (incorrectly called the \"ankle\") is actually the proximal sesamoid bones between the cannon bones (a single equivalent to the human metacarpal or metatarsal bones) and the proximal phalanges, located where one finds the \"knuckles\" of a human. A horse also has no muscles in its legs below the knees and hocks, only skin, hair, bone, tendons, ligaments, cartilage, and the assorted specialized tissues that make up the hoof.\n\nThe critical importance of the feet and legs is summed up by the traditional adage, \"no foot, no horse\". The horse hoof begins with the distal phalanges, the equivalent of the human fingertip or tip of the toe, surrounded by cartilage and other specialized, blood-rich soft tissues such as the laminae. The exterior hoof wall and horn of the sole is made of keratin, the same material as a human fingernail. The end result is that a horse, weighing on average , travels on the same bones as would a human on tiptoe. For the protection of the hoof under certain conditions, some horses have horseshoes placed on their feet by a professional farrier. The hoof continually grows, and in most domesticated horses needs to be trimmed (and horseshoes reset, if used) every five to eight weeks, though the hooves of horses in the wild wear down and regrow at a rate suitable for their terrain.\n\nHorses are adapted to grazing. In an adult horse, there are 12 incisors at the front of the mouth, adapted to biting off the grass or other vegetation. There are 24 teeth adapted for chewing, the premolars and molars, at the back of the mouth. Stallions and geldings have four additional teeth just behind the incisors, a type of canine teeth called \"tushes\". Some horses, both male and female, will also develop one to four very small vestigial teeth in front of the molars, known as \"wolf\" teeth, which are generally removed because they can interfere with the bit. There is an empty interdental space between the incisors and the molars where the bit rests directly on the gums, or \"bars\" of the horse's mouth when the horse is bridled.\n\nAn estimate of a horse's age can be made from looking at its teeth. The teeth continue to erupt throughout life and are worn down by grazing. Therefore, the incisors show changes as the horse ages; they develop a distinct wear pattern, changes in tooth shape, and changes in the angle at which the chewing surfaces meet. This allows a very rough estimate of a horse's age, although diet and veterinary care can also affect the rate of tooth wear.\n\nHorses are herbivores with a digestive system adapted to a forage diet of grasses and other plant material, consumed steadily throughout the day. Therefore, compared to humans, they have a relatively small stomach but very long intestines to facilitate a steady flow of nutrients. A horse will eat of food per day and, under normal use, drink of water. Horses are not ruminants, they have only one stomach, like humans, but unlike humans, they can utilize cellulose, a major component of grass. Horses are hindgut fermenters. Cellulose fermentation by symbiotic bacteria occurs in the cecum, or \"water gut\", which food goes through before reaching the large intestine. Horses cannot vomit, so digestion problems can quickly cause colic, a leading cause of death.\n\nThe horses' senses are based on their status as prey animals, where they must be aware of their surroundings at all times. They have the largest eyes of any land mammal, and are lateral-eyed, meaning that their eyes are positioned on the sides of their heads. This means that horses have a range of vision of more than 350°, with approximately 65° of this being binocular vision and the remaining 285° monocular vision. Horses have excellent day and night vision, but they have two-color, or dichromatic vision; their color vision is somewhat like red-green color blindness in humans, where certain colors, especially red and related colors, appear as a shade of green.\n\nTheir sense of smell, while much better than that of humans, is not quite as good as that of a dog. It is believed to play a key role in the social interactions of horses as well as detecting other key scents in the environment. Horses have two olfactory centers. The first system is in the nostrils and nasal cavity, which analyze a wide range of odors. The second, located under the nasal cavity, are the Vomeronasal organs, also called Jacobson's organs. These have a separate nerve pathway to the brain and appear to primarily analyze pheromones.\n\nA horse's hearing is good, and the pinna of each ear can rotate up to 180°, giving the potential for 360° hearing without having to move the head. Noise impacts the behavior of horses and certain kinds of noise may contribute to stress: A 2013 study in the UK indicated that stabled horses were calmest in a quiet setting, or if listening to country or classical music, but displayed signs of nervousness when listening to jazz or rock music. This study also recommended keeping music under a volume of 21 decibels. An Australian study found that stabled racehorses listening to talk radio had a higher rate of gastric ulcers than horses listening to music, and racehorses stabled where a radio was played had a higher overall rate of ulceration than horses stabled where there was no radio playing.\n\nHorses have a great sense of balance, due partly to their ability to feel their footing and partly to highly developed proprioception—the unconscious sense of where the body and limbs are at all times. A horse's sense of touch is well developed. The most sensitive areas are around the eyes, ears, and nose. Horses are able to sense contact as subtle as an insect landing anywhere on the body.\n\nHorses have an advanced sense of taste, which allows them to sort through fodder and choose what they would most like to eat, and their prehensile lips can easily sort even small grains. Horses generally will not eat poisonous plants, however, there are exceptions; horses will occasionally eat toxic amounts of poisonous plants even when there is adequate healthy food.\n\nAll horses move naturally with four basic gaits: the four-beat walk, which averages ; the two-beat trot or jog at (faster for harness racing horses); the canter or lope, a three-beat gait that is ; and the gallop. The gallop averages , but the world record for a horse galloping over a short, sprint distance is . Besides these basic gaits, some horses perform a two-beat pace, instead of the trot. There also are several four-beat \"ambling\" gaits that are approximately the speed of a trot or pace, though smoother to ride. These include the lateral rack, running walk, and tölt as well as the diagonal fox trot. Ambling gaits are often genetic in some breeds, known collectively as gaited horses. Often, gaited horses replace the trot with one of the ambling gaits.\n\nHorses are prey animals with a strong fight-or-flight response. Their first reaction to threat is to startle and usually flee, although they will stand their ground and defend themselves when flight is impossible or if their young are threatened. They also tend to be curious; when startled, they will often hesitate an instant to ascertain the cause of their fright, and may not always flee from something that they perceive as non-threatening. Most light horse riding breeds were developed for speed, agility, alertness and endurance; natural qualities that extend from their wild ancestors. However, through selective breeding, some breeds of horses are quite docile, particularly certain draft horses.\n\nHorses are herd animals, with a clear hierarchy of rank, led by a dominant individual, usually a mare. They are also social creatures that are able to form companionship attachments to their own species and to other animals, including humans. They communicate in various ways, including vocalizations such as nickering or whinnying, mutual grooming, and body language. Many horses will become difficult to manage if they are isolated, but with training, horses can learn to accept a human as a companion, and thus be comfortable away from other horses. However, when confined with insufficient companionship, exercise, or stimulation, individuals may develop stable vices, an assortment of bad habits, mostly stereotypies of psychological origin, that include wood chewing, wall kicking, \"weaving\" (rocking back and forth), and other problems.\n\nStudies have indicated that horses perform a number of cognitive tasks on a daily basis, meeting mental challenges that include food procurement and identification of individuals within a social system. They also have good spatial discrimination abilities. They are naturally curious and apt to investigate things they have not seen before. Studies have assessed equine intelligence in areas such as problem solving, speed of learning, and memory. Horses excel at simple learning, but also are able to use more advanced cognitive abilities that involve categorization and concept learning. They can learn using habituation, desensitization, classical conditioning, and operant conditioning, and positive and negative reinforcement. One study has indicated that horses can differentiate between \"more or less\" if the quantity involved is less than four.\n\nDomesticated horses may face greater mental challenges than wild horses, because they live in artificial environments that prevent instinctive behavior whilst also learning tasks that are not natural. Horses are animals of habit that respond well to regimentation, and respond best when the same routines and techniques are used consistently. One trainer believes that \"intelligent\" horses are reflections of intelligent trainers who effectively use response conditioning techniques and positive reinforcement to train in the style that best fits with an individual animal's natural inclinations.\n\nHorses are mammals, and as such are warm-blooded, or endothermic creatures, as opposed to cold-blooded, or poikilothermic animals. However, these words have developed a separate meaning in the context of equine terminology, used to describe temperament, not body temperature. For example, the \"hot-bloods\", such as many race horses, exhibit more sensitivity and energy, while the \"cold-bloods\", such as most draft breeds, are quieter and calmer. Sometimes \"hot-bloods\" are classified as \"light horses\" or \"riding horses\", with the \"cold-bloods\" classified as \"draft horses\" or \"work horses\".\n\"Hot blooded\" breeds include \"oriental horses\" such as the Akhal-Teke, Arabian horse, Barb and now-extinct Turkoman horse, as well as the Thoroughbred, a breed developed in England from the older oriental breeds. Hot bloods tend to be spirited, bold, and learn quickly. They are bred for agility and speed. They tend to be physically refined—thin-skinned, slim, and long-legged. The original oriental breeds were brought to Europe from the Middle East and North Africa when European breeders wished to infuse these traits into racing and light cavalry horses.\n\nMuscular, heavy draft horses are known as \"cold bloods\", as they are bred not only for strength, but also to have the calm, patient temperament needed to pull a plow or a heavy carriage full of people. They are sometimes nicknamed \"gentle giants\". Well-known draft breeds include the Belgian and the Clydesdale. Some, like the Percheron, are lighter and livelier, developed to pull carriages or to plow large fields in drier climates. Others, such as the Shire, are slower and more powerful, bred to plow fields with heavy, clay-based soils. The cold-blooded group also includes some pony breeds.\n\n\"Warmblood\" breeds, such as the Trakehner or Hanoverian, developed when European carriage and war horses were crossed with Arabians or Thoroughbreds, producing a riding horse with more refinement than a draft horse, but greater size and milder temperament than a lighter breed. Certain pony breeds with warmblood characteristics have been developed for smaller riders. Warmbloods are considered a \"light horse\" or \"riding horse\".\n\nToday, the term \"Warmblood\" refers to a specific subset of sport horse breeds that are used for competition in dressage and show jumping. Strictly speaking, the term \"warm blood\" refers to any cross between cold-blooded and hot-blooded breeds. Examples include breeds such as the Irish Draught or the Cleveland Bay. The term was once used to refer to breeds of light riding horse other than Thoroughbreds or Arabians, such as the Morgan horse.\n\nHorses are able to sleep both standing up and lying down. In an adaptation from life in the wild, horses are able to enter light sleep by using a \"stay apparatus\" in their legs, allowing them to doze without collapsing. Horses sleep better when in groups because some animals will sleep while others stand guard to watch for predators. A horse kept alone will not sleep well because its instincts are to keep a constant eye out for danger.\n\nUnlike humans, horses do not sleep in a solid, unbroken period of time, but take many short periods of rest. Horses spend four to fifteen hours a day in standing rest, and from a few minutes to several hours lying down. Total sleep time in a 24-hour period may range from several minutes to a couple of hours, mostly in short intervals of about 15 minutes each. The average sleep time of a domestic horse is said to be 2.9 hours per day.\n\nHorses must lie down to reach REM sleep. They only have to lie down for an hour or two every few days to meet their minimum REM sleep requirements. However, if a horse is never allowed to lie down, after several days it will become sleep-deprived, and in rare cases may suddenly collapse as it involuntarily slips into REM sleep while still standing. This condition differs from narcolepsy, although horses may also suffer from that disorder.\n\nThe horse adapted to survive in areas of wide-open terrain with sparse vegetation, surviving in an ecosystem where other large grazing animals, especially ruminants, could not. Horses and other equids are odd-toed ungulates of the order Perissodactyla, a group of mammals that was dominant during the Tertiary period. In the past, this order contained 14 families, but only three—Equidae (the horse and related species), Tapiridae (the tapir), and Rhinocerotidae (the rhinoceroses)—have survived to the present day.\n\nThe earliest known member of the family Equidae was the \"Hyracotherium\", which lived between 45 and 55 million years ago, during the Eocene period. It had 4 toes on each front foot, and 3 toes on each back foot. The extra toe on the front feet soon disappeared with the \"Mesohippus\", which lived 32 to 37 million years ago. Over time, the extra side toes shrank in size until they vanished. All that remains of them in modern horses is a set of small vestigial bones on the leg below the knee, known informally as splint bones. Their legs also lengthened as their toes disappeared until they were a hooved animal capable of running at great speed. By about 5 million years ago, the modern \"Equus\" had evolved. Equid teeth also evolved from browsing on soft, tropical plants to adapt to browsing of drier plant material, then to grazing of tougher plains grasses. Thus proto-horses changed from leaf-eating forest-dwellers to grass-eating inhabitants of semi-arid regions worldwide, including the steppes of Eurasia and the Great Plains of North America.\n\nBy about 15,000 years ago, \"Equus ferus\" was a widespread holarctic species. Horse bones from this time period, the late Pleistocene, are found in Europe, Eurasia, Beringia, and North America. Yet between 10,000 and 7,600 years ago, the horse became extinct in North America and rare elsewhere. The reasons for this extinction are not fully known, but one theory notes that extinction in North America paralleled human arrival. Another theory points to climate change, noting that approximately 12,500 years ago, the grasses characteristic of a steppe ecosystem gave way to shrub tundra, which was covered with unpalatable plants.\n\nA truly wild horse is a species or subspecies with no ancestors that were ever domesticated. Therefore, most \"wild\" horses today are actually feral horses, animals that escaped or were turned loose from domestic herds and the descendants of those animals. Only two never-domesticated subspecies, the Tarpan and the Przewalski's Horse, survived into recorded history and only the latter survives today.\n\nThe Przewalski's horse (\"Equus ferus przewalskii\"), named after the Russian explorer Nikolai Przhevalsky, is a rare Asian animal. It is also known as the Mongolian wild horse; Mongolian people know it as the \"taki\", and the Kyrgyz people call it a \"kirtag\". The subspecies was presumed extinct in the wild between 1969 and 1992, while a small breeding population survived in zoos around the world. In 1992, it was reestablished in the wild due to the conservation efforts of numerous zoos. Today, a small wild breeding population exists in Mongolia. There are additional animals still maintained at zoos throughout the world.\n\nThe tarpan or European wild horse (\"Equus ferus ferus\") was found in Europe and much of Asia. It survived into the historical era, but became extinct in 1909, when the last captive died in a Russian zoo. Thus, the genetic line was lost. Attempts have been made to recreate the tarpan, which resulted in horses with outward physical similarities, but nonetheless descended from domesticated ancestors and not true wild horses.\n\nPeriodically, populations of horses in isolated areas are speculated to be relict populations of wild horses, but generally have been proven to be feral or domestic. For example, the Riwoche horse of Tibet was proposed as such, but testing did not reveal genetic differences from domesticated horses. Similarly, the Sorraia of Portugal was proposed as a direct descendant of the Tarpan based on shared characteristics, but genetic studies have shown that the Sorraia is more closely related to other horse breeds and that the outward similarity is an unreliable measure of relatedness.\n\nBesides the horse, there are seven other species of genus \"Equus\" in the Equidae family. These are the ass or donkey, \"Equus asinus\"; the mountain zebra, \"Equus zebra\"; plains zebra, \"Equus quagga\"; Grévy's zebra, \"Equus grevyi\"; the kiang, \"Equus kiang\"; and the onager, \"Equus hemionus\".\n\nHorses can crossbreed with other members of their genus. The most common hybrid is the mule, a cross between a \"jack\" (male donkey) and a mare. A related hybrid, a hinny, is a cross between a stallion and a jenny (female donkey). Other hybrids include the zorse, a cross between a zebra and a horse. With rare exceptions, most hybrids are sterile and cannot reproduce.\n\nDomestication of the horse most likely took place in central Asia prior to 3500 BC. Two major sources of information are used to determine where and when the horse was first domesticated and how the domesticated horse spread around the world. The first source is based on palaeological and archaeological discoveries; the second source is a comparison of DNA obtained from modern horses to that from bones and teeth of ancient horse remains.\n\nThe earliest archaeological evidence for the domestication of the horse comes from sites in Ukraine and Kazakhstan, dating to approximately 3500–4000 BC. By 3000 BC, the horse was completely domesticated and by 2000 BC there was a sharp increase in the number of horse bones found in human settlements in northwestern Europe, indicating the spread of domesticated horses throughout the continent. The most recent, but most irrefutable evidence of domestication comes from sites where horse remains were interred with chariots in graves of the Sintashta and Petrovka cultures c. 2100 BC.\n\nDomestication is also studied by using the genetic material of present-day horses and comparing it with the genetic material present in the bones and teeth of horse remains found in archaeological and palaeological excavations. The variation in the genetic material shows that very few wild stallions contributed to the domestic horse, while many mares were part of early domesticated herds. This is reflected in the difference in genetic variation between the DNA that is passed on along the paternal, or sire line (Y-chromosome) versus that passed on along the maternal, or dam line (mitochondrial DNA). There are very low levels of Y-chromosome variability, but a great deal of genetic variation in mitochondrial DNA. There is also regional variation in mitochondrial DNA due to the inclusion of wild mares in domestic herds. Another characteristic of domestication is an increase in coat color variation. In horses, this increased dramatically between 5000 and 3000 BC.\n\nBefore the availability of DNA techniques to resolve the questions related to the domestication of the horse, various hypotheses were proposed. One classification was based on body types and conformation, suggesting the presence of four basic prototypes that had adapted to their environment prior to domestication. Another hypothesis held that the four prototypes originated from a single wild species and that all different body types were entirely a result of selective breeding after domestication. However, the lack of a detectable substructure in the horse has resulted in a rejection of both hypotheses.\n\nFeral horses are born and live in the wild, but are descended from domesticated animals. Many populations of feral horses exist throughout the world. Studies of feral herds have provided useful insights into the behavior of prehistoric horses, as well as greater understanding of the instincts and behaviors that drive horses that live in domesticated conditions.\n\nThere are also semi-feral horses in many parts of the world, such as Dartmoor and the New Forest in the UK, where the animals are all privately owned but live for significant amounts of time in \"wild\" conditions on undeveloped, often public, lands. Owners of such animals often pay a fee for grazing rights.\n\nThe concept of purebred bloodstock and a controlled, written breed registry has come to be particularly significant and important in modern times. Sometimes purebred horses are incorrectly or inaccurately called \"thoroughbreds\". Thoroughbred is a specific breed of horse, while a \"purebred\" is a horse (or any other animal) with a defined pedigree recognized by a breed registry. Horse breeds are groups of horses with distinctive characteristics that are transmitted consistently to their offspring, such as conformation, color, performance ability, or disposition. These inherited traits result from a combination of natural crosses and artificial selection methods. Horses have been selectively bred since their domestication. An early example of people who practiced selective horse breeding were the Bedouin, who had a reputation for careful practices, keeping extensive pedigrees of their Arabian horses and placing great value upon pure bloodlines. These pedigrees were originally transmitted via an oral tradition. In the 14th century, Carthusian monks of southern Spain kept meticulous pedigrees of bloodstock lineages still found today in the Andalusian horse.\n\nBreeds developed due to a need for \"form to function\", the necessity to develop certain characteristics in order to perform a particular type of work. Thus, a powerful but refined breed such as the Andalusian developed as riding horses with an aptitude for dressage. Heavy draft horses developed out of a need to perform demanding farm work and pull heavy wagons. Other horse breeds developed specifically for light agricultural work, carriage and road work, various sport disciplines, or simply as pets. Some breeds developed through centuries of crossing other breeds, while others descended from a single foundation sire, or other limited or restricted foundation bloodstock. One of the earliest formal registries was General Stud Book for Thoroughbreds, which began in 1791 and traced back to the foundation bloodstock for the breed. There are more than 300 horse breeds in the world today.\n\nWorldwide, horses play a role within human cultures and have done so for millennia. Horses are used for leisure activities, sports, and working purposes. The Food and Agriculture Organization (FAO) estimates that in 2008, there were almost 59,000,000 horses in the world, with around 33,500,000 in the Americas, 13,800,000 in Asia and 6,300,000 in Europe and smaller portions in Africa and Oceania. There are estimated to be 9,500,000 horses in the United States alone. The American Horse Council estimates that horse-related activities have a direct impact on the economy of the United States of over $39 billion, and when indirect spending is considered, the impact is over $102 billion. In a 2004 \"poll\" conducted by Animal Planet, more than 50,000 viewers from 73 countries voted for the horse as the world's 4th favorite animal.\n\nCommunication between human and horse is paramount in any equestrian activity; to aid this process horses are usually ridden with a saddle on their backs to assist the rider with balance and positioning, and a bridle or related headgear to assist the rider in maintaining control. Sometimes horses are ridden without a saddle, and occasionally, horses are trained to perform without a bridle or other headgear. Many horses are also driven, which requires a harness, bridle, and some type of vehicle.\n\nHistorically, equestrians honed their skills through games and races. Equestrian sports provided entertainment for crowds and honed the excellent horsemanship that was needed in battle. Many sports, such as dressage, eventing and show jumping, have origins in military training, which were focused on control and balance of both horse and rider. Other sports, such as rodeo, developed from practical skills such as those needed on working ranches and stations. Sport hunting from horseback evolved from earlier practical hunting techniques. Horse racing of all types evolved from impromptu competitions between riders or drivers. All forms of competition, requiring demanding and specialized skills from both horse and rider, resulted in the systematic development of specialized breeds and equipment for each sport. The popularity of equestrian sports through the centuries has resulted in the preservation of skills that would otherwise have disappeared after horses stopped being used in combat.\n\nHorses are trained to be ridden or driven in a variety of sporting competitions. Examples include show jumping, dressage, three-day eventing, competitive driving, endurance riding, gymkhana, rodeos, and fox hunting. Horse shows, which have their origins in medieval European fairs, are held around the world. They host a huge range of classes, covering all of the mounted and harness disciplines, as well as \"In-hand\" classes where the horses are led, rather than ridden, to be evaluated on their conformation. The method of judging varies with the discipline, but winning usually depends on style and ability of both horse and rider.\nSports such as polo do not judge the horse itself, but rather use the horse as a partner for human competitors as a necessary part of the game. Although the horse requires specialized training to participate, the details of its performance are not judged, only the result of the rider's actions—be it getting a ball through a goal or some other task. Examples of these sports of partnership between human and horse include jousting, in which the main goal is for one rider to unseat the other, and buzkashi, a team game played throughout Central Asia, the aim being to capture a goat carcass while on horseback.\n\nHorse racing is an equestrian sport and major international industry, watched in almost every nation of the world. There are three types: \"flat\" racing; steeplechasing, i.e. racing over jumps; and harness racing, where horses trot or pace while pulling a driver in a small, light cart known as a sulky. A major part of horse racing's economic importance lies in the gambling associated with it.\n\nThere are certain jobs that horses do very well, and no technology has yet developed to fully replace them. For example, mounted police horses are still effective for certain types of patrol duties and crowd control. Cattle ranches still require riders on horseback to round up cattle that are scattered across remote, rugged terrain. Search and rescue organizations in some countries depend upon mounted teams to locate people, particularly hikers and children, and to provide disaster relief assistance. Horses can also be used in areas where it is necessary to avoid vehicular disruption to delicate soil, such as nature reserves. They may also be the only form of transport allowed in wilderness areas. Horses are quieter than motorized vehicles. Law enforcement officers such as park rangers or game wardens may use horses for patrols, and horses or mules may also be used for clearing trails or other work in areas of rough terrain where vehicles are less effective.\n\nAlthough machinery has replaced horses in many parts of the world, an estimated 100 million horses, donkeys and mules are still used for agriculture and transportation in less developed areas. This number includes around 27 million working animals in Africa alone. Some land management practices such as cultivating and logging can be efficiently performed with horses. In agriculture, less fossil fuel is used and increased environmental conservation occurs over time with the use of draft animals such as horses. Logging with horses can result in reduced damage to soil structure and less damage to trees due to more selective logging.\n\nModern horses are often used to reenact many of their historical work purposes. Horses are used, complete with equipment that is authentic or a meticulously recreated replica, in various live action historical reenactments of specific periods of history, especially recreations of famous battles. Horses are also used to preserve cultural traditions and for ceremonial purposes. Countries such as the United Kingdom still use horse-drawn carriages to convey royalty and other VIPs to and from certain culturally significant events. Public exhibitions are another example, such as the Budweiser Clydesdales, seen in parades and other public settings, a team of draft horses that pull a beer wagon similar to that used before the invention of the modern motorized truck.\n\nHorses are frequently seen in television, films and literature. They are sometimes featured as a major character in films about particular animals, but also used as visual elements that assure the accuracy of historical stories. Both live horses and iconic images of horses are used in advertising to promote a variety of products. The horse frequently appears in coats of arms in heraldry, in a variety of poses and equipment. The mythologies of many cultures, including Greco-Roman, Hindu, Islamic, and Norse, include references to both normal horses and those with wings or additional limbs, and multiple myths also call upon the horse to draw the chariots of the Moon and Sun. The horse also appears in the 12-year cycle of animals in the Chinese zodiac related to the Chinese calendar.\n\nPeople of all ages with physical and mental disabilities obtain beneficial results from association with horses. Therapeutic riding is used to mentally and physically stimulate disabled persons and help them improve their lives through improved balance and coordination, increased self-confidence, and a greater feeling of freedom and independence. The benefits of equestrian activity for people with disabilities has also been recognized with the addition of equestrian events to the Paralympic Games and recognition of para-equestrian events by the International Federation for Equestrian Sports (FEI). Hippotherapy and therapeutic horseback riding are names for different physical, occupational, and speech therapy treatment strategies that utilize equine movement. In hippotherapy, a therapist uses the horse's movement to improve their patient's cognitive, coordination, balance, and fine motor skills, whereas therapeutic horseback riding uses specific riding skills.\n\nHorses also provide psychological benefits to people whether they actually ride or not. \"Equine-assisted\" or \"equine-facilitated\" therapy is a form of experiential psychotherapy that uses horses as companion animals to assist people with mental illness, including anxiety disorders, psychotic disorders, mood disorders, behavioral difficulties, and those who are going through major life changes. There are also experimental programs using horses in prison settings. Exposure to horses appears to improve the behavior of inmates and help reduce recidivism when they leave.\n\nHorses have been used in warfare for most of recorded history. The first archaeological evidence of horses used in warfare dates to between 4000 and 3000 BC, and the use of horses in warfare was widespread by the end of the Bronze Age. Although mechanization has largely replaced the horse as a weapon of war, horses are still seen today in limited military uses, mostly for ceremonial purposes, or for reconnaissance and transport activities in areas of rough terrain where motorized vehicles are ineffective. Horses have been used in the 21st century by the Janjaweed militias in the War in Darfur.\n\nHorses are raw material for many products made by humans throughout history, including byproducts from the slaughter of horses as well as materials collected from living horses.\n\nProducts collected from living horses include mare's milk, used by people with large horse herds, such as the Mongols, who let it ferment to produce kumis. Horse blood was once used as food by the Mongols and other nomadic tribes, who found it a convenient source of nutrition when traveling. Drinking their own horses' blood allowed the Mongols to ride for extended periods of time without stopping to eat. The drug Premarin is a mixture of estrogens extracted from the urine of pregnant mares (pregnant mares' urine), and was previously a widely used drug for hormone replacement therapy. The tail hair of horses can be used for making bows for string instruments such as the violin, viola, cello, and double bass.\n\nHorse meat has been used as food for humans and carnivorous animals throughout the ages. It is eaten in many parts of the world, though consumption is taboo in some cultures, and a subject of political controversy in others. Horsehide leather has been used for boots, gloves, jackets, baseballs, and baseball gloves. Horse hooves can also be used to produce animal glue. Horse bones can be used to make implements. Specifically, in Italian cuisine, the horse tibia is sharpened into a probe called a \"spinto\", which is used to test the readiness of a (pig) ham as it cures. In Asia, the saba is a horsehide vessel used in the production of kumis.\n\nHorses are grazing animals, and their major source of nutrients is good-quality forage from hay or pasture. They can consume approximately 2% to 2.5% of their body weight in dry feed each day. Therefore, a adult horse could eat up to of food. Sometimes, concentrated feed such as grain is fed in addition to pasture or hay, especially when the animal is very active. When grain is fed, equine nutritionists recommend that 50% or more of the animal's diet by weight should still be forage.\n\nHorses require a plentiful supply of clean water, a minimum of to per day. Although horses are adapted to live outside, they require shelter from the wind and precipitation, which can range from a simple shed or shelter to an elaborate stable.\n\nHorses require routine hoof care from a farrier, as well as vaccinations to protect against various diseases, and dental examinations from a veterinarian or a specialized equine dentist. If horses are kept inside in a barn, they require regular daily exercise for their physical health and mental well-being. When turned outside, they require well-maintained, sturdy fences to be safely contained. Regular grooming is also helpful to help the horse maintain good health of the hair coat and underlying skin.\n\n\n",
                "Ungulate\n\nUngulates (pronounced ) are any members of a diverse group of primarily large mammals that includes odd-toed ungulates such as horses and rhinoceroses, and even-toed ungulates such as cattle, pigs, giraffes, camels, deer, and hippopotamuses. Most terrestrial ungulates use the tips of their toes, usually hoofed, to sustain their whole body weight while moving.\n\nThe term means, roughly, \"being hoofed\" or \"hoofed animal\". As a descriptive term, \"ungulate\" normally excludes cetaceans (whales, dolphins, porpoises), as they do not possess most of the typical morphological characteristics of ungulates, but recent discoveries indicate that they are descended from early artiodactyls. Ungulates are typically herbivorous (though some species are omnivorous, such as pigs), and many employ specialized gut bacteria to allow them to digest cellulose, as in the case of ruminants. They inhabit a wide range of habitats, including jungles, plains and rivers.\n\nUngulata, which used to be considered an order, has been split into the following: Perissodactyla (odd-toed ungulates), Artiodactyla (even-toed ungulates), Tubulidentata (aardvarks), Hyracoidea (hyraxes), Sirenia (dugongs and manatees), Proboscidea (elephants) and occasionally Cetacea (whales and dolphins).\n\nHowever, in 2009 morphological and molecular work has found that aardvarks, hyraxes, sea cows, and elephants are more closely related to sengis, tenrecs, and golden moles than to the perissodactyls and artiodactyls, and form Afrotheria. Elephants, sea cows, and hyraxes are grouped together in the clade Paenungulata, while the aardvark has been considered as either a close relative to them or a close relative to sengis in the clade Afroinsectiphilia. This is a striking example of convergent evolution.\n\nThere is now some dispute as to whether this smaller Ungulata is a cladistic (evolution-based) group, or merely a phenetic group (form taxon) or folk taxon (similar, but not necessarily related). Some studies have indeed found the mesaxonian ungulates and paraxonian ungulates to form a monophyletic lineage, closely related to either the Ferae (the carnivorans and the pangolins) in the clade Fereuungulata or to the bats. Other studies found the two orders not that closely related, as some place the perissodactyls as close relatives to bats and Ferae in Pegasoferae and others place the artiodactyls as close relatives to bats.\n\nBelow is a simplified taxonomy (assuming that ungulates do indeed form a natural grouping) with the extant families, in order of the relationships. Keep in mind that there are still some grey areas of conflict, such as the case with relationship of the pecoran families and the baleen whale families. See each family for the relationships of the species as well as the controversies in their respective article.\n\n\nBelow is the general consensus of the phylogeny of the ungulate families.\nPerissodactyla and Artiodactyla include the majority of large land mammals. These two groups first appeared during the late Paleocene, rapidly spreading to a wide variety of species on numerous continents, and have developed in parallel since that time. Some scientists believed that modern ungulates are descended from an evolutionary grade of mammals known as the condylarths; the earliest known member of the group was the tiny \"Protungulatum\", an ungulate that co-existed with the last of non-avian dinosaurs 66 million years ago; however, many authorities do not consider it a true placental, let alone an ungulate. The enigmatic dinoceratans were among the first large herbivorous mammals, although their exact relationship with other mammals is still debated with one of the theories being that they might just be distant relatives to living ungulates; the most recent study recovers them as within the true ungulate assemblage, closest to \"Carodnia\".\n\nIn Australia, the marsupial \"Chaeropus\" also developed hooves, convergent those of artiodactyls.\n\nPerissodactyls are said to have evolved from the Phenacodontidae, small, sheep-sized animals that were already showing signs of anatomical features that their descendents would inherit (the reduction of digit I and V for example). By the start of the Eocene, 55 million years ago (Mya), they had diversified and spread out to occupy several continents. Horses and tapirs both evolved in North America; rhinoceroses appear to have developed in Asia from tapir-like animals and then colonised the Americas during the middle Eocene (about 45 Mya). Of the approximately 15 families, only three survive (McKenna and Bell, 1997; Hooker, 2005). These families were very diverse in form and size; they included the enormous brontotheres and the bizarre chalicotheres. The largest perissodactyl, an Asian rhinoceros called \"Paraceratherium\", reached , more than twice the weight of an elephant.\n\nIt has been found in a cladistic study that the anthracobunids and the desmostylians - two lineages that have been previously classified as Afrotherians (more specifically closer to elephants) - have been classified as a clade that is closely related to the perissodactyls. The desmostylians were large amphibious quadrupeds with massive limbs and a short tail. They grew to in length and are thought to have weighed more than . Their fossils are known from the northern Pacific Rim, from southern Japan through Russia, the Aleutian Islands and the Pacific coast of North America to the southern tip of Baja California. Their dental and skeletal form suggests desmostylians were aquatic herbivores dependent on littoral habitats. Their name refers to their highly distinctive molars, in which each cusp was modified into hollow columns, so that a typical molar would have resembled a cluster of pipes, or in the case of worn molars, volcanoes. They are the only marine mammals to have gone extinct.\n\nThe South American meridiungulates, which contain the somewhat tapir-like pyrotheres and astrapotheres, the mesaxonic litopterns and the diverse notoungulates; As a whole, meridiungulates are said to have evolved from animals like \"Hyopsodus\". For a while their relationships with other ungulates was a mystery. Some paleontologists have even challenged the monophyly of Meridiungulata by suggesting that the pyrotheres may be more closely related to other mammals, such as Embrithopoda (an African order that are related to elephants) than to other South American ungulates. A recent study based on bone collagen as also found that to suggest that at least litopterns and the notoungulates were closely related to the perissodactyls.\n\nThe oldest known fossils assigned to Equidae date from the early Eocene, 54 million years ago. They had been assigned to the genus \"Hyracotherium\", but the type species of that genus is now considered not a member of this family, but the other species have been split off into different genera. These early Equidae were fox-sized animals with three toes on the hind feet, and four on the front feet. They were herbivorous browsers on relatively soft plants, and already adapted for running. The complexity of their brains suggest that they already were alert and intelligent animals. Later species reduced the number of toes, and developed teeth more suited for grinding up grasses and other tough plant food.\n\nRhinocerotoids diverged from other perissodactyls by the early Eocene. Fossils of \"Hyrachyus eximus\" found in North America date to this period. This small hornless ancestor resembled a tapir or small horse more than a rhino. Three families, sometimes grouped together as the superfamily Rhinocerotoidea, evolved in the late Eocene: Hyracodontidae, Amynodontidae and Rhinocerotidae, thus creating an explosion of diversity unmatched for a while until environmental changes drastically eliminated several species.\n\nThe first tapirids, such as \"Heptodon\", appeared in the early Eocene. They appeared very similar to modern forms, but were about half the size, and lacked the proboscis. The first true tapirs appeared in the Oligocene. By the Miocene, such genera as \"Miotapirus\" were almost indistinguishable from the extant species. Asian and American tapirs are believed to have diverged around 20 to 30 million years ago; and tapirs migrated from North America to South America around 3 million years ago, as part of the Great American Interchange.\n\nPerissodactyls were the dominant group of large terrestrial browsers right through the Oligocene. However, the rise of grasses in the Miocene (about 20 Mya) saw a major change: the artiodactyl species with their more complex stomachs were better able to adapt to a coarse, low-nutrition diet, and soon rose to prominence. Nevertheless, many perissodactyl species survived and prospered until the late Pleistocene (about 10,000 years ago) when they faced the pressure of human hunting and habitat change.\n\nThe artiodactyls are thought to have evolved from a small group of condylarths, Arctocyonidae, which were unspecialized, superficially raccoon-like to bear-like omnivores from the Early Paleocene\n(about 65 to 60 million years ago). They had relatively short limbs lacking specializations associated with their relatives (e.g. reduced side digits, fused bones, and hoofs), and long, heavy tails. Their primitive anatomy makes it unlikely that they were able to run down prey, but with their powerful proportions, claws, and long canines, they may have been able to overpower smaller animals in surprise attacks. Evidently these mammals soon evolved into two separate lineages: the mesonychians and the artiodactyls.\n\nMesonychians are depicted as \"wolves on hooves\" and were the first major mammalian predators, appearing in the Paleocene. Early mesonychids had five digits on their feet, which probably rested flat on the ground during walking (plantigrade locomotion), but later mesonychids had four digits that ended in tiny hoofs on all of their toes and were increasingly well adapted to running. Like running members of the even-toed ungulates, mesonychids (\"Pachyaena\", for example) walked on their digits (digitigrade locomotion). Mesonychians fared very poorly at the close of the Eocene epoch, with only one genus, \"Mongolestes\", surviving into the Early Oligocene epoch, as the climate changed and fierce competition arose from the better adapted creodonts.\n\nThe first artiodactyls looked like today's chevrotains or pigs: small, short-legged creatures that ate leaves and the soft parts of plants. By the Late Eocene (46 million years ago), the three modern suborders had already developed: Suina (the pig group); Tylopoda (the camel group); and Ruminantia (the goat and cattle group). Nevertheless, artiodactyls were far from dominant at that time: the perissodactyls were much more successful and far more numerous. Artiodactyls survived in niche roles, usually occupying marginal habitats, and it is presumably at that time that they developed their complex digestive systems, which allowed them to survive on lower-grade food. While most artiodactyls were taking over the niches left behind by several extinct perissodactyls, one lineage of artiodactyls began to venture out into the seas.\n\nThe traditional theory of cetacean evolution was that cetaceans were related to the mesonychids. These animals had unusual triangular teeth very similar to those of primitive cetaceans. This is why scientists long believed that cetaceans evolved from a form of mesonychid. Today many scientists believe cetaceans evolved from the same stock that gave rise to hippopotamuses. This hypothesized ancestral group likely split into two branches around . One branch would evolve into cetaceans, possibly beginning about with the proto-whale \"Pakicetus\" and other early cetacean ancestors collectively known as Archaeoceti, which eventually underwent aquatic adaptation into the completely aquatic cetaceans. The other branch became the anthracotheres, a large family of four-legged beasts, the earliest of whom in the late Eocene would have resembled skinny hippopotamuses with comparatively small and narrow heads. All branches of the anthracotheres, except that which evolved into Hippopotamidae, became extinct during the Pliocene without leaving any descendants. The family Raoellidae is said to be the closest artiodactyl family to the cetaceans. Consequentially, new theories in cetacean evolution hypothesize that whales and their ancestors escaped predation, not competition, by slowly adapting to the ocean.\n\nUngulates are in high diversity in response to sexual selection and ecological events; the majority of ungulates lack a collar bone. Terrestrial ungulates are for the most part herbivores, with some of them being grazers. However, there are exceptions to this as pigs, peccaries, hippos and duikers are known to have an omnivorous diet. Some cetaceans are the only modern ungulates that are carnivores; baleen whales consume significantly smaller animals in relation to their body size, such as small species of fish and krill; toothed whales, depending on the species, can consume a wide range of species: squid, fish, sharks, and other species of mammals such as seals and other whales. In terms of ecosystem ungulates have colonized all corners of the planets, from mountains to the ocean depths; grasslands to deserts and have been domesticated by humans.\n\nUngulates have developed specialized adaptations, especially in the areas of cranial appendages, dentition, and leg morphology including the modification of the astragalus (one of the ankle bones at the end of the lower leg) with a short, robust head.\n\nThe hoof is the tip of a toe of an ungulate mammal, strengthened by a thick horny (keratin) covering. The hoof consists of a hard or rubbery sole, and a hard wall formed by a thick nail rolled around the tip of the toe. The weight of the animal is normally borne by both the sole and the edge of the hoof wall. Hooves grow continuously, and are constantly worn down by use. In most modern ungulates, the radius and ulna are fused along the length of the forelimb; early ungulates, such as the arctocyonids, did not share this unique skeletal structure. The fusion of the radius and ulna prevents an ungulate from rotating its forelimb. Since this skeletal structure has no specific function in ungulates, it is considered a homologous characteristic that ungulates share with other mammals. This trait would have been passed down from a common ancestor. While the two orders of ungulates colloquial names are based on the number of toes of their members (\"odd-toed\" for the perissodactyls and \"even-toed\" for the terrestrial artiodactyls), it is not an accurate reason they are grouped. Tapirs have four toes in the front, yet they are members of the \"odd-toed\" order; peccaries and modern cetaceans are members of the \"even-toed\" order, yet peccaries have three toes in the front and whales are an extreme example as they have flippers instead of hooves. Scientists had classified them according to the distribution of their weight to their toes.\n\nPerissodactyls have a mesaxonic foot meaning that the weight is distributed on the third toe on all legs thanks to the plane symmetry of their feet. There has been reduction of toes from the common ancestor, with the classic example being horses with their single hooves. In consequence, there was an alternative name for the perissodactyls the nearly obsolete Mesaxonia. Perissodactyls are not the only lineage of mammals to have evolved this trait; the meridiungulates have evolved mesaxonic feet numerous times.\n\nTerrestrial artiodactyls have a paraxonic foot meaning that the weight is distributed on the third and the fourth toe on all legs. The majority of these mammals have cloven hooves, with two smaller ones known as the dewclaws that are located further up on the leg. The earliest cetaceans (the archaeocetes), also have this characteristic in the addition of also having both an astragalus and cuboid bone in the ankle, which are further diagnostic traits of artiodactyls.\nIn modern cetaceans, the front limbs have become pectoral fins and the hind parts are internal and reduced. Occasionally, the genes that code for longer extremities cause a modern cetacean to develop miniature legs (known as atavism). The main method of moving is an up-and-down motion with the tail fin, called the fluke, which is used for propulsion, while the pectoral fins together with the entire tail section provide directional control. All modern cetaceans still retain their digits despite the external appearance suggesting otherwise.\n\nMost ungulates have developed reduced canine teeth and specialized molars, including bunodont (low, rounded cusps) and hypsodont (high crowned) teeth. The development of hypsodonty has been of particular interest as this adaptation was strongly associated with the spread of grasslands during the Miocene about 25 million years. As forest biomes declined, grasslands spread, opening new niches for mammals. Many ungulates switched from browsing diets to grazing diets, and possibly driven by abrasive silica in grass, hypsodonty became common. However, recent evidence ties the evolution of hyspodonty to open, gritty habitats and not the grass itself. This is termed the Grit, not grass hypothesis.\n\nSome ungulates completely lack upper incisors and instead have a dental pad to assist in browsing. It can be found in camels, ruminants, and some toothed whales; modern baleen whales are remarkable in that they have baleen instead to filter out the krill from the water. On the other spectrum teeth have been evolved as weapons or sexual display seen in pigs and peccaries, some species of deer, musk deer, hippopotamuses, beaked whales and the Narwhal, with its long canine tooth.\n\nUngulates evolved a variety of cranial appendages that today can be found in cervoids (with the exception of musk deer). In oxen and antelope, the size and shape of the horns vary greatly, but the basic structure is always a pair of simple bony protrusions without branches, often having a spiral, twisted or fluted form, each covered in a permanent sheath of keratin. The unique horn structure is the only unambiguous morphological feature of bovids that distinguishes them from other pecorans. Male horn development has been linked to sexual selection, while the presence of horns in females is likely due to natural selection. The horns of females are usually smaller than those of males, and are sometimes of a different shape. The horns of female bovids are thought to have evolved for defense against predators or to express territoriality, as nonterritorial females, which are able to use crypsis for predator defense, often do not have horns.\n\nRhinoceros horns, unlike those of other horned mammals, only consist of keratin. The horns rest on the nasal ridge of the animals skull.\n\nAntlers are unique to cervids and found mostly on males: only caribou and reindeer have antlers on the females, and these are normally smaller than those of the males. Nevertheless, fertile does from other species of deer have the capacity to produce antlers on occasion, usually due to increased testosterone levels. Each antler grows from an attachment point on the skull called a pedicle. While an antler is growing, it is covered with highly vascular skin called velvet, which supplies oxygen and nutrients to the growing bone. Antlers are considered one of the most exaggerated cases of male secondary sexual traits in the animal kingdom, and grow faster than any other mammal bone. Growth occurs at the tip, and is initially cartilage, which is mineralized to become bone. Once the antler has achieved its full size, the velvet is lost and the antler's bone dies. This dead bone structure is the mature antler. In most cases, the bone at the base is destroyed by osteoclasts and the antlers fall off at some point. As a result of their fast growth rate, antlers are considered a handicap since there is an incredible nutritional demand on deer to re-grow antlers annually, and thus can be honest signals of metabolic efficiency and food gathering capability.\n\nOssicones are horn-like (or antler-like) protuberances that can be found on the heads of giraffes and male okapis today. They are similar to the horns of antelopes and cattle, save that they are derived from ossified cartilage, and that the ossicones remain covered in skin and fur, rather than horn. Antlers (such as on deer) are derived from bone tissue: when mature, the skin and fur covering of the antlers, termed \"velvet\", is sloughed and scraped off to expose the bone of the antlers.\n\nPronghorn are unique when compared to their relatives. Each \"horn\" of the pronghorn is composed of a slender, laterally flattened blade of bone that grows from the frontal bones of the skull, forming a permanent core. As in the Giraffidae, skin covers the bony cores, but in the pronghorn it develops into a keratinous sheath which is shed and regrown on an annual basis. Unlike the horns of the family Bovidae, the horn sheaths of the pronghorn are branched, each sheath possessing a forward-pointing tine (hence the name pronghorn). The horns of males are well developed.\n\nMany of the world’s ungulate species exist only in relatively small populations in which some degree of inbreeding inevitably occurs. A study of 16 species of captive ungulates revealed that juvenile survival of inbred young is generally lower than that of non-inbred young. (Also see Inbreeding depression). These findings have implications for the genetic management of small ungulate populations.\n\n\n",
                "Odd-toed ungulate\n\nMembers of the order Perissodactyla, otherwise known as odd-toed ungulates, are mammals characterized by an odd number of toes and by hindgut fermentation with somewhat simple stomachs. Perissodactyla comes from the Ancient Greek περισσός (\"perissós\", “uneven”) + δάκτυλος (\"dáktulos\", “a finger, toe”). Unlike the even-toed ungulates, they digest plant cellulose in their intestines rather than in one or more stomach chambers. The order includes three extant families: Equidae (horses, asses, and zebras), Rhinocerotidae (rhinoceroses), and Tapiridae (tapirs), with a total of about 17 species. Despite their very different appearances, they were recognized as related families in the 19th century by the zoologist Richard Owen, who also coined the order name.\n\nThe largest odd-toed ungulates are rhinoceroses, and the extinct \"Paraceratherium\", a hornless rhino from the Oligocene, is considered one of the largest land mammals of all time. At the other extreme, an early member of the order, the prehistoric horse \"Hyracotherium\", had a withers height of only . Apart from dwarf varieties of the domestic horse and donkey, perissodactyls reach a body length of and a weight of . While rhinos have only sparse hair and exhibit a thick epidermis, tapirs and horses have dense, short coats. Most species are grey or brown, although zebras and young tapirs are striped.\n\nThe main axes of both the front and rear feet pass through the third toe, which is always the largest. The remaining toes have been reduced in size to varying degrees. Tapirs, which are adapted to walking on soft ground, have four toes on their fore feet and three on their hind feet. Living rhinos have three toes on both the front and hind feet. Modern equines possess only a single toe; however, their feet are equipped with hooves, which almost completely cover the toe. Rhinos and tapirs, by contrast, have hooves covering only the leading edge of the toes, with the bottom being soft.\n\nThe ulnae and fibulae are reduced in horses. A common feature that clearly distinguishes this group from other mammals is the saddle-shaped ankle between the astragalus and the scaphoid, which greatly restricts the mobility of the foot. The thigh is relatively short, and the clavicle is absent.\n\nOdd-toed ungulates have a long upper jaw with an extended diastema between the front and cheek teeth, giving them an elongated head. The various forms of snout between families are due to differences in the form of the premaxilla. The lacrimal bone has projecting cusps in the eye sockets and a wide contact with the nasal bone. The temporomandibular joint is high and the mandible is enlarged.\n\nRhinos have one or two horns made of agglutinated keratin, unlike the horns of even-toed ungulates, which have a bony core.\n\nThe number and form of the teeth vary according to diet. The incisors and canines can be very small or completely absent, as in the two African species of rhinoceros. In the horses, usually only the males possess canines. The surface shape and height of the molars is heavily dependent on whether soft leaves or hard grass makes up the main component of their diets. Three or four cheek teeth are present on each jaw half, so the dental formula of odd-toed ungulates is: \nAll perissodactyls are hindgut fermenters. In contrast to ruminants, hindgut fermenters store digested food that has left the stomach in an enlarged cecum, where the food is digested by bacteria. No gallbladder is present. The stomach of perissodactyls is simply built, while the cecum accommodates up to in horses. The intestine is very long, reaching up to in horses. Extraction of nutrients from food is relatively inefficient, which probably explains why no odd-toed ungulates are small; for large animals, nutritional requirements per unit of body weight are lower and the surface-to-volume ratio is smaller.\n\nThe present distribution of most perissodactyl species is only a small fraction of their original range. Members of this group are now found only in Central and South America, eastern and southern Africa, and central, southern, and southeastern Asia. During the peak of odd-toed ungulate existence, from the Eocene to the Oligocene, perissodactyls were distributed over much of the globe, the only exceptions being Australia and Antarctica. Horses and tapirs arrived in South America after the formation of the Isthmus of Panama in the Pliocene, around 3 million years ago. In North America, they died out around 10,000 years ago, while in Europe, the tarpans disappeared in the 19th century. Hunting and habitat restriction have reduced the present-day species to fragmented relict populations. In contrast, domesticated horses and donkeys have gained a worldwide distribution, and feral animals of both species are now also found in regions outside of their original range, such as in Australia.\n\nPerissodactyls inhabit a number of different habitats, leading to different lifestyles. Tapirs are solitary and inhabit mainly tropical rainforests. Rhinos tend to live alone in rather dry savannas, and in Asia, wet marsh or forest areas. Horses inhabit open areas such as grasslands, steppes, or semideserts, and live together in groups. Odd-toed ungulates are exclusively herbivores that feed, to varying degrees, on grasses, leaves, and other plant parts. A distinction is often made between primarily grass feeders (white rhinos, equines) and leaf feeders (tapirs, other rhinos).\n\nOdd-toed ungulates are characterized by a long gestation period and a small litter size, usually delivering a single young. The gestation period is 330–500 days, being longest in the rhinos. Newborn perissodactyls are precocial; young horses and rhinos can follow the mother after a few hours. The young are nursed for a relatively long time, often into their second year, reaching sexual maturity around eight or ten years old. Perissodactyls are long-lived, with several species reaching an age of almost 50 years in captivity.\n\nTraditionally, the odd-toed ungulates were classified with other mammals such as artiodactyls, hyraxes, mammals with a proboscis, and other \"ungulates\". A close family relationship with hyraxes was suspected based on similarities in the construction of the ear and the course of the carotid artery.\n\nRecent molecular genetic studies, however, have shown the ungulates to be polyphyletic, meaning that in some cases the similarities are the result of convergent evolution rather than common ancestry. Elephants and hyraxes are now considered to belong to Afrotheria, so are not closely related to the perissodactyls. These, in turn, are in the Laurasiatheria, a superorder that had its origin in the former supercontinent Laurasia. Molecular genetic findings suggest that the cloven Artiodactyla (containing the cetaceans as a deeply nested subclade) are the sister taxon of the Perissodactyla; together, the two groups form the Euungulata. More distant are the bats (Chiroptera) and Ferae (a common taxon of carnivorans, Carnivora, and pangolins, Pholidota). In a discredited alternative scenario, a close relationship exists between perissodactyls, carnivores, and bats, this assembly comprising the Pegasoferae.\n\nAccording to studies published in March 2015, odd-toed ungulates are in a close family relationship with at least some of the so-called Meridiungulata, a very diverse group of mammals living from the Paleocene to the Pleistocene in South America, whose systematic unity is largely unexplained. Some of these were classified on the basis of their paleogeographic distribution. However, a close relationship can be worked out to perissodactyls by means of protein sequencing and comparison with fossil collagen from remnants of phylogenetically young members of the Meridiungulata (specifically \"Macrauchenia\" from the Litopterna and \"Toxodon\" from the Notoungulata). Both kinship groups, the odd-toed ungulates and the Litopterna-Notoungulata, are now in the higher-level taxon of Panperissodactyla. This kinship group is included among the Euungulata which also contains the even-toed ungulates and whales (Artiodactyla). The separation of the Litopterna-Notoungulata group from the perissodactyls probably took place before the Cretaceous-Paleogene extinction event. \"Condylarths\" can probably be considered the starting point for the development of the two groups, as they represent a heterogeneous group of primitive ungulates that mainly inhabited the northern hemisphere in the Paleogene.\n\nOdd-toed ungulates (Perissodactyla) comprise three living families with around 17 species—in the horse the exact count is still controversial. Rhinos and tapirs are more closely related to each other than to the horses. The separation of horses from other perissodactyls took place according to molecular genetic analysis in the Paleocene some 56 million years ago, while the rhinos and tapirs split off in the lower middle Eocene, about 47 million years ago. \n\nThere are many perissodactyl fossils of multivariant form. The major lines of development include the following groups:\n\n\nRelationships within the large group of odd-toed ungulates are not fully understood. Initially, after the establishment of \"Perissodactyla\" by Richard Owen in 1848, the present-day representatives were considered equal in rank. In the first half of the 20th century, a more systematic differentiation of odd-toed ungulates began, based on a consideration of fossil forms, and they were placed in two major suborders: Hippomorpha and Ceratomorpha. The Hippomorpha comprises today's horses and their extinct members (Equoidea); the Ceratomorpha consist of tapirs and rhinos plus their extinct members (Tapiroidea and Rhinocerotoidea). The names Hippomorpha and Ceratomorpha were introduced in 1937 by Horace Elmer Wood, in response to criticism of the name \"Solidungula\" that he proposed three years previously. It had been based on the grouping of horses and Tridactyla and on the rhinoceros/tapir complex. The extinct brontotheriidae were also classified under Hippomorpha and therefore possess a close relationship to horses. Some researchers accept this assignment because of similar dental features, but there is also the view that a very basal position within the odd-toed ungulates places them rather in the group of \"Titanotheriomorpha\".\n\nOriginally, the Chalicotheriidae were seen as members of Hippomorpha, and presented as such in 1941. William Berryman Scott thought that, as claw-bearing perissodactyls, they belong in the new suborder Ancylopoda (where Ceratomorpha and Hippomorpha as odd-toed ungulates were combined in the group of Chelopoda). The term Ancylopoda, coined by Edward Drinker Cope in 1889, had been established for chalicotheres. However, further morphological studies from the 1960s showed a middle position of Ancylopoda between Hippomorpha and Ceratomorpha. Leonard Burton Radinsky saw all three major groups of odd-toed ungulates as peers, based on the extremely long and independent phylogenetic development of the three lines. In the 1980s, Jeremy J. Hooker saw a general similarity of Ancylopoda and \"Ceratomorpha\" based on dentition, especially in the earliest members, leading to the unification in 1984 of the two submissions in the interim order, \"Tapiromorpha\". At the same time he expanded the Ancylopoda to include the \"Lophiodontidae\". The name \"Tapiromorpha\" goes back to Ernst Haeckel, who coined it in 1873, but it was long considered synonymous to Ceratomorpha because Wood had not considered it in 1937 when Ceratomorpha were named, since the term had been used quite differently in the past. Also in 1984, Robert M. Schoch used the conceptually similar term Moropomorpha, which today applies synonymously to Tapiromorpha. Included within the Tapiromorpha are the now extinct Isectolophidae, a sister group of the Ancylopoda-Ceratomorpha group and thus the most primitive members of this relationship complex.\n\nThe evolutionary development of Perissodactyla is well documented in the fossil record. Numerous finds are evidence of the adaptive radiation of this group, which was once much more varied and widely dispersed. \"Radinskya\" from the late Paleocene of East Asia is often considered to be one of the oldest close relatives of the ungulates. Its only 8 cm skull must have belonged to a very small and primitive animal with a π-shaped crown pattern on the enamel of its rear molars similar to that of perissodactyls and their relatives, especially the rhinos. Finds of \"Cambaytherium\" and \"Kalitherium\" in the Cambay shale of western India indicate an origin in Asia dating to the Lower Eocene roughly 54.5 million years ago. Their teeth also show similarities to \"Radinskya\" as well as to the Tethytheria clade. The saddle-shaped configuration of the navicular joints and the mesaxonic construction of the front and hind feet also indicates a close relationship to Tethytheria. However, this construction deviates from that of Cambaytherium, indicating that it is actually a member of a sister group. Ancestors of Perissodactyla may have arrived via an island bridge from the Afro-Arab landmass onto the Indian subcontinent as it drifted north towards Asia.\n\nThe alignment of hyopsodontids and phenacodontids to Perissodactyla in general suggests an older Laurasian origin and distribution for the clade, dispersed across the northern continents already in the early Paleocene. These forms already show a fairly well-developed molar morphology, with no intermediary forms as evidence of the course of its development. The close relationship between meridiungulate mammals and perissoodactyls in particular is of interest since the latter appear in South America soon after the KT event, implying rapid ecological radiation and dispersal after the mass extinction.\n\nThe Perissodactyla appear relatively abruptly at the beginning of the Lower Paleocene before about 63 million years ago, both in North America and Asia, in the form of phenacodontids and hyopsodontids. The oldest finds from an extant group originate among other sources from \"Sifrhippus\", an ancestor of the horses from the Willswood lineup in northwestern Wyoming. The distant ancestors of tapirs appeared not too long after that in the Ghazij lineup in Balochistan, such as \"Ganderalophus\", as well as \"Litolophus\" from the Chalicotheriidae line, or \"Eotitanops\" from the group of brontotheriidae. Initially, the members of the different lineages looked quite similar with an arched back and generally four toes on the front and three on the hind feet. \"Hyracotherium\", which is considered a member of the horse family, outwardly resembled \"Hyrachyus\", the first representative of the rhino and tapir line. All were small compared to later forms and lived as fruit and foliage eaters in forests. The first of the mega-fauna to emerge were the brontotheres, in the Middle and Upper Eocene. \"Megacerops\", known from North America, reached a withers height of and could have weighed just over . The decline of brontotheres at the end of the Eocene is associated with competition arising from the advent of more successful herbivores.\n\nMore successful lines of odd-toed ungulates emerged at the end of the Eocene when dense jungles gave way to steppe, such as the chalicotheriid rhinos, and their immediate relatives; their development also began with very small forms. \"Paraceratherium\", one of the largest mammals ever to walk the earth, evolved during this era. They weighed up to and lived throughout the Oligocene in Eurasia. About 20 million years ago at the onset of the Miocene the perissodactyls first reached Africa when it became connected to Eurasia because of the closing of the Tethys Ocean. For the same reason, however, new animals such as the mammoths also entered the ancient settlement areas of odd-toed ungulates, creating competition that led to the extinction of some of their lines. The rise of ruminants, which occupied similar ecological niches and had a much more efficient digestive system, is also associated with the decline in diversity of odd-toed ungulates. A significant cause for the decline of perissodactyls was climate change during the Miocene, leading to a cooler and drier climate accompanied by the spread of open landscapes. However, some lines flourished, such as the horses and rhinos; anatomical adaptations made it possible for them to consume tougher grass food. This led to open land forms that dominated the newly created landscapes. With the emergence of the Isthmus of Panama in the Pliocene, perissodactyls and other megafauna were given access to one of their last habitable continents: South America. However, many perissodactyls became extinct at the end of the ice ages, including American horses and the \"Elasmotherium\". Whether over-hunting by humans (overkill hypothesis), climatic change, or a combination of both factors was responsible for the extinction of ice age mega-fauna, remains controversial.\n\nIn 1758, in his seminal work \"Systema Naturae\", Linnaeus (1707–1778) classified horses (\"Equus\") together with hippos (\"Hippopotamus\"). At that time, this category also included the tapirs (\"Tapirus\"), more precisely the lowland or South American tapir (\"Tapirus terrestus\"), the only tapir then known in Europe. Linnaeus classified this tapir as \"Hippopotamus terrestris\" and put both genera in the group of the \"Belluae\" (\"beasts\"). He combined the rhinos with the Glires, a group now consisting of the lagomorphs and rodents. Mathurin Jacques Brisson (1723–1806) first separated the tapirs and hippos in 1762 with the introduction of the concept \"le tapir\". He also separated the rhinos from the rodents, but did not combine the three families now known as the odd-toed ungulates. In the transition to the 19th century, the individual perissodactyl genera were associated with various other groups, such as the proboscidean and even-toed ungulates. In 1795, Étienne Geoffroy Saint-Hilaire (1772- 1844) and Georges Cuvier (1769–1832) introduced the term \"pachyderm\" (Pachydermata), including in it not only the rhinos and elephants, but also the hippos, pigs, peccaries, tapirs and hyrax . The horses were still generally regarded as a group separate from other mammals and were often classified under the name \"Solidungula\" or \"Solipèdes\", meaning \"one-hoof animal\".\n\nIn 1861, Henri Marie Ducrotay de Blainville (1777–1850) classified ungulates by the structure of their feet, differentiating those with an even number of toes from those with an odd number. He moved the horses as \"solidungulate\" over to the tapirs and rhinos as \"multungulate\" animals and referred to all of them together as \"onguligrades à doigts impairs\", coming close to the concept of the odd-toed ungulate as a systematic unit. Richard Owen (1804–1892) quoted Blainville in his study on fossil mammals of the Isle of Wight and introduced the name \"Perissodactyla\".\n\nIn 1884, Othniel Charles Marsh (1831–1899) came up with the concept \"Mesaxonia\", which he used for what are today called the odd-toed ungulates, including their extinct relatives, but explicitly excluding the hyrax. \"Mesaxonia\" is now considered a synonym of \"Perissodactyla\", but it was sometimes also used for the true odd-toed ungulates as a subcategory (rhinos, horses, tapirs), while \"Perissodactyla\" stood for the entire order, including the hyrax. The assumption that hyraxes were \"Perissodactyla\" was held well into the 20th century. Only with the advent of molecular genetic research methods had it been recognized that the hyrax is not closely related to perissodactyls but rather to elephants and manatees.\n\nThe domestic horse and the donkey play an important role in human history particularly as transport, work and pack animals. The domestication of both species began several millennia B.C. Due to the motorisation of agriculture and the spread of automobile traffic, such use has declined sharply in Western industrial countries; riding is usually undertaken more as a hobby or sport. In less developed regions of the world, the traditional uses for these animals are, however, still widespread. To a lesser extent, horses and donkeys are also kept for their meat and their milk.\n\nIn contrast, the existence in the wild of almost all other odd-toed ungulates species has declined dramatically because of hunting and habitat destruction. The quagga is extinct and Przewalski's horse has been eradicated in the wild.\n\nPresent threat levels, according to the International Union for Conservation of Nature (2012):\n\n\n"
            ],
            "good_message": false,
            "loggedData": [],
            "offensive": null,
            "researchTask": true,
            "selectedData": [],
            "shown_passages": [
                [
                    "Horse",
                    [
                        "The horse (\"Equus ferus caballus\") is one of two extant subspecies of \"Equus ferus\".",
                        "It is an odd-toed ungulate mammal belonging to the taxonomic family Equidae.",
                        "The horse has evolved over the past 45 to 55 million years from a small multi-toed creature, \"Eohippus\", into the large, single-toed animal of today.",
                        "Humans began to domesticate horses around 4000 BC, and their domestication is believed to have been widespread by 3000 BC.",
                        "Horses in the subspecies \"caballus\" are domesticated, although some domesticated populations live in the wild as feral horses.",
                        "These feral populations are not true wild horses, as this term is used to describe horses that have never been domesticated, such as the endangered Przewalski's horse, a separate subspecies, and the only remaining true wild horse."
                    ]
                ],
                [
                    "Ungulate",
                    [
                        "Ungulates (pronounced ) are any members of a diverse group of primarily large mammals that includes odd-toed ungulates such as horses and rhinoceroses, and even-toed ungulates such as cattle, pigs, giraffes, camels, deer, and hippopotamuses.",
                        "Most terrestrial ungulates use the tips of their toes, usually hoofed, to sustain their whole body weight while moving.",
                        "The term means, roughly, \"being hoofed\" or \"hoofed animal\".",
                        "As a descriptive term, \"ungulate\" normally excludes cetaceans (whales, dolphins, porpoises), as they do not possess most of the typical morphological characteristics of ungulates, but recent discoveries indicate that they are descended from early artiodactyls."
                    ]
                ],
                [
                    "Odd-toed ungulate",
                    [
                        "Members of the order Perissodactyla, otherwise known as odd-toed ungulates, are mammals characterized by an odd number of toes and by hindgut fermentation with somewhat simple stomachs.",
                        "Perissodactyla comes from the Ancient Greek περισσός (\"perissós\", “uneven”) + δάκτυλος (\"dáktulos\", “a finger, toe”).",
                        "Unlike the even-toed ungulates, they digest plant cellulose in their intestines rather than in one or more stomach chambers.",
                        "The order includes three extant families: Equidae (horses, asses, and zebras), Rhinocerotidae (rhinoceroses), and Tapiridae (tapirs), with a total of about 17 species.",
                        "Despite their very different appearances, they were recognized as related families in the 19th century by the zoologist Richard Owen, who also coined the order name."
                    ]
                ]
            ],
            "speaker": "0_Wizard",
            "suggestionsConfig": "show",
            "suggestionsNumber": 5,
            "suggestionsShown": true,
            "text": " To be specific, it is an odd-toed ungulate mammal part of the taxonomic family Equidae",
            "textboxFilled": 0,
            "time": 1580233383.2549708,
            "timeTaken": 3.448,
            "totalTasks": 120,
            "turn": 19
        },
        {
            "checked_passage": {
                "no_passages_used": "no_passages_used"
            },
            "checked_sentence": {
                "no_passages_used": "no_passages_used"
            },
            "clicked_passages": {},
            "context": {
                "conversation": [
                    "I have a horse named Beauty. Do you love horses?",
                    "I don't, but I've rode the subspecies of Equus ferus before. What kind?",
                    "I'm not sure what kind it is. My dad got it for me. I don't know much about them. What is Equus ferus?",
                    " To be specific, it is an odd-toed ungulate mammal part of the taxonomic family Equidae"
                ],
                "index": 27
            },
            "contextCount": 0,
            "full_passages": [
                "Wild horse\n\nThe wild horse (\"Equus ferus\") is a species of the genus \"Equus\", which includes as subspecies the modern domesticated horse (\"Equus ferus caballus\") as well as the undomesticated tarpan (\"Equus ferus ferus\", now extinct), and the endangered Przewalski's horse (\"Equus ferus przewalskii\"). Przewalski's horse had reached the brink of extinction but was reintroduced successfully back into the wild. The tarpan became extinct in the 19th century, though it is a possible ancestor of the domestic horse; it roamed the steppes of Eurasia at the time of domestication. However, other subspecies of \"Equus ferus\" may have existed and could have been the stock from which domesticated horses are descended. Since the extinction of the tarpan, attempts have been made to reconstruct its phenotype, resulting in horse breeds such as the Konik and Heck horse. However, the genetic makeup and foundation bloodstock of those breeds is substantially derived from domesticated horses, so these breeds possess domesticated traits.\n\nThe term \"wild horse\" is also used colloquially in reference to free-roaming herds of feral horses such as the mustang in the United States, the brumby in Australia, and many others. These feral horses are untamed members of the domestic horse subspecies (\"Equus ferus caballus\"), not to be confused with the truly \"wild\" horse subspecies extant into modern times.\n\n\"E. ferus\" had several subspecies. Three survived into modern times:\nThe latter two are the only never-domesticated \"wild\" groups that survived into historic times. However, other subspecies of \"Equus ferus\" may have existed and could have been the stock from which domesticated horses are descended.\n\nPrzewalski's horse occupied the eastern Eurasian Steppes, perhaps from the Urals to Mongolia, although the ancient border between tarpan and Przewalski distributions has not been clearly defined. Przewalski's horse was limited to Dzungaria and western Mongolia in the same period, and became extinct in the wild during the 1960s, but was reintroduced in the late 1980s to two preserves in Mongolia. Although researchers such as Marija Gimbutas theorized that the horses of the Chalcolithic period were Przewalski's, more recent genetic studies indicate that Przewalski's horse is not an ancestor to modern domesticated horses.\n\nPrzewalski's horse is still found today, though it is an endangered species and for a time was considered extinct in the wild. Roughly 2000 Przewalski's horses are in zoos around the world. A small breeding population has been reintroduced in Mongolia. As of 2005, a cooperative venture between the Zoological Society of London and Mongolian scientists has resulted in a population of 248 animals in the wild.\n\nPrzewalski's horse has some biological differences from the domestic horse; unlike domesticated horses and the tarpan, which both have 64 chromosomes, Przewalski's horse has 66 chromosomes due to a Robertsonian translocation. However, the offspring of Przewalski and domestic horses are fertile, possessing 65 chromosomes.\n\nThe horse family Equidae and the genus \"Equus\" evolved in North America, before the species moved into the Eastern Hemisphere. Studies using ancient DNA, as well as DNA of recent individuals, shows the presence of two closely related horse species in North America, the wild horse and \"Equus francisci\", the \"New World stilt-legged horse\"; the latter is taxonomically assigned to various names.\n\nCurrently, three subspecies that lived during recorded human history are recognized. One subspecies is the widespread domestic horse (\"Equus ferus caballus\"), as well as two wild subspecies: the recently extinct tarpan (\"E. f. ferus\") and the endangered Przewalski's horse (\"E. f. przewalskii\").\n\nGenetically, the pre-domestication horse, \"E. f. ferus\", and the domesticated horse, \"E. f. caballus\", form a single homogeneous group (clade) and are genetically indistinguishable from each other. The genetic variation within this clade shows only a limited regional variation, with the notable exception of Przewalski's horse. Przewalski's horse has several unique genetic differences that distinguish it from the other subspecies, including 66 instead of 64 chromosomes, unique Y-chromosome gene haplotypes, and unique mtDNA haplotypes.\n\nBesides genetic differences, osteological evidence from across the Eurasian wild horse range, based on cranial and metacarpal differences, indicates the presence of only two subspecies in postglacial times, the tarpan and Przewalski's horse.\n\nAt present, the domesticated and wild horses are considered a single species, with the valid scientific name for the horse species being \"Equus ferus.\" The wild tarpan subspecies is \"E. f. ferus\", Przewalski's horse is \"E. f. przewalskii\", and the domesticated horse is \"E. f. caballus\". The rules for the scientific naming of animal species are determined in the International Code of Zoological Nomenclature, which stipulates that the oldest available valid scientific name is used to name the species. Previously, when taxonomists considered domesticated and wild horse two subspecies of the same species, the valid scientific name was \"Equus caballus\" Linnaeus 1758, with the subspecies labeled \"E. c. caballus\" (domesticated horse), \"E. c. ferus\" Boddaert, 1785 (tarpan) and \"E. c. przewalskii\" Poliakov, 1881 (Przewalski's Horse). However, in 2003, the International Commission on Zoological Nomenclature decided that the scientific names of the wild species have priority over the scientific names of domesticated species, therefore mandating the use of \"Equus ferus\" for the horse, independent of the position of the domesticated horse.\n\n \nHorses that live in an untamed state but have ancestors that have been domesticated are not truly \"wild\" horses; they are feral horses. For instance, when the Spanish reintroduced the horse to the Americas, beginning in the late 15th century, some horses escaped, forming feral herds; the best-known being the mustang. Similarly, the brumby descended from horses strayed or let loose in Australia by English settlers. Isolated populations of feral horses occur in a number of places, including Portugal, Scotland, and a number of barrier islands along the Atlantic coast of North America from Sable Island off Nova Scotia, to Cumberland Island, off the coast of Georgia. Even though these are often referred to as \"wild\" horses, they are not truly \"wild\" in the biological sense of having no domesticated ancestors.\n\nIn 1995, British and French explorers discovered a new population of horses in the Riwoche Valley of Tibet, unknown to the rest of the world, but apparently used by the local Khamba people. It was speculated that the Riwoche horse might be a relict population of wild horses, but testing did not reveal genetic differences with domesticated horses, which is in line with news reports indicating that they are used as pack and riding animals by the local villagers. These horses only stand tall and are said to resemble the images known as \"horse no 2\" depicted in cave paintings alongside images of Przewalski's horse.\n\n",
                "Tarpan\n\nThe tarpan (\"Equus ferus ferus\"), also known as Eurasian wild horse, was a subspecies of wild horse. It is now extinct. The last individual believed to be of this subspecies died in captivity in Russia in 1909, although some sources claim that it was not a genuine wild horse due to its resemblance to domesticated horses.\n\nBeginning in the 1930s, several attempts were made to develop horses that looked like tarpans through selective breeding, called \"breeding back\" by advocates. The breeds that resulted included the Heck horse, the Hegardt or Stroebel's horse, and a derivation of the Konik breed, all of which have a primitive appearance, particularly in having the grullo coat color. Some of these horses are now commercially promoted as \"tarpans\". However, those who study the history of the ancient wild horse assert that the word \"tarpan\" only describes the true wild horse.\n\nThe name \"tarpan\" or \"tarpani\" is from a Turkic language (Kazakh or Kyrgyz) name meaning \"wild horse\". The Tatars and the Cossacks distinguished the wild horse from the feral horse; the latter was called \"Takja\" or \"Muzin\".\n\nIn modern use, the term has been loosely used to refer to the predomesticated ancestor of the modern horse, \"Equus ferus,\" to the predomestic subspecies believed to have lived into the historic era, \"Equus ferus ferus,\" and to all European primitive or \"wild\" horses in general. The modern \"bred-back\" horse breeds are also promoted as \"tarpan\" by their supporters, though researchers discourage this use of the word, which they believe should only apply to the ancient \"E. ferus ferus\".\n\nThe tarpan was first described by Johann Friedrich Gmelin in 1774; he had seen the animals in 1769 in the district of Bobrov, near Voronezh. In 1784 Pieter Boddaert named the species \"Equus ferus\", referring to Gmelin's description. Unaware of Boddaert's name, Otto Antonius published the name \"Equus gmelini\" in 1912, again referring to Gmelin's description. Since Antonius' name refers to the same description as Boddaert's it is a junior objective synonym. It is now thought that the domesticated horse, named \"Equus caballus\" by Carl Linnaeus in 1758, is descended from the tarpan; indeed, many taxonomists consider them to belong to the same species. By a strict application of the rules of the International Code of Zoological Nomenclature, the tarpan ought to be named \"E. caballus\", or if considered a subspecies, \"E. caballus ferus\". However, biologists have generally ignored the letter of the rule and used \"E. ferus\" for the tarpan to avoid confusion with the domesticated subspecies.\n\nIt is debated if the small, free-roaming horses seen in the forests of Europe during 18th and 19th centuries and called \"tarpan\" were indeed wild, never-domesticated horses, hybrids of the Przewalski's horse and local domestic animals, or simply feral horses. Most studies have been based on only two preserved specimens and research to date has not positively linked the tarpan to Pleistocene or Holocene-era animals. \n\nIn 2003, the International Commission on Zoological Nomenclature \"conserved the usage of 17 specific names based on wild species, which are predated by or contemporary with those based on domestic forms\", confirming \"E. ferus\" for the tarpan. Taxonomists who consider the domestic horse a subspecies of the wild tarpan should use \"Equus ferus caballus\"; the name \"Equus caballus\" remains available for the domestic horse where it is considered to be a separate species.\n\nTraditionally, two tarpan subtypes have been proposed, the forest tarpan and steppe tarpan, although there seem to be only minor differences in type. The general view is that there was only one subspecies, the tarpan, \"Equus ferus ferus\". The last individual, which died in captivity in 1909, was between tall at the shoulders, had a thick falling mane, a grullo coat colour, dark legs, and primitive markings, including a dorsal stripe and shoulder stripes. \n\nA number of genotypes have been identified within European wild horses from the Pleistocene and Holocene; their coat color genes including those creating bay, black and leopard complex are known to be present in the wild horse population in Europe and were depicted in cave paintings of wild horses during the Pleistocene. The dun gene, a dilution gene seen in Przewalski's horse that also creates the grullo or \"blue dun\" coat, seen in the Konik has genetic markers identified in modern horses, but has not yet been studied in European wild horses. It is considered likely that at least some wild horses had a dun coat.\n\nSome theorize that the tarpan had a standing mane because all other extant wild equines display this feature, and falling manes are considered an indication of domestication. However, historical accounts do not unambiguously describe a standing mane in European wild horses, and it is likely that they had a short, falling mane. This feature is advantageous in regions with much rainfall because it diverts rain and snow from the neck and face and prevents a loss of heat, as much as a bushy tail. Mummified Siberian wild horses display a hanging mane as well.\n\nThe appearance of European wild horses is reconstructed with genetic, osteologic and historic data. One genetic study suggests that bay was the predominant color in European wild horses. During the Mesolithic, a gene coding a black coat color appeared on the Iberian peninsula. This color spread east but was less common than bay in the investigated sample and never reached Siberia. Bay in combination with dun results in the \"bay dun\" color seen in Przewalski's horses, black with dun creates the grullo coat. A loss of dun dilution may have been advantageous in more forested western European landscapes, as dark colors were a better camouflage in forests. Pangaré or \"mealy\" coloration, a characteristic of other wild equines, might have been present in at least some tarpans, as historic accounts report a light-colored belly. It is also likely that European wild horses had primitive markings, consisting of stripes on the shoulders, legs and spine.\n\nWild horses have been present in Europe since the Pleistocene and ranged from southern France and Spain east to central Russia. There are cave drawings of primitive predomestication horses at Lascaux, France and in Cave of Altamira, Spain, as well as artifacts believed to show the species in southern Russia, where a horse of this type was domesticated around 3000 BCE. \"Equus ferus\" had a continuous range from western Europe to Alaska; historic material suggests wild horses lived in most parts of Holocene Europe and the Eurasian steppe, except for parts of Scandinavia, Iceland and Ireland. \n\nThe \"forest horse\" or \"forest tarpan\" was a hypothesis of various 19th-century natural scientists, including Tadeusz Vetulani, who suggested that the continuous forestation of Europe after the last ice age created forest-adapted subtype of the wild horse, which he named \"Equus sylvestris.\" However, historic references do not describe any major difference between the populations, therefore most authors assume there was only one subspecies of western Eurasian wild horse, \"Equus ferus ferus.\" Nevertheless, a stocky type of horse living in forests and highlands was described during the 19th century in Spain, the Pyrenees, the Camargue, the Ardennes, Great Britain, and the southern Swedish upland. They had a robust head and strong body, and a long frizzy mane. The color was described as faint brown or yellowish brown with eel stripe and leg stripes, or wholly black legs. The flanks and shoulders were spotted, some of them tended to an ashy colour. They dwelled in rocky habitats and showed intelligent and fierce behaviour. In Dutch swamps, black wild horses were found with a large skull, small eyes and a bristly muzzle. The mane was full, with broad hooves and curly hair. However, it is possible that these were feral and not wild horses. \n\nHerodotus described lightly-coloured wild horses in an area now part of the Ukraine in the 5th century BCE. In the 12th century, Albertus Magnus states that mouse-coloured wild horses with a dark eel stripe lived in the German territory, and in Denmark, large herds were hunted. Wild horses still were common in the east of Prussia during the 15th and early 16th centuries. During the 16th century, wild horses disappeared from most of the mainland of western Europe and became less common in eastern Europe as well. Belsazar Hacquet saw wild horses in the Polish zoo in Zamość during the Seven Years' War. According to him, those wild horses were of small body size, had a blackish brown colour, a large and thick head, short dark manes and tail hair, and a “beard”. They were absolutely untameable and defended themselves harshly against predators. Kajetan Kozmian visited the population at Zamość as well and reported that they were small and strong, had robust limbs and a constantly dark mouse colour. Samuel Gottlieb Gmelin witnessed herds in Voronezh in 1768. Those wild horses were described as very fast and shy and would flee by any noise, small with small pinned ears, and a short frizzly mane. The tail was shorter than in domestic horses. They were described as mouse-colored with a light belly and legs becoming black, although gray and white horses were mentioned as well. The coat was long and dense. Peter Simon Pallas witnessed possible wild tarpans in the same year in southern Russia. He thought they were feral animals that escaped during the confusions of wars. These herds were important game of the Tatars and numbered between five and 20 animals. The horses he described had a small body, large and thick heads, short frizzly manes and short tail hair, as well as pinned ears. The colour was described as faint brownish, sometimes brown or black. He also reported of obvious domestic hybrids with lightly colored legs or grays. \n\"The Natural History of Horses\" by 19th-century author Charles Hamilton Smith also described tarpans. According to Smith, the herds of wild horses numbered from a few to hundred individuals. They often were mixed with domestic horses, and alongside pure herds there were herds of feral horses or hybrids. The color of pure tarpans was described as constantly brown, cream-colored or mouse-colored. The short frizzy mane was reported to be black, as were the tail and legs. The ears either were of varying size, but set up high at the skull. The eyes were small. According to Smith, tarpans made stronger sounds than domestic horses and the overall appearance of these wild horses was mule-like. A tarpan herd survived in the Zoo of Zamość until 1806, when the reserve had to sell them because of economic problems. They were dispersed onto the local farms at the Biłgoraj region, tamed and bred to domestic horses. According to Kozmian, wild horses had been exterminated in the Polish wilderness shortly before, because they damaged hay collected for livestock.\n\nThe human-caused extinction of the tarpan began in Southern Europe, possibly in antiquity. While humans had been hunting wild horses since the Paleolithic, during historic times horse meat was an important source of protein for many cultures. As large herbivores, the range of the tarpan was continuously decreased by the increasing civilization of the Eurasian continent. Wild horses were further persecuted because they caused damage to hay storages and often took domestic mares from pastures. Furthermore, interbreeding with wild horses was an economic loss for farmers since the foals of such matings were intractable. Tarpans survived the longest in the southern parts of the Russian steppe. By 1880, when most \"tarpans\" may have become hybrids, wild horses became very rare. In 1879 the last scientifically confirmed individual was killed. After that, only dubious sightings were documented. As the tarpan horse died out in the wild between 1875 and 1890, the last considered-wild mare was accidentally killed during an attempt at capture. The last captive tarpan died in 1909 in a Russian zoo. \n\nAn early 19th-century attempt was made by the Polish government to save the tarpan type by establishing a preserve for animals descended from the tarpan in a forested area in Białowieża. In 1780, a wildlife park was established protecting a population of tarpans until the beginning of the 19th century. When the preserve had to close down in 1806, the last remaining tarpans were donated to local farmers and it is claimed that they survived through crossbreeding with domestic horses. The Konik is claimed to descend from these hybrid horses. However, there is no evidence that the Konik is genetically different in any significant degree from other domestic breeds, and thus claims that it is a descendant of the tarpan cannot be substantiated.\n\nThe oldest archaeological evidence for domesticated horses is from Kazakhstan and Ukraine between 6000 and 5500 years BP (Before Present). The diverse mitochondrial DNA of the domestic horse coinciding with the very low diversity on the Y chromosome suggests that many mares but only very few stallions were used, and local use of wild mares or even secondary sites of domestication are likely. Therefore, the European tarpan may have contributed to the domestic horse.\n\nSome researchers consider the tarpans of the last two centuries of their existence to be mixed wild and feral population or completely feral horses. Few consider the more recent animals historically called \"tarpans\" to be genuine wild horses without domestic influence. Historic references to \"wild horses\" may actually refer to feral domestic horses or hybrids. Some 19th-century authors wrote that local \"wild\" horses had hoof problems, leading to crippled legs, and therefore they assumed these were feral horses. Other contemporary authors claimed all \"wild\" horses between the Volga River and the Ural were actually feral. However, others thought that this was too speculative and assumed that wild, undomesticated horses still lived into the 19th century. Domestic horses used in warfare often were turned loose when they were not needed. Also, remaining wild stallions could steal domestic mares. There are some accounts from the 18th and 19th centuries of wild herds with typical wild horse features such as large heads, pinned ears, short frizzy mane and tail, but mentioned animals with domestic influence as well.\nThe only known individual to be photographed was the so-called Cherson-tarpan, which was caught as a foal near Novovorontsovka in 1866. It died in 1887 in the Moscow Zoo. The nature of this horse was dubious in its lifetime, because it showed almost none of the wild horse features described in the historic sources. Today it is assumed this individual either was a hybrid or a feral domestic horse.\nThree attempts have been made to use selective breeding to create a type of horse that resembles the tarpan phenotype, though recreating an extinct subspecies is not genetically possible with current technology. In 1936, Polish university professor Tadeusz Vetulani selected Polish farm horses that he believed resembled the historic tarpan and started a selective breeding program. This horse breed is now called the Konik, which clusters genetically with other domestic horse breeds, including those as diverse as the Mongolian horse and the Thoroughbred. In the early 1930s, Berlin Zoo Director Lutz Heck and Heinz Heck of the Munich Zoo began a program crossbreeding Koniks with Przewalski horses, Gotland Ponies, and Icelandic horses. By the 1960s they produced the Heck horse. In the mid-1960s, Harry Hegard started a similar program in the United States using Mustangs and local working ranch horses that has resulted in the Hegardt or Stroebel's Horse.\n\nWhile all three breeds have a primitive look that resembles the wild type tarpan in some respects, they are not genetically tarpans and the wild, predomestic European horse remains extinct. However, this does not prevent some modern breeders from marketing horses with these features as a \"tarpan\". In spite of sharing primitive external features, the Konik and Hucul horses have markedly different conformation with differently-proportioned body measurements, thought in part to be linked to living in different habitats. Other breeds sometimes alleged to be surviving tarpans include the Exmoor pony and the Dülmen pony. However, genetic studies do not set any of these breeds apart from other domestic horses. On the other hand, there has not yet been a study comparing domestic breeds directly with the European wild horse.\n\n\n",
                "Horse\n\nThe horse (\"Equus ferus caballus\") is one of two extant subspecies of \"Equus ferus\". It is an odd-toed ungulate mammal belonging to the taxonomic family Equidae. The horse has evolved over the past 45 to 55 million years from a small multi-toed creature, \"Eohippus\", into the large, single-toed animal of today. Humans began to domesticate horses around 4000 BC, and their domestication is believed to have been widespread by 3000 BC. Horses in the subspecies \"caballus\" are domesticated, although some domesticated populations live in the wild as feral horses. These feral populations are not true wild horses, as this term is used to describe horses that have never been domesticated, such as the endangered Przewalski's horse, a separate subspecies, and the only remaining true wild horse. There is an extensive, specialized vocabulary used to describe equine-related concepts, covering everything from anatomy to life stages, size, colors, markings, breeds, locomotion, and behavior.\n\nHorses' anatomy enables them to make use of speed to escape predators and they have a well-developed sense of balance and a strong fight-or-flight response. Related to this need to flee from predators in the wild is an unusual trait: horses are able to sleep both standing up and lying down, with younger horse tending to sleep significantly more than adults. Female horses, called mares, carry their young for approximately 11 months, and a young horse, called a foal, can stand and run shortly following birth. Most domesticated horses begin training under saddle or in harness between the ages of two and four. They reach full adult development by age five, and have an average lifespan of between 25 and 30 years.\n\nHorse breeds are loosely divided into three categories based on general temperament: spirited \"hot bloods\" with speed and endurance; \"cold bloods\", such as draft horses and some ponies, suitable for slow, heavy work; and \"warmbloods\", developed from crosses between hot bloods and cold bloods, often focusing on creating breeds for specific riding purposes, particularly in Europe. There are more than 300 breeds of horse in the world today, developed for many different uses.\n\nHorses and humans interact in a wide variety of sport competitions and non-competitive recreational pursuits, as well as in working activities such as police work, agriculture, entertainment, and therapy. Horses were historically used in warfare, from which a wide variety of riding and driving techniques developed, using many different styles of equipment and methods of control. Many products are derived from horses, including meat, milk, hide, hair, bone, and pharmaceuticals extracted from the urine of pregnant mares. Humans provide domesticated horses with food, water and shelter, as well as attention from specialists such as veterinarians and farriers.\n\nSpecific terms and specialized language are used to describe equine anatomy, different life stages, colors and breeds.\n\nDepending on breed, management and environment, the modern domestic horse has a life expectancy of 25 to 30 years. Uncommonly, a few animals live into their 40s and, occasionally, beyond. The oldest verifiable record was \"Old Billy\", a 19th-century horse that lived to the age of 62. In modern times, Sugar Puff, who had been listed in \"Guinness World Records\" as the world's oldest living pony, died in 2007 at age 56.\n\nRegardless of a horse or pony's actual birth date, for most competition purposes a year is added to its age each January 1 of each year in the Northern Hemisphere and each August 1 in the Southern Hemisphere. The exception is in endurance riding, where the minimum age to compete is based on the animal's actual calendar age.\n\nThe following terminology is used to describe horses of various ages:\n\n\nIn horse racing, these definitions may differ: For example, in the British Isles, Thoroughbred horse racing defines colts and fillies as less than five years old. However, Australian Thoroughbred racing defines colts and fillies as less than four years old.\n\nThe height of horses is usually measured at the highest point of the withers, where the neck meets the back. This point is used because it is a stable point of the anatomy, unlike the head or neck, which move up and down in relation to the body of the horse.\n\nIn English-speaking countries, the height of horses is often stated in units of hands and inches: one hand is equal to . The height is expressed as the number of full hands, followed by a point, then the number of additional inches, and ending with the abbreviation \"h\" or \"hh\" (for \"hands high\"). Thus, a horse described as \"15.2 h\" is 15 hands plus 2 inches, for a total of in height.\nThe size of horses varies by breed, but also is influenced by nutrition. Light riding horses usually range in height from and can weigh from . Larger riding horses usually start at about and often are as tall as , weighing from . Heavy or draft horses are usually at least high and can be as tall as high. They can weigh from about .\n\nThe largest horse in recorded history was probably a Shire horse named Mammoth, who was born in 1848. He stood high and his peak weight was estimated at . The current record holder for the world's smallest horse is Thumbelina, a fully mature miniature horse affected by dwarfism. She is tall and weighs .\n\nPonies are taxonomically the same animals as horses. The distinction between a horse and pony is commonly drawn on the basis of height, especially for competition purposes. However, height alone is not dispositive; the difference between horses and ponies may also include aspects of phenotype, including conformation and temperament.\n\nThe traditional standard for height of a horse or a pony at maturity is . An animal 14.2 h or over is usually considered to be a horse and one less than 14.2 h a pony, but there are many exceptions to the traditional standard. In Australia, ponies are considered to be those under . For competition in the Western division of the United States Equestrian Federation, the cutoff is . The International Federation for Equestrian Sports, the world governing body for horse sport, uses metric measurements and defines a pony as being any horse measuring less than at the withers without shoes, which is just over 14.2 h, and , or just over 14.2½ h, with shoes.\n\nHeight is not the sole criterion for distinguishing horses from ponies. Breed registries for horses that typically produce individuals both under and over 14.2 h consider all animals of that breed to be horses regardless of their height. Conversely, some pony breeds may have features in common with horses, and individual animals may occasionally mature at over 14.2 h, but are still considered to be ponies.\n\nPonies often exhibit thicker manes, tails, and overall coat. They also have proportionally shorter legs, wider barrels, heavier bone, shorter and thicker necks, and short heads with broad foreheads. They may have calmer temperaments than horses and also a high level of intelligence that may or may not be used to cooperate with human handlers. Small size, by itself, is not an exclusive determinant. For example, the Shetland pony which averages , is considered a pony. Conversely, breeds such as the Falabella and other miniature horses, which can be no taller than , are classified by their registries as very small horses, not ponies.\n\nHorses have 64 chromosomes. The horse genome was sequenced in 2007. It contains 2.7 billion DNA base pairs, which is larger than the dog genome, but smaller than the human genome or the bovine genome. The map is available to researchers.\n\nHorses exhibit a diverse array of coat colors and distinctive markings, described by a specialized vocabulary. Often, a horse is classified first by its coat color, before breed or sex. Horses of the same color may be distinguished from one another by white markings, which, along with various spotting patterns, are inherited separately from coat color.\n\nMany genes that create horse coat colors and patterns have been identified. Current genetic tests can identify at least 13 different alleles influencing coat color, and research continues to discover new genes linked to specific traits. The basic coat colors of chestnut and black are determined by the gene controlled by the Melanocortin 1 receptor, also known as the \"extension gene\" or \"red factor,\" as its recessive form is \"red\" (chestnut) and its dominant form is black. Additional genes control suppression of black color to point coloration that results in a bay, spotting patterns such as pinto or leopard, dilution genes such as palomino or dun, as well as graying, and all the other factors that create the many possible coat colors found in horses.\n\nHorses that have a white coat color are often mislabeled; a horse that looks \"white\" is usually a middle-aged or older gray. Grays are born a darker shade, get lighter as they age, but usually keep black skin underneath their white hair coat (with the exception of pink skin under white markings). The only horses properly called white are born with a predominantly white hair coat and pink skin, a fairly rare occurrence. Different and unrelated genetic factors can produce white coat colors in horses, including several different alleles of dominant white and the sabino-1 gene. However, there are no \"albino\" horses, defined as having both pink skin and red eyes.\n\nGestation lasts approximately 340 days, with an average range 320–370 days, and usually results in one foal; twins are rare. Horses are a precocial species, and foals are capable of standing and running within a short time following birth. Foals are usually born in the spring. The estrous cycle of a mare occurs roughly every 19–22 days and occurs from early spring into autumn. Most mares enter an \"anestrus\" period during the winter and thus do not cycle in this period. Foals are generally weaned from their mothers between four and six months of age.\n\nHorses, particularly colts, sometimes are physically capable of reproduction at about 18 months, but domesticated horses are rarely allowed to breed before the age of three, especially females. Horses four years old are considered mature, although the skeleton normally continues to develop until the age of six; maturation also depends on the horse's size, breed, sex, and quality of care. Larger horses have larger bones; therefore, not only do the bones take longer to form bone tissue, but the epiphyseal plates are larger and take longer to convert from cartilage to bone. These plates convert after the other parts of the bones, and are crucial to development.\n\nDepending on maturity, breed, and work expected, horses are usually put under saddle and trained to be ridden between the ages of two and four. Although Thoroughbred race horses are put on the track as young as the age of two in some countries, horses specifically bred for sports such as dressage are generally not put under saddle until they are three or four years old, because their bones and muscles are not solidly developed. For endurance riding competition, horses are not deemed mature enough to compete until they are a full 60 calendar months (five years) old.\n\nThe horse skeleton averages 205 bones. A significant difference between the horse skeleton and that of a human is the lack of a collarbone—the horse's forelimbs are attached to the spinal column by a powerful set of muscles, tendons, and ligaments that attach the shoulder blade to the torso. The horse's legs and hooves are also unique structures. Their leg bones are proportioned differently from those of a human. For example, the body part that is called a horse's \"knee\" is actually made up of the carpal bones that correspond to the human wrist. Similarly, the hock contains bones equivalent to those in the human ankle and heel. The lower leg bones of a horse correspond to the bones of the human hand or foot, and the fetlock (incorrectly called the \"ankle\") is actually the proximal sesamoid bones between the cannon bones (a single equivalent to the human metacarpal or metatarsal bones) and the proximal phalanges, located where one finds the \"knuckles\" of a human. A horse also has no muscles in its legs below the knees and hocks, only skin, hair, bone, tendons, ligaments, cartilage, and the assorted specialized tissues that make up the hoof.\n\nThe critical importance of the feet and legs is summed up by the traditional adage, \"no foot, no horse\". The horse hoof begins with the distal phalanges, the equivalent of the human fingertip or tip of the toe, surrounded by cartilage and other specialized, blood-rich soft tissues such as the laminae. The exterior hoof wall and horn of the sole is made of keratin, the same material as a human fingernail. The end result is that a horse, weighing on average , travels on the same bones as would a human on tiptoe. For the protection of the hoof under certain conditions, some horses have horseshoes placed on their feet by a professional farrier. The hoof continually grows, and in most domesticated horses needs to be trimmed (and horseshoes reset, if used) every five to eight weeks, though the hooves of horses in the wild wear down and regrow at a rate suitable for their terrain.\n\nHorses are adapted to grazing. In an adult horse, there are 12 incisors at the front of the mouth, adapted to biting off the grass or other vegetation. There are 24 teeth adapted for chewing, the premolars and molars, at the back of the mouth. Stallions and geldings have four additional teeth just behind the incisors, a type of canine teeth called \"tushes\". Some horses, both male and female, will also develop one to four very small vestigial teeth in front of the molars, known as \"wolf\" teeth, which are generally removed because they can interfere with the bit. There is an empty interdental space between the incisors and the molars where the bit rests directly on the gums, or \"bars\" of the horse's mouth when the horse is bridled.\n\nAn estimate of a horse's age can be made from looking at its teeth. The teeth continue to erupt throughout life and are worn down by grazing. Therefore, the incisors show changes as the horse ages; they develop a distinct wear pattern, changes in tooth shape, and changes in the angle at which the chewing surfaces meet. This allows a very rough estimate of a horse's age, although diet and veterinary care can also affect the rate of tooth wear.\n\nHorses are herbivores with a digestive system adapted to a forage diet of grasses and other plant material, consumed steadily throughout the day. Therefore, compared to humans, they have a relatively small stomach but very long intestines to facilitate a steady flow of nutrients. A horse will eat of food per day and, under normal use, drink of water. Horses are not ruminants, they have only one stomach, like humans, but unlike humans, they can utilize cellulose, a major component of grass. Horses are hindgut fermenters. Cellulose fermentation by symbiotic bacteria occurs in the cecum, or \"water gut\", which food goes through before reaching the large intestine. Horses cannot vomit, so digestion problems can quickly cause colic, a leading cause of death.\n\nThe horses' senses are based on their status as prey animals, where they must be aware of their surroundings at all times. They have the largest eyes of any land mammal, and are lateral-eyed, meaning that their eyes are positioned on the sides of their heads. This means that horses have a range of vision of more than 350°, with approximately 65° of this being binocular vision and the remaining 285° monocular vision. Horses have excellent day and night vision, but they have two-color, or dichromatic vision; their color vision is somewhat like red-green color blindness in humans, where certain colors, especially red and related colors, appear as a shade of green.\n\nTheir sense of smell, while much better than that of humans, is not quite as good as that of a dog. It is believed to play a key role in the social interactions of horses as well as detecting other key scents in the environment. Horses have two olfactory centers. The first system is in the nostrils and nasal cavity, which analyze a wide range of odors. The second, located under the nasal cavity, are the Vomeronasal organs, also called Jacobson's organs. These have a separate nerve pathway to the brain and appear to primarily analyze pheromones.\n\nA horse's hearing is good, and the pinna of each ear can rotate up to 180°, giving the potential for 360° hearing without having to move the head. Noise impacts the behavior of horses and certain kinds of noise may contribute to stress: A 2013 study in the UK indicated that stabled horses were calmest in a quiet setting, or if listening to country or classical music, but displayed signs of nervousness when listening to jazz or rock music. This study also recommended keeping music under a volume of 21 decibels. An Australian study found that stabled racehorses listening to talk radio had a higher rate of gastric ulcers than horses listening to music, and racehorses stabled where a radio was played had a higher overall rate of ulceration than horses stabled where there was no radio playing.\n\nHorses have a great sense of balance, due partly to their ability to feel their footing and partly to highly developed proprioception—the unconscious sense of where the body and limbs are at all times. A horse's sense of touch is well developed. The most sensitive areas are around the eyes, ears, and nose. Horses are able to sense contact as subtle as an insect landing anywhere on the body.\n\nHorses have an advanced sense of taste, which allows them to sort through fodder and choose what they would most like to eat, and their prehensile lips can easily sort even small grains. Horses generally will not eat poisonous plants, however, there are exceptions; horses will occasionally eat toxic amounts of poisonous plants even when there is adequate healthy food.\n\nAll horses move naturally with four basic gaits: the four-beat walk, which averages ; the two-beat trot or jog at (faster for harness racing horses); the canter or lope, a three-beat gait that is ; and the gallop. The gallop averages , but the world record for a horse galloping over a short, sprint distance is . Besides these basic gaits, some horses perform a two-beat pace, instead of the trot. There also are several four-beat \"ambling\" gaits that are approximately the speed of a trot or pace, though smoother to ride. These include the lateral rack, running walk, and tölt as well as the diagonal fox trot. Ambling gaits are often genetic in some breeds, known collectively as gaited horses. Often, gaited horses replace the trot with one of the ambling gaits.\n\nHorses are prey animals with a strong fight-or-flight response. Their first reaction to threat is to startle and usually flee, although they will stand their ground and defend themselves when flight is impossible or if their young are threatened. They also tend to be curious; when startled, they will often hesitate an instant to ascertain the cause of their fright, and may not always flee from something that they perceive as non-threatening. Most light horse riding breeds were developed for speed, agility, alertness and endurance; natural qualities that extend from their wild ancestors. However, through selective breeding, some breeds of horses are quite docile, particularly certain draft horses.\n\nHorses are herd animals, with a clear hierarchy of rank, led by a dominant individual, usually a mare. They are also social creatures that are able to form companionship attachments to their own species and to other animals, including humans. They communicate in various ways, including vocalizations such as nickering or whinnying, mutual grooming, and body language. Many horses will become difficult to manage if they are isolated, but with training, horses can learn to accept a human as a companion, and thus be comfortable away from other horses. However, when confined with insufficient companionship, exercise, or stimulation, individuals may develop stable vices, an assortment of bad habits, mostly stereotypies of psychological origin, that include wood chewing, wall kicking, \"weaving\" (rocking back and forth), and other problems.\n\nStudies have indicated that horses perform a number of cognitive tasks on a daily basis, meeting mental challenges that include food procurement and identification of individuals within a social system. They also have good spatial discrimination abilities. They are naturally curious and apt to investigate things they have not seen before. Studies have assessed equine intelligence in areas such as problem solving, speed of learning, and memory. Horses excel at simple learning, but also are able to use more advanced cognitive abilities that involve categorization and concept learning. They can learn using habituation, desensitization, classical conditioning, and operant conditioning, and positive and negative reinforcement. One study has indicated that horses can differentiate between \"more or less\" if the quantity involved is less than four.\n\nDomesticated horses may face greater mental challenges than wild horses, because they live in artificial environments that prevent instinctive behavior whilst also learning tasks that are not natural. Horses are animals of habit that respond well to regimentation, and respond best when the same routines and techniques are used consistently. One trainer believes that \"intelligent\" horses are reflections of intelligent trainers who effectively use response conditioning techniques and positive reinforcement to train in the style that best fits with an individual animal's natural inclinations.\n\nHorses are mammals, and as such are warm-blooded, or endothermic creatures, as opposed to cold-blooded, or poikilothermic animals. However, these words have developed a separate meaning in the context of equine terminology, used to describe temperament, not body temperature. For example, the \"hot-bloods\", such as many race horses, exhibit more sensitivity and energy, while the \"cold-bloods\", such as most draft breeds, are quieter and calmer. Sometimes \"hot-bloods\" are classified as \"light horses\" or \"riding horses\", with the \"cold-bloods\" classified as \"draft horses\" or \"work horses\".\n\"Hot blooded\" breeds include \"oriental horses\" such as the Akhal-Teke, Arabian horse, Barb and now-extinct Turkoman horse, as well as the Thoroughbred, a breed developed in England from the older oriental breeds. Hot bloods tend to be spirited, bold, and learn quickly. They are bred for agility and speed. They tend to be physically refined—thin-skinned, slim, and long-legged. The original oriental breeds were brought to Europe from the Middle East and North Africa when European breeders wished to infuse these traits into racing and light cavalry horses.\n\nMuscular, heavy draft horses are known as \"cold bloods\", as they are bred not only for strength, but also to have the calm, patient temperament needed to pull a plow or a heavy carriage full of people. They are sometimes nicknamed \"gentle giants\". Well-known draft breeds include the Belgian and the Clydesdale. Some, like the Percheron, are lighter and livelier, developed to pull carriages or to plow large fields in drier climates. Others, such as the Shire, are slower and more powerful, bred to plow fields with heavy, clay-based soils. The cold-blooded group also includes some pony breeds.\n\n\"Warmblood\" breeds, such as the Trakehner or Hanoverian, developed when European carriage and war horses were crossed with Arabians or Thoroughbreds, producing a riding horse with more refinement than a draft horse, but greater size and milder temperament than a lighter breed. Certain pony breeds with warmblood characteristics have been developed for smaller riders. Warmbloods are considered a \"light horse\" or \"riding horse\".\n\nToday, the term \"Warmblood\" refers to a specific subset of sport horse breeds that are used for competition in dressage and show jumping. Strictly speaking, the term \"warm blood\" refers to any cross between cold-blooded and hot-blooded breeds. Examples include breeds such as the Irish Draught or the Cleveland Bay. The term was once used to refer to breeds of light riding horse other than Thoroughbreds or Arabians, such as the Morgan horse.\n\nHorses are able to sleep both standing up and lying down. In an adaptation from life in the wild, horses are able to enter light sleep by using a \"stay apparatus\" in their legs, allowing them to doze without collapsing. Horses sleep better when in groups because some animals will sleep while others stand guard to watch for predators. A horse kept alone will not sleep well because its instincts are to keep a constant eye out for danger.\n\nUnlike humans, horses do not sleep in a solid, unbroken period of time, but take many short periods of rest. Horses spend four to fifteen hours a day in standing rest, and from a few minutes to several hours lying down. Total sleep time in a 24-hour period may range from several minutes to a couple of hours, mostly in short intervals of about 15 minutes each. The average sleep time of a domestic horse is said to be 2.9 hours per day.\n\nHorses must lie down to reach REM sleep. They only have to lie down for an hour or two every few days to meet their minimum REM sleep requirements. However, if a horse is never allowed to lie down, after several days it will become sleep-deprived, and in rare cases may suddenly collapse as it involuntarily slips into REM sleep while still standing. This condition differs from narcolepsy, although horses may also suffer from that disorder.\n\nThe horse adapted to survive in areas of wide-open terrain with sparse vegetation, surviving in an ecosystem where other large grazing animals, especially ruminants, could not. Horses and other equids are odd-toed ungulates of the order Perissodactyla, a group of mammals that was dominant during the Tertiary period. In the past, this order contained 14 families, but only three—Equidae (the horse and related species), Tapiridae (the tapir), and Rhinocerotidae (the rhinoceroses)—have survived to the present day.\n\nThe earliest known member of the family Equidae was the \"Hyracotherium\", which lived between 45 and 55 million years ago, during the Eocene period. It had 4 toes on each front foot, and 3 toes on each back foot. The extra toe on the front feet soon disappeared with the \"Mesohippus\", which lived 32 to 37 million years ago. Over time, the extra side toes shrank in size until they vanished. All that remains of them in modern horses is a set of small vestigial bones on the leg below the knee, known informally as splint bones. Their legs also lengthened as their toes disappeared until they were a hooved animal capable of running at great speed. By about 5 million years ago, the modern \"Equus\" had evolved. Equid teeth also evolved from browsing on soft, tropical plants to adapt to browsing of drier plant material, then to grazing of tougher plains grasses. Thus proto-horses changed from leaf-eating forest-dwellers to grass-eating inhabitants of semi-arid regions worldwide, including the steppes of Eurasia and the Great Plains of North America.\n\nBy about 15,000 years ago, \"Equus ferus\" was a widespread holarctic species. Horse bones from this time period, the late Pleistocene, are found in Europe, Eurasia, Beringia, and North America. Yet between 10,000 and 7,600 years ago, the horse became extinct in North America and rare elsewhere. The reasons for this extinction are not fully known, but one theory notes that extinction in North America paralleled human arrival. Another theory points to climate change, noting that approximately 12,500 years ago, the grasses characteristic of a steppe ecosystem gave way to shrub tundra, which was covered with unpalatable plants.\n\nA truly wild horse is a species or subspecies with no ancestors that were ever domesticated. Therefore, most \"wild\" horses today are actually feral horses, animals that escaped or were turned loose from domestic herds and the descendants of those animals. Only two never-domesticated subspecies, the Tarpan and the Przewalski's Horse, survived into recorded history and only the latter survives today.\n\nThe Przewalski's horse (\"Equus ferus przewalskii\"), named after the Russian explorer Nikolai Przhevalsky, is a rare Asian animal. It is also known as the Mongolian wild horse; Mongolian people know it as the \"taki\", and the Kyrgyz people call it a \"kirtag\". The subspecies was presumed extinct in the wild between 1969 and 1992, while a small breeding population survived in zoos around the world. In 1992, it was reestablished in the wild due to the conservation efforts of numerous zoos. Today, a small wild breeding population exists in Mongolia. There are additional animals still maintained at zoos throughout the world.\n\nThe tarpan or European wild horse (\"Equus ferus ferus\") was found in Europe and much of Asia. It survived into the historical era, but became extinct in 1909, when the last captive died in a Russian zoo. Thus, the genetic line was lost. Attempts have been made to recreate the tarpan, which resulted in horses with outward physical similarities, but nonetheless descended from domesticated ancestors and not true wild horses.\n\nPeriodically, populations of horses in isolated areas are speculated to be relict populations of wild horses, but generally have been proven to be feral or domestic. For example, the Riwoche horse of Tibet was proposed as such, but testing did not reveal genetic differences from domesticated horses. Similarly, the Sorraia of Portugal was proposed as a direct descendant of the Tarpan based on shared characteristics, but genetic studies have shown that the Sorraia is more closely related to other horse breeds and that the outward similarity is an unreliable measure of relatedness.\n\nBesides the horse, there are seven other species of genus \"Equus\" in the Equidae family. These are the ass or donkey, \"Equus asinus\"; the mountain zebra, \"Equus zebra\"; plains zebra, \"Equus quagga\"; Grévy's zebra, \"Equus grevyi\"; the kiang, \"Equus kiang\"; and the onager, \"Equus hemionus\".\n\nHorses can crossbreed with other members of their genus. The most common hybrid is the mule, a cross between a \"jack\" (male donkey) and a mare. A related hybrid, a hinny, is a cross between a stallion and a jenny (female donkey). Other hybrids include the zorse, a cross between a zebra and a horse. With rare exceptions, most hybrids are sterile and cannot reproduce.\n\nDomestication of the horse most likely took place in central Asia prior to 3500 BC. Two major sources of information are used to determine where and when the horse was first domesticated and how the domesticated horse spread around the world. The first source is based on palaeological and archaeological discoveries; the second source is a comparison of DNA obtained from modern horses to that from bones and teeth of ancient horse remains.\n\nThe earliest archaeological evidence for the domestication of the horse comes from sites in Ukraine and Kazakhstan, dating to approximately 3500–4000 BC. By 3000 BC, the horse was completely domesticated and by 2000 BC there was a sharp increase in the number of horse bones found in human settlements in northwestern Europe, indicating the spread of domesticated horses throughout the continent. The most recent, but most irrefutable evidence of domestication comes from sites where horse remains were interred with chariots in graves of the Sintashta and Petrovka cultures c. 2100 BC.\n\nDomestication is also studied by using the genetic material of present-day horses and comparing it with the genetic material present in the bones and teeth of horse remains found in archaeological and palaeological excavations. The variation in the genetic material shows that very few wild stallions contributed to the domestic horse, while many mares were part of early domesticated herds. This is reflected in the difference in genetic variation between the DNA that is passed on along the paternal, or sire line (Y-chromosome) versus that passed on along the maternal, or dam line (mitochondrial DNA). There are very low levels of Y-chromosome variability, but a great deal of genetic variation in mitochondrial DNA. There is also regional variation in mitochondrial DNA due to the inclusion of wild mares in domestic herds. Another characteristic of domestication is an increase in coat color variation. In horses, this increased dramatically between 5000 and 3000 BC.\n\nBefore the availability of DNA techniques to resolve the questions related to the domestication of the horse, various hypotheses were proposed. One classification was based on body types and conformation, suggesting the presence of four basic prototypes that had adapted to their environment prior to domestication. Another hypothesis held that the four prototypes originated from a single wild species and that all different body types were entirely a result of selective breeding after domestication. However, the lack of a detectable substructure in the horse has resulted in a rejection of both hypotheses.\n\nFeral horses are born and live in the wild, but are descended from domesticated animals. Many populations of feral horses exist throughout the world. Studies of feral herds have provided useful insights into the behavior of prehistoric horses, as well as greater understanding of the instincts and behaviors that drive horses that live in domesticated conditions.\n\nThere are also semi-feral horses in many parts of the world, such as Dartmoor and the New Forest in the UK, where the animals are all privately owned but live for significant amounts of time in \"wild\" conditions on undeveloped, often public, lands. Owners of such animals often pay a fee for grazing rights.\n\nThe concept of purebred bloodstock and a controlled, written breed registry has come to be particularly significant and important in modern times. Sometimes purebred horses are incorrectly or inaccurately called \"thoroughbreds\". Thoroughbred is a specific breed of horse, while a \"purebred\" is a horse (or any other animal) with a defined pedigree recognized by a breed registry. Horse breeds are groups of horses with distinctive characteristics that are transmitted consistently to their offspring, such as conformation, color, performance ability, or disposition. These inherited traits result from a combination of natural crosses and artificial selection methods. Horses have been selectively bred since their domestication. An early example of people who practiced selective horse breeding were the Bedouin, who had a reputation for careful practices, keeping extensive pedigrees of their Arabian horses and placing great value upon pure bloodlines. These pedigrees were originally transmitted via an oral tradition. In the 14th century, Carthusian monks of southern Spain kept meticulous pedigrees of bloodstock lineages still found today in the Andalusian horse.\n\nBreeds developed due to a need for \"form to function\", the necessity to develop certain characteristics in order to perform a particular type of work. Thus, a powerful but refined breed such as the Andalusian developed as riding horses with an aptitude for dressage. Heavy draft horses developed out of a need to perform demanding farm work and pull heavy wagons. Other horse breeds developed specifically for light agricultural work, carriage and road work, various sport disciplines, or simply as pets. Some breeds developed through centuries of crossing other breeds, while others descended from a single foundation sire, or other limited or restricted foundation bloodstock. One of the earliest formal registries was General Stud Book for Thoroughbreds, which began in 1791 and traced back to the foundation bloodstock for the breed. There are more than 300 horse breeds in the world today.\n\nWorldwide, horses play a role within human cultures and have done so for millennia. Horses are used for leisure activities, sports, and working purposes. The Food and Agriculture Organization (FAO) estimates that in 2008, there were almost 59,000,000 horses in the world, with around 33,500,000 in the Americas, 13,800,000 in Asia and 6,300,000 in Europe and smaller portions in Africa and Oceania. There are estimated to be 9,500,000 horses in the United States alone. The American Horse Council estimates that horse-related activities have a direct impact on the economy of the United States of over $39 billion, and when indirect spending is considered, the impact is over $102 billion. In a 2004 \"poll\" conducted by Animal Planet, more than 50,000 viewers from 73 countries voted for the horse as the world's 4th favorite animal.\n\nCommunication between human and horse is paramount in any equestrian activity; to aid this process horses are usually ridden with a saddle on their backs to assist the rider with balance and positioning, and a bridle or related headgear to assist the rider in maintaining control. Sometimes horses are ridden without a saddle, and occasionally, horses are trained to perform without a bridle or other headgear. Many horses are also driven, which requires a harness, bridle, and some type of vehicle.\n\nHistorically, equestrians honed their skills through games and races. Equestrian sports provided entertainment for crowds and honed the excellent horsemanship that was needed in battle. Many sports, such as dressage, eventing and show jumping, have origins in military training, which were focused on control and balance of both horse and rider. Other sports, such as rodeo, developed from practical skills such as those needed on working ranches and stations. Sport hunting from horseback evolved from earlier practical hunting techniques. Horse racing of all types evolved from impromptu competitions between riders or drivers. All forms of competition, requiring demanding and specialized skills from both horse and rider, resulted in the systematic development of specialized breeds and equipment for each sport. The popularity of equestrian sports through the centuries has resulted in the preservation of skills that would otherwise have disappeared after horses stopped being used in combat.\n\nHorses are trained to be ridden or driven in a variety of sporting competitions. Examples include show jumping, dressage, three-day eventing, competitive driving, endurance riding, gymkhana, rodeos, and fox hunting. Horse shows, which have their origins in medieval European fairs, are held around the world. They host a huge range of classes, covering all of the mounted and harness disciplines, as well as \"In-hand\" classes where the horses are led, rather than ridden, to be evaluated on their conformation. The method of judging varies with the discipline, but winning usually depends on style and ability of both horse and rider.\nSports such as polo do not judge the horse itself, but rather use the horse as a partner for human competitors as a necessary part of the game. Although the horse requires specialized training to participate, the details of its performance are not judged, only the result of the rider's actions—be it getting a ball through a goal or some other task. Examples of these sports of partnership between human and horse include jousting, in which the main goal is for one rider to unseat the other, and buzkashi, a team game played throughout Central Asia, the aim being to capture a goat carcass while on horseback.\n\nHorse racing is an equestrian sport and major international industry, watched in almost every nation of the world. There are three types: \"flat\" racing; steeplechasing, i.e. racing over jumps; and harness racing, where horses trot or pace while pulling a driver in a small, light cart known as a sulky. A major part of horse racing's economic importance lies in the gambling associated with it.\n\nThere are certain jobs that horses do very well, and no technology has yet developed to fully replace them. For example, mounted police horses are still effective for certain types of patrol duties and crowd control. Cattle ranches still require riders on horseback to round up cattle that are scattered across remote, rugged terrain. Search and rescue organizations in some countries depend upon mounted teams to locate people, particularly hikers and children, and to provide disaster relief assistance. Horses can also be used in areas where it is necessary to avoid vehicular disruption to delicate soil, such as nature reserves. They may also be the only form of transport allowed in wilderness areas. Horses are quieter than motorized vehicles. Law enforcement officers such as park rangers or game wardens may use horses for patrols, and horses or mules may also be used for clearing trails or other work in areas of rough terrain where vehicles are less effective.\n\nAlthough machinery has replaced horses in many parts of the world, an estimated 100 million horses, donkeys and mules are still used for agriculture and transportation in less developed areas. This number includes around 27 million working animals in Africa alone. Some land management practices such as cultivating and logging can be efficiently performed with horses. In agriculture, less fossil fuel is used and increased environmental conservation occurs over time with the use of draft animals such as horses. Logging with horses can result in reduced damage to soil structure and less damage to trees due to more selective logging.\n\nModern horses are often used to reenact many of their historical work purposes. Horses are used, complete with equipment that is authentic or a meticulously recreated replica, in various live action historical reenactments of specific periods of history, especially recreations of famous battles. Horses are also used to preserve cultural traditions and for ceremonial purposes. Countries such as the United Kingdom still use horse-drawn carriages to convey royalty and other VIPs to and from certain culturally significant events. Public exhibitions are another example, such as the Budweiser Clydesdales, seen in parades and other public settings, a team of draft horses that pull a beer wagon similar to that used before the invention of the modern motorized truck.\n\nHorses are frequently seen in television, films and literature. They are sometimes featured as a major character in films about particular animals, but also used as visual elements that assure the accuracy of historical stories. Both live horses and iconic images of horses are used in advertising to promote a variety of products. The horse frequently appears in coats of arms in heraldry, in a variety of poses and equipment. The mythologies of many cultures, including Greco-Roman, Hindu, Islamic, and Norse, include references to both normal horses and those with wings or additional limbs, and multiple myths also call upon the horse to draw the chariots of the Moon and Sun. The horse also appears in the 12-year cycle of animals in the Chinese zodiac related to the Chinese calendar.\n\nPeople of all ages with physical and mental disabilities obtain beneficial results from association with horses. Therapeutic riding is used to mentally and physically stimulate disabled persons and help them improve their lives through improved balance and coordination, increased self-confidence, and a greater feeling of freedom and independence. The benefits of equestrian activity for people with disabilities has also been recognized with the addition of equestrian events to the Paralympic Games and recognition of para-equestrian events by the International Federation for Equestrian Sports (FEI). Hippotherapy and therapeutic horseback riding are names for different physical, occupational, and speech therapy treatment strategies that utilize equine movement. In hippotherapy, a therapist uses the horse's movement to improve their patient's cognitive, coordination, balance, and fine motor skills, whereas therapeutic horseback riding uses specific riding skills.\n\nHorses also provide psychological benefits to people whether they actually ride or not. \"Equine-assisted\" or \"equine-facilitated\" therapy is a form of experiential psychotherapy that uses horses as companion animals to assist people with mental illness, including anxiety disorders, psychotic disorders, mood disorders, behavioral difficulties, and those who are going through major life changes. There are also experimental programs using horses in prison settings. Exposure to horses appears to improve the behavior of inmates and help reduce recidivism when they leave.\n\nHorses have been used in warfare for most of recorded history. The first archaeological evidence of horses used in warfare dates to between 4000 and 3000 BC, and the use of horses in warfare was widespread by the end of the Bronze Age. Although mechanization has largely replaced the horse as a weapon of war, horses are still seen today in limited military uses, mostly for ceremonial purposes, or for reconnaissance and transport activities in areas of rough terrain where motorized vehicles are ineffective. Horses have been used in the 21st century by the Janjaweed militias in the War in Darfur.\n\nHorses are raw material for many products made by humans throughout history, including byproducts from the slaughter of horses as well as materials collected from living horses.\n\nProducts collected from living horses include mare's milk, used by people with large horse herds, such as the Mongols, who let it ferment to produce kumis. Horse blood was once used as food by the Mongols and other nomadic tribes, who found it a convenient source of nutrition when traveling. Drinking their own horses' blood allowed the Mongols to ride for extended periods of time without stopping to eat. The drug Premarin is a mixture of estrogens extracted from the urine of pregnant mares (pregnant mares' urine), and was previously a widely used drug for hormone replacement therapy. The tail hair of horses can be used for making bows for string instruments such as the violin, viola, cello, and double bass.\n\nHorse meat has been used as food for humans and carnivorous animals throughout the ages. It is eaten in many parts of the world, though consumption is taboo in some cultures, and a subject of political controversy in others. Horsehide leather has been used for boots, gloves, jackets, baseballs, and baseball gloves. Horse hooves can also be used to produce animal glue. Horse bones can be used to make implements. Specifically, in Italian cuisine, the horse tibia is sharpened into a probe called a \"spinto\", which is used to test the readiness of a (pig) ham as it cures. In Asia, the saba is a horsehide vessel used in the production of kumis.\n\nHorses are grazing animals, and their major source of nutrients is good-quality forage from hay or pasture. They can consume approximately 2% to 2.5% of their body weight in dry feed each day. Therefore, a adult horse could eat up to of food. Sometimes, concentrated feed such as grain is fed in addition to pasture or hay, especially when the animal is very active. When grain is fed, equine nutritionists recommend that 50% or more of the animal's diet by weight should still be forage.\n\nHorses require a plentiful supply of clean water, a minimum of to per day. Although horses are adapted to live outside, they require shelter from the wind and precipitation, which can range from a simple shed or shelter to an elaborate stable.\n\nHorses require routine hoof care from a farrier, as well as vaccinations to protect against various diseases, and dental examinations from a veterinarian or a specialized equine dentist. If horses are kept inside in a barn, they require regular daily exercise for their physical health and mental well-being. When turned outside, they require well-maintained, sturdy fences to be safely contained. Regular grooming is also helpful to help the horse maintain good health of the hair coat and underlying skin.\n\n\n"
            ],
            "good_message": false,
            "loggedData": [
                {
                    "curr_message": " The horse (\"Equus ferus caballus\") is one of two extant subspecies of \"Equus ferus\".",
                    "last_key_pressed": "<<selected>>"
                }
            ],
            "offensive": null,
            "researchTask": true,
            "selectedData": [
                "The horse (\"Equus ferus caballus\") is one of two extant subspecies of \"Equus ferus\"."
            ],
            "shown_passages": [
                [
                    "Wild horse",
                    [
                        "The wild horse (\"Equus ferus\") is a species of the genus \"Equus\", which includes as subspecies the modern domesticated horse (\"Equus ferus caballus\") as well as the undomesticated tarpan (\"Equus ferus ferus\", now extinct), and the endangered Przewalski's horse (\"Equus ferus przewalskii\").",
                        "Przewalski's horse had reached the brink of extinction but was reintroduced successfully back into the wild.",
                        "The tarpan became extinct in the 19th century, though it is a possible ancestor of the domestic horse; it roamed the steppes of Eurasia at the time of domestication.",
                        "However, other subspecies of \"Equus ferus\" may have existed and could have been the stock from which domesticated horses are descended."
                    ]
                ],
                [
                    "Tarpan",
                    [
                        "The tarpan (\"Equus ferus ferus\"), also known as Eurasian wild horse, was a subspecies of wild horse.",
                        "It is now extinct.",
                        "The last individual believed to be of this subspecies died in captivity in Russia in 1909, although some sources claim that it was not a genuine wild horse due to its resemblance to domesticated horses.",
                        "Beginning in the 1930s, several attempts were made to develop horses that looked like tarpans through selective breeding, called \"breeding back\" by advocates.",
                        "The breeds that resulted included the Heck horse, the Hegardt or Stroebel's horse, and a derivation of the Konik breed, all of which have a primitive appearance, particularly in having the grullo coat color."
                    ]
                ],
                [
                    "Horse",
                    [
                        "The horse (\"Equus ferus caballus\") is one of two extant subspecies of \"Equus ferus\".",
                        "It is an odd-toed ungulate mammal belonging to the taxonomic family Equidae.",
                        "The horse has evolved over the past 45 to 55 million years from a small multi-toed creature, \"Eohippus\", into the large, single-toed animal of today.",
                        "Humans began to domesticate horses around 4000 BC, and their domestication is believed to have been widespread by 3000 BC.",
                        "Horses in the subspecies \"caballus\" are domesticated, although some domesticated populations live in the wild as feral horses.",
                        "These feral populations are not true wild horses, as this term is used to describe horses that have never been domesticated, such as the endangered Przewalski's horse, a separate subspecies, and the only remaining true wild horse."
                    ]
                ]
            ],
            "speaker": "0_Wizard",
            "suggestionsConfig": "show",
            "suggestionsNumber": 5,
            "suggestionsShown": true,
            "text": " The horse (\"Equus ferus caballus\") is one of two extant subspecies of \"Equus ferus\".",
            "textboxFilled": 0,
            "time": 1580233417.1601634,
            "timeTaken": 33.963,
            "totalTasks": 120,
            "turn": 20
        },
        {
            "checked_passage": {
                "no_passages_used": "no_passages_used"
            },
            "checked_sentence": {
                "no_passages_used": "no_passages_used"
            },
            "clicked_passages": {},
            "context": {
                "conversation": [
                    "Good Morning.  I'm thinking of getting a job in the court system.",
                    "ARe you planning on working on a specific level, there are three?",
                    "I hadn't thought of that.  What levels are there to choose from?",
                    "The Supreme Court of the United States is the court of last resort, so it is the higher one."
                ],
                "index": 1
            },
            "contextCount": 0,
            "full_passages": [
                "Supreme court\n\nA supreme court is the highest court within the hierarchy of courts in many legal jurisdictions. Other descriptions for such courts include court of last resort, apex court, and highest (or final) court of appeal. Broadly speaking, the decisions of a supreme court are not subject to further review by any other court. Supreme courts typically function primarily as appellate courts, hearing appeals from decisions of lower trial courts, or from intermediate-level appellate courts.\n\nHowever, not all highest courts are named as such. Civil law states tend not to have a single highest court. Additionally, the highest court in some jurisdictions is not named the \"Supreme Court\", for example, the High Court of Australia; this is because decisions by the High Court could formerly be appealed to the Privy Council. On the other hand, in some places the court named the \"Supreme Court\" is not in fact the highest court; examples include the New York Supreme Court, the Supreme Courts of several Canadian provinces/territories and the former Supreme Court of Judicature of England and Wales and Supreme Court of Judicature of Northern Ireland, which are all subordinate to higher courts of appeal.\n\nSome countries have multiple \"supreme courts\" whose respective jurisdictions have different geographical extents, or which are restricted to particular areas of law. Some countries with a federal system of government may have both a federal supreme court (such as the Supreme Court of the United States), and supreme courts for each member state (such as the Supreme Court of Nevada), with the former having jurisdiction over the latter only to the extent that the federal constitution extends federal law over state law. However, other federations, such as Canada, may have a supreme court of general jurisdiction, able to decide any question of law. Jurisdictions with a civil law system often have a hierarchy of administrative courts separate from the ordinary courts, headed by a supreme administrative court as is the case in the Netherlands. A number of jurisdictions also maintain a separate constitutional court (first developed in the Czechoslovak Constitution of 1920), such as Austria, France, Germany, Luxembourg, Portugal, Russia, Spain and South Africa. Within the former British Empire, the highest court within a colony was often called the \"Supreme Court\", even though appeals could be made from that court to the United Kingdom's Privy Council (based in London). A number of Commonwealth jurisdictions retain this system, but many others have reconstituted their own highest court as a court of last resort, with the right of appeal to the Privy Council being abolished.\n\nIn jurisdictions using a common law system, the doctrine of \"stare decisis\" applies, whereby the principles applied by the supreme court in its decisions are binding upon all lower courts; this is intended to apply a uniform interpretation and implementation of the law. In civil law jurisdictions the doctrine of \"stare decisis\" is not generally considered to apply, so the decisions of the supreme court are not necessarily binding beyond the immediate case before it; however, in practice the decisions of the supreme court usually provide a very strong precedent, or \"jurisprudence constante\", for both itself and all lower courts.\n\nThe Supreme Court of Bangladesh is created by the provisions of the Constitution of Bangladesh, 1972. There are two Divisions of the Supreme Court, i.e. (a) Appellate Division and (b) High Court Division. Appellate Division is the highest Court of Appeal and usually does not exercise the powers of a court of first instance. Whereas, the High Court Division is a Court of first instance in writ/judicial review, company and admiralty matters.\n\nThe Supreme Court of Canada was established in 1875 but only became the highest court in the country in 1949 when the right of appeal to the Judicial Committee of the Privy Council was abolished. This court hears appeals from the courts of appeal from the provinces and territories, and also appeals from the Federal Court of Appeal. The Supreme Court is a \"General Court of Appeal.\" It can decide any question of law considered by the lower courts, including constitutional law, federal law, and provincial law. The court's decisions are final and binding on the federal courts and the courts from all provinces and territories. The title \"Supreme\" can be confusing because, for example, the Supreme Court of British Columbia does not have the final say and controversial cases heard there often get appealed in higher courts - it is in fact one of the lower courts in such a process.\n\nIn Hong Kong, the Supreme Court of Hong Kong (now known as the High Court of Hong Kong) was the final court of appeal during its colonial times which ended with transfer of sovereignty in 1997. The final adjudication power, as in any other British Colonies, rested with the Judicial Committee of the Privy Council (JCPC) in London, United Kingdom. Now the power of final adjudication is vested in the Court of Final Appeal created in 1997. Under the Basic Law, its constitution, the territory remains a common law jurisdiction. Consequently, judges from other common law jurisdictions (including England and Wales) can be recruited and continue to serve in the judiciary according to Article 92 of the Basic Law. On the other hand, the power of interpretation of the Basic Law itself is vested in the Standing Committee of the National People's Congress (NPCSC) in Beijing (without retroactive effect), and the courts are authorised to interpret the Basic Law when trying cases, in accordance with Article 158 of the Basic Law. This arrangement became controversial in light of the right of abode issue in 1999, raising concerns for judicial independence.\n\nIn India, the Supreme Court of India was created on January 28, 1950 after adoption of the Constitution.\nArticle 141 of the Constitution of India states that the law declared by Supreme Court is to be binding on all Courts within the territory of India. It is the highest court in India and has ultimate judicial authority to interpret the Constitution and decide questions of national law (including local bylaws). The Supreme Court is also vested with the power of judicial review to ensure the application of the rule of law.\n\nNote that within the constitutional framework of India, Jammu and Kashmir (J&K) has a special status vis-a-vis the other states of India. Article 370 of the Constitution of India carves out certain exceptions for J&K. However, the Constitution (Application to Jammu and Kashmir) Order 1954 makes Article 141 applicable to the state of J&K and hence law declared by the Supreme Court of India is equally applicable to all courts of J&K including the High Court.\n\nThe Supreme Court is the highest court in the Republic of Ireland. It has authority to interpret the constitution, and strike down laws and activities of the state that it finds to be unconstitutional. It is also the highest authority in the interpretation of the law. Constitutionally it must have authority to interpret the constitution but its further appellate jurisdiction from lower courts is defined by law. The Irish Supreme Court consists of its presiding member, the Chief Justice, and seven other judges. Judges of the Supreme Court are appointed by the President in accordance with the binding advice of the Government. The Supreme Court sits in the Four Courts in Dublin.\n\nIsrael's Supreme Court is at the head of the court system in the State of Israel. It is the highest judicial instance. The Supreme Court sits in Jerusalem. The area of its jurisdiction is the entire State. A ruling of the Supreme Court is binding upon every court, other than the Supreme Court itself. The Israeli supreme court is both an appellate court and the high court of justice. As an appellate court, the Supreme Court considers cases on appeal (both criminal and civil) on judgments and other decisions of the District Courts. It also considers appeals on judicial and quasi-judicial decisions of various kinds, such as matters relating to the legality of Knesset elections and disciplinary rulings of the Bar Association. As the High Court of Justice (Hebrew: Beit Mishpat Gavoha Le'Zedek בית משפט גבוה לצדק; also known by its initials as Bagatz בג\"ץ), the Supreme Court rules as a court of first instance, primarily in matters regarding the legality of decisions of State authorities: Government decisions, those of local authorities and other bodies and persons performing public functions under the law, and direct challenges to the constitutionality of laws enacted by the Knesset. The court has broad discretionary authority to rule on matters in which it considers it necessary to grant relief in the interests of justice, and which are not within the jurisdiction of another court or tribunal. The High Court of Justice grants relief through orders such as injunction, mandamus and Habeas Corpus, as well as through declaratory judgments. The Supreme Court can also sit at a further hearing on its own judgment. In a matter on which the Supreme Court has ruled - whether as a court of appeals or as the High Court of Justice - with a panel of three or more justices, it may rule at a further hearing with a panel of a larger number of justices. A further hearing may be held if the Supreme Court makes a ruling inconsistent with a previous ruling or if the Court deems that the importance, difficulty or novelty of a ruling of the Court justifies such hearing. The Supreme Court also holds the unique power of being able to order \"trial de novo\" (a retrial).\n\nIn Nauru, there is no single highest court for all types of cases. The Supreme Court has final jurisdiction on constitutional matters, but any other case may be appealed further to the Appellate Court. In addition, an agreement between Nauru and Australia in 1976 provides for appeals from the Supreme Court of Nauru to the High Court of Australia in both criminal and civil cases, with the notable exception of constitutional cases.\n\nIn New Zealand, the right of appeal to the Privy Council was abolished following the passing of the Supreme Court Act (2003). A right of appeal to the Privy Council remains for criminal cases which were decided before the Supreme Court was created, but it is likely that the successful appeal by Mark Lundy to the Privy Council in 2013 will be the last appeal to the Board from New Zealand.\n\nThe new Supreme Court of New Zealand was officially established at the beginning of 2004, although it did not come into operation until July. The High Court of New Zealand was until 1980 known as the Supreme Court. The Supreme Court has a purely appellate jurisdiction and hears appeals from the Court of Appeal of New Zealand. In some cases, an appeal may be removed directly to the Supreme Court from the High Court. For certain cases, particularly cases which commenced in the District Court, a lower court (typically the High Court or the Court of Appeal) may be the court of final jurisdiction.\n\nThe Supreme Court has been the apex court for Pakistan since the declaration of the republic in 1956 (previously the Privy Council had that function). The Supreme Court has the final say on matters of constitutional law, federal law or on matters of mixed federal and provincial competence. It can hear appeals on matters of provincial competence only if a matter of a constitutional nature is raised.\n\nWith respect to Pakistan's territories (i.e. FATA, Azad Kashmir, Northern Areas and Islamabad Capital Territory (ICT)) the Supreme Court's jurisdiction is rather limited and varies from territory to territory; it can hear appeals only of a constitutional nature from FATA and Northern Areas, while ICT generally functions the same as provinces. Azad Kashmir has its own courts system and the constitution of Pakistan does not apply to it as such; appeals from Azad Kashmir relate to its relationship with Pakistan.\n\nThe provinces have their own courts system, with the High Court as the apex court, except insofar as where an appeal can go to the Supreme Court as mentioned above.\n\nThe Supreme Court of the United Kingdom is the ultimate court for criminal and civil matters in England, Wales and Northern Ireland and for civil matters in Scotland. (The supreme court for criminal matters in Scotland is the High Court of Justiciary.) The Supreme Court was established by the Constitutional Reform Act 2005 with effect from 1 October 2009, replacing and assuming the judicial functions of the House of Lords. Devolution issues under the Scotland Act 1998, Government of Wales Act and Northern Ireland Act were also transferred to the new Supreme Court by the Constitutional Reform Act, from the Judicial Committee of the Privy Council.\n\nIn respect of Community Law the Supreme Court is subject to the decisions of the European Court of Justice. Since there can be no appeal from the Supreme Court, there is an interlocutory procedure by which the Supreme Court may refer to the European Court questions of European law which arise in cases before it, and obtain a definitive ruling before the Supreme Court gives its judgment.\n\nThe Supreme Court shares its members and accommodation at the Middlesex Guildhall in London with the Judicial Committee of the Privy Council which hears final appeals from certain smaller Commonwealth countries, admiralty cases, and certain appeals from the ecclesiastical courts and statutory private jurisdictions, such as professional and academic bodies.\n\n(The Constitutional Reform Act also renamed the \"Supreme Court of Judicature of Northern Ireland\" to the Court of Judicature, and the rarely cited \"Supreme Court of Judicature for England and Wales\" as the Senior Courts of England and Wales).\n\nThe Supreme Court was set up in 2009; until then the House of Lords was the ultimate court in addition to being a legislative body, and the Lord Chancellor, with legislative and executive functions, was also a senior judge in the House of Lords.\n\nThe Supreme Court of the United States, established in 1789, is the highest Federal court in the United States, with powers of judicial review first asserted in \"Calder v. Bull\" (1798) in Justice Iredell's dissenting opinion. The power was later given binding authority by Justice Marshall in \"Marbury v. Madison\" (1803). There are currently nine seats on the US Supreme Court.\n\nEach U.S. state has a state supreme court, which is the highest authority interpreting that state's law and administering that state's judiciary. Two states, Oklahoma and Texas, each have two separate highest courts that respectively specialize in criminal cases and civil cases. Although Delaware has a specialized court, the Court of Chancery, to hear cases in equity, it is not a supreme court because the Delaware Supreme Court has appellate jurisdiction over it.\n\nThe titles of state supreme court vary, which can cause confusion between jurisdictions because one state may use a name for its highest court that another uses for a lower court. In New York, Maryland, and the District of Columbia the highest court is called the Court of Appeals, a name used by many states for their intermediate appellate courts. Further, trial courts of general jurisdiction in New York are called the Supreme Court, and the intermediate appellate court is called the Supreme Court, Appellate Division. In West Virginia, the highest court of the state is the Supreme Court of Appeals. In Maine and Massachusetts the highest court is styled the \"Supreme Judicial Court\"; the last is the oldest appellate court of continuous operation in the Western Hemisphere.\n\nThe Roman law and the Corpus Juris Civilis are generally held to be the historical model for civil law. From the late 18th century onwards, civil law jurisdictions began to codify their laws, most of all in civil codes.\n\nIn Austria, the Austrian Constitution of 1920 (based on a draft by Hans Kelsen) introduced judicial review of legislative acts for their constitutionality. This function is performed by the Constitutional Court (\"Verfassungsgerichtshof\"), which is also charged with the review of administrative acts on whether they violate constitutionally guaranteed rights.\nOther than that, administrative acts are reviewed by the Administrative Court (\"Verwaltungsgerichtshof\"). The Supreme Court (\"Oberste Gerichtshof (OGH)\"), stands at the top of Austria's system of \"ordinary courts\" (\"ordentliche Gerichte\") as the final instance in issues of private law and criminal law.\n\nIn Brazil, the Supreme Federal Tribunal (\"Supremo Tribunal Federal\") is the highest court. It is both the constitutional court and the court of last resort in Brazilian law. It only reviews cases that may be unconstitutional or final \"habeas corpus\" pleads for criminal cases. It also judges, in original jurisdiction, cases involving members of congress, senators, ministers of state, members of the high courts and the President and Vice-President of the Republic. The Superior Court of Justice (\"Tribunal Superior de Justiça\") reviews State and Federal Circuit courts decisions for civil law and criminal law cases, when dealing with federal law or conflicting rulings. The Superior Labour Tribunal (\"Tribunal Superior do Trabalho\") reviews cases involving labour law. The Superior Electoral Tribunal (\"Tribunal Superior Eleitoral\") is the court of last resort of electoral law, and also oversees general elections. The Superior Military Tribunal (\"Tribunal Superior Militar\") is the highest court in matters of federal military law.\n\nIn Croatia, the supreme jurisdiction is given to the Supreme Court, which secures a uniform application of laws. The Constitutional Court exists to verify constitutionality of laws and regulations, as well as decide on individual complaints on decisions on governmental bodies. It also decides on jurisdictional disputes between the legislative, executive and judicial branches.\n\nIn Denmark, all ordinary courts have original jurisdiction to hear all types of cases, including cases of a constitutional or administrative nature. As a result, there exists no special constitutional court, and therefore final jurisdiction is vested with the Danish Supreme Court (\"Højesteret\") which was established 14 February 1661 by king Frederik III.\n\nIn France, supreme appellate jurisdiction is divided among three judicial bodies:\n\nWhen there is jurisdictional dispute between judicial and administrative courts: the Court of Arbitration (\"Tribunal des conflits\"), which is empanelled half from the Court of Cassation and half from the Council of State and presided over by the Minister of Justice, is called together to settle the dispute or hand down a final decision.\n\nThe High Court (\"Haute Cour\") exists only to impeach the President of the French Republic in case of \"breach of his duties patently incompatible with his continuing in office\". Since a constitutional amendment of 2007, the French Constitution states that the High Court is composed of all members of both Houses of Parliament. As of 2012, it has never been convened.\n\nIn Germany, there is no \"de jure\" single supreme court. Instead, cases are handled by numerous federal courts, depending on their nature.\n\nFinal interpretation of the German Constitution, the \"Grundgesetz\", is the task of the \"Bundesverfassungsgericht\" (Federal Constitutional Court), which is the \"de facto\" highest German court, as it can declare both federal and state legislation ineffective, and has the power to overrule decisions of all other federal courts, despite not being a regular court of appeals on itself in the German court system. It is also the only court possessing the power and authority to outlaw political parties, if it is deemed that these parties have repeatedly violated articles of the Constitution.\n\nWhen it comes to civil and criminal cases, the \"Bundesgerichtshof\" (Federal Court of Justice) is at the top of the hierarchy of courts. The other branches of the German judicial system each have their own appellate systems, each topped by a high court; these are the \"Bundessozialgericht\" (Federal Social Court) for matters of social security, the \"Bundesarbeitsgericht\" (Federal Labour Court) for employment and labour, the \"Bundesfinanzhof\" (Federal Fiscal Court) for taxation and financial issues, and the \"Bundesverwaltungsgericht\" (Federal Administrative Court) for administrative law. The so-called \"Gemeinsamer Senat der Obersten Gerichtshöfe\" (Joint Senate of the Supreme Courts) is not a supreme court in itself, but an ad-hoc body that is convened in only when one supreme court intends to diverge from another supreme court's legal opinion or when a certain case exceeds the authority of one court. As the courts have well-defined areas of responsibility, situations like these are rather rare and so, the Joint Senate gathers very infrequently, and only to consider matters which are mostly definitory.\n\nIn the Netherlands, the Supreme Court of the Netherlands is the highest court. Its decisions, known as \"arresten\", are absolutely final. The court is banned from testing legislation against the constitution, pursuant to the principle of the sovereignty of the States-General; the court can, however, test legislation against some treaties. Also, the ordinary courts in the Netherlands, including the Hoge Raad, do not deal with administrative law, which is dealt with in separate administrative courts, the highest of which is the Council of State (Raad van State)\n\nThe Supreme Court of Iceland (, lit. \"Highest Court of Iceland\") was founded under Act No. 22/1919 and held its first session on 16 February 1920. The Court holds the highest judicial power in Iceland, where the court system has two levels.\n\nThe Supreme Court of India, also known colloquially as the 'apex court', is the highest judicial body in the Republic of India. Any decision taken by it is final and binding, and can only be modified in some cases (death sentence, etc.) by the President of India. It has several jurisdiction like \n1. Original\n2.Appellate \n3. Advisory\n\nIt is also known as court of records, i. e. all judgements are recorded and printed. These are cited in lower courts as case - law in various cases.\n\nItaly follows the French system of different supreme courts.\n\nThe Italian court of last resort for most disputes is the \"Corte Suprema di Cassazione\". There is also a separate constitutional court, the \"Corte costituzionale\", which has a duty of judicial review, and which can strike down legislation as being in conflict with the Constitution.\n\nIn Japan, the Supreme Court of Japan is called (Saikō-Saibansho; called 最高裁 Saikō-Sai for short), located in Chiyoda, Tokyo, and is the highest court in Japan. It has ultimate judicial authority within Japan to interpret the Constitution and decide questions of national law (including local by laws). It has the power of judicial review (i.e., it can declare Acts of Diet and Local Assembly, and administrative actions, unconstitutional).\n\nIn Luxembourg, challenges on the conformity of the law to the Constitution are brought before the \"Cour Constitutionnelle\" (Constitutional Court). — The most used and common procedure to present these challenges is by way of the \"\"question préjudicielle\"\" (prejudicial question). The Court of last resort for civil and criminal proceedings is the \"\"Cour de Cassation\"\".\nFor administrative proceedings the highest court is the \"\"Cour Administrative\"\" (Administrative Court).\n\nThe supreme court of Macau is the Court of Final Appeal (; ).\n\nWhile the Philippines is generally considered a civil law nation, its Supreme Court is heavily modelled after the American Supreme Court. This can be attributed to the fact that the Philippines was colonized by both Spain and the United States, and the system of laws of both nations strongly influenced the development of Philippine laws and jurisprudence. Even as the body of Philippine laws remain mostly codified, the Philippine Civil Code expressly recognizes that decisions of the Supreme Court \"form part of the law of the land\", belonging to the same class as statutes. The 1987 Philippine Constitution also explicitly grants to the Supreme Court the power of judicial review over laws and executive actions. The Supreme Court is composed of 1 Chief Justice and 14 Associate Justices. The court sits either en banc or in divisions, depending on the nature of the case to be decided.\n\nIn the judicial system of mainland China the highest court of appeal is the Supreme People's Court. This supervises the administration of justice by all subordinate \"local\" and \"special\" people's courts, and is the court of last resort for the whole People's Republic of China except for Macau and Hong Kong\n\nIn Portugal, there are several supreme courts, each with a specific jurisdiction:\n\nUntil 2003, a fifth supreme court also existed for the military jurisdiction, this being the Supreme Military Court (\"Supremo Tribunal Militar\"). Presently, in time of peace, the supreme court for military justice matters is the Supreme Court of Justice, which now includes four military judges.\n\nIn the Republic of China (Taiwan), there are three different courts of last resort:\n\nThe Council of Grand Justices, consisting of 15 justices and mainly dealing with constitutional issues, is the counterpart of constitutional courts in some countries.\n\nAll three courts are directly under the Judicial Yuan, whose president also serves as Chief Justice in the Council of Grand Justices.\n\nFounded by papal bull in 1532, the Court of Session is the supreme civil court of Scotland, and the High Court of Justiciary is the supreme criminal court. However, the absolute highest court (excluding criminal matters) is the Supreme Court of the United Kingdom.\n\nSpanish Supreme Court is the highest court for all cases in Spain (both private and public). Only those cases related to human rights can be appealed at the Constitutional Court (which also decides about acts accordance with Spanish Constitution). \nIn Spain, high courts cannot create binding precedents; however, lower rank courts usually observe Supreme Court interpretations. In most private law cases, two Supreme Court judgements supporting a claim are needed to appeal at the Supreme Court.\nFive sections form the Spanish Supreme court:\n\nIn Sweden, the Supreme Court and the Supreme Administrative Court respectively function as the highest courts of the land. The Supreme Administrative Court considers cases concerning disputes between individuals and administrative organs, as well as disputes among administrative organs, while the Supreme Court considers all other cases. The judges are appointed by the Government. In most cases, the Supreme Courts will only grant leave to appeal a case (\"prövningstillstånd\") if the case involves setting a precedent in the interpretation of the law. Exceptions are issues where the Supreme Court is the court of first instance. Such cases include an application for a retrial of a criminal case in the light of new evidence, and prosecutions made against an incumbent minister of the Government for severe neglect of duty. If a lower court has to try a case which involves a question where there is no settled interpretation of the law, it can also refer the question to the relevant Supreme Court for an answer.\n\nIn Switzerland, the Federal Supreme Court of Switzerland is the final court of appeals. Due to Switzerland's system of direct democracy, it has no authority to review the constitutionality of federal statutes, but the people can strike down a proposed law by referendum. According to settled case law, however, the Court is authorised to review the compliance of all Swiss law with certain categories of international law, especially the European Convention of Human Rights.\n\nIn Sri Lanka, the Supreme Court of Sri Lanka was created in 1972 after the adoption of a new Constitution. The Supreme Court is the highest and final superior court of record and is empowered to exercise its powers, subject to the provisions of the Constitution. The court rulings take precedence over all lower Courts. The Sri Lanka judicial system is complex blend of both common-law and civil-law. In some cases such as capital punishment, the decision may be passed on to the President of the Republic for clemency petitions. However, when there is 2/3 majority in the parliament in favour of president (as with present), the supreme court and its judges' powers become nullified as they could be fired from their positions according to the Constitution, if the president wants. Therefore, in such situations, Civil law empowerment vanishes.\n\nIn South Africa, a \"two apex\" system existed from 1994 to 2013. The Supreme Court of Appeal (SCA) was created in 1994 and replaced the Appellate Division of the Supreme Court of South Africa as the highest court of appeal in non-constitutional matters. The SCA is subordinate to the Constitutional Court, which is the highest court in matters involving the interpretation and application of the Constitution. But in August 2013 the Constitution was amended to make the Constitutional Court the country's single apex court, superior to the SCA in all matters, both constitutional and non-constitutional.\n\nHistorically, citizens appealed directly to the King along his route to places out of the Palace. A Thai King would adjudicate all disputes. During the reign of King Chulalongkorn, an official department for appeals was set up, and, after Thailand adopted a western-styled government, Thai Supreme Court was established in 1891.\n\nAt present, the Supreme Court of Thailand retains the important status as the highest court of justice in the country. Operating separately from the Administrative Court and the Constitutional Court, the judgement of the Supreme Court is considered as final.\n\nIn the United Arab Emirates, the Federal Supreme Court of the United Arab Emirates was created in 1973 after the adoption of the Constitution. The Supreme Court is the highest and final superior court of record and is empowered to exercise its powers, subject to the provisions of the Constitution. The court rulings take precedence over all lower Courts. The Emirati judicial system is complex blend of both Islamic law and civil law. In some cases such as capital punishment, the decision may be passed on to the President of the country (currently Khalifa bin Zayed Al Nahyan).\n\n\nLaw of Indonesia at the national level is based on a combination of civil law from the tradition of Roman-Dutch law and customary law from the tradition of Adat. Law in regional jurisdictions can vary from province to province, including even Sharia law, for example Islamic criminal law in Aceh, though even at the national level, individual justices can cite sharia or other forms of non-Dutch law in their legal opinions.\n\nThe Supreme Court of Indonesia is the main judicial arm of the state, functioning as the final court of appeal as well as a means to re-open cases previously closed. The Supreme Court, which consists of a total of 51 justices, also oversees the regional high courts. It was founded at the country's independence in 1945.\n\nThe Constitutional Court of Indonesia, on the other hand, is a part of the judicial branch tasked with review of bills and government actions for constitutionality, as well as regulation of the interactions between various arms of the state. The constitutional amendment to establish the court was passed in 2001, and the court itself was established in 2003. The Constitutional Court consists of nine justices serving nine year terms, and they're appointed in tandem by the Supreme Court, the President of Indonesia and the People's Representative Council.\n\nIn most nations with constitutions modelled after the Soviet Union, the legislature was given the power of being the \"court of last resort\". In the People's Republic of China, the final power to interpret the law is vested in the Standing Committee of the National People's Congress (NPCSC). This power includes the power to interpret the basic laws of Hong Kong and Macau, the constitutional documents of the two special administrative regions which are common law and Portuguese-based legal system jurisdictions respectively. This power is a legislative power and not a judicial one in that an interpretation by the NPCSC does not affect cases which have already been decided.\n\n",
                "Precedent\n\nIn legal systems based on common law, a precedent, or authority, is a principle or rule established in a previous legal case that is either binding on or persuasive for a court or other tribunal when deciding subsequent cases with similar issues or facts . Common law legal systems place great value on deciding cases according to consistent principled rules so that similar facts will yield similar and predictable outcomes, and observance of precedent is the mechanism by which that goal is attained. The principle by which judges are bound to precedents is known as stare decisis. Black's Law Dictionary defines \"precedent\" as a \"rule of law established for the first time by a court for a particular type of case and thereafter referred to in deciding similar cases\". Common law precedent is a third kind of law, on equal footing with statutory law (statutes and codes enacted by legislative bodies), and delegated legislation (in U.K. parlance) or regulatory law (in U.S. parlance) (regulations promulgated by executive branch agencies).\n\nCase law, in common law jurisdictions, is the set of decisions of adjudicatory tribunals or other rulings that can be cited as precedent. In most countries, including most European countries, the term is applied to any set of rulings on law which is guided by previous rulings, for example, previous decisions of a government agency.\n\nEssential to the development of case law is the publication and indexing of decisions for use by lawyers, courts and the general public, in the form of law reports. While all decisions are precedent (though at varying levels of authority as discussed throughout this article), some become \"leading cases\" or \"landmark decisions\" that are cited especially often.\n\n\"Stare decisis\" (Anglo-Latin pronunciation: ) is a legal principle by which judges are obligated to respect the precedent established by prior decisions. The words originate from the phrasing of the principle in the Latin maxim Stare decisis et non quieta movere: \"to stand by decisions and not disturb the undisturbed\". In a legal context, this is understood to mean that courts should generally abide by precedent and not disturb settled matters. The principle of \"stare decisis\" can be divided into two components.\n\nThe first is the rule that a decision made by a superior court, or by the same court in an earlier decision, is binding precedent that the court itself and all its inferior courts are obligated to follow. The second is the principle that a court should not overturn its own precedent unless there is a strong reason to do so and should be guided by principles from lateral and inferior courts. The second principle, regarding persuasive precedent, is an advisory one that courts can and do ignore occasionally.\n\nIn the common law tradition, courts decide the law applicable to a case by interpreting statutes and applying precedent which record how and why prior cases have been decided. Unlike most civil law systems, common law systems follow the doctrine of \"stare decisis\", by which most courts are bound by their own previous decisions in similar cases, and all lower courts should make decisions consistent with previous decisions of higher courts. For example, in England, the High Court and the Court of Appeal are each bound by their own previous decisions, but the Supreme Court of the United Kingdom is able to deviate from its earlier decisions, although in practice it rarely does so.\n\nGenerally speaking, higher courts do not have direct oversight over day-to-day proceedings in lower courts, in that they cannot reach out on their own initiative (\"sua sponte\") at any time to reverse or overrule judgments of the lower courts. Normally, the burden rests with litigants to appeal rulings (including those in clear violation of established case law) to the higher courts. If a judge acts against precedent and the case is not appealed, the decision will stand.\n\nA lower court may not rule against a binding precedent, even if the lower court feels that the precedent is unjust; the lower court may only express the hope that a higher court or the legislature will reform the rule in question. If the court believes that developments or trends in legal reasoning render the precedent unhelpful, and wishes to evade it and help the law evolve, the court may either hold that the precedent is inconsistent with subsequent authority, or that the precedent should be \"distinguished\" by some material difference between the facts of the cases. If that judgment goes to appeal, the appellate court will have the opportunity to review both the precedent and the case under appeal, perhaps overruling the previous case law by setting a new precedent of higher authority. This may happen several times as the case works its way through successive appeals. Lord Denning, first of the High Court of Justice, later of the Court of Appeal, provided a famous example of this evolutionary process in his development of the concept of estoppel starting in the \"High Trees\" case: \"Central London Property Trust Ltd v. High Trees House Ltd\" [1947] K.B. 130.\n\nJudges may refer to various types of persuasive authority to reach a decision in a case. Widely cited non-binding sources include legal encyclopedias such as \"Corpus Juris Secundum\" and \"Halsbury's Laws of England\", or the published work of the Law Commission or the American Law Institute. Some bodies are given statutory powers to issue Guidance with persuasive authority or similar statutory effect, such as the Highway Code.\n\nIn federal or multi-jurisdictional law systems there may exist conflicts between the various lower appellate courts. Sometimes these differences may not be resolved and it may be necessary to distinguish how the law is applied in one district, province, division or appellate department. Usually only an appeal accepted by the court of last resort will resolve such differences and, for many reasons, such appeals are often not granted.\n\nAny court may seek to distinguish its present case from that of a binding precedent, in order to reach a different conclusion. The validity of such a distinction may or may not be accepted on appeal. An appellate court may also propound an entirely new and different analysis from that of junior courts, and may or may not be bound by its own previous decisions, or in any case may distinguish the decisions based on significant differences in the facts applicable to each case. Or, a court may view the matter before it as one of \"first impression,\" not governed by any controlling precedent.\n\nWhere there are several members of a court, there may be one or more judgments given; only the ratio decidendi of the majority can constitute a binding precedent, but all may be cited as persuasive, or their reasoning may be adopted in argument. Quite apart from the rules of precedent, the weight actually given to any reported judgment may depend on the reputation of both the court and the judges.\n\nGenerally, a common law court system has trial courts, intermediate appellate courts and a supreme court. The inferior courts conduct almost all trial proceedings. The inferior courts are bound to obey precedent established by the appellate court for their jurisdiction, and all supreme court precedent.\n\nThe Supreme Court of California's explanation of this principle is that\n\nAn Intermediate state appellate court is generally bound to follow the decisions of the highest court of that state.\n\nThe application of the doctrine of \"stare decisis\" from a superior court to an inferior court is sometimes called \"vertical stare decisis\".\n\nThe idea that a judge is bound by (or at least should respect) decisions of earlier judges of similar or coordinate level is called horizontal \"stare decisis\".\n\nIn the United States federal court system, the intermediate appellate courts are divided into thirteen \"circuits,\" each covering some range of territory ranging in size from the District of Columbia alone up to seven states. Each panel of judges on the court of appeals for a circuit is bound to obey the prior appellate decisions of the same circuit. Precedent of a United States court of appeals may be overruled only by the court \"en banc,\" that is, a session of all the active appellate judges of the circuit, or by the United States Supreme Court, not simply by a different three-judge panel.\n\nWhen a court binds itself, this application of the doctrine of precedent is sometimes called \"horizontal stare decisis\". The state of New York has a similar appellate structure as it is divided into four appellate departments supervised by the final New York Court of Appeals. Decisions of one appellate department are not binding upon another, and in some cases the departments differ considerably on interpretations of law.\n\nIn federal systems the division between federal and state law may result in complex interactions. In the United States, state courts are not considered inferior to federal courts but rather constitute a parallel court system.\n\n\nIn practice, however, judges in one system will almost always choose to follow relevant case law in the other system to prevent divergent results and to minimize forum shopping.\n\nPrecedent that must be applied or followed is known as \"binding precedent\" (alternately \"metaphorically precedent\", \"mandatory\" or \"binding authority\", etc.). Under the doctrine of \"stare decisis\", a lower court must honor findings of law made by a higher court that is within the appeals path of cases the court hears. In state and federal courts in the United States of America, jurisdiction is often divided geographically among local trial courts, several of which fall under the territory of a regional appeals court. All appellate courts fall under a highest court (sometimes but not always called a \"supreme court\"). By definition, decisions of lower courts are not binding on courts higher in the system, nor are appeals court decisions binding on local courts that fall under a different appeals court. Further, courts must follow their own proclamations of law made earlier on other cases, and honor rulings made by other courts in disputes among the parties before them pertaining to the same pattern of facts or events, unless they have a strong reason to change these rulings (see Law of the case re: a court's previous holding being binding precedent for that court).\n\nIn law, a binding precedent (also known as a mandatory precedent or binding authority) is a precedent which must be followed by all lower courts under common law legal systems. In English law it is usually created by the decision of a higher court, such as the Supreme Court of the United Kingdom, which took over the judicial functions of the House of Lords in 2009. In Civil law and pluralist systems precedent is not binding but case law is taken into account by the courts.\n\nBinding precedent relies on the legal principle of \"stare decisis\". \"Stare decisis\" means to stand by things decided. It ensures certainty and consistency in the application of law. Existing binding precedent from past cases are applied in principle to new situations by analogy.\n\nOne law professor has described mandatory precedent as follows:\n\nIn extraordinary circumstances a higher court may overturn or overrule mandatory precedent, but will often attempt to distinguish the precedent before overturning it, thereby limiting the scope of the precedent.\n\nUnder the U.S. legal system, courts are set up in a hierarchy. At the top of the federal or national system is the Supreme Court, and underneath are lower federal courts. The state court systems have hierarchy structures similar to that of the federal system.\n\nThe U.S. Supreme Court has final authority on questions about the meaning of federal law, including the U.S. Constitution. For example, when the Supreme Court says that the First Amendment applies in a specific way to suits for slander, then every court is bound by that precedent in its interpretation of the First Amendment as it applies to suits for slander. If a lower court judge disagrees with a higher court precedent on what the First Amendment should mean, the lower court judge must rule according to the binding precedent. Until the higher court changes the ruling (or the law itself is changed), the binding precedent is authoritative on the meaning of the law.\n\nLower courts are bound by the precedent set by higher courts within their region. Thus, a federal district court that falls within the geographic boundaries of the Third Circuit Court of Appeals (the mid-level appeals court that hears appeals from district court decisions from Delaware, New Jersey, Pennsylvania, and the Virgin Islands) is bound by rulings of the Third Circuit Court, but not by rulings in the Ninth Circuit (Alaska, Arizona, California, Guam, Hawaii, Idaho, Montana, Nevada, Northern Mariana Islands, Oregon, and Washington), since the Circuit Courts of Appeals have jurisdiction defined by geography. The Circuit Courts of Appeals can interpret the law how they want, so long as there is no binding Supreme Court precedent. One of the common reasons the Supreme Court grants certiorari (that is, they agree to hear a case) is if there is a conflict among the circuit courts as to the meaning of a federal law.\n\nThere are three elements needed for a precedent to work. Firstly, the hierarchy of the courts needs to be accepted, and an efficient system of law reporting. 'A balance must be struck between the need on one side for the legal certainty resulting from the binding effect of previous decisions, and on the other side the avoidance of undue restriction on the proper development of the law (1966 Practice Statement (Judicial Precedent) by Lord Gardiner L.C.)'.\n\nJudges are bound by the law of binding precedent in England and Wales and other common law jurisdictions. This is a distinctive feature of the English legal system. In Scotland and many countries throughout the world, particularly in mainland Europe, civil law means that judges take case law into account in a similar way, but are not obliged to do so and are required to consider the precedent in terms of principle. Their fellow judges' decisions may be persuasive but are not binding. Under the English legal system, judges are not necessarily entitled to make their own decisions about the development or interpretations of the law. They may be bound by a decision reached in a previous case. Two facts are crucial to determining whether a precedent is binding:\n\n\"Super \"stare decisis\"\" is a term used for important precedent that is resistant or immune from being overturned, without regard to whether correctly decided in the first place. It may be viewed as one extreme in a range of precedential power, or alternatively, to express a belief, or a critique of that belief, that some decisions should not be overturned.\n\nIn 1976, Richard Posner and William Landes coined the term \"super-precedent,\" in an article they wrote about testing theories of precedent by counting citations. Posner and Landes used this term to describe the influential effect of a cited decision. The term \"super-precedent\" later became associated with different issue: the difficulty of overturning a decision. In 1992, Rutgers professor Earl Maltz criticized the Supreme Court's decision in \"Planned Parenthood v. Casey\" for endorsing the idea that if one side can take control of the Court on an issue of major national importance (as in \"Roe v. Wade\"), that side can protect its position from being reversed \"by a kind of super-stare decisis\". The controversial idea that some decisions are virtually immune from being overturned, regardless of whether they were decided correctly in the first place, is the idea to which the term \"super \"stare decisis\"\" now usually refers.\n\nThe concept of super-\"stare decisis\" (or \"super-precedent\") was mentioned during the interrogations of Chief Justice John Roberts and Justice Samuel Alito before the Senate Judiciary Committee. Prior to the commencement of the Roberts hearings, the chair of that committee, Senator Arlen Specter of Pennsylvania, wrote an op/ed in the \"New York Times\" referring to \"Roe\" as a \"super-precedent\". He revisited this concept during the hearings, but neither Roberts nor Alito endorsed the term or the concept.\n\nPersuasive precedent (also persuasive authority) is precedent or other legal writing that is not binding precedent but that is useful or relevant and that may guide the judge in making the decision in a current case. Persuasive precedent includes cases decided by lower courts, by peer or higher courts from other geographic jurisdictions, cases made in other parallel systems (for example, military courts, administrative courts, indigenous/tribal courts, state courts versus federal courts in the United States), statements made in dicta, treatises or academic law reviews, and in some exceptional circumstances, cases of other nations, treaties, world judicial bodies, etc.\n\nIn a \"case of first impression\", courts often rely on persuasive precedent from courts in other jurisdictions that have previously dealt with similar issues. Persuasive precedent may become binding through its adoption by a higher court.\n\nIn civil law and pluralist systems, as under Scots law, precedent is not binding but case law is taken into account by the courts.\n\nA lower court's opinion may be considered as persuasive authority if the judge believes they have applied the correct legal principle and reasoning.\n\nA court may consider the ruling of a higher court that is not binding. For example, a district court in the United States First Circuit could consider a ruling made by the United States Court of Appeals for the Ninth Circuit as persuasive authority.\n\nCourts may consider rulings made in other courts that are of equivalent authority in the legal system. For example, an appellate court for one district could consider a ruling issued by an appeals court in another district.\n\nCourts may consider \"obiter dicta\" in opinions of higher courts. Dicta of a higher court, though not binding, will often be persuasive to lower courts. The phrase \"obiter dicta\" is usually translated as \"other things said\", but due to the high number of judges and individual concurring opinions, it is often hard to distinguish from the \"ratio decidendi\" (reason for the decision). For these reasons, the obiter dicta may often be taken into consideration by a court. A litigant may also consider \"obiter dicta\" if a court has previously signaled that a particular legal argument is weak and may even warrant sanctions if repeated.\n\nA case decided by a multi-judge panel could result in a split decision. While only the majority opinion is considered precedential, an outvoted judge can still publish a dissenting opinion. Common patterns for dissenting opinions include:\nA judge in a subsequent case, particularly in a different jurisdiction, could find the dissenting judge's reasoning persuasive. In the jurisdiction of the original decision, however, a judge should only overturn the holding of a court lower or equivalent in the hierarchy. A district court, for example, could not rely on a Supreme Court dissent as a basis to depart from the reasoning of the majority opinion. However, lower courts occasionally cite dissents, either for a limiting principle on the majority, or for propositions that are not stated in the majority opinion and not inconsistent with that majority, or to explain a disagreement with the majority and to urge reform (while following the majority in the outcome).\n\nCourts may consider the writings of eminent legal scholars in treatises, restatements of the law, and law reviews. The extent to which judges find these types of writings persuasive will vary widely with elements such as the reputation of the author and the relevance of the argument.\n\nThe courts of England and Wales are free to consider decisions of other jurisdictions, and give them whatever persuasive weight the English court sees fit, even though these other decisions are not binding precedent. Jurisdictions that are closer to modern English common law are more likely to be given persuasive weight (for example Commonwealth states such as Canada, Australia, or New Zealand). Persuasive weight might be given to other common law courts, such as from the United States, most often where the American courts have been particularly innovative, e.g. in product liability and certain areas of contract law.\n\nIn the United States, in the late 20th and early 21st centuries, the concept of a U.S. court considering foreign law or precedent has been considered controversial by some parties. The Supreme Court splits on this issue. This critique is recent, as in the early history of the United States, citation of English authority was ubiquitous. One of the first acts of many of the new state legislatures was to adopt the body of English common law into the law of the state. See here. Citation to English cases was common through the 19th and well into the 20th centuries. Even in the late 20th and early 21st centuries, it is relatively uncontroversial for American state courts to rely on English decisions for matters of pure common (i.e. judge-made) law. \n\nWithin the federal legal systems of several common-law countries, and most especially the United States, it is relatively common for the distinct lower-level judicial systems (e.g. state courts in the United States and Australia, provincial courts in Canada) to regard the decisions of other jurisdictions within the same country as persuasive precedent. Particularly in the United States, the adoption of a legal doctrine by a large number of other state judiciaries is regarded as highly persuasive evidence that such doctrine is preferred. A good example is the adoption in Tennessee of comparative negligence (replacing contributory negligence as a complete bar to recovery) by the 1992 Tennessee Supreme Court decision \"McIntyre v. Balentine\" (by this point all US jurisdictions save Tennessee, five other states, and the District of Columbia had adopted comparative negligence schemes). Moreover, in American law, the \"Erie\" doctrine requires federal courts sitting in diversity actions to apply state substantive law, but in a manner consistent with how the court believes the state's highest court would rule in that case. Since such decisions are not binding on state courts, but are often very well-reasoned and useful, state courts cite federal interpretations of state law fairly often as persuasive precedent, although it is also fairly common for a state high court to reject a federal court's interpretation of its jurisprudence.\n\nNon-publication of opinions, or unpublished opinions, are those decisions of courts that are not available for citation as precedent because the judges making the opinion deem the case as having less precedential value. Selective publication is the legal process which a judge or justices of a court decide whether a decision is to be or not published in a reporter. \"Unpublished\" federal appellate decisions are published in the Federal Appendix. Depublication is the power of a court to make a previously published order or opinion unpublished.\n\nLitigation that is settled out of court generates no written decision, and thus has no precedential effect. As one practical effect, the U.S. Department of Justice settles many cases against the federal government simply to avoid creating adverse precedent.\n\nSeveral rules may cause a decision to apply as narrow \"precedent\" to preclude future legal positions of the specific parties to a case, even if a decision is non-precedential with respect to all other parties.\n\nOnce a case is decided, the same plaintiff cannot sue the same defendant again on any claim arising out of the same facts. The law requires plaintiffs to put all issues on the table in a single case, not split the case. For example, in a case of an auto accident, the plaintiff cannot sue first for property damage, and then personal injury in a separate case. This is called \"res judicata\" or claim preclusion (\"'Res judicata'\" is the traditional name going back centuries; the name shifted to \"claim preclusion\" in the United States over the late 20th century). Claim preclusion applies whether the plaintiff wins or loses the earlier case, even if the later case raises a different legal theory, even the second claim is unknown at the time of the first case. Exceptions are extremely limited, for example if the two claims for relief must necessarily be brought in different courts (for example, one claim might be exclusively federal, and the other exclusively state).\n\nOnce a case is finally decided, any issues decided in the previous case may be binding against the party that lost the issue in later cases, even in cases involving other parties. For example, if a first case decides that a party was negligent, then other plaintiffs may rely on that earlier determination in later cases, and need not re-prove the issue of negligence. For another example, if a patent is shown to be invalid in a case against one accused infringer, that same patent is invalid against all other accused infringers—invalidity need not be re-proved. Again, there are limits and exceptions on this principle. The principle is called collateral estoppel or issue preclusion.\n\nWithin a single case, once there's been a first appeal, both the lower court and the appellate court itself will not further review the same issue, and will not re-review an issue that could have been appealed in the first appeal. Exceptions are limited to three \"exceptional circumstances:\" (1) when substantially different evidence is raised at a subsequent trial, (2) when the law changes after the first appeal, for example by a decision of a higher court, or (3) when a decision is clearly erroneous and would result in a manifest injustice. This principle is called \"law of the case\".\n\nOn many questions, reasonable people may differ. When two of those people are judges, the tension among two lines of precedent may be resolved as follows.\n\nIf the two courts are in separate, parallel jurisdictions, there is no conflict, and two lines of precedent may persist. Courts in one jurisdiction are influenced by decisions in others, and notably better rules may be adopted over time.\n\nCourts try to formulate the common law as a \"seamless web\" so that principles in one area of the law apply to other areas. However, this principle does not apply uniformly. Thus, a word may have different definitions in different areas of the law, or different rules may apply so that a question has different answers in different legal contexts. Judges try to minimize these conflicts, but they arise from time to time, and under principles of 'stare decisis', may persist for some time.\n\nA matter of first impression (known as \"primae impressionis\" in Latin) is a legal case in which there is no binding authority on the matter presented. Such a case can set forth a completely original issue of law for decision by the courts. A first impression case may be a first impression in only a particular jurisdiction. In that situation, courts will look to holdings of other jurisdictions for persuasive authority.\n\nIn the latter meaning, the case in question cannot be decided through referring to and/or relying on precedent. Since the legal issue under consideration has never been decided by an appeals court and, therefore, there is no precedent for the court to follow, the court uses analogies from prior rulings by appeals courts, refers to commentaries and articles by legal scholars, and applies its own logic. In cases of first impression, the trial judge will often ask both sides' attorneys for legal briefs.\n\nIn some situations, a case of first impression may exist in a jurisdiction until a reported appellate court decision is rendered.\n\nThe different roles of case law in civil law and common law traditions create differences in the way that courts render decisions. Common law courts generally explain in detail the legal rationale behind their decisions, with citations of both legislation and previous relevant judgments, and often an exegesis of the wider legal principles. These are called \"ratio decidendi\" and constitute a precedent binding on other courts; further analyses not strictly necessary to the determination of the current case are called \"obiter dicta\", which have persuasive authority but are not technically binding. By contrast, decisions in civil law jurisdictions are generally very short, referring only to statutes. The reason for this difference is that these civil law jurisdictions apply legislative positivism — a form of extreme legal positivism — which holds that legislation is the only valid source of law because it has been voted on democratically; thus, it is not the judiciary's role to create law, but rather to interpret and apply statute, and therefore their decisions must reflect that.\n\n\"Stare decisis\" is not usually a doctrine used in civil law systems, because it violates the legislative positivist principle that only the legislature may make law. Instead, the civil law system relies on the doctrine of \"jurisprudence constante\", according to which if a court has adjudicated a consistent line of cases that arrive at the same holdings using sound reasoning, then the previous decisions are highly persuasive but not controlling on issues of law. This doctrine is similar to \"stare decisis\" insofar as it dictates that a court's decision must condone a cohesive and predictable result. In theory, lower courts are generally not bound by the precedents of higher courts. In practice, the need for predictability means that lower courts generally defer to the precedent of higher courts. As a result, the precedent of courts of last resort, such as the French Cassation Court and the Council of State, is recognized as being \"de facto\" binding on lower courts.\n\nThe doctrine of \"jurisprudence constante\" also influences how court decisions are structured. In general, court decisions of common law jurisdictions give a sufficient \"ratio decidendi\" as to guide future courts. The ratio is used to justify a court decision on the basis of previous case law as well as to make it easier to use the decision as a precedent for future cases. By contrast, court decisions in some civil law jurisdictions (most prominently France) tend to be extremely brief, mentioning only the relevant legislation and codal provisions and not going into the \"ratio decidendi\" in any great detail. This is the result of the legislative positivist view that the court is only interpreting the legislature's intent and therefore detailed exposition is unnecessary. Because of this, \"ratio decidendi\" is carried out by legal academics (doctrinal writers) who provide the explanations that in common law jurisdictions would be provided by the judges themselves.\n\nIn other civil law jurisdictions, such as the German-speaking countries, \"ratio decidendi\" tend to be much more developed than in France, and courts will frequently cite previous cases and doctrinal writers. However, some courts (such as German courts) have less emphasis on the particular facts of the case than common law courts, but have more emphasis on the discussion of various doctrinal arguments and on finding what the correct interpretation of the law is.\n\nThe mixed systems of the Nordic countries are sometimes considered a branch of the civil law, but they are sometimes counted as separate from the civil law tradition. In Sweden, for instance, case law arguably plays a more important role than in some of the continental civil law systems. The two highest courts, the Supreme Court (\"Högsta domstolen\") and the Supreme Administrative Court (\"Högsta förvaltningsdomstolen\"), have the right to set precedent which has persuasive authority on all future application of the law. Appellate courts, be they judicial (\"hovrätter\") or administrative (\"kammarrätter\"), may also issue decisions that act as guides for the application of the law, but these decisions are persuasive, not controlling, and may therefore be overturned by higher courts.\n\nSome mixed systems, such as Scots law in Scotland, South-African law, and the law of Quebec and Louisiana, do not fit into the civil vs. common law dichotomy because they mix portions of both. Such systems may have been heavily influenced by the common law tradition; however, their private law is firmly rooted in the civil law tradition. Because of their position between the two main systems of law, these types of legal systems are sometimes referred to as \"mixed\" systems of law. Louisiana courts, for instance, operate under both \"stare decisis\" and \"jurisprudence constante\". In South Africa, the precedent of higher courts is absolutely or fully binding on lower courts, whereas the precedent of lower courts only has persuasive authority on higher courts; horizontally, precedent is \"prima facie\" or presumptively binding between courts.\n\nLaw professors in common law traditions play a much smaller role in developing case law than professors in civil law traditions. Because court decisions in civil law traditions are brief and not amenable to establishing precedent, much of the exposition of the law in civil law traditions is done by academics rather than by judges; this is called doctrine and may be published in treatises or in journals such as \"Recueil Dalloz\" in France. Historically, common law courts relied little on legal scholarship; thus, at the turn of the twentieth century, it was very rare to see an academic writer quoted in a legal decision (except perhaps for the academic writings of prominent judges such as Coke and Blackstone). Today academic writers are often cited in legal argument and decisions as persuasive authority; often, they are cited when judges are attempting to implement reasoning that other courts have not yet adopted, or when the judge believes the academic's restatement of the law is more compelling than can be found in precedent. Thus common law systems are adopting one of the approaches long common in civil law jurisdictions.\n\nJustice Louis Brandeis, in a heavily footnoted dissent to \"Burnet v. Coronado Oil & Gas Co.\", 285 U.S. 393, 405-411 (1932), explained (citations and quotations omitted):\n\nThe United States Court of Appeals for the Third Circuit has stated:\n\nThe United States Court of Appeals for the Ninth Circuit has stated:\n\nJustice McHugh of the High Court of Australia in relation to precedents remarked in \"Perre v Apand\":\n\nPrecedent viewed against passing time can serve to establish trends, thus indicating the next logical step in evolving interpretations of the law. For instance, if immigration has become more and more restricted under the law, then the next legal decision on that subject may serve to restrict it further still. The existence of submerged precedent (reasoned opinions not made available through conventional legal research sources) has been identified as a potentially distorting force in the evolution of law.\n\nScholars have recently attempted to apply network theory to precedent in order to establish which precedent is most important or authoritative, and how the court's interpretations and priorities have changed over time.\n\nEarly English common law did not have or require the \"stare decisis\" doctrine for a range of legal and technological reasons:\n\nThese features changed over time, opening the door to the doctrine of \"stare decisis\":\n\n\"Stare decisis\" applies to the holding of a case, rather than to obiter dicta (\"things said by the way\"). As the United States Supreme Court has put it: \"dicta may be followed if sufficiently persuasive but are not binding.\"\n\nIn the United States Supreme Court, the principle of stare decisis is most flexible in constitutional cases:\n\nFor example, in the years 1946–1992, the U.S. Supreme Court reversed itself in about 130 cases. The U.S. Supreme Court has further explained as follows:\n\nThe United States Supreme Court has stated that where a court gives multiple reasons for a given result, each alternative reason that is \"explicitly\" labeled by the court as an \"independent\" ground for the decision is not treated as \"simply a dictum\".\n\nThe doctrine of binding precedent or \"stare decisis\" is basic to the English legal system. Special features of the English legal system include the following:\n\nThe British House of Lords, as the court of last appeal outside Scotland before it was replaced by the UK Supreme Court, was not strictly bound to always follow its own decisions until the case \"London Street Tramways v London County Council [1898] AC 375\". After this case, once the Lords had given a ruling on a point of law, the matter was closed unless and until Parliament made a change by statute. This is the most strict form of the doctrine of \"stare decisis\" (one not applied, previously, in common law jurisdictions, where there was somewhat greater flexibility for a court of last resort to review its own precedent).\n\nThis situation changed, however, after the issuance of the Practice Statement of 1966. It enabled the House of Lords to adapt English law to meet changing social conditions. In \"R v G & R\" 2003, the House of Lords overruled its decision in \"Caldwell\" 1981, which had allowed the Lords to establish mens rea (\"guilty mind\") by measuring a defendant's conduct against that of a \"reasonable person,\" regardless of the defendant's actual state of mind.\n\nHowever, the Practice Statement has been seldom applied by the House of Lords, usually only as a last resort. As of 2005, the House of Lords has rejected its past decisions no more than 20 times. They are reluctant to use it because they fear to introduce uncertainty into the law. In particular, the Practice Statement stated that the Lords would be especially reluctant to overrule themselves in criminal cases because of the importance of certainty of that law. The first case involving criminal law to be overruled with the Practice Statement was \"Anderton v Ryan\" (1985), which was overruled by \"R v Shivpuri\" (1986), two decades after the Practice Statement. Remarkably, the precedent overruled had been made only a year before, but it had been criticised by several academic lawyers. As a result, Lord Bridge stated he was \"undeterred by the consideration that the decision in \"Anderton v Ryan\" was so recent. The Practice Statement is an effective abandonment of our pretention to infallibility. If a serious error embodied in a decision of this House has distorted the law, the sooner it is corrected the better.\" Still, the House of Lords has remained reluctant to overrule itself in some cases; in \"R v Kansal\" (2002), the majority of House members adopted the opinion that \"R v Lambert\" had been wrongly decided and agreed to depart from their earlier decision.\n\nA precedent does not bind a court if it finds there was a lack of care in the original \"Per Incuriam\". For example, if a statutory provision or precedent had not been brought to the previous court's attention before its decision, the precedent would not be binding.\n\nOne of the most important roles of precedent is to resolve ambiguities in other legal texts, such as constitutions, statutes, and regulations. The process involves, first and foremost, consultation of the plain language of the text, as enlightened by the legislative history of enactment, subsequent precedent, and experience with various interpretations of similar texts.\n\nA judge's normal aids include access to all previous cases in which a precedent has been set, and a good English dictionary.\n\nJudges and barristers in the U.K use three primary rules for interpreting the law.\n\nUnder the literal rule, the judge should do what the actual legislation states rather than trying to do what the judge thinks that it means. The judge should use the plain everyday ordinary meaning of the words, even if this produces an unjust or undesirable outcome. A good example of problems with this method is \"R v Maginnis\" (1987), in which several judges in separate opinions found several different dictionary meanings of the word \"supply\". Another example is \"Fisher v Bell\", where it was held that a shopkeeper who placed an illegal item in a shop window with a price tag did not make an offer to sell it, because of the specific meaning of \"offer for sale\" in contract law. As a result of this case, Parliament amended the statute concerned to end this discrepancy.\n\nThe golden rule is used when use of the literal rule would obviously create an absurd result. The court must find genuine difficulties before it declines to use the literal rule. There are two ways in which the golden rule can be applied: the narrow method, and the broad method. Under the narrow method, when there are apparently two contradictory meanings to a word used in a legislative provision or it is ambiguous, the least absurd is to be used. For example, in \"Adler v George\" (1964), the defendant was found guilty under the Official Secrets Act of 1920. The act said it was an offence to obstruct HM Forces in the vicinity of a prohibited place. Adler argued that he was not in the \"vicinity\" of a prohibited place but was actually \"in\" a prohibited place. The court chose not to accept the wording literally. Under the broad method, the court may reinterpret the law at will when it is clear that there is only one way to read the statute. This occurred in \"Re Sigsworth\" (1935) where a man who murdered his mother was forbidden from inheriting her estate, despite a statute to the contrary.\n\nThe mischief rule is the most flexible of the interpretation methods. Stemming from \"Heydon's Case\" (1584), it allows the court to enforce what the statute is intended to remedy rather than what the words actually say. For example, in \"Corkery v Carpenter\" (1950), a man was found guilty of being drunk in charge of a carriage, although in fact he only had a bicycle.\n\nIn the United States, the courts have stated consistently that the text of the statute is read as it is written, using the ordinary meaning of the words of the statute.\n\nHowever, most legal texts have some lingering ambiguity—inevitably, situations arise in which the words chosen by the legislature do not address the precise facts in issue, or there is some tension among two or more statutes. In such cases, a court must analyze the various available sources, and reach a resolution of the ambiguity. The \"Canons of statutory construction\" are discussed in a separate article. Once the ambiguity is resolved, that resolution has binding effect as described in the rest of this article.\n\nAlthough inferior courts are bound in theory by superior court precedent, in practice a judge may believe that justice requires an outcome at some variance with precedent, and may distinguish the facts of the individual case on reasoning that does not appear in the binding precedent. On appeal, the appellate court may either adopt the new reasoning, or reverse on the basis of precedent. On the other hand, if the losing party does not appeal (typically because of the cost of the appeal), the lower court decision may remain in effect, at least as to the individual parties.\n\nOccasionally, a lower court judge explicitly states personal disagreement with the judgment he or she has rendered, but that he or she is required to do so by binding precedent. Note that inferior courts cannot evade binding precedent of superior courts, but a court can depart from its own prior decisions.\n\nIn the United States, \"stare decisis\" can interact in counterintuitive ways with the federal and state court systems. On an issue of federal law, a state court is not bound by an interpretation of federal law at the district or circuit level, but is bound by an interpretation by the United States Supreme Court. On an interpretation of state law, whether common law or statutory law, the federal courts are bound by the interpretation of a state court of last resort, and are required normally to defer to the precedent of intermediate state courts as well.\n\nCourts may choose to obey precedent of international jurisdictions, but this is not an application of the doctrine of \"stare decisis\", because foreign decisions are not binding. Rather, a foreign decision that is obeyed on the basis of the soundness of its reasoning will be called \"persuasive authority\" — indicating that its effect is limited to the persuasiveness of the reasons it provides.\n\nOriginalism is an approach to interpretation of a legal text in which controlling weight is given to the intent of the original authors (at least the intent as inferred by a modern judge). In contrast, a non-originalist looks at other cues to meaning, including the current meaning of the words, the pattern and trend of other judicial decisions, changing context and improved scientific understanding, observation of practical outcomes and \"what works,\" contemporary standards of justice, and \"stare decisis\". Both are directed at \"interpreting\" the text, not changing it—interpretation is the process of resolving ambiguity and choosing from among possible meanings, not changing the text.\n\nThe two approaches look at different sets of underlying facts that may or may not point in the same direction--\"stare decisis\" gives most weight to the newest understanding of a legal text, while originalism gives most weight to the oldest. While they don't necessarily reach different results in every case, the two approaches are in direct tension. Originalists such as Justice Antonin Scalia argue that \"\"Stare decisis\" is not usually a doctrine used in civil law systems, because it violates the principle that only the legislature may make law.\" Justice Scalia argues that America is a civil law nation, not a common law nation. By principle, originalists are generally unwilling to defer to precedent when precedent seems to come into conflict with the originalist's own interpretation of the Constitutional text or inferences of original intent (even in situations where there is no original source statement of that original intent). However, there is still room within an originalist paradigm for \"stare decisis\"; whenever the plain meaning of the text has alternative constructions, past precedent is generally considered a valid guide, with the qualifier being that it cannot change what the text actually says.\n\nOriginalists vary in the degree to which they defer to precedent. In his confirmation hearings, Justice Clarence Thomas answered a question from Senator Strom Thurmond, qualifying his willingness to change precedent in this way:\n\nPossibly he has changed his mind, or there are a very large body of cases which merit \"the additional step\" of ignoring the doctrine; according to Scalia, \"Clarence Thomas doesn't believe in stare decisis, period. If a constitutional line of authority is wrong, he would say, let's get it right.\"\n\nProfessor Caleb Nelson, a former clerk for Justice Thomas and law professor at the University of Virginia, has elaborated on the role of \"stare decisis\" in originalist jurisprudence:\n\nThere are disadvantages and advantages of binding precedent, as noted by scholars and jurists.\n\nIn a 1997 book, attorney Michael Trotter blamed over-reliance by American lawyers on binding and persuasive authority, rather than the merits of the case at hand, as a major factor behind the escalation of legal costs during the 20th century. He argued that courts should ban the citation of persuasive precedent from outside their jurisdiction, with two exceptions:\n\nThe disadvantages of \"stare decisis\" include its rigidity, the complexity of learning law, the differences between some cases may be very small and appear illogical, and the slow growth or incremental changes to the law that are in need of major overhaul.\n\nAn argument often used against the system is that it is undemocratic as it allows judges, which may or may not be elected, to make law.\n\nRegarding constitutional interpretations, there is concern that over-reliance on the doctrine of \"stare decisis\" can be subversive. An erroneous precedent may at first be only slightly inconsistent with the Constitution, and then this error in interpretation can be propagated and increased by further precedent until a result is obtained that is greatly different from the original understanding of the Constitution. \"Stare decisis\" is not mandated by the Constitution, and if it causes unconstitutional results then the historical evidence of original understanding can be re-examined. In this opinion, predictable fidelity to the Constitution is more important than fidelity to unconstitutional precedent. See also the living tree doctrine.\n\nA counter-argument (in favor of the advantages of \"stare decisis\") is that if the legislature wishes to alter the case law (other than constitutional interpretations) by statute, the legislature is empowered to do so. Critics sometimes accuse particular judges of applying the doctrine selectively, invoking it to support precedent that the judge supported anyway, but ignoring it in order to change precedent with which the judge disagreed.\n\nThere is much discussion about the virtue of using \"stare decisis\". Supporters of the system, such as minimalists, argue that obeying precedent makes decisions \"predictable\". For example, a business person can be reasonably assured of predicting a decision where the facts of his or her case are sufficiently similar to a case decided previously. This parallels the arguments against retroactive (ex post facto) laws banned by the U.S. Constitution.\n\n",
                "Last resort rule\n\nIn Constitutional law, the Last Resort Rule is a largely prudential rule which gives a federal court the power to avoid a constitutional issue in some circumstances. This rule dictates that, even if all other jurisdictional and justiciability obstacles are surmounted, federal courts still must avoid a constitutional issue if there is any other ground upon which to render a final judgment. The last resort rule can function as a distinct barrier to Constitutional avoidance. It is articulated by Justice Brandeis in Ashwander v. Tennessee Valley Authority.\n\nBrandeis cited two examples in Ashwander of the \"most varied application\" of the last resort rule. First, as between two potential grounds, one involving a constitutional question, the other a question of statutory construction or general law, the Court will only decide the latter. To the extent the question involves statutory construction and a plausible interpretation of the statute might obviate the need for constitutional review, this example replicates the seventh rule of the avoidance doctrine.\n\nTo illustrate this first application, Brandeis relied primarily on Siler v. Louisville & Nashville Railroad Co. In Siler, a railroad company challenged an order by the Kentucky railroad commission setting maximum rates on commodities transported by rail within the state. The company asserted a takings claim and a Commerce Clause claim under the United States Constitution, as well as state law claims, including a claim that the commission had exceeded its statutory authorization in making such an order. The Supreme Court upheld the lower federal court's order enjoining enforcement of the maximum rate order. The Court indicated, however, that the lower court should have enjoined the rate order on state law grounds, without reaching the federal constitutional grounds.\n\nThe Court in Siler confirmed that once the lower court properly determined that it had federal question jurisdiction, the court had the right to decide either all questions or only the state law questions. The Siler Court stated that where a case can be decided without reference to questions arising under the federal Constitution, that course is \"usually pursued and is not departed from without important reasons.\" The Court declared it better to decide the case with regard to the construction of the state statute, and the authority therein given to the Commission to make the order in question, rather than to unnecessarily decide the various constitutional questions appearing in the record.\n\nThe Siler Court offered no case precedent or doctrinal ground for this policy decision. The discretionary nature of the Court's decision limits the extent to which Siler serves as a primary basis for an absolute last resort rule. After recognizing the lower court's authority to decide the constitutional questions, the Court decided to follow the \"usual course\" of avoiding such questions if questions of local law would resolve the dispute. This purely prudential formulation of the rule allows courts to dispense with the rule for \"important reasons.\" 1 Although Brandeis prefaced his avoidance doctrine discussion in Ashwander by casting the seven rules as prudential, his formulation of the last resort rule omits this \"important reasons\" qualification. Thus, an evaluation of the proper scope of the last resort rule requires a determination of whether the qualifying phrase should be employed, or whether the rule should be viewed as an absolute.\n\n\"Pullman abstention\" represents the most prominent development of this initial application of the last resort rule after Ashwander. \n\nThe second application Brandeis furnished to demonstrate the last resort rule in Ashwander is the adequate and independent state ground doctrine: \"Appeals [to the United States Supreme Court] from the highest court of a state challenging its decision of a question under the Federal Constitution are frequently dismissed because the judgment can be sustained on an independent state ground.\" When reviewing judgments of state courts, the United States Supreme Court only reviews questions of federal law. The Court will decline to hear a case if an adequate and independent state ground supports the judgment of the state court. The Court reasons that, if a state ground independently supports the judgment, a decision by the Court on federal law grounds will have no effect on the outcome of the case and will amount to an advisory opinion.\n\nThe Supreme Court has provided six closely related justifications for the general doctrine of avoiding constitutional questions, noting their grounding in \"the unique place and character . . . of judicial review of governmental action for constitutionality.\"\n\nThe Court has often called judicial review of legislative acts the most important and delicate of its responsibilities. The Court's characterization of judicial review of legislative acts as a \"delica[te]\" function, \"particularly in view of possible consequences for others stemming also from constitutional roots,\" fundamentally justifies the general avoidance doctrine. An evaluation of the force of this assertion as a justification for avoiding constitutional questions must be linked to evaluation of a second justification offered for the avoidance doctrine, that such review is a \"final\" function. If the Court renders a final, binding conclusion as to constitutional interpretation each time it speaks on a constitutional issue, the arduous task of amending the Constitution may provide the only counter to the Court's ruling. If, however, the Court acts as more or less an equal participant with other political actors in an ongoing dialogue, those other non-judicial actors can reinterpret and reapply a constitutional provision.\n\nThe avoidance doctrine is also premised on \"the inherent limitations of the judicial process, arising especially from its largely negative character and limited resources of enforcement.\" Additionally, federal courts are vulnerable to the extent their jurisdiction and the work of their judges are subject to control by the other branches. Proponents of avoidance techniques such as the last resort rule believe that the federal judiciary must exercise its powers cautiously to conserve the fragile credibility of the least dangerous branch. \n\nIn 1947, evaluating the avoidance doctrine generally, the Supreme Court speculated that to pursue another policy -- a policy of \"accelerated decision\" -- \"might have put an end to, or seriously impaired, the distinctively American institution of judicial review.\" The Court continued: \"It is not without significance for the [avoidance] policy's validity that the periods when the power [of judicial review of legislative acts] has been exercised most readily and broadly have been the ones in which this Court and the institution of judicial review had their stormiest experiences.\"\n\nAckerman notes that Bickel's countermajoritarian difficulty \"recalls the Old Court's long, and ultimately futile, judicial struggle against the New Deal.\" By using the last resort rule frequently, the Court can live with a constitutional problem and let a solution simmer until widespread acceptance is at hand. Bickel argued that the avoidance doctrine, by allowing the judiciary to render unpopular decisions cautiously, rather than suddenly or haphazardly, preserves judicial credibility and increases public acceptance of Court decisions. The last resort rule allows judges to determine when widespread acceptance is at hand or when more simmering is necessary.\n\nAnother justification for the avoidance doctrine is the \"paramount importance of constitutional adjudication in our system.\" This justification overlaps to some extent with the delicate and final nature of the constitutional function, discussed above, but it also implicates the role of constitutional rights. The Court sometimes claims that the ability to declare constitutional rights is the most important power the federal judiciary wields. But many individual rights depend on administrative and statutory claims. Justice Antonin Scalia has argued that not \"every constitutional claim is ipso facto more worthy, and every statutory claim less worthy, of judicial review.\" A decision by a court clarifying a statutory or procedural entitlement to relief may have a tremendous effect on a great number of individuals, or on the workings of an administrative agency.\n\nTwo forceful justifications for the avoidance doctrine are \"the necessity, if government is to function constitutionally, for each [branch] to keep within its power, including the courts\" and \"the consideration due to the judgment of other repositories of constitutional power [*1048] concerning the scope of their authority.\" These justifications are grounded in the separation of powers principle in a constitutional and prudential sense.\n\nIn addition to maintaining appropriate power relations among the national branches, the final two justifications for the avoidance doctrine also encompass federalism concerns. Federal courts must defer appropriately to the powers retained by states and their courts. This comity concern implicates two important applications of the last resort rule: Pullman abstention and the adequate and independent state ground doctrine.\nIn Railroad Commission of Texas v. Pullman Co., the Texas Railroad Commission issued an order requiring that white Pullman conductors, not black Pullman porters, operate sleeping cars. Several railroad companies, and the intervening Pullman porters, challenged the order as unauthorized by state law and unconstitutional under the Equal Protection, Due Process and Commerce Clauses of the federal Constitution. The Court acknowledged that the \"complaint of the Pullman porters undoubtedly tendered a substantial constitutional issue.\" But the Court avoided the issue by abstaining from decision. Justice Frankfurter wrote:\n\nIf the Texas Commission had acted beyond the scope of its authority, the order would be declared invalid under Texas law and no court would need to reach the equal protection issue. Finding the state law unclear, the Court balked at \"making a tentative answer\" regarding Texas law which the Texas Supreme Court could displace the next day. So the Court handed the politically explosive case to the state court for resolution of state law issues.\n\nToday, if a federal court were presented with a case identical to Pullman and the parties chose not to press the nonconstitutional claims, the court, relying on Zobrest, could reach the equal protection claim. Relying on Siler, the court could decide the state law issues itself; or, alternatively, it could apply Pullman abstention. The Court in Pullman used abstention both to avoid wasting federal resources on a \"tentative\" state law decision and to avoid the \"friction of a premature constitutional adjudication.\" Abstention furthered harmony between state and federal courts \"without the need of rigorous congressional restriction of those powers.\" \n\nIn contrast to Pullman abstention, one branch of the adequate and independent state ground doctrine constitutes appropriate application of the last resort rule. The branch dealing with parallel state and federal constitutional provisions has developed in a manner that accords sufficient regard for comity interests while preserving adequate federal court review of constitutional claims. The branch of the doctrine dealing with state procedural foreclosure, however, is more problematic.\n\nThe Supreme Court is entitled to review all federal issues, including constitutional issues, on appeal from a final judgment of the highest state courts in order to preserve federal supremacy and advance uniformity in federal law. The Court will refuse to hear a case, however, if an adequate and independent state ground supports the decision. By deferring to state court decisions based on an adequate and independent state ground, the doctrine addresses Brandeis' concern of federal judicial interference with state authority. The doctrine is generally grounded in efforts to avoid advisory opinions and unnecessary constitutional rulings, and the premise of according sufficient respect to the authority of state courts. It applies only when litigation begins in state courts rather than the lower federal courts. This could be because of a desire to proceed in state court or because Congress has limited the jurisdiction of the lower federal courts.\n\nThe first branch of the doctrine commonly applies where state and federal constitutional provisions are implicated. The application of the adequate and independent state ground doctrine in cases involving state procedural foreclosure is more troublesome. In such instances, failure to adhere to a state procedural rule is often deemed an adequate basis to avoid Supreme Court review of a federal constitutional claim. State procedural law is thus allowed to frustrate federal constitutional rights because of the decision to respect state procedural rules. The second major criticism of the adequate and independent state ground doctrine is that it contributes to inaccuracy and inconsistency in federal law because state courts' erroneous interpretations of federal law remain on the books as long as the judgment is supported by an adequate and independent state ground. The state court's rulings on federal law, however, arguably amount to no more than dicta because those rulings do not provide the basis for the judgment. The Supreme Court might promote uniformity by addressing constitutional claims even when a judgment is supported by adequate and independent state grounds. The ability of the Court, however, to promote uniformity effectively is questionable in our large nation. In any event, uniformity may not always be desirable. The constitutional dialogue may be advanced by a multiplicity of pronouncements from state and federal courts on federal constitutional law.\n\nLike Pullman abstention, the adequate and independent state ground doctrine may disrupt and delay the vindication of federal rights, and make litigation of federal rights less efficient because of prolonged state proceedings and federal review. These concerns are less troubling in this context because the litigation begins in state court. Either that choice is voluntary and litigants could avoid the adequate and independent state ground doctrine by going to federal court initially, or that initial choice is restricted by congressional jurisdictional allocations and deference to state law in particular areas. Congress can alter such allocations in light of restrictions imposed by the adequate and independent state ground doctrine.\n\nBecause the Supreme Court may ultimately review state decisions denying relief under the federal Constitution based on parallel state constitutional grounds, the first branch of the doctrine is a more easily justified application of the last resort rule.\n\n"
            ],
            "good_message": false,
            "loggedData": [],
            "offensive": null,
            "researchTask": true,
            "selectedData": [],
            "shown_passages": [
                [
                    "Supreme court",
                    [
                        "A supreme court is the highest court within the hierarchy of courts in many legal jurisdictions.",
                        "Other descriptions for such courts include court of last resort, apex court, and highest (or final) court of appeal.",
                        "Broadly speaking, the decisions of a supreme court are not subject to further review by any other court.",
                        "Supreme courts typically function primarily as appellate courts, hearing appeals from decisions of lower trial courts, or from intermediate-level appellate courts.",
                        "However, not all highest courts are named as such.",
                        "Civil law states tend not to have a single highest court.",
                        "Additionally, the highest court in some jurisdictions is not named the \"Supreme Court\", for example, the High Court of Australia; this is because decisions by the High Court could formerly be appealed to the Privy Council."
                    ]
                ],
                [
                    "Precedent",
                    [
                        "In legal systems based on common law, a precedent, or authority, is a principle or rule established in a previous legal case that is either binding on or persuasive for a court or other tribunal when deciding subsequent cases with similar issues or facts .",
                        "Common law legal systems place great value on deciding cases according to consistent principled rules so that similar facts will yield similar and predictable outcomes, and observance of precedent is the mechanism by which that goal is attained.",
                        "The principle by which judges are bound to precedents is known as stare decisis.",
                        "Black's Law Dictionary defines \"precedent\" as a \"rule of law established for the first time by a court for a particular type of case and thereafter referred to in deciding similar cases\"."
                    ]
                ],
                [
                    "Last resort rule",
                    [
                        "In Constitutional law, the Last Resort Rule is a largely prudential rule which gives a federal court the power to avoid a constitutional issue in some circumstances.",
                        "This rule dictates that, even if all other jurisdictional and justiciability obstacles are surmounted, federal courts still must avoid a constitutional issue if there is any other ground upon which to render a final judgment.",
                        "The last resort rule can function as a distinct barrier to Constitutional avoidance.",
                        "It is articulated by Justice Brandeis in Ashwander v. Tennessee Valley Authority.",
                        "Brandeis cited two examples in Ashwander of the \"most varied application\" of the last resort rule."
                    ]
                ]
            ],
            "speaker": "0_Wizard",
            "suggestionsConfig": "show",
            "suggestionsNumber": 5,
            "suggestionsShown": true,
            "text": "The Supreme Court of the United States is the court of last resort, so it is the higher one.",
            "textboxFilled": 0,
            "time": 1580233420.710421,
            "timeTaken": 3.377,
            "totalTasks": 120,
            "turn": 21
        },
        {
            "checked_passage": {
                "no_passages_used": "no_passages_used"
            },
            "checked_sentence": {
                "no_passages_used": "no_passages_used"
            },
            "clicked_passages": {},
            "context": {
                "conversation": [
                    "Good Morning.  I'm thinking of getting a job in the court system.",
                    "ARe you planning on working on a specific level, there are three?",
                    "I hadn't thought of that.  What levels are there to choose from?",
                    "The Supreme Court of the United States is the court of last resort, so it is the higher one."
                ],
                "index": 1
            },
            "contextCount": 0,
            "full_passages": [
                "Supreme court\n\nA supreme court is the highest court within the hierarchy of courts in many legal jurisdictions. Other descriptions for such courts include court of last resort, apex court, and highest (or final) court of appeal. Broadly speaking, the decisions of a supreme court are not subject to further review by any other court. Supreme courts typically function primarily as appellate courts, hearing appeals from decisions of lower trial courts, or from intermediate-level appellate courts.\n\nHowever, not all highest courts are named as such. Civil law states tend not to have a single highest court. Additionally, the highest court in some jurisdictions is not named the \"Supreme Court\", for example, the High Court of Australia; this is because decisions by the High Court could formerly be appealed to the Privy Council. On the other hand, in some places the court named the \"Supreme Court\" is not in fact the highest court; examples include the New York Supreme Court, the Supreme Courts of several Canadian provinces/territories and the former Supreme Court of Judicature of England and Wales and Supreme Court of Judicature of Northern Ireland, which are all subordinate to higher courts of appeal.\n\nSome countries have multiple \"supreme courts\" whose respective jurisdictions have different geographical extents, or which are restricted to particular areas of law. Some countries with a federal system of government may have both a federal supreme court (such as the Supreme Court of the United States), and supreme courts for each member state (such as the Supreme Court of Nevada), with the former having jurisdiction over the latter only to the extent that the federal constitution extends federal law over state law. However, other federations, such as Canada, may have a supreme court of general jurisdiction, able to decide any question of law. Jurisdictions with a civil law system often have a hierarchy of administrative courts separate from the ordinary courts, headed by a supreme administrative court as is the case in the Netherlands. A number of jurisdictions also maintain a separate constitutional court (first developed in the Czechoslovak Constitution of 1920), such as Austria, France, Germany, Luxembourg, Portugal, Russia, Spain and South Africa. Within the former British Empire, the highest court within a colony was often called the \"Supreme Court\", even though appeals could be made from that court to the United Kingdom's Privy Council (based in London). A number of Commonwealth jurisdictions retain this system, but many others have reconstituted their own highest court as a court of last resort, with the right of appeal to the Privy Council being abolished.\n\nIn jurisdictions using a common law system, the doctrine of \"stare decisis\" applies, whereby the principles applied by the supreme court in its decisions are binding upon all lower courts; this is intended to apply a uniform interpretation and implementation of the law. In civil law jurisdictions the doctrine of \"stare decisis\" is not generally considered to apply, so the decisions of the supreme court are not necessarily binding beyond the immediate case before it; however, in practice the decisions of the supreme court usually provide a very strong precedent, or \"jurisprudence constante\", for both itself and all lower courts.\n\nThe Supreme Court of Bangladesh is created by the provisions of the Constitution of Bangladesh, 1972. There are two Divisions of the Supreme Court, i.e. (a) Appellate Division and (b) High Court Division. Appellate Division is the highest Court of Appeal and usually does not exercise the powers of a court of first instance. Whereas, the High Court Division is a Court of first instance in writ/judicial review, company and admiralty matters.\n\nThe Supreme Court of Canada was established in 1875 but only became the highest court in the country in 1949 when the right of appeal to the Judicial Committee of the Privy Council was abolished. This court hears appeals from the courts of appeal from the provinces and territories, and also appeals from the Federal Court of Appeal. The Supreme Court is a \"General Court of Appeal.\" It can decide any question of law considered by the lower courts, including constitutional law, federal law, and provincial law. The court's decisions are final and binding on the federal courts and the courts from all provinces and territories. The title \"Supreme\" can be confusing because, for example, the Supreme Court of British Columbia does not have the final say and controversial cases heard there often get appealed in higher courts - it is in fact one of the lower courts in such a process.\n\nIn Hong Kong, the Supreme Court of Hong Kong (now known as the High Court of Hong Kong) was the final court of appeal during its colonial times which ended with transfer of sovereignty in 1997. The final adjudication power, as in any other British Colonies, rested with the Judicial Committee of the Privy Council (JCPC) in London, United Kingdom. Now the power of final adjudication is vested in the Court of Final Appeal created in 1997. Under the Basic Law, its constitution, the territory remains a common law jurisdiction. Consequently, judges from other common law jurisdictions (including England and Wales) can be recruited and continue to serve in the judiciary according to Article 92 of the Basic Law. On the other hand, the power of interpretation of the Basic Law itself is vested in the Standing Committee of the National People's Congress (NPCSC) in Beijing (without retroactive effect), and the courts are authorised to interpret the Basic Law when trying cases, in accordance with Article 158 of the Basic Law. This arrangement became controversial in light of the right of abode issue in 1999, raising concerns for judicial independence.\n\nIn India, the Supreme Court of India was created on January 28, 1950 after adoption of the Constitution.\nArticle 141 of the Constitution of India states that the law declared by Supreme Court is to be binding on all Courts within the territory of India. It is the highest court in India and has ultimate judicial authority to interpret the Constitution and decide questions of national law (including local bylaws). The Supreme Court is also vested with the power of judicial review to ensure the application of the rule of law.\n\nNote that within the constitutional framework of India, Jammu and Kashmir (J&K) has a special status vis-a-vis the other states of India. Article 370 of the Constitution of India carves out certain exceptions for J&K. However, the Constitution (Application to Jammu and Kashmir) Order 1954 makes Article 141 applicable to the state of J&K and hence law declared by the Supreme Court of India is equally applicable to all courts of J&K including the High Court.\n\nThe Supreme Court is the highest court in the Republic of Ireland. It has authority to interpret the constitution, and strike down laws and activities of the state that it finds to be unconstitutional. It is also the highest authority in the interpretation of the law. Constitutionally it must have authority to interpret the constitution but its further appellate jurisdiction from lower courts is defined by law. The Irish Supreme Court consists of its presiding member, the Chief Justice, and seven other judges. Judges of the Supreme Court are appointed by the President in accordance with the binding advice of the Government. The Supreme Court sits in the Four Courts in Dublin.\n\nIsrael's Supreme Court is at the head of the court system in the State of Israel. It is the highest judicial instance. The Supreme Court sits in Jerusalem. The area of its jurisdiction is the entire State. A ruling of the Supreme Court is binding upon every court, other than the Supreme Court itself. The Israeli supreme court is both an appellate court and the high court of justice. As an appellate court, the Supreme Court considers cases on appeal (both criminal and civil) on judgments and other decisions of the District Courts. It also considers appeals on judicial and quasi-judicial decisions of various kinds, such as matters relating to the legality of Knesset elections and disciplinary rulings of the Bar Association. As the High Court of Justice (Hebrew: Beit Mishpat Gavoha Le'Zedek בית משפט גבוה לצדק; also known by its initials as Bagatz בג\"ץ), the Supreme Court rules as a court of first instance, primarily in matters regarding the legality of decisions of State authorities: Government decisions, those of local authorities and other bodies and persons performing public functions under the law, and direct challenges to the constitutionality of laws enacted by the Knesset. The court has broad discretionary authority to rule on matters in which it considers it necessary to grant relief in the interests of justice, and which are not within the jurisdiction of another court or tribunal. The High Court of Justice grants relief through orders such as injunction, mandamus and Habeas Corpus, as well as through declaratory judgments. The Supreme Court can also sit at a further hearing on its own judgment. In a matter on which the Supreme Court has ruled - whether as a court of appeals or as the High Court of Justice - with a panel of three or more justices, it may rule at a further hearing with a panel of a larger number of justices. A further hearing may be held if the Supreme Court makes a ruling inconsistent with a previous ruling or if the Court deems that the importance, difficulty or novelty of a ruling of the Court justifies such hearing. The Supreme Court also holds the unique power of being able to order \"trial de novo\" (a retrial).\n\nIn Nauru, there is no single highest court for all types of cases. The Supreme Court has final jurisdiction on constitutional matters, but any other case may be appealed further to the Appellate Court. In addition, an agreement between Nauru and Australia in 1976 provides for appeals from the Supreme Court of Nauru to the High Court of Australia in both criminal and civil cases, with the notable exception of constitutional cases.\n\nIn New Zealand, the right of appeal to the Privy Council was abolished following the passing of the Supreme Court Act (2003). A right of appeal to the Privy Council remains for criminal cases which were decided before the Supreme Court was created, but it is likely that the successful appeal by Mark Lundy to the Privy Council in 2013 will be the last appeal to the Board from New Zealand.\n\nThe new Supreme Court of New Zealand was officially established at the beginning of 2004, although it did not come into operation until July. The High Court of New Zealand was until 1980 known as the Supreme Court. The Supreme Court has a purely appellate jurisdiction and hears appeals from the Court of Appeal of New Zealand. In some cases, an appeal may be removed directly to the Supreme Court from the High Court. For certain cases, particularly cases which commenced in the District Court, a lower court (typically the High Court or the Court of Appeal) may be the court of final jurisdiction.\n\nThe Supreme Court has been the apex court for Pakistan since the declaration of the republic in 1956 (previously the Privy Council had that function). The Supreme Court has the final say on matters of constitutional law, federal law or on matters of mixed federal and provincial competence. It can hear appeals on matters of provincial competence only if a matter of a constitutional nature is raised.\n\nWith respect to Pakistan's territories (i.e. FATA, Azad Kashmir, Northern Areas and Islamabad Capital Territory (ICT)) the Supreme Court's jurisdiction is rather limited and varies from territory to territory; it can hear appeals only of a constitutional nature from FATA and Northern Areas, while ICT generally functions the same as provinces. Azad Kashmir has its own courts system and the constitution of Pakistan does not apply to it as such; appeals from Azad Kashmir relate to its relationship with Pakistan.\n\nThe provinces have their own courts system, with the High Court as the apex court, except insofar as where an appeal can go to the Supreme Court as mentioned above.\n\nThe Supreme Court of the United Kingdom is the ultimate court for criminal and civil matters in England, Wales and Northern Ireland and for civil matters in Scotland. (The supreme court for criminal matters in Scotland is the High Court of Justiciary.) The Supreme Court was established by the Constitutional Reform Act 2005 with effect from 1 October 2009, replacing and assuming the judicial functions of the House of Lords. Devolution issues under the Scotland Act 1998, Government of Wales Act and Northern Ireland Act were also transferred to the new Supreme Court by the Constitutional Reform Act, from the Judicial Committee of the Privy Council.\n\nIn respect of Community Law the Supreme Court is subject to the decisions of the European Court of Justice. Since there can be no appeal from the Supreme Court, there is an interlocutory procedure by which the Supreme Court may refer to the European Court questions of European law which arise in cases before it, and obtain a definitive ruling before the Supreme Court gives its judgment.\n\nThe Supreme Court shares its members and accommodation at the Middlesex Guildhall in London with the Judicial Committee of the Privy Council which hears final appeals from certain smaller Commonwealth countries, admiralty cases, and certain appeals from the ecclesiastical courts and statutory private jurisdictions, such as professional and academic bodies.\n\n(The Constitutional Reform Act also renamed the \"Supreme Court of Judicature of Northern Ireland\" to the Court of Judicature, and the rarely cited \"Supreme Court of Judicature for England and Wales\" as the Senior Courts of England and Wales).\n\nThe Supreme Court was set up in 2009; until then the House of Lords was the ultimate court in addition to being a legislative body, and the Lord Chancellor, with legislative and executive functions, was also a senior judge in the House of Lords.\n\nThe Supreme Court of the United States, established in 1789, is the highest Federal court in the United States, with powers of judicial review first asserted in \"Calder v. Bull\" (1798) in Justice Iredell's dissenting opinion. The power was later given binding authority by Justice Marshall in \"Marbury v. Madison\" (1803). There are currently nine seats on the US Supreme Court.\n\nEach U.S. state has a state supreme court, which is the highest authority interpreting that state's law and administering that state's judiciary. Two states, Oklahoma and Texas, each have two separate highest courts that respectively specialize in criminal cases and civil cases. Although Delaware has a specialized court, the Court of Chancery, to hear cases in equity, it is not a supreme court because the Delaware Supreme Court has appellate jurisdiction over it.\n\nThe titles of state supreme court vary, which can cause confusion between jurisdictions because one state may use a name for its highest court that another uses for a lower court. In New York, Maryland, and the District of Columbia the highest court is called the Court of Appeals, a name used by many states for their intermediate appellate courts. Further, trial courts of general jurisdiction in New York are called the Supreme Court, and the intermediate appellate court is called the Supreme Court, Appellate Division. In West Virginia, the highest court of the state is the Supreme Court of Appeals. In Maine and Massachusetts the highest court is styled the \"Supreme Judicial Court\"; the last is the oldest appellate court of continuous operation in the Western Hemisphere.\n\nThe Roman law and the Corpus Juris Civilis are generally held to be the historical model for civil law. From the late 18th century onwards, civil law jurisdictions began to codify their laws, most of all in civil codes.\n\nIn Austria, the Austrian Constitution of 1920 (based on a draft by Hans Kelsen) introduced judicial review of legislative acts for their constitutionality. This function is performed by the Constitutional Court (\"Verfassungsgerichtshof\"), which is also charged with the review of administrative acts on whether they violate constitutionally guaranteed rights.\nOther than that, administrative acts are reviewed by the Administrative Court (\"Verwaltungsgerichtshof\"). The Supreme Court (\"Oberste Gerichtshof (OGH)\"), stands at the top of Austria's system of \"ordinary courts\" (\"ordentliche Gerichte\") as the final instance in issues of private law and criminal law.\n\nIn Brazil, the Supreme Federal Tribunal (\"Supremo Tribunal Federal\") is the highest court. It is both the constitutional court and the court of last resort in Brazilian law. It only reviews cases that may be unconstitutional or final \"habeas corpus\" pleads for criminal cases. It also judges, in original jurisdiction, cases involving members of congress, senators, ministers of state, members of the high courts and the President and Vice-President of the Republic. The Superior Court of Justice (\"Tribunal Superior de Justiça\") reviews State and Federal Circuit courts decisions for civil law and criminal law cases, when dealing with federal law or conflicting rulings. The Superior Labour Tribunal (\"Tribunal Superior do Trabalho\") reviews cases involving labour law. The Superior Electoral Tribunal (\"Tribunal Superior Eleitoral\") is the court of last resort of electoral law, and also oversees general elections. The Superior Military Tribunal (\"Tribunal Superior Militar\") is the highest court in matters of federal military law.\n\nIn Croatia, the supreme jurisdiction is given to the Supreme Court, which secures a uniform application of laws. The Constitutional Court exists to verify constitutionality of laws and regulations, as well as decide on individual complaints on decisions on governmental bodies. It also decides on jurisdictional disputes between the legislative, executive and judicial branches.\n\nIn Denmark, all ordinary courts have original jurisdiction to hear all types of cases, including cases of a constitutional or administrative nature. As a result, there exists no special constitutional court, and therefore final jurisdiction is vested with the Danish Supreme Court (\"Højesteret\") which was established 14 February 1661 by king Frederik III.\n\nIn France, supreme appellate jurisdiction is divided among three judicial bodies:\n\nWhen there is jurisdictional dispute between judicial and administrative courts: the Court of Arbitration (\"Tribunal des conflits\"), which is empanelled half from the Court of Cassation and half from the Council of State and presided over by the Minister of Justice, is called together to settle the dispute or hand down a final decision.\n\nThe High Court (\"Haute Cour\") exists only to impeach the President of the French Republic in case of \"breach of his duties patently incompatible with his continuing in office\". Since a constitutional amendment of 2007, the French Constitution states that the High Court is composed of all members of both Houses of Parliament. As of 2012, it has never been convened.\n\nIn Germany, there is no \"de jure\" single supreme court. Instead, cases are handled by numerous federal courts, depending on their nature.\n\nFinal interpretation of the German Constitution, the \"Grundgesetz\", is the task of the \"Bundesverfassungsgericht\" (Federal Constitutional Court), which is the \"de facto\" highest German court, as it can declare both federal and state legislation ineffective, and has the power to overrule decisions of all other federal courts, despite not being a regular court of appeals on itself in the German court system. It is also the only court possessing the power and authority to outlaw political parties, if it is deemed that these parties have repeatedly violated articles of the Constitution.\n\nWhen it comes to civil and criminal cases, the \"Bundesgerichtshof\" (Federal Court of Justice) is at the top of the hierarchy of courts. The other branches of the German judicial system each have their own appellate systems, each topped by a high court; these are the \"Bundessozialgericht\" (Federal Social Court) for matters of social security, the \"Bundesarbeitsgericht\" (Federal Labour Court) for employment and labour, the \"Bundesfinanzhof\" (Federal Fiscal Court) for taxation and financial issues, and the \"Bundesverwaltungsgericht\" (Federal Administrative Court) for administrative law. The so-called \"Gemeinsamer Senat der Obersten Gerichtshöfe\" (Joint Senate of the Supreme Courts) is not a supreme court in itself, but an ad-hoc body that is convened in only when one supreme court intends to diverge from another supreme court's legal opinion or when a certain case exceeds the authority of one court. As the courts have well-defined areas of responsibility, situations like these are rather rare and so, the Joint Senate gathers very infrequently, and only to consider matters which are mostly definitory.\n\nIn the Netherlands, the Supreme Court of the Netherlands is the highest court. Its decisions, known as \"arresten\", are absolutely final. The court is banned from testing legislation against the constitution, pursuant to the principle of the sovereignty of the States-General; the court can, however, test legislation against some treaties. Also, the ordinary courts in the Netherlands, including the Hoge Raad, do not deal with administrative law, which is dealt with in separate administrative courts, the highest of which is the Council of State (Raad van State)\n\nThe Supreme Court of Iceland (, lit. \"Highest Court of Iceland\") was founded under Act No. 22/1919 and held its first session on 16 February 1920. The Court holds the highest judicial power in Iceland, where the court system has two levels.\n\nThe Supreme Court of India, also known colloquially as the 'apex court', is the highest judicial body in the Republic of India. Any decision taken by it is final and binding, and can only be modified in some cases (death sentence, etc.) by the President of India. It has several jurisdiction like \n1. Original\n2.Appellate \n3. Advisory\n\nIt is also known as court of records, i. e. all judgements are recorded and printed. These are cited in lower courts as case - law in various cases.\n\nItaly follows the French system of different supreme courts.\n\nThe Italian court of last resort for most disputes is the \"Corte Suprema di Cassazione\". There is also a separate constitutional court, the \"Corte costituzionale\", which has a duty of judicial review, and which can strike down legislation as being in conflict with the Constitution.\n\nIn Japan, the Supreme Court of Japan is called (Saikō-Saibansho; called 最高裁 Saikō-Sai for short), located in Chiyoda, Tokyo, and is the highest court in Japan. It has ultimate judicial authority within Japan to interpret the Constitution and decide questions of national law (including local by laws). It has the power of judicial review (i.e., it can declare Acts of Diet and Local Assembly, and administrative actions, unconstitutional).\n\nIn Luxembourg, challenges on the conformity of the law to the Constitution are brought before the \"Cour Constitutionnelle\" (Constitutional Court). — The most used and common procedure to present these challenges is by way of the \"\"question préjudicielle\"\" (prejudicial question). The Court of last resort for civil and criminal proceedings is the \"\"Cour de Cassation\"\".\nFor administrative proceedings the highest court is the \"\"Cour Administrative\"\" (Administrative Court).\n\nThe supreme court of Macau is the Court of Final Appeal (; ).\n\nWhile the Philippines is generally considered a civil law nation, its Supreme Court is heavily modelled after the American Supreme Court. This can be attributed to the fact that the Philippines was colonized by both Spain and the United States, and the system of laws of both nations strongly influenced the development of Philippine laws and jurisprudence. Even as the body of Philippine laws remain mostly codified, the Philippine Civil Code expressly recognizes that decisions of the Supreme Court \"form part of the law of the land\", belonging to the same class as statutes. The 1987 Philippine Constitution also explicitly grants to the Supreme Court the power of judicial review over laws and executive actions. The Supreme Court is composed of 1 Chief Justice and 14 Associate Justices. The court sits either en banc or in divisions, depending on the nature of the case to be decided.\n\nIn the judicial system of mainland China the highest court of appeal is the Supreme People's Court. This supervises the administration of justice by all subordinate \"local\" and \"special\" people's courts, and is the court of last resort for the whole People's Republic of China except for Macau and Hong Kong\n\nIn Portugal, there are several supreme courts, each with a specific jurisdiction:\n\nUntil 2003, a fifth supreme court also existed for the military jurisdiction, this being the Supreme Military Court (\"Supremo Tribunal Militar\"). Presently, in time of peace, the supreme court for military justice matters is the Supreme Court of Justice, which now includes four military judges.\n\nIn the Republic of China (Taiwan), there are three different courts of last resort:\n\nThe Council of Grand Justices, consisting of 15 justices and mainly dealing with constitutional issues, is the counterpart of constitutional courts in some countries.\n\nAll three courts are directly under the Judicial Yuan, whose president also serves as Chief Justice in the Council of Grand Justices.\n\nFounded by papal bull in 1532, the Court of Session is the supreme civil court of Scotland, and the High Court of Justiciary is the supreme criminal court. However, the absolute highest court (excluding criminal matters) is the Supreme Court of the United Kingdom.\n\nSpanish Supreme Court is the highest court for all cases in Spain (both private and public). Only those cases related to human rights can be appealed at the Constitutional Court (which also decides about acts accordance with Spanish Constitution). \nIn Spain, high courts cannot create binding precedents; however, lower rank courts usually observe Supreme Court interpretations. In most private law cases, two Supreme Court judgements supporting a claim are needed to appeal at the Supreme Court.\nFive sections form the Spanish Supreme court:\n\nIn Sweden, the Supreme Court and the Supreme Administrative Court respectively function as the highest courts of the land. The Supreme Administrative Court considers cases concerning disputes between individuals and administrative organs, as well as disputes among administrative organs, while the Supreme Court considers all other cases. The judges are appointed by the Government. In most cases, the Supreme Courts will only grant leave to appeal a case (\"prövningstillstånd\") if the case involves setting a precedent in the interpretation of the law. Exceptions are issues where the Supreme Court is the court of first instance. Such cases include an application for a retrial of a criminal case in the light of new evidence, and prosecutions made against an incumbent minister of the Government for severe neglect of duty. If a lower court has to try a case which involves a question where there is no settled interpretation of the law, it can also refer the question to the relevant Supreme Court for an answer.\n\nIn Switzerland, the Federal Supreme Court of Switzerland is the final court of appeals. Due to Switzerland's system of direct democracy, it has no authority to review the constitutionality of federal statutes, but the people can strike down a proposed law by referendum. According to settled case law, however, the Court is authorised to review the compliance of all Swiss law with certain categories of international law, especially the European Convention of Human Rights.\n\nIn Sri Lanka, the Supreme Court of Sri Lanka was created in 1972 after the adoption of a new Constitution. The Supreme Court is the highest and final superior court of record and is empowered to exercise its powers, subject to the provisions of the Constitution. The court rulings take precedence over all lower Courts. The Sri Lanka judicial system is complex blend of both common-law and civil-law. In some cases such as capital punishment, the decision may be passed on to the President of the Republic for clemency petitions. However, when there is 2/3 majority in the parliament in favour of president (as with present), the supreme court and its judges' powers become nullified as they could be fired from their positions according to the Constitution, if the president wants. Therefore, in such situations, Civil law empowerment vanishes.\n\nIn South Africa, a \"two apex\" system existed from 1994 to 2013. The Supreme Court of Appeal (SCA) was created in 1994 and replaced the Appellate Division of the Supreme Court of South Africa as the highest court of appeal in non-constitutional matters. The SCA is subordinate to the Constitutional Court, which is the highest court in matters involving the interpretation and application of the Constitution. But in August 2013 the Constitution was amended to make the Constitutional Court the country's single apex court, superior to the SCA in all matters, both constitutional and non-constitutional.\n\nHistorically, citizens appealed directly to the King along his route to places out of the Palace. A Thai King would adjudicate all disputes. During the reign of King Chulalongkorn, an official department for appeals was set up, and, after Thailand adopted a western-styled government, Thai Supreme Court was established in 1891.\n\nAt present, the Supreme Court of Thailand retains the important status as the highest court of justice in the country. Operating separately from the Administrative Court and the Constitutional Court, the judgement of the Supreme Court is considered as final.\n\nIn the United Arab Emirates, the Federal Supreme Court of the United Arab Emirates was created in 1973 after the adoption of the Constitution. The Supreme Court is the highest and final superior court of record and is empowered to exercise its powers, subject to the provisions of the Constitution. The court rulings take precedence over all lower Courts. The Emirati judicial system is complex blend of both Islamic law and civil law. In some cases such as capital punishment, the decision may be passed on to the President of the country (currently Khalifa bin Zayed Al Nahyan).\n\n\nLaw of Indonesia at the national level is based on a combination of civil law from the tradition of Roman-Dutch law and customary law from the tradition of Adat. Law in regional jurisdictions can vary from province to province, including even Sharia law, for example Islamic criminal law in Aceh, though even at the national level, individual justices can cite sharia or other forms of non-Dutch law in their legal opinions.\n\nThe Supreme Court of Indonesia is the main judicial arm of the state, functioning as the final court of appeal as well as a means to re-open cases previously closed. The Supreme Court, which consists of a total of 51 justices, also oversees the regional high courts. It was founded at the country's independence in 1945.\n\nThe Constitutional Court of Indonesia, on the other hand, is a part of the judicial branch tasked with review of bills and government actions for constitutionality, as well as regulation of the interactions between various arms of the state. The constitutional amendment to establish the court was passed in 2001, and the court itself was established in 2003. The Constitutional Court consists of nine justices serving nine year terms, and they're appointed in tandem by the Supreme Court, the President of Indonesia and the People's Representative Council.\n\nIn most nations with constitutions modelled after the Soviet Union, the legislature was given the power of being the \"court of last resort\". In the People's Republic of China, the final power to interpret the law is vested in the Standing Committee of the National People's Congress (NPCSC). This power includes the power to interpret the basic laws of Hong Kong and Macau, the constitutional documents of the two special administrative regions which are common law and Portuguese-based legal system jurisdictions respectively. This power is a legislative power and not a judicial one in that an interpretation by the NPCSC does not affect cases which have already been decided.\n\n",
                "Precedent\n\nIn legal systems based on common law, a precedent, or authority, is a principle or rule established in a previous legal case that is either binding on or persuasive for a court or other tribunal when deciding subsequent cases with similar issues or facts . Common law legal systems place great value on deciding cases according to consistent principled rules so that similar facts will yield similar and predictable outcomes, and observance of precedent is the mechanism by which that goal is attained. The principle by which judges are bound to precedents is known as stare decisis. Black's Law Dictionary defines \"precedent\" as a \"rule of law established for the first time by a court for a particular type of case and thereafter referred to in deciding similar cases\". Common law precedent is a third kind of law, on equal footing with statutory law (statutes and codes enacted by legislative bodies), and delegated legislation (in U.K. parlance) or regulatory law (in U.S. parlance) (regulations promulgated by executive branch agencies).\n\nCase law, in common law jurisdictions, is the set of decisions of adjudicatory tribunals or other rulings that can be cited as precedent. In most countries, including most European countries, the term is applied to any set of rulings on law which is guided by previous rulings, for example, previous decisions of a government agency.\n\nEssential to the development of case law is the publication and indexing of decisions for use by lawyers, courts and the general public, in the form of law reports. While all decisions are precedent (though at varying levels of authority as discussed throughout this article), some become \"leading cases\" or \"landmark decisions\" that are cited especially often.\n\n\"Stare decisis\" (Anglo-Latin pronunciation: ) is a legal principle by which judges are obligated to respect the precedent established by prior decisions. The words originate from the phrasing of the principle in the Latin maxim Stare decisis et non quieta movere: \"to stand by decisions and not disturb the undisturbed\". In a legal context, this is understood to mean that courts should generally abide by precedent and not disturb settled matters. The principle of \"stare decisis\" can be divided into two components.\n\nThe first is the rule that a decision made by a superior court, or by the same court in an earlier decision, is binding precedent that the court itself and all its inferior courts are obligated to follow. The second is the principle that a court should not overturn its own precedent unless there is a strong reason to do so and should be guided by principles from lateral and inferior courts. The second principle, regarding persuasive precedent, is an advisory one that courts can and do ignore occasionally.\n\nIn the common law tradition, courts decide the law applicable to a case by interpreting statutes and applying precedent which record how and why prior cases have been decided. Unlike most civil law systems, common law systems follow the doctrine of \"stare decisis\", by which most courts are bound by their own previous decisions in similar cases, and all lower courts should make decisions consistent with previous decisions of higher courts. For example, in England, the High Court and the Court of Appeal are each bound by their own previous decisions, but the Supreme Court of the United Kingdom is able to deviate from its earlier decisions, although in practice it rarely does so.\n\nGenerally speaking, higher courts do not have direct oversight over day-to-day proceedings in lower courts, in that they cannot reach out on their own initiative (\"sua sponte\") at any time to reverse or overrule judgments of the lower courts. Normally, the burden rests with litigants to appeal rulings (including those in clear violation of established case law) to the higher courts. If a judge acts against precedent and the case is not appealed, the decision will stand.\n\nA lower court may not rule against a binding precedent, even if the lower court feels that the precedent is unjust; the lower court may only express the hope that a higher court or the legislature will reform the rule in question. If the court believes that developments or trends in legal reasoning render the precedent unhelpful, and wishes to evade it and help the law evolve, the court may either hold that the precedent is inconsistent with subsequent authority, or that the precedent should be \"distinguished\" by some material difference between the facts of the cases. If that judgment goes to appeal, the appellate court will have the opportunity to review both the precedent and the case under appeal, perhaps overruling the previous case law by setting a new precedent of higher authority. This may happen several times as the case works its way through successive appeals. Lord Denning, first of the High Court of Justice, later of the Court of Appeal, provided a famous example of this evolutionary process in his development of the concept of estoppel starting in the \"High Trees\" case: \"Central London Property Trust Ltd v. High Trees House Ltd\" [1947] K.B. 130.\n\nJudges may refer to various types of persuasive authority to reach a decision in a case. Widely cited non-binding sources include legal encyclopedias such as \"Corpus Juris Secundum\" and \"Halsbury's Laws of England\", or the published work of the Law Commission or the American Law Institute. Some bodies are given statutory powers to issue Guidance with persuasive authority or similar statutory effect, such as the Highway Code.\n\nIn federal or multi-jurisdictional law systems there may exist conflicts between the various lower appellate courts. Sometimes these differences may not be resolved and it may be necessary to distinguish how the law is applied in one district, province, division or appellate department. Usually only an appeal accepted by the court of last resort will resolve such differences and, for many reasons, such appeals are often not granted.\n\nAny court may seek to distinguish its present case from that of a binding precedent, in order to reach a different conclusion. The validity of such a distinction may or may not be accepted on appeal. An appellate court may also propound an entirely new and different analysis from that of junior courts, and may or may not be bound by its own previous decisions, or in any case may distinguish the decisions based on significant differences in the facts applicable to each case. Or, a court may view the matter before it as one of \"first impression,\" not governed by any controlling precedent.\n\nWhere there are several members of a court, there may be one or more judgments given; only the ratio decidendi of the majority can constitute a binding precedent, but all may be cited as persuasive, or their reasoning may be adopted in argument. Quite apart from the rules of precedent, the weight actually given to any reported judgment may depend on the reputation of both the court and the judges.\n\nGenerally, a common law court system has trial courts, intermediate appellate courts and a supreme court. The inferior courts conduct almost all trial proceedings. The inferior courts are bound to obey precedent established by the appellate court for their jurisdiction, and all supreme court precedent.\n\nThe Supreme Court of California's explanation of this principle is that\n\nAn Intermediate state appellate court is generally bound to follow the decisions of the highest court of that state.\n\nThe application of the doctrine of \"stare decisis\" from a superior court to an inferior court is sometimes called \"vertical stare decisis\".\n\nThe idea that a judge is bound by (or at least should respect) decisions of earlier judges of similar or coordinate level is called horizontal \"stare decisis\".\n\nIn the United States federal court system, the intermediate appellate courts are divided into thirteen \"circuits,\" each covering some range of territory ranging in size from the District of Columbia alone up to seven states. Each panel of judges on the court of appeals for a circuit is bound to obey the prior appellate decisions of the same circuit. Precedent of a United States court of appeals may be overruled only by the court \"en banc,\" that is, a session of all the active appellate judges of the circuit, or by the United States Supreme Court, not simply by a different three-judge panel.\n\nWhen a court binds itself, this application of the doctrine of precedent is sometimes called \"horizontal stare decisis\". The state of New York has a similar appellate structure as it is divided into four appellate departments supervised by the final New York Court of Appeals. Decisions of one appellate department are not binding upon another, and in some cases the departments differ considerably on interpretations of law.\n\nIn federal systems the division between federal and state law may result in complex interactions. In the United States, state courts are not considered inferior to federal courts but rather constitute a parallel court system.\n\n\nIn practice, however, judges in one system will almost always choose to follow relevant case law in the other system to prevent divergent results and to minimize forum shopping.\n\nPrecedent that must be applied or followed is known as \"binding precedent\" (alternately \"metaphorically precedent\", \"mandatory\" or \"binding authority\", etc.). Under the doctrine of \"stare decisis\", a lower court must honor findings of law made by a higher court that is within the appeals path of cases the court hears. In state and federal courts in the United States of America, jurisdiction is often divided geographically among local trial courts, several of which fall under the territory of a regional appeals court. All appellate courts fall under a highest court (sometimes but not always called a \"supreme court\"). By definition, decisions of lower courts are not binding on courts higher in the system, nor are appeals court decisions binding on local courts that fall under a different appeals court. Further, courts must follow their own proclamations of law made earlier on other cases, and honor rulings made by other courts in disputes among the parties before them pertaining to the same pattern of facts or events, unless they have a strong reason to change these rulings (see Law of the case re: a court's previous holding being binding precedent for that court).\n\nIn law, a binding precedent (also known as a mandatory precedent or binding authority) is a precedent which must be followed by all lower courts under common law legal systems. In English law it is usually created by the decision of a higher court, such as the Supreme Court of the United Kingdom, which took over the judicial functions of the House of Lords in 2009. In Civil law and pluralist systems precedent is not binding but case law is taken into account by the courts.\n\nBinding precedent relies on the legal principle of \"stare decisis\". \"Stare decisis\" means to stand by things decided. It ensures certainty and consistency in the application of law. Existing binding precedent from past cases are applied in principle to new situations by analogy.\n\nOne law professor has described mandatory precedent as follows:\n\nIn extraordinary circumstances a higher court may overturn or overrule mandatory precedent, but will often attempt to distinguish the precedent before overturning it, thereby limiting the scope of the precedent.\n\nUnder the U.S. legal system, courts are set up in a hierarchy. At the top of the federal or national system is the Supreme Court, and underneath are lower federal courts. The state court systems have hierarchy structures similar to that of the federal system.\n\nThe U.S. Supreme Court has final authority on questions about the meaning of federal law, including the U.S. Constitution. For example, when the Supreme Court says that the First Amendment applies in a specific way to suits for slander, then every court is bound by that precedent in its interpretation of the First Amendment as it applies to suits for slander. If a lower court judge disagrees with a higher court precedent on what the First Amendment should mean, the lower court judge must rule according to the binding precedent. Until the higher court changes the ruling (or the law itself is changed), the binding precedent is authoritative on the meaning of the law.\n\nLower courts are bound by the precedent set by higher courts within their region. Thus, a federal district court that falls within the geographic boundaries of the Third Circuit Court of Appeals (the mid-level appeals court that hears appeals from district court decisions from Delaware, New Jersey, Pennsylvania, and the Virgin Islands) is bound by rulings of the Third Circuit Court, but not by rulings in the Ninth Circuit (Alaska, Arizona, California, Guam, Hawaii, Idaho, Montana, Nevada, Northern Mariana Islands, Oregon, and Washington), since the Circuit Courts of Appeals have jurisdiction defined by geography. The Circuit Courts of Appeals can interpret the law how they want, so long as there is no binding Supreme Court precedent. One of the common reasons the Supreme Court grants certiorari (that is, they agree to hear a case) is if there is a conflict among the circuit courts as to the meaning of a federal law.\n\nThere are three elements needed for a precedent to work. Firstly, the hierarchy of the courts needs to be accepted, and an efficient system of law reporting. 'A balance must be struck between the need on one side for the legal certainty resulting from the binding effect of previous decisions, and on the other side the avoidance of undue restriction on the proper development of the law (1966 Practice Statement (Judicial Precedent) by Lord Gardiner L.C.)'.\n\nJudges are bound by the law of binding precedent in England and Wales and other common law jurisdictions. This is a distinctive feature of the English legal system. In Scotland and many countries throughout the world, particularly in mainland Europe, civil law means that judges take case law into account in a similar way, but are not obliged to do so and are required to consider the precedent in terms of principle. Their fellow judges' decisions may be persuasive but are not binding. Under the English legal system, judges are not necessarily entitled to make their own decisions about the development or interpretations of the law. They may be bound by a decision reached in a previous case. Two facts are crucial to determining whether a precedent is binding:\n\n\"Super \"stare decisis\"\" is a term used for important precedent that is resistant or immune from being overturned, without regard to whether correctly decided in the first place. It may be viewed as one extreme in a range of precedential power, or alternatively, to express a belief, or a critique of that belief, that some decisions should not be overturned.\n\nIn 1976, Richard Posner and William Landes coined the term \"super-precedent,\" in an article they wrote about testing theories of precedent by counting citations. Posner and Landes used this term to describe the influential effect of a cited decision. The term \"super-precedent\" later became associated with different issue: the difficulty of overturning a decision. In 1992, Rutgers professor Earl Maltz criticized the Supreme Court's decision in \"Planned Parenthood v. Casey\" for endorsing the idea that if one side can take control of the Court on an issue of major national importance (as in \"Roe v. Wade\"), that side can protect its position from being reversed \"by a kind of super-stare decisis\". The controversial idea that some decisions are virtually immune from being overturned, regardless of whether they were decided correctly in the first place, is the idea to which the term \"super \"stare decisis\"\" now usually refers.\n\nThe concept of super-\"stare decisis\" (or \"super-precedent\") was mentioned during the interrogations of Chief Justice John Roberts and Justice Samuel Alito before the Senate Judiciary Committee. Prior to the commencement of the Roberts hearings, the chair of that committee, Senator Arlen Specter of Pennsylvania, wrote an op/ed in the \"New York Times\" referring to \"Roe\" as a \"super-precedent\". He revisited this concept during the hearings, but neither Roberts nor Alito endorsed the term or the concept.\n\nPersuasive precedent (also persuasive authority) is precedent or other legal writing that is not binding precedent but that is useful or relevant and that may guide the judge in making the decision in a current case. Persuasive precedent includes cases decided by lower courts, by peer or higher courts from other geographic jurisdictions, cases made in other parallel systems (for example, military courts, administrative courts, indigenous/tribal courts, state courts versus federal courts in the United States), statements made in dicta, treatises or academic law reviews, and in some exceptional circumstances, cases of other nations, treaties, world judicial bodies, etc.\n\nIn a \"case of first impression\", courts often rely on persuasive precedent from courts in other jurisdictions that have previously dealt with similar issues. Persuasive precedent may become binding through its adoption by a higher court.\n\nIn civil law and pluralist systems, as under Scots law, precedent is not binding but case law is taken into account by the courts.\n\nA lower court's opinion may be considered as persuasive authority if the judge believes they have applied the correct legal principle and reasoning.\n\nA court may consider the ruling of a higher court that is not binding. For example, a district court in the United States First Circuit could consider a ruling made by the United States Court of Appeals for the Ninth Circuit as persuasive authority.\n\nCourts may consider rulings made in other courts that are of equivalent authority in the legal system. For example, an appellate court for one district could consider a ruling issued by an appeals court in another district.\n\nCourts may consider \"obiter dicta\" in opinions of higher courts. Dicta of a higher court, though not binding, will often be persuasive to lower courts. The phrase \"obiter dicta\" is usually translated as \"other things said\", but due to the high number of judges and individual concurring opinions, it is often hard to distinguish from the \"ratio decidendi\" (reason for the decision). For these reasons, the obiter dicta may often be taken into consideration by a court. A litigant may also consider \"obiter dicta\" if a court has previously signaled that a particular legal argument is weak and may even warrant sanctions if repeated.\n\nA case decided by a multi-judge panel could result in a split decision. While only the majority opinion is considered precedential, an outvoted judge can still publish a dissenting opinion. Common patterns for dissenting opinions include:\nA judge in a subsequent case, particularly in a different jurisdiction, could find the dissenting judge's reasoning persuasive. In the jurisdiction of the original decision, however, a judge should only overturn the holding of a court lower or equivalent in the hierarchy. A district court, for example, could not rely on a Supreme Court dissent as a basis to depart from the reasoning of the majority opinion. However, lower courts occasionally cite dissents, either for a limiting principle on the majority, or for propositions that are not stated in the majority opinion and not inconsistent with that majority, or to explain a disagreement with the majority and to urge reform (while following the majority in the outcome).\n\nCourts may consider the writings of eminent legal scholars in treatises, restatements of the law, and law reviews. The extent to which judges find these types of writings persuasive will vary widely with elements such as the reputation of the author and the relevance of the argument.\n\nThe courts of England and Wales are free to consider decisions of other jurisdictions, and give them whatever persuasive weight the English court sees fit, even though these other decisions are not binding precedent. Jurisdictions that are closer to modern English common law are more likely to be given persuasive weight (for example Commonwealth states such as Canada, Australia, or New Zealand). Persuasive weight might be given to other common law courts, such as from the United States, most often where the American courts have been particularly innovative, e.g. in product liability and certain areas of contract law.\n\nIn the United States, in the late 20th and early 21st centuries, the concept of a U.S. court considering foreign law or precedent has been considered controversial by some parties. The Supreme Court splits on this issue. This critique is recent, as in the early history of the United States, citation of English authority was ubiquitous. One of the first acts of many of the new state legislatures was to adopt the body of English common law into the law of the state. See here. Citation to English cases was common through the 19th and well into the 20th centuries. Even in the late 20th and early 21st centuries, it is relatively uncontroversial for American state courts to rely on English decisions for matters of pure common (i.e. judge-made) law. \n\nWithin the federal legal systems of several common-law countries, and most especially the United States, it is relatively common for the distinct lower-level judicial systems (e.g. state courts in the United States and Australia, provincial courts in Canada) to regard the decisions of other jurisdictions within the same country as persuasive precedent. Particularly in the United States, the adoption of a legal doctrine by a large number of other state judiciaries is regarded as highly persuasive evidence that such doctrine is preferred. A good example is the adoption in Tennessee of comparative negligence (replacing contributory negligence as a complete bar to recovery) by the 1992 Tennessee Supreme Court decision \"McIntyre v. Balentine\" (by this point all US jurisdictions save Tennessee, five other states, and the District of Columbia had adopted comparative negligence schemes). Moreover, in American law, the \"Erie\" doctrine requires federal courts sitting in diversity actions to apply state substantive law, but in a manner consistent with how the court believes the state's highest court would rule in that case. Since such decisions are not binding on state courts, but are often very well-reasoned and useful, state courts cite federal interpretations of state law fairly often as persuasive precedent, although it is also fairly common for a state high court to reject a federal court's interpretation of its jurisprudence.\n\nNon-publication of opinions, or unpublished opinions, are those decisions of courts that are not available for citation as precedent because the judges making the opinion deem the case as having less precedential value. Selective publication is the legal process which a judge or justices of a court decide whether a decision is to be or not published in a reporter. \"Unpublished\" federal appellate decisions are published in the Federal Appendix. Depublication is the power of a court to make a previously published order or opinion unpublished.\n\nLitigation that is settled out of court generates no written decision, and thus has no precedential effect. As one practical effect, the U.S. Department of Justice settles many cases against the federal government simply to avoid creating adverse precedent.\n\nSeveral rules may cause a decision to apply as narrow \"precedent\" to preclude future legal positions of the specific parties to a case, even if a decision is non-precedential with respect to all other parties.\n\nOnce a case is decided, the same plaintiff cannot sue the same defendant again on any claim arising out of the same facts. The law requires plaintiffs to put all issues on the table in a single case, not split the case. For example, in a case of an auto accident, the plaintiff cannot sue first for property damage, and then personal injury in a separate case. This is called \"res judicata\" or claim preclusion (\"'Res judicata'\" is the traditional name going back centuries; the name shifted to \"claim preclusion\" in the United States over the late 20th century). Claim preclusion applies whether the plaintiff wins or loses the earlier case, even if the later case raises a different legal theory, even the second claim is unknown at the time of the first case. Exceptions are extremely limited, for example if the two claims for relief must necessarily be brought in different courts (for example, one claim might be exclusively federal, and the other exclusively state).\n\nOnce a case is finally decided, any issues decided in the previous case may be binding against the party that lost the issue in later cases, even in cases involving other parties. For example, if a first case decides that a party was negligent, then other plaintiffs may rely on that earlier determination in later cases, and need not re-prove the issue of negligence. For another example, if a patent is shown to be invalid in a case against one accused infringer, that same patent is invalid against all other accused infringers—invalidity need not be re-proved. Again, there are limits and exceptions on this principle. The principle is called collateral estoppel or issue preclusion.\n\nWithin a single case, once there's been a first appeal, both the lower court and the appellate court itself will not further review the same issue, and will not re-review an issue that could have been appealed in the first appeal. Exceptions are limited to three \"exceptional circumstances:\" (1) when substantially different evidence is raised at a subsequent trial, (2) when the law changes after the first appeal, for example by a decision of a higher court, or (3) when a decision is clearly erroneous and would result in a manifest injustice. This principle is called \"law of the case\".\n\nOn many questions, reasonable people may differ. When two of those people are judges, the tension among two lines of precedent may be resolved as follows.\n\nIf the two courts are in separate, parallel jurisdictions, there is no conflict, and two lines of precedent may persist. Courts in one jurisdiction are influenced by decisions in others, and notably better rules may be adopted over time.\n\nCourts try to formulate the common law as a \"seamless web\" so that principles in one area of the law apply to other areas. However, this principle does not apply uniformly. Thus, a word may have different definitions in different areas of the law, or different rules may apply so that a question has different answers in different legal contexts. Judges try to minimize these conflicts, but they arise from time to time, and under principles of 'stare decisis', may persist for some time.\n\nA matter of first impression (known as \"primae impressionis\" in Latin) is a legal case in which there is no binding authority on the matter presented. Such a case can set forth a completely original issue of law for decision by the courts. A first impression case may be a first impression in only a particular jurisdiction. In that situation, courts will look to holdings of other jurisdictions for persuasive authority.\n\nIn the latter meaning, the case in question cannot be decided through referring to and/or relying on precedent. Since the legal issue under consideration has never been decided by an appeals court and, therefore, there is no precedent for the court to follow, the court uses analogies from prior rulings by appeals courts, refers to commentaries and articles by legal scholars, and applies its own logic. In cases of first impression, the trial judge will often ask both sides' attorneys for legal briefs.\n\nIn some situations, a case of first impression may exist in a jurisdiction until a reported appellate court decision is rendered.\n\nThe different roles of case law in civil law and common law traditions create differences in the way that courts render decisions. Common law courts generally explain in detail the legal rationale behind their decisions, with citations of both legislation and previous relevant judgments, and often an exegesis of the wider legal principles. These are called \"ratio decidendi\" and constitute a precedent binding on other courts; further analyses not strictly necessary to the determination of the current case are called \"obiter dicta\", which have persuasive authority but are not technically binding. By contrast, decisions in civil law jurisdictions are generally very short, referring only to statutes. The reason for this difference is that these civil law jurisdictions apply legislative positivism — a form of extreme legal positivism — which holds that legislation is the only valid source of law because it has been voted on democratically; thus, it is not the judiciary's role to create law, but rather to interpret and apply statute, and therefore their decisions must reflect that.\n\n\"Stare decisis\" is not usually a doctrine used in civil law systems, because it violates the legislative positivist principle that only the legislature may make law. Instead, the civil law system relies on the doctrine of \"jurisprudence constante\", according to which if a court has adjudicated a consistent line of cases that arrive at the same holdings using sound reasoning, then the previous decisions are highly persuasive but not controlling on issues of law. This doctrine is similar to \"stare decisis\" insofar as it dictates that a court's decision must condone a cohesive and predictable result. In theory, lower courts are generally not bound by the precedents of higher courts. In practice, the need for predictability means that lower courts generally defer to the precedent of higher courts. As a result, the precedent of courts of last resort, such as the French Cassation Court and the Council of State, is recognized as being \"de facto\" binding on lower courts.\n\nThe doctrine of \"jurisprudence constante\" also influences how court decisions are structured. In general, court decisions of common law jurisdictions give a sufficient \"ratio decidendi\" as to guide future courts. The ratio is used to justify a court decision on the basis of previous case law as well as to make it easier to use the decision as a precedent for future cases. By contrast, court decisions in some civil law jurisdictions (most prominently France) tend to be extremely brief, mentioning only the relevant legislation and codal provisions and not going into the \"ratio decidendi\" in any great detail. This is the result of the legislative positivist view that the court is only interpreting the legislature's intent and therefore detailed exposition is unnecessary. Because of this, \"ratio decidendi\" is carried out by legal academics (doctrinal writers) who provide the explanations that in common law jurisdictions would be provided by the judges themselves.\n\nIn other civil law jurisdictions, such as the German-speaking countries, \"ratio decidendi\" tend to be much more developed than in France, and courts will frequently cite previous cases and doctrinal writers. However, some courts (such as German courts) have less emphasis on the particular facts of the case than common law courts, but have more emphasis on the discussion of various doctrinal arguments and on finding what the correct interpretation of the law is.\n\nThe mixed systems of the Nordic countries are sometimes considered a branch of the civil law, but they are sometimes counted as separate from the civil law tradition. In Sweden, for instance, case law arguably plays a more important role than in some of the continental civil law systems. The two highest courts, the Supreme Court (\"Högsta domstolen\") and the Supreme Administrative Court (\"Högsta förvaltningsdomstolen\"), have the right to set precedent which has persuasive authority on all future application of the law. Appellate courts, be they judicial (\"hovrätter\") or administrative (\"kammarrätter\"), may also issue decisions that act as guides for the application of the law, but these decisions are persuasive, not controlling, and may therefore be overturned by higher courts.\n\nSome mixed systems, such as Scots law in Scotland, South-African law, and the law of Quebec and Louisiana, do not fit into the civil vs. common law dichotomy because they mix portions of both. Such systems may have been heavily influenced by the common law tradition; however, their private law is firmly rooted in the civil law tradition. Because of their position between the two main systems of law, these types of legal systems are sometimes referred to as \"mixed\" systems of law. Louisiana courts, for instance, operate under both \"stare decisis\" and \"jurisprudence constante\". In South Africa, the precedent of higher courts is absolutely or fully binding on lower courts, whereas the precedent of lower courts only has persuasive authority on higher courts; horizontally, precedent is \"prima facie\" or presumptively binding between courts.\n\nLaw professors in common law traditions play a much smaller role in developing case law than professors in civil law traditions. Because court decisions in civil law traditions are brief and not amenable to establishing precedent, much of the exposition of the law in civil law traditions is done by academics rather than by judges; this is called doctrine and may be published in treatises or in journals such as \"Recueil Dalloz\" in France. Historically, common law courts relied little on legal scholarship; thus, at the turn of the twentieth century, it was very rare to see an academic writer quoted in a legal decision (except perhaps for the academic writings of prominent judges such as Coke and Blackstone). Today academic writers are often cited in legal argument and decisions as persuasive authority; often, they are cited when judges are attempting to implement reasoning that other courts have not yet adopted, or when the judge believes the academic's restatement of the law is more compelling than can be found in precedent. Thus common law systems are adopting one of the approaches long common in civil law jurisdictions.\n\nJustice Louis Brandeis, in a heavily footnoted dissent to \"Burnet v. Coronado Oil & Gas Co.\", 285 U.S. 393, 405-411 (1932), explained (citations and quotations omitted):\n\nThe United States Court of Appeals for the Third Circuit has stated:\n\nThe United States Court of Appeals for the Ninth Circuit has stated:\n\nJustice McHugh of the High Court of Australia in relation to precedents remarked in \"Perre v Apand\":\n\nPrecedent viewed against passing time can serve to establish trends, thus indicating the next logical step in evolving interpretations of the law. For instance, if immigration has become more and more restricted under the law, then the next legal decision on that subject may serve to restrict it further still. The existence of submerged precedent (reasoned opinions not made available through conventional legal research sources) has been identified as a potentially distorting force in the evolution of law.\n\nScholars have recently attempted to apply network theory to precedent in order to establish which precedent is most important or authoritative, and how the court's interpretations and priorities have changed over time.\n\nEarly English common law did not have or require the \"stare decisis\" doctrine for a range of legal and technological reasons:\n\nThese features changed over time, opening the door to the doctrine of \"stare decisis\":\n\n\"Stare decisis\" applies to the holding of a case, rather than to obiter dicta (\"things said by the way\"). As the United States Supreme Court has put it: \"dicta may be followed if sufficiently persuasive but are not binding.\"\n\nIn the United States Supreme Court, the principle of stare decisis is most flexible in constitutional cases:\n\nFor example, in the years 1946–1992, the U.S. Supreme Court reversed itself in about 130 cases. The U.S. Supreme Court has further explained as follows:\n\nThe United States Supreme Court has stated that where a court gives multiple reasons for a given result, each alternative reason that is \"explicitly\" labeled by the court as an \"independent\" ground for the decision is not treated as \"simply a dictum\".\n\nThe doctrine of binding precedent or \"stare decisis\" is basic to the English legal system. Special features of the English legal system include the following:\n\nThe British House of Lords, as the court of last appeal outside Scotland before it was replaced by the UK Supreme Court, was not strictly bound to always follow its own decisions until the case \"London Street Tramways v London County Council [1898] AC 375\". After this case, once the Lords had given a ruling on a point of law, the matter was closed unless and until Parliament made a change by statute. This is the most strict form of the doctrine of \"stare decisis\" (one not applied, previously, in common law jurisdictions, where there was somewhat greater flexibility for a court of last resort to review its own precedent).\n\nThis situation changed, however, after the issuance of the Practice Statement of 1966. It enabled the House of Lords to adapt English law to meet changing social conditions. In \"R v G & R\" 2003, the House of Lords overruled its decision in \"Caldwell\" 1981, which had allowed the Lords to establish mens rea (\"guilty mind\") by measuring a defendant's conduct against that of a \"reasonable person,\" regardless of the defendant's actual state of mind.\n\nHowever, the Practice Statement has been seldom applied by the House of Lords, usually only as a last resort. As of 2005, the House of Lords has rejected its past decisions no more than 20 times. They are reluctant to use it because they fear to introduce uncertainty into the law. In particular, the Practice Statement stated that the Lords would be especially reluctant to overrule themselves in criminal cases because of the importance of certainty of that law. The first case involving criminal law to be overruled with the Practice Statement was \"Anderton v Ryan\" (1985), which was overruled by \"R v Shivpuri\" (1986), two decades after the Practice Statement. Remarkably, the precedent overruled had been made only a year before, but it had been criticised by several academic lawyers. As a result, Lord Bridge stated he was \"undeterred by the consideration that the decision in \"Anderton v Ryan\" was so recent. The Practice Statement is an effective abandonment of our pretention to infallibility. If a serious error embodied in a decision of this House has distorted the law, the sooner it is corrected the better.\" Still, the House of Lords has remained reluctant to overrule itself in some cases; in \"R v Kansal\" (2002), the majority of House members adopted the opinion that \"R v Lambert\" had been wrongly decided and agreed to depart from their earlier decision.\n\nA precedent does not bind a court if it finds there was a lack of care in the original \"Per Incuriam\". For example, if a statutory provision or precedent had not been brought to the previous court's attention before its decision, the precedent would not be binding.\n\nOne of the most important roles of precedent is to resolve ambiguities in other legal texts, such as constitutions, statutes, and regulations. The process involves, first and foremost, consultation of the plain language of the text, as enlightened by the legislative history of enactment, subsequent precedent, and experience with various interpretations of similar texts.\n\nA judge's normal aids include access to all previous cases in which a precedent has been set, and a good English dictionary.\n\nJudges and barristers in the U.K use three primary rules for interpreting the law.\n\nUnder the literal rule, the judge should do what the actual legislation states rather than trying to do what the judge thinks that it means. The judge should use the plain everyday ordinary meaning of the words, even if this produces an unjust or undesirable outcome. A good example of problems with this method is \"R v Maginnis\" (1987), in which several judges in separate opinions found several different dictionary meanings of the word \"supply\". Another example is \"Fisher v Bell\", where it was held that a shopkeeper who placed an illegal item in a shop window with a price tag did not make an offer to sell it, because of the specific meaning of \"offer for sale\" in contract law. As a result of this case, Parliament amended the statute concerned to end this discrepancy.\n\nThe golden rule is used when use of the literal rule would obviously create an absurd result. The court must find genuine difficulties before it declines to use the literal rule. There are two ways in which the golden rule can be applied: the narrow method, and the broad method. Under the narrow method, when there are apparently two contradictory meanings to a word used in a legislative provision or it is ambiguous, the least absurd is to be used. For example, in \"Adler v George\" (1964), the defendant was found guilty under the Official Secrets Act of 1920. The act said it was an offence to obstruct HM Forces in the vicinity of a prohibited place. Adler argued that he was not in the \"vicinity\" of a prohibited place but was actually \"in\" a prohibited place. The court chose not to accept the wording literally. Under the broad method, the court may reinterpret the law at will when it is clear that there is only one way to read the statute. This occurred in \"Re Sigsworth\" (1935) where a man who murdered his mother was forbidden from inheriting her estate, despite a statute to the contrary.\n\nThe mischief rule is the most flexible of the interpretation methods. Stemming from \"Heydon's Case\" (1584), it allows the court to enforce what the statute is intended to remedy rather than what the words actually say. For example, in \"Corkery v Carpenter\" (1950), a man was found guilty of being drunk in charge of a carriage, although in fact he only had a bicycle.\n\nIn the United States, the courts have stated consistently that the text of the statute is read as it is written, using the ordinary meaning of the words of the statute.\n\nHowever, most legal texts have some lingering ambiguity—inevitably, situations arise in which the words chosen by the legislature do not address the precise facts in issue, or there is some tension among two or more statutes. In such cases, a court must analyze the various available sources, and reach a resolution of the ambiguity. The \"Canons of statutory construction\" are discussed in a separate article. Once the ambiguity is resolved, that resolution has binding effect as described in the rest of this article.\n\nAlthough inferior courts are bound in theory by superior court precedent, in practice a judge may believe that justice requires an outcome at some variance with precedent, and may distinguish the facts of the individual case on reasoning that does not appear in the binding precedent. On appeal, the appellate court may either adopt the new reasoning, or reverse on the basis of precedent. On the other hand, if the losing party does not appeal (typically because of the cost of the appeal), the lower court decision may remain in effect, at least as to the individual parties.\n\nOccasionally, a lower court judge explicitly states personal disagreement with the judgment he or she has rendered, but that he or she is required to do so by binding precedent. Note that inferior courts cannot evade binding precedent of superior courts, but a court can depart from its own prior decisions.\n\nIn the United States, \"stare decisis\" can interact in counterintuitive ways with the federal and state court systems. On an issue of federal law, a state court is not bound by an interpretation of federal law at the district or circuit level, but is bound by an interpretation by the United States Supreme Court. On an interpretation of state law, whether common law or statutory law, the federal courts are bound by the interpretation of a state court of last resort, and are required normally to defer to the precedent of intermediate state courts as well.\n\nCourts may choose to obey precedent of international jurisdictions, but this is not an application of the doctrine of \"stare decisis\", because foreign decisions are not binding. Rather, a foreign decision that is obeyed on the basis of the soundness of its reasoning will be called \"persuasive authority\" — indicating that its effect is limited to the persuasiveness of the reasons it provides.\n\nOriginalism is an approach to interpretation of a legal text in which controlling weight is given to the intent of the original authors (at least the intent as inferred by a modern judge). In contrast, a non-originalist looks at other cues to meaning, including the current meaning of the words, the pattern and trend of other judicial decisions, changing context and improved scientific understanding, observation of practical outcomes and \"what works,\" contemporary standards of justice, and \"stare decisis\". Both are directed at \"interpreting\" the text, not changing it—interpretation is the process of resolving ambiguity and choosing from among possible meanings, not changing the text.\n\nThe two approaches look at different sets of underlying facts that may or may not point in the same direction--\"stare decisis\" gives most weight to the newest understanding of a legal text, while originalism gives most weight to the oldest. While they don't necessarily reach different results in every case, the two approaches are in direct tension. Originalists such as Justice Antonin Scalia argue that \"\"Stare decisis\" is not usually a doctrine used in civil law systems, because it violates the principle that only the legislature may make law.\" Justice Scalia argues that America is a civil law nation, not a common law nation. By principle, originalists are generally unwilling to defer to precedent when precedent seems to come into conflict with the originalist's own interpretation of the Constitutional text or inferences of original intent (even in situations where there is no original source statement of that original intent). However, there is still room within an originalist paradigm for \"stare decisis\"; whenever the plain meaning of the text has alternative constructions, past precedent is generally considered a valid guide, with the qualifier being that it cannot change what the text actually says.\n\nOriginalists vary in the degree to which they defer to precedent. In his confirmation hearings, Justice Clarence Thomas answered a question from Senator Strom Thurmond, qualifying his willingness to change precedent in this way:\n\nPossibly he has changed his mind, or there are a very large body of cases which merit \"the additional step\" of ignoring the doctrine; according to Scalia, \"Clarence Thomas doesn't believe in stare decisis, period. If a constitutional line of authority is wrong, he would say, let's get it right.\"\n\nProfessor Caleb Nelson, a former clerk for Justice Thomas and law professor at the University of Virginia, has elaborated on the role of \"stare decisis\" in originalist jurisprudence:\n\nThere are disadvantages and advantages of binding precedent, as noted by scholars and jurists.\n\nIn a 1997 book, attorney Michael Trotter blamed over-reliance by American lawyers on binding and persuasive authority, rather than the merits of the case at hand, as a major factor behind the escalation of legal costs during the 20th century. He argued that courts should ban the citation of persuasive precedent from outside their jurisdiction, with two exceptions:\n\nThe disadvantages of \"stare decisis\" include its rigidity, the complexity of learning law, the differences between some cases may be very small and appear illogical, and the slow growth or incremental changes to the law that are in need of major overhaul.\n\nAn argument often used against the system is that it is undemocratic as it allows judges, which may or may not be elected, to make law.\n\nRegarding constitutional interpretations, there is concern that over-reliance on the doctrine of \"stare decisis\" can be subversive. An erroneous precedent may at first be only slightly inconsistent with the Constitution, and then this error in interpretation can be propagated and increased by further precedent until a result is obtained that is greatly different from the original understanding of the Constitution. \"Stare decisis\" is not mandated by the Constitution, and if it causes unconstitutional results then the historical evidence of original understanding can be re-examined. In this opinion, predictable fidelity to the Constitution is more important than fidelity to unconstitutional precedent. See also the living tree doctrine.\n\nA counter-argument (in favor of the advantages of \"stare decisis\") is that if the legislature wishes to alter the case law (other than constitutional interpretations) by statute, the legislature is empowered to do so. Critics sometimes accuse particular judges of applying the doctrine selectively, invoking it to support precedent that the judge supported anyway, but ignoring it in order to change precedent with which the judge disagreed.\n\nThere is much discussion about the virtue of using \"stare decisis\". Supporters of the system, such as minimalists, argue that obeying precedent makes decisions \"predictable\". For example, a business person can be reasonably assured of predicting a decision where the facts of his or her case are sufficiently similar to a case decided previously. This parallels the arguments against retroactive (ex post facto) laws banned by the U.S. Constitution.\n\n",
                "Common law\n\nCommon law (also known as judicial precedent or judge-made law, or case law) is that body of law derived from judicial decisions of courts and similar tribunals. The defining characteristic of “common law” is that it arises as precedent. In cases where the parties disagree on what the law is, a common law court looks to past precedential decisions of relevant courts, and synthesizes the principles of those past cases as applicable to the current facts. If a similar dispute has been resolved in the past, the court is usually bound to follow the reasoning used in the prior decision (a principle known as \"stare decisis\"). If, however, the court finds that the current dispute is fundamentally distinct from all previous cases (called a \"matter of first impression\"), and legislative statutes are either silent or ambiguous on the question, judges have the authority and duty to resolve the issue (one party or the other has to win, and on disagreements of law, judges make that decision). The court states an opinion that gives reasons for the decision, and those reasons agglomerate with past decisions as precedent to bind future judges and litigants. Common law, as the body of law made by judges, stands in contrast to and on equal footing with statutes which are adopted through the legislative process, and regulations which are promulgated by the executive branch (the interactions are explained later in this article). \"Stare decisis\", the principle that cases should be decided according to consistent principled rules so that similar facts will yield similar results, lies at the heart of all common law systems..\n\nToday, one-third of the world's population lives in common law jurisdictions or in systems mixed with civil law, including Antigua and Barbuda, Australia, Bahamas, Bangladesh, Barbados, Belize, Botswana, Burma, Cameroon, Canada (both the federal system and all its provinces except Quebec), Cyprus, Dominica, Fiji, Ghana, Grenada, Guyana, Hong Kong, India, Ireland, Israel, Jamaica, Kenya, Liberia, Malaysia, Marshall Islands, Micronesia, Namibia, Nauru, New Zealand, Nigeria, Pakistan, Palau, Papua New Guinea, Sierra Leone, Singapore, South Africa, Sri Lanka, Trinidad and Tobago, the United Kingdom (including its overseas territories such as Gibraltar), the United States (both the federal system and 49 of its 50 states), and Zimbabwe. Some of these countries have variants on common law systems.\n\nThe term \"common law\" has many connotations. The first three set out here are the most-common usages within the legal community. Other connotations from past centuries are sometimes seen, and are sometimes heard in everyday speech.\n\nBlack's Law Dictionary, 10th Ed., gives as definition 1, \"1. The body of law derived from judicial decisions, rather than from statutes or constitutions; [synonym] CASELAW, [contrast] STATUTORY LAW.\" (Black's Law Dictionary is the most-used legal dictionary used among legal professionals in the U.S.) This usage is given as the first definition in modern legal dictionaries, is characterized as the “most common” usage among legal professionals, and is the usage frequently seen in decisions of courts. In this connotation, \"common law\" distinguishes the authority that promulgated a law. For example, the law in most Anglo-American jurisdictions includes \"statutory law\" enacted by a legislature, \"regulatory law\" (in the U.S.) or “delegated legislation” (in the U.K.) promulgated by executive branch agencies pursuant to delegation of rule-making authority from the legislature, and common law or \"case law\", \"i.e.\", decisions issued by courts (or quasi-judicial tribunals within agencies). This first connotation can be further differentiated into\nPublication of decisions, and indexing, is essential to the development of common law, and thus governments and private publishers publish law reports. While all decisions in common law jurisdictions are precedent (at varying levels and scope as discussed throughout the article on precedent), some become \"leading cases\" or \"landmark decisions\" that are cited especially often.\n\nBlack's 10th Ed., definition 2, differentiates \"common law\" jurisdictions and legal systems from \"civil law\" or \"code\" jurisdictions.<ref name=\"GarnerUsageDef1\"/ Common law systems place great weight on court decisions, which are considered \"law\" with the same force of law as statutes—for nearly a millennium, common law courts have had the authority to make law where no legislative statute exists, and statutes mean what courts interpret them to mean.\n\nBy contrast, in civil law jurisdictions (the legal tradition that prevails, or is combined with common law, in Europe and most non-Islamic, non-common law countries), courts lack authority to act if there is no statute. Judicial precedent is given less interpretive weight, which means that a judge deciding a given case has more freedom to interpret the text of a statute independently, and less predictably. For example, the Napoleonic code expressly forbade French judges to pronounce general principles of law. The role of providing overarching principles, which in common law jurisdictions is provided in judicial opinions, in civil law jurisdictions is filled by giving greater weight to scholarly literature, as explained below.\n\nCommon law systems trace their history to England, while civil law systems trace their history through the Napoleonic Code back to the Corpus Juris Civilis of Roman law.\n\nBlack's 10th Ed., definition 4, differentiates \"common law\" (or just \"law\") from \"equity\". Additional legal dictionary cites include. Before 1873, England had two complementary court systems: courts of \"law\" which could only award money damages and recognized only the legal owner of property, and courts of \"equity\" (courts of chancery) that could issue injunctive relief (that is, a court order to a party to do something, give something to someone, or stop doing something) and recognized trusts of property. This split propagated to many of the colonies, including the United States. For most purposes, most jurisdictions, including the U.S. federal system and most states, have merged the two courts. Additionally, even before the separate courts were merged, most courts were permitted to apply both law and equity, though under potentially different procedural law. Nonetheless, the historical distinction between \"law\" and \"equity\" remains important today when the case involves issues such as the following:\nCourts of equity rely on common law principles of binding precedent.\n\nIn addition, there are several historical (but now archaic) uses of the term that, while no longer current, provide background context that assists in understanding the meaning of \"common law\" today.\n\nIn one usage that is now archaic, but that gives insight into the history of the common law, \"common law\" referred to the pre-Christian system of law, imported by the Saxons to England, and dating to before the Norman conquest, and before there was any consistent law to be applied. That usage is obsolete today. It is both underinclusive and overinclusive, as discussed in the section on \"misconceptions\".\n\n\"Common law\" as the term is used today in common law countries contrasts with \"ius commune\". While historically the \"ius commune\" became a secure point of reference in continental European legal systems, in England it was not a point of reference at all.\n\nThe English Court of Common Pleas dealt with lawsuits in which the Monarch had no interest, i.e., between commoners.\n\nBlack's definition 3 is \"3. General law common to a country as a whole, as opposed to special law that has only local application.\" From at least the 11th century and continuing for several centuries after that, there were several different circuits in the royal court system, served by itinerant judges who would travel from town to town dispensing the King's justice in \"assizes\". The term \"common law\" was used to describe the law held in common between the circuits and the different stops in each circuit. The more widely a particular law was recognized, the more weight it held, whereas purely local customs were generally subordinate to law recognized in a plurality of jurisdictions.\n\nA number of misconceptions of the term \"common law\" exist in popular culture and non-lawyer sources.\n\nIn a common law jurisdiction several stages of research and analysis are required to determine \"what the law is\" in a given situation. First, one must ascertain the facts. Then, one must locate any relevant statutes and cases. Then one must extract the principles, analogies and statements by various courts of what they consider important to determine how the next court is likely to rule on the facts of the present case. Later decisions, and decisions of higher courts or legislatures carry more weight than earlier cases and those of lower courts. Finally, one integrates all the lines drawn and reasons given, and determines \"what the law is\". Then, one applies that law to the facts.\n\nIn practice, common law systems are considerably more complicated than the simplified system described above. The decisions of a court are binding only in a particular jurisdiction, and even within a given jurisdiction, some courts have more power than others. For example, in most jurisdictions, decisions by appellate courts are binding on lower courts in the same jurisdiction, and on future decisions of the same appellate court, but decisions of lower courts are only non-binding persuasive authority. Interactions between common law, constitutional law, statutory law and regulatory law also give rise to considerable complexity.\n\nOliver Wendell Holmes, Jr. cautioned that \"the proper derivation of general principles in both common and constitutional law ... arise gradually, in the emergence of a consensus from a multitude of particularized prior decisions.\" Justice Cardozo noted the \"common law does not work from pre-established truths of universal and inflexible validity to conclusions derived from them deductively\", but \"[i]ts method is inductive, and it draws its generalizations from particulars\".\n\nThe common law is more malleable than statutory law. First, common law courts are not absolutely bound by precedent, but can (when extraordinarily good reason is shown) reinterpret and revise the law, without legislative intervention, to adapt to new trends in political, legal and social philosophy. Second, the common law evolves through a series of gradual steps, that gradually works out all the details, so that over a decade or more, the law can change substantially but without a sharp break, thereby reducing disruptive effects. In contrast to common law incrementalism, the legislative process is very difficult to get started, as legislatures tend to delay action until a situation is totally intolerable. For these reasons, legislative changes tend to be large, jarring and disruptive (sometimes positively, sometimes negatively, and sometimes with unintended consequences).\n\nOne example of the gradual change that typifies evolution of the common law is the gradual change in liability for negligence. The traditional common law rule through most of the 19th century was that a plaintiff could not recover for a defendant's negligent production or distribution of a harmful instrumentality unless the two were in privity of contract. Thus, only the immediate purchaser could recover for a product defect, and if a part was built up out of parts from parts manufacturers, the ultimate buyer could not recover for injury caused by a defect in the part. In an 1842 English case, \"Winterbottom v. Wright\", the postal service had contracted with Wright to maintain its coaches. Winterbottom was a driver for the post. When the coach failed and injured Winterbottom, he sued Wright. The \"Winterbottom\" court recognized that there would be \"absurd and outrageous consequences\" if an injured person could sue any person peripherally involved, and knew it had to draw a line somewhere, a limit on the causal connection between the negligent conduct and the injury. The court looked to the contractual relationships, and held that liability would only flow as far as the person in immediate contract (\"privity\") with the negligent party.\n\nA first exception to this rule arose in 1852, in the case of \"Thomas v. Winchester\", when New York's highest court held that mislabeling a poison as an innocuous herb, and then selling the mislabeled poison through a dealer who would be expected to resell it, put \"human life in imminent danger\". \"Thomas\" relied on this reason to create an exception to the \"privity\" rule. In, 1909, New York held in \"Statler v. Ray Mfg. Co.\" that a coffee urn manufacturer was liable to a person injured when the urn exploded, because the urn \"was of such a character inherently that, when applied to the purposes for which it was designed, it was liable to become a source of great danger to many people if not carefully and properly constructed\".\n\nYet the privity rule survived. In \"Cadillac Motor Car Co. v. Johnson\", (decided in 1915 by the federal appeals court for New York and several neighboring states), the court held that a car owner could not recover for injuries from a defective wheel, when the automobile owner had a contract only with the automobile dealer and not with the manufacturer, even though there was \"no question that the wheel was made of dead and ‘dozy‘ wood, quite insufficient for its purposes.\" The \"Cadillac\" court was willing to acknowledge that the case law supported exceptions for \"an article dangerous in its nature or likely to become so in the course of the ordinary usage to be contemplated by the vendor\". However, held the \"Cadillac\" court, \"one who manufactures articles dangerous only if defectively made, or installed, e.g., tables, chairs, pictures or mirrors hung on the walls, carriages, automobiles, and so on, is not liable to third parties for injuries caused by them, except in case of willful injury or fraud,\"\n\nFinally, in the famous case of \"MacPherson v. Buick Motor Co.\", in 1916, Judge Benjamin Cardozo for New York's highest court pulled a broader principle out of these predecessor cases. The facts were almost identical to \"Cadillac\" a year earlier: a wheel from a wheel manufacturer was sold to Buick, to a dealer, to MacPherson, and the wheel failed, injuring MacPherson. Judge Cardozo held:\n\nCardozo's new \"rule\" exists in no prior case, but is inferrable as a synthesis of the \"thing of danger\" principle stated in them, merely extending it to \"foreseeable danger\" even if \"the purposes for which it was designed\" were not themselves \"a source of great danger\". \"MacPherson\" takes some care to present itself as foreseeable progression, not a wild departure. Cardozo continues to adhere to the original principle of \"Winterbottom\", that \"absurd and outrageous consequences\" must be avoided, and he does so by drawing a new line in the last sentence quoted above: \"There must be knowledge of a danger, not merely possible, but probable.\" But while adhering to the underlying principle that \"some\" boundary is necessary, \"MacPherson\" overruled the prior common law by rendering the formerly dominant factor in the boundary, that is, the privity formality arising out of a contractual relationship between persons, totally irrelevant. Rather, the most important factor in the boundary would be the nature of the thing sold and the foreseeable uses that downstream purchasers would make of the thing.\n\nThe example of the evolution of the law of negligence in the preceding paragraphs illustrates two crucial principles: (a) The common law evolves, this evolution is in the hands of judges, and judges have \"made law\" for hundreds of years. (b) The reasons given for a decision are often more important in the long run than the outcome in a particular case. This is the reason that judicial opinions are usually quite long, and give rationales and policies that can be balanced with judgment in future cases, rather than the bright-line rules usually embodied in statutes.\n\nAll law systems rely on written publication of the law, so that it is accessible to all. Common law decisions are published in law reports for use by lawyers, courts and the general public.\n\nAfter the American Revolution, Massachusetts became the first state to establish an official Reporter of Decisions. As newer states needed law, they often looked first to the Massachusetts Reports for authoritative precedents as a basis for their own common law. The United States federal courts relied on private publishers until after the Civil War, and only began publishing as a government function in 1874. West Publishing in Minnesota is the largest private-sector publisher of law reports in the United States. Government publishers typically issue only decisions \"in the raw,\" while private sector publishers often add indexing, editorial analysis, and similar finding aids.\n\nIn common law legal systems, the common law is crucial to understanding almost all important areas of law. For example, in England and Wales, in English Canada, and in most states of the United States, the basic law of contracts, torts and property do not exist in statute, but only in common law (though there may be isolated modifications enacted by statute). As another example, the Supreme Court of the United States in 1877, held that a Michigan statute that established rules for solemnization of marriages did not abolish pre-existing common-law marriage, because the statute did not affirmatively require statutory solemnization and was silent as to preexisting common law.\n\nIn almost all areas of the law (even those where there is a statutory framework, such as contracts for the sale of goods, or the criminal law), legislature-enacted statutes generally give only terse statements of general principle, and the fine boundaries and definitions exist only in the interstitial common law. To find out what the precise law is that applies to a particular set of facts, one has to locate precedential decisions on the topic, and reason from those decisions by analogy.\n\nIn (common law jurisdictions (in the sense opposed to \"civil law\"), legislatures operate under the assumption that statutes will be interpreted against the backdrop of the pre-existing common law. As the United States Supreme Court explained in \"United States v Texas\", 507 U.S. 529 (1993):\n\nFor example, in most U.S. states, the criminal statutes are primarily codification of pre-existing common law. (Codification is the process of enacting a statute that collects and restates pre-existing law in a single document—when that pre-existing law is common law, the common law remains relevant to the interpretation of these statutes.) In reliance on this assumption, modern statutes often leave a number of terms and fine distinctions unstated—for example, a statute might be very brief, leaving the precise definition of terms unstated, under the assumption that these fine distinctions will be inherited from pre-existing common law. (For this reason, many modern American law schools teach the common law of crime as it stood in England in 1789, because that centuries-old English common law is a necessary foundation to interpreting modern criminal statutes.)\n\nWith the transition from English law, which had common law crimes, to the new legal system under the U.S. Constitution, which prohibited \"ex post facto\" laws at both the federal and state level, the question was raised whether there could be common law crimes in the United States. It was settled in the case of \"United States v. Hudson\", which decided that federal courts had no jurisdiction to define new common law crimes, and that there must always be a (constitutional) statute defining the offense and the penalty for it.\n\nStill, many states retain selected common law crimes. For example, in Virginia, the definition of the conduct that constitutes the crime of robbery exists only in the common law, and the robbery statute only sets the punishment. Virginia Code section 1-200 establishes the continued existence and vitality of common law principles and provides that \"The common law of England, insofar as it is not repugnant to the principles of the Bill of Rights and Constitution of this Commonwealth, shall continue in full force within the same, and be the rule of decision, except as altered by the General Assembly.\"\n\nBy contrast to statutory codification of common law, some statutes displace common law, for example to create a new cause of action that did not exist in the common law, or to legislatively overrule the common law. An example is the tort of wrongful death, which allows certain persons, usually a spouse, child or estate, to sue for damages on behalf of the deceased. There is no such tort in English common law; thus, any jurisdiction that lacks a wrongful death statute will not allow a lawsuit for the wrongful death of a loved one. Where a wrongful death statute exists, the compensation or other remedy available is limited to the remedy specified in the statute (typically, an upper limit on the amount of damages). Courts generally interpret statutes that create new causes of action narrowly—that is, limited to their precise terms—because the courts generally recognize the legislature as being supreme in deciding the reach of judge-made law unless such statute should violate some \"second order\" constitutional law provision (\"cf\". judicial activism).\n\nWhere a tort is rooted in common law, all traditionally recognized damages for that tort may be sued for, whether or not there is mention of those damages in the current statutory law. For instance, a person who sustains bodily injury through the negligence of another may sue for medical costs, pain, suffering, loss of earnings or earning capacity, mental and/or emotional distress, loss of quality of life, disfigurement and more. These damages need not be set forth in statute as they already exist in the tradition of common law. However, without a wrongful death statute, most of them are extinguished upon death.\n\nIn the United States, the power of the federal judiciary to review and invalidate unconstitutional acts of the federal executive branch is stated in the constitution, Article III sections 1 and 2: \"The judicial Power of the United States, shall be vested in one supreme Court, and in such inferior Courts as the Congress may from time to time ordain and establish. ... The judicial Power shall extend to all Cases, in Law and Equity, arising under this Constitution, the Laws of the United States, and Treaties made, or which shall be made, under their Authority...\" The first landmark decision on \"the judicial power\" was \"Marbury v. Madison\", . Later cases interpreted the \"judicial power\" of Article III to establish the power of federal courts to consider or overturn any action of Congress or of any state that conflicts with the Constitution.\n\nThe interactions between decisions of different courts is discussed further in the article on precedent.\n\nThe United States federal courts are divided into twelve regional circuits, each with a circuit court of appeals (plus a thirteenth, the Court of Appeals for the Federal Circuit, which hears appeals in patent cases and cases against the federal government, without geographic limitation). Decisions of one circuit court are binding on the district courts within the circuit and on the circuit court itself, but are only persuasive authority on sister circuits. District court decisions are not binding precedent at all, only persuasive.\n\nMost of the U.S. federal courts of appeal have adopted a rule under which, in the event of any conflict in decisions of panels (most of the courts of appeal almost always sit in panels of three), the earlier panel decision is controlling, and a panel decision may only be overruled by the court of appeals sitting \"en banc\" (that is, all active judges of the court) or by a higher court. In these courts, the older decision remains controlling when an issue comes up the third time.\n\nOther courts, for example, the Court of Customs and Patent Appeals and the Supreme Court, always sit \"en banc\", and thus the \"later\" decision controls. These courts essentially overrule all previous cases in each new case, and older cases survive only to the extent they do not conflict with newer cases. The interpretations of these courts—for example, Supreme Court interpretations of the constitution or federal statutes—are stable only so long as the older interpretation maintains the support of a majority of the court. Older decisions persist through some combination of belief that the old decision is right, and that it is not sufficiently wrong to be overruled.\n\nIn the UK, since 2009, the Supreme Court of the United Kingdom has the authority to overrule and unify decisions of lower courts. From 1966 to 2009, this power lay with the House of Lords, granted by the Practice Statement of 1966.\n\nCanada's federal system, described below, avoids regional variability of federal law by giving national jurisdiction to both layers of appellate courts.\n\nThe reliance on judicial opinion is a strength of common law systems, and is a significant contributor to the robust commercial systems in the United Kingdom and United States. Because there is reasonably precise guidance on almost every issue, parties (especially commercial parties) can predict whether a proposed course of action is likely to be lawful or unlawful, and have some assurance of consistency. As Justice Brandeis famously expressed it, \"in most matters it is more important that the applicable rule of law be settled than that it be settled right.\" This ability to predict gives more freedom to come close to the boundaries of the law. For example, many commercial contracts are more economically efficient, and create greater wealth, because the parties know ahead of time that the proposed arrangement, though perhaps close to the line, is almost certainly legal. Newspapers, taxpayer-funded entities with some religious affiliation, and political parties can obtain fairly clear guidance on the boundaries within which their freedom of expression rights apply.\n\nIn contrast, in jurisdictions with very weak respect for precedent, fine questions of law are redetermined anew each time they arise, making consistency and prediction more difficult, and procedures far more protracted than necessary because parties cannot rely on written statements of law as reliable guides. In jurisdictions that do not have a strong allegiance to a large body of precedent, parties have less \"a priori\" guidance (unless the written law is very clear and kept updated) and must often leave a bigger \"safety margin\" of unexploited opportunities, and final determinations are reached only after far larger expenditures on legal fees by the parties.\n\nThis is the reason for the frequent choice of the law of the State of New York in commercial contracts, even when neither entity has extensive contacts with New York—and remarkably often even when neither party has contacts with the United States. Commercial contracts almost always include a \"choice of law clause\" to reduce uncertainty. Somewhat surprisingly, contracts throughout the world (for example, contracts involving parties in Japan, France and Germany, and from most of the other states of the United States) often choose the law of New York, even where the relationship of the parties and transaction to New York is quite attenuated. Because of its history as the United States' commercial center, New York common law has a depth and predictability not (yet) available in any other jurisdictions of the United States. Similarly, American corporations are often formed under Delaware corporate law, and American contracts relating to corporate law issues (merger and acquisitions of companies, rights of shareholders, and so on.) include a Delaware choice of law clause, because of the deep body of law in Delaware on these issues. On the other hand, some other jurisdictions have sufficiently developed bodies of law so that parties have no real motivation to choose the law of a foreign jurisdiction (for example, England and Wales, and the state of California), but not yet so fully developed that parties with no relationship to the jurisdiction choose that law. Outside the United States, parties that are in different jurisdictions from each other often choose the law of England and Wales, particularly when the parties are each in former British colonies and members of the Commonwealth. The common theme in all cases is that commercial parties seek predictability and simplicity in their contractual relations, and frequently choose the law of a common law jurisdiction with a well-developed body of common law to achieve that result.\n\nLikewise, for litigation of commercial disputes arising out of unpredictable torts (as opposed to the prospective choice of law clauses in contracts discussed in the previous paragraph), certain jurisdictions attract an unusually high fraction of cases, because of the predictability afforded by the depth of decided cases. For example, London is considered the pre-eminent centre for litigation of admiralty cases.\n\nThis is not to say that common law is better in every situation. For example, civil law can be clearer than case law when the legislature has had the foresight and diligence to address the precise set of facts applicable to a particular situation. For that reason, civil law statutes tend to be somewhat more detailed than statutes written by common law legislatures—but, conversely, that tends to make the statute more difficult to read (the United States tax code is an example).\n\nThe common lawso named because it was \"common\" to all the king's courts across Englandoriginated in the practices of the courts of the English kings in the centuries following the Norman Conquest in 1066. Prior to the Norman Conquest, much of England's legal business took place in the local folk courts of its various counties and hundreds. A variety of other individual courts also existed across the land: urban boroughs and merchant fairs held their own courts, as did the universities of Oxford and Cambridge, and large landholders also held their own manorial and seigniorial courts as needed. Additionally, the Catholic Church operated its own court system that adjudicated issues of canon law.\n\nThe main sources for the history of the common law in the Middle Ages are the plea rolls and the Year Books. The plea rolls, which were the official court records for the Courts of Common Pleas and King's Bench, were written in Latin. The rolls were made up in bundles by law term: Hilary, Easter, Trinity, and Michaelmas, or winter, spring, summer, and autumn. They are currently deposited in the UK National Archives, by whose permission images of the rolls for the Courts of Common Pleas, King's Bench, and Exchequer of Pleas, from the 13th century to the 17th, can be viewed online at the Anglo-American Legal Tradition site (The O'Quinn Law Library of the University of Houston Law Center).\n\nThe doctrine of precedent developed during the 12th and 13th centuries, as the collective judicial decisions that were based in tradition, custom and precedent.\n\nThe form of reasoning used in common law is known as casuistry or case-based reasoning. The common law, as applied in civil cases (as distinct from criminal cases), was devised as a means of compensating someone for wrongful acts known as torts, including both intentional torts and torts caused by negligence, and as developing the body of law recognizing and regulating contracts. The type of procedure practiced in common law courts is known as the adversarial system; this is also a development of the common law.\n\nThe early development of case-law in the thirteenth century has been traced to Bracton's \"On the Laws and Customs of England\" and led to the yearly compilations of court cases known as Year Books, of which the first extant was published in 1268, the same year that Bracton died. The Year Books are known as the law reports of medieval England, and are a principal source for knowledge of the developing legal doctrines, concepts, and methods in the period from the 13th to the 16th centuries, when the common law developed into recognizable form.\nIn 1154, Henry II became the first Plantagenet king. Among many achievements, Henry institutionalized common law by creating a unified system of law \"common\" to the country through incorporating and elevating local custom to the national, ending local control and peculiarities, eliminating arbitrary remedies and reinstating a jury system—citizens sworn on oath to investigate reliable criminal accusations and civil claims. The jury reached its verdict through evaluating common local knowledge, not necessarily through the presentation of evidence, a distinguishing factor from today's civil and criminal court systems.\n\nHenry II developed the practice of sending judges from his own central court to hear the various disputes throughout the country. His judges would resolve disputes on an ad hoc basis according to what they interpreted the customs to be. The king's judges would then return to London and often discuss their cases and the decisions they made with the other judges. These decisions would be recorded and filed. In time, a rule, known as \"stare decisis\" (also commonly known as precedent) developed, whereby a judge would be bound to follow the decision of an earlier judge; he was required to adopt the earlier judge's interpretation of the law and apply the same principles promulgated by that earlier judge if the two cases had similar facts to one another. Once judges began to regard each other's decisions to be binding precedent, the pre-Norman system of local customs and law varying in each locality was replaced by a system that was (at least in theory, though not always in practice) common throughout the whole country, hence the name \"common law\".\n\nHenry II's creation of a powerful and unified court system, which curbed somewhat the power of canonical (church) courts, brought him (and England) into conflict with the church, most famously with Thomas Becket, the Archbishop of Canterbury. The murder of the Archbishop gave rise to a wave of popular outrage against the King. Henry was forced to repeal the disputed laws and to abandon his efforts to hold church members accountable for secular crimes (see also Constitutions of Clarendon).\n\nThe English Court of Common Pleas was established after Magna Carta to try lawsuits between commoners in which the monarch had no interest. Its judges sat in open court in the Great Hall of the king's Palace of Westminster, permanently except in the vacations between the four terms of the Legal year.\n\nJudge-made common law operated as the primary source of law for several hundred years, before Parliament acquired legislative powers to create statutory law. It is important to understand that common law is the older and more traditional source of law, and legislative power is simply a layer applied on top of the older common law foundation. Since the 12th century, courts have had parallel and co-equal authority to make law—\"legislating from the bench\" is a traditional and essential function of courts, which was carried over into the U.S. system as an essential component of the \"judicial power\" specified by Article III of the U.S. constitution. Justice Oliver Wendell Holmes, Jr. summarized centuries of history in 1917, \"judges do and must legislate.\" There are legitimate debates on how the powers of courts and legislatures should be balanced. However, a view that courts lack law-making power is historically inaccurate and constitutionally unsupportable.\n\nThe term \"common law\" is often used as a contrast to Roman-derived \"civil law\", and the fundamental processes and forms of reasoning in the two are quite different. Nonetheless, there has been considerable cross-fertilization of ideas, while the two traditions and sets of foundational principles remain distinct.\n\nBy the time of the rediscovery of the Roman law in Europe in the 12th and 13th centuries, the common law had already developed far enough to prevent a Roman law reception as it occurred on the continent. However, the first common law scholars, most notably Glanvill and Bracton, as well as the early royal common law judges, had been well accustomed with Roman law. Often, they were clerics trained in the Roman canon law. One of the first and throughout its history one of the most significant treatises of the common law, Bracton's \"De Legibus et Consuetudinibus Angliae\" (On the Laws and Customs of England), was heavily influenced by the division of the law in Justinian's \"Institutes\". The impact of Roman law had decreased sharply after the age of Bracton, but the Roman divisions of actions into \"in rem\" (typically, actions against a \"thing\" or property for the purpose of gaining title to that property; must be filed in a court where the property is located) and \"in personam\" (typically, actions directed against a person; these can affect a person's rights and, since a person often owns things, his property too) used by Bracton had a lasting effect and laid the groundwork for a return of Roman law structural concepts in the 18th and 19th centuries. Signs of this can be found in Blackstone's \"Commentaries on the Laws of England\", and Roman law ideas regained importance with the revival of academic law schools in the 19th century. As a result, today, the main systematic divisions of the law into property, contract, and tort (and to some extent unjust enrichment) can be found in the civil law as well as in the common law.\n\nThe first attempt at a comprehensive compilation of centuries of common law was by Lord Chief Justice Edward Coke, in his treatise, \"Institutes of the Lawes of England\" in the 17th century.\n\nThe next definitive historical treatise on the common law is \"Commentaries on the Laws of England\", written by Sir William Blackstone and first published in 1765–1769.\n\nA reception statute is a statutory law adopted as a former British colony becomes independent, by which the new nation adopts (i.e. receives) pre-independence common law, to the extent not explicitly rejected by the legislative body or constitution of the new nation. Reception statutes generally consider the English common law dating prior to independence, and the precedent originating from it, as the default law, because of the importance of using an extensive and predictable body of law to govern the conduct of citizens and businesses in a new state. All U.S. states, with the partial exception of Louisiana, have either implemented reception statutes or adopted the common law by judicial opinion.\n\nOther examples of reception statutes in the United States, the states of the U.S., Canada and its provinces, and Hong Kong, are discussed in the reception statute article.\n\nYet, adoption of the common law in the newly-independent nation was not a foregone conclusion, and was controversial. Immediately after the American Revolution, there was widespread distrust and hostility to anything British, and the common law was no exception. Jeffersonians decried lawyers and their common law tradition as threats to the new republic. The Jeffersonians preferred a legislatively-enacted civil law under the control of the political process, rather than the common law developed by judges that—by design—were insulated from the political process. The Federalists believed that the common law was the birthright of Independence: after all, the natural rights to \"life, liberty, and the pursuit of happiness\" were the rights protected by common law. Even advocates for the common law approach noted that it was not an ideal fit for the newly-independent colonies: judges and lawyers alike were severely hindered by a lack of printed legal materials. Before Independence, the most comprehensive law libraries had been maintained by Tory lawyers, and those libraries vanished with the loyalist expatriation, and the ability to print books was limited. Lawyer (later president) John Adams complained that he \"suffered very much for the want of books\". To bootstrap this most basic need of a common law system—knowable, written law—in 1803, lawyers in Massachusetts donated their books to found a law library. A Jeffersonian newspaper criticized the library, as it would carry forward \"all the old authorities practiced in England for centuries back ... whereby a new system of jurisprudence [will be founded] on the high monarchical system [to] become the Common Law of this Commonwealth... [The library] may hereafter have a very unsocial purpose.\"\n\nWell into the 19th century, ancient maxims played a large role in common law adjudication. Many of these maxims had originated in Roman Law, migrated to England before the introduction of Christianity to the British Isles, and were typically stated in Latin even in English decisions. Many examples are familiar in everyday speech even today, \"One cannot be a judge in one's own cause\" (see Dr. Bonham's Case), rights are reciprocal to obligations, and the like. Judicial decisions and treatises of the 17th and 18th centuries, such at those of Lord Chief Justice Edward Coke, presented the common law as a collection of such maxims.\n\nReliance on old maxims and rigid adherence to precedent, no matter how old or ill-considered, came under critical discussion in the late 19th century, starting in the United States. Oliver Wendell Holmes, Jr. in his famous article, \"The Path of the Law\", commented, \"It is revolting to have no better reason for a rule of law than that so it was laid down in the time of Henry IV. It is still more revolting if the grounds upon which it was laid down have vanished long since, and the rule simply persists from blind imitation of the past.\" Justice Holmes noted that study of maxims might be sufficient for \"the man of the present\", but \"the man of the future is the man of statistics and the master of economics\". In an 1880 lecture at Harvard, he wrote:\n\nThe life of the law has not been logic; it has been experience. The felt necessities of the time, the prevalent moral and political theories, intuitions of public policy, avowed or unconscious, even the prejudices which judges share with their fellow men, have had a good deal more to do than the syllogism in determining the rules by which men should be governed. The law embodies the story of a nation's development through many centuries, and it cannot be dealt with as if it contained only the axioms and corollaries of a book of mathematics.\n\nIn the early 20th century, Louis Brandeis, later appointed to the United States Supreme Court, became noted for his use of policy-driving facts and economics in his briefs, and extensive appendices presenting facts that lead a judge to the advocate's conclusion. By this time, briefs relied more on facts than on Latin maxims.\n\nReliance on old maxims is now deprecated. Common law decisions today reflect both precedent and policy judgment drawn from economics, the social sciences, business, decisions of foreign courts, and the like. The degree to which these external factors \"should\" influence adjudication is the subject of active debate, but it is indisputable that judges \"do\" draw on experience and learning from everyday life, from other fields, and from other jurisdictions.\n\nAs early as the 15th century, it became the practice that litigants who felt they had been cheated by the common law system would petition the King in person. For example, they might argue that an award of damages (at common law (as opposed to equity)) was not sufficient redress for a trespasser occupying their land, and instead request that the trespasser be evicted. From this developed the system of equity, administered by the Lord Chancellor, in the courts of chancery. By their nature, equity and law were frequently in conflict and litigation would frequently continue for years as one court countermanded the other, even though it was established by the 17th century that equity should prevail.\n\nIn England, courts of law (as opposed to equity) were combined with courts of equity by the Judicature Acts of 1873 and 1875, with equity prevailing in case of conflict.\n\nIn the United States, parallel systems of law (providing money damages, with cases heard by a jury upon either party's request) and equity (fashioning a remedy to fit the situation, including injunctive relief, heard by a judge) survived well into the 20th century. The United States federal courts procedurally separated law and equity: the same judges could hear either kind of case, but a given case could only pursue causes in law or in equity, and the two kinds of cases proceeded under different procedural rules. This became problematic when a given case required both money damages and injunctive relief. In 1937, the new Federal Rules of Civil Procedure combined law and equity into one form of action, the \"civil action\". Fed.R.Civ.P. . The distinction survives to the extent that issues that were \"common law (as opposed to equity)\" as of 1791 (the date of adoption of the Seventh Amendment) are still subject to the right of either party to request a jury, and \"equity\" issues are decided by a judge.\n\nDelaware, Mississippi, and Tennessee still have separate courts of law and equity, for example, the Court of Chancery. In many states there are separate divisions for law and equity within one court.\n\nFor centuries, through the 19th century, the common law recognized only specific forms of action, and required very careful drafting of the opening pleading (called a writ) to slot into exactly one of them: Debt, Detinue, Covenant, Special Assumpsit, General Assumpsit, Trespass, Trover, Replevin, Case (or Trespass on the Case), and Ejectment. To initiate a lawsuit, a pleading had to be drafted to meet myriad technical requirements: correctly categorizing the case into the correct legal pigeonhole (pleading in the alternative was not permitted), and using specific \"magic words\" encrusted over the centuries. Under the old common law pleading standards, a suit by a \"pro se\" (\"for oneself,\" without a lawyer) party was all but impossible, and there was often considerable procedural jousting at the outset of a case over minor wording issues.\n\nOne of the major reforms of the late 19th century and early 20th century was the abolition of common law pleading requirements. A plaintiff can initiate a case by giving the defendant \"a short and plain statement\" of facts that constitute an alleged wrong. This reform moved the attention of courts from technical scrutiny of words to a more rational consideration of the facts, and opened access to justice far more broadly.\n\nThe main alternative to the common law system is the civil law system, which is used in Continental Europe, and most of the rest of the world.\n\nThe primary contrast between the two systems is the role of written decisions and precedent.\n\nIn common law jurisdictions, nearly every case that presents a \"bona fide\" disagreement on the law is resolved in a written opinion. The legal reasoning for the decision, known as \"ratio decidendi\", not only determines the court's judgment between the parties, but also stands as precedent for resolving future disputes. In contrast, civil law decisions typically do not include explanatory opinions, and thus no precedent flows from one decision to the next.\nIn common law systems, a single decided case is binding common law (connotation 1) to the same extent as statute or regulation, under the principle of \"stare decisis\". In contrast, in civil law systems, individual decisions have only advisory, not binding effect. In civil law systems, case law only acquires weight when a long series of cases use consistent reasoning, called \"jurisprudence constante\". Civil law lawyers consult case law to obtain their best prediction of how a court will rule, but comparatively, civil law judges are less bound to follow it.\n\nFor that reason, statutes in civil law systems are more comprehensive, detailed, and continuously updated, covering all matters capable of being brought before a court.\n\nCommon law systems tend to give more weight to separation of powers between the judicial branch and the executive branch. In contrast, civil law systems are typically more tolerant of allowing individual officials to exercise both powers. One example of this contrast is the difference between the two systems in allocation of responsibility between prosecutor and adjudicator.\n\nCommon law courts usually use an adversarial system, in which two sides present their cases to a neutral judge. In contrast, in civil law systems, criminal proceedings proceed under an inquisitorial system in which an examining magistrate serves two roles by developing the evidence and arguments for one side and then the other during the investigation phase.\n\nThe examining magistrate then presents the dossier detailing his or her findings to the president of the bench that will adjudicate on the case where it has been decided that a trial shall be conducted. Therefore, the president of the bench's view of the case is not neutral and may be biased while conducting the trial after the reading of the dossier. Unlike the common law proceedings, the president of the bench in the inquisitorial system is not merely an umpire and is entitled to directly interview the witnesses or express comments during the trial, as long as he or she does not express his or her view on the guilt of the accused.\n\nThe proceeding in the inquisitorial system is essentially by writing. Most of the witnesses would have given evidence in the investigation phase and such evidence will be contained in the dossier under the form of police reports. In the same way, the accused would have already put his or her case at the investigation phase but he or she will be free to change her or his evidence at trial. Whether the accused pleads guilty or not, a trial will be conducted. Unlike the adversarial system, the conviction and sentence to be served (if any) will be released by the trial jury together with the president of the trial bench, following their common deliberation.\n\nThere are many exceptions in both directions. For example, most proceedings before U.S. federal and state agencies are inquisitorial in nature, at least the initial stages (\"e.g.\", a patent examiner, a social security hearing officer, and so on), even though the law to be applied is developed through common law processes.\n\nThe role of the legal academy presents a significant \"cultural\" difference between common law (connotation 2) and civil law jurisdictions. In both systems, treatises compile decisions and state overarching principles that (in the author's opinion) explain the results of the cases. In neither system are treatises considered \"law,\" but the weight given them is nonetheless quite different.\n\nIn common law jurisdictions, lawyers and judges tend to use these treatises as only \"finding aids\" to locate the relevant cases. In common law jurisdictions, scholarly work is seldom cited as authority for what the law is. Chief Justice Roberts noted the \"great disconnect between the academy and the profession.\" When common law courts rely on scholarly work, it is almost always only for factual findings, policy justification, or the history and evolution of the law, but the court's legal conclusion is reached through analysis of relevant statutes and common law, seldom scholarly commentary.\n\nIn contrast, in civil law jurisdictions, courts give the writings of law professors significant weight, partly because civil law decisions traditionally were very brief, sometimes no more than a paragraph stating who wins and who loses. The rationale had to come from somewhere else: the academy often filled that role.\n\nThe contrast between civil law and common law legal systems has become increasingly blurred, with the growing importance of jurisprudence (similar to case law but not binding) in civil law countries, and the growing importance of statute law and codes in common law countries.\n\nExamples of common law being replaced by statute or codified rule in the United States include criminal law (since 1812, U.S. federal courts and most but not all of the States have held that criminal law must be embodied in statute if the public is to have fair notice), commercial law (the Uniform Commercial Code in the early 1960s) and procedure (the Federal Rules of Civil Procedure in the 1930s and the Federal Rules of Evidence in the 1970s). But note that in each case, the statute sets the general principles, but the interstitial common law process determines the scope and application of the statute.\n\nAn example of convergence from the other direction is shown in the 1982 decision \"Srl CILFIT and Lanificio di Gavardo SpA v Ministry of Health\" (), in which the European Court of Justice held that questions it has already answered need not be resubmitted. This showed how a historically distinctly common law principle is used by a court composed of judges (at that time) of essentially civil law jurisdiction.\n\nThe former Soviet Bloc and other Socialist countries used a Socialist law system.\n\nMuch of the Muslim world uses Sharia (also called Islamic law).\n\nThe common law constitutes the basis of the legal systems of:\n\nand many other generally English-speaking countries or Commonwealth countries (except the UK's Scotland, which is bijuridicial, and Malta). Essentially, every country that was colonised at some time by England, Great Britain, or the United Kingdom uses common law except those that were formerly colonised by other nations, such as Quebec (which follows the law of France in part), South Africa and Sri Lanka (which follow Roman Dutch law), where the prior civil law system was retained to respect the civil rights of the local colonists. Guyana and Saint Lucia have mixed Common Law and Civil Law systems.\n\nThe remainder of this section discusses jurisdiction-specific variants, arranged chronologically.\n\nScotland is often said to use the civil law system, but it has a unique system that combines elements of an uncodified civil law dating back to the Corpus Juris Civilis with an element of its own common law long predating the Treaty of Union with England in 1707 (see Legal institutions of Scotland in the High Middle Ages), founded on the customary laws of the tribes residing there. Historically, Scottish common law differed in that the use of \"precedent\" was subject to the courts' seeking to discover the principle that justifies a law rather than searching for an example as a \"precedent\", and principles of natural justice and fairness have always played a role in Scots Law. From the 19th century, the Scottish approach to precedent developed into a \"stare decisis\" akin to that already established in England thereby reflecting a narrower, more modern approach to the application of case law in subsequent instances. This is not to say that the substantive rules of the common laws of both countries are the same although in many matters (particularly those of UK-wide interest) they are similar.\n\nScotland shares the Supreme Court, with England, Wales and Northern Ireland for civil cases; and the Court's decisions are binding throughout the UK for civil cases and throughout England and Wales and Northern Ireland for criminal cases. This has had the effect of homogenising the law in certain areas. For instance, the modern UK law of negligence is based on \"Donoghue v Stevenson\", a case originating in Paisley, Scotland.\n\nScotland maintains a separate criminal law system from the rest of the UK, with the High Court of Justiciary being the final court for criminal appeals. The highest court of appeal in civil cases brought in Scotland is now the Supreme Court of the United Kingdom (before October 2009, final appellate jurisdiction lay with the House of Lords).\n\nThe centuries-old authority of the common law courts in England to develop law case by case and to apply statute law—\"legislating from the bench\"— is a traditional function of courts, which was carried over into the U.S. system as an essential component of the \"judicial power\" specified by Article III of the U.S. constitution. Justice Oliver Wendell Holmes, Jr. summarized centuries of history in 1917, \"judges do and must legislate” (in the federal courts, only interstitially, in state courts, to the full limits of common law adjudicatory authority).\n\nThe state of New York, which also has a civil law history from its Dutch colonial days, began a codification of its law in the 19th century. The only part of this codification process that was considered complete is known as the Field Code applying to civil procedure. The original colony of New Netherland was settled by the Dutch and the law was also Dutch. When the English captured pre-existing colonies they continued to allow the local settlers to keep their civil law. However, the Dutch settlers revolted against the English and the colony was recaptured by the Dutch. When the English finally regained control of New Netherland they forced, as a punishment unique in the history of the British Empire, the English imposed common law upon all the colonists, including the Dutch. This was problematic, as the patroon system of land holding, based on the feudal system and civil law, continued to operate in the colony until it was abolished in the mid-19th century. The influence of Roman-Dutch law continued in the colony well into the late 19th century. The codification of a law of general obligations shows how remnants of the civil law tradition in New York continued on from the Dutch days.\n\nUnder Louisiana's codified system, the Louisiana Civil Code, private law—that is, substantive law between private sector parties—is based on principles of law from continental Europe, with some common law influences. These principles derive ultimately from Roman law, transmitted through French law and Spanish law, as the state's current territory intersects the area of North America colonized by Spain and by France. Contrary to popular belief, the Louisiana code does not directly derive from the Napoleonic Code, as the latter was enacted in 1804, one year after the Louisiana Purchase. However, the two codes are similar in many respects due to common roots.\n\nLouisiana's criminal law largely rests on English common law. Louisiana's administrative law is generally similar to the administrative law of the U.S. federal government and other U.S. states. Louisiana's procedural law is generally in line with that of other U.S. states, which in turn is generally based on the U.S. Federal Rules of Civil Procedure.\n\nHistorically notable among the Louisiana code's differences from common law is the role of property rights among women, particularly in inheritance gained by widows.\n\nThe U.S. state of California has a system based on common law, but it has codified the law in the manner of the civil law jurisdictions. The reason for the enactment of the California Codes in the 19th century was to replace a pre-existing system based on Spanish civil law with a system based on common law, similar to that in most other states. California and a number of other Western states, however, have retained the concept of community property derived from civil law. The California courts have treated portions of the codes as an extension of the common-law tradition, subject to judicial development in the same manner as judge-made common law. (Most notably, in the case \"Li v. Yellow Cab Co.\", 13 Cal.3d 804 (1975), the California Supreme Court adopted the principle of comparative negligence in the face of a California Civil Code provision codifying the traditional common-law doctrine of contributory negligence.)\n\nThe United States federal government (as opposed to the states) has a variant on a common law system. United States federal courts only act as interpreters of statutes and the constitution by elaborating and precisely defining broad statutory language (connotation 1(b) above), but, unlike state courts, do not act as an independent source of common law.\n\nBefore 1938, the federal courts, like almost all other common law courts, decided the law on any issue where the relevant legislature (either the U.S. Congress or state legislature, depending on the issue), had not acted, by looking to courts in the same system, that is, other federal courts, even on issues of state law, and even where there was no express grant of authority from Congress or the Constitution.\n\nIn 1938, the U.S. Supreme Court in \"Erie Railroad Co. v. Tompkins\" 304 U.S. 64, 78 (1938), overruled earlier precedent, and held \"There is no federal general common law,\" thus confining the federal courts to act only as interpreters of law originating elsewhere. \"E.g.\", \"Texas Industries v. Radcliff\", (without an express grant of statutory authority, federal courts cannot create rules of intuitive justice, for example, a right to contribution from co-conspirators). Post-1938, federal courts deciding issues that arise under state law are required to defer to state court interpretations of state statutes, or reason what a state's highest court would rule if presented with the issue, or to certify the question to the state's highest court for resolution.\n\nLater courts have limited \"Erie\" slightly, to create a few situations where United States federal courts are permitted to create federal common law rules without express statutory authority, for example, where a federal rule of decision is necessary to protect uniquely federal interests, such as foreign affairs, or financial instruments issued by the federal government. \"See, e.g.\", \"Clearfield Trust Co. v. United States\", (giving federal courts the authority to fashion common law rules with respect to issues of federal power, in this case negotiable instruments backed by the federal government); \"see also\" \"International News Service v. Associated Press\", 248 U.S. 215 (1918) (creating a cause of action for misappropriation of \"hot news\" that lacks any statutory grounding); \"but see National Basketball Association v. Motorola, Inc.\", 105 F.3d 841, 843–44, 853 (2d Cir. 1997) (noting continued vitality of \"INS\" \"hot news\" tort under New York state law, but leaving open the question of whether it survives under federal law). Except on Constitutional issues, Congress is free to legislatively overrule federal courts' common law.\n\nMost executive branch agencies in the United States federal government have some adjudicatory authority. To greater or lesser extent, agencies honor their own precedent to ensure consistent results. Agency decision making is governed by the Administrative Procedure Act of 1946.\n\nFor example, the National Labor Relations Board issues relatively few regulations, but instead promulgates most of its substantive rules through common law (connotation 1).\n\nThe law of India, Pakistan, and Bangladesh are largely based on English common law because of the long period of British colonial influence during the period of the British Raj.\n\nAncient India represented a distinct tradition of law, and had an historically independent school of legal theory and practice. The \"Arthashastra\", dating from 400 BCE and the \"Manusmriti\", from 100 CE, were influential treatises in India, texts that were considered authoritative legal guidance. Manu's central philosophy was tolerance and pluralism, and was cited across Southeast Asia. Early in this period, which finally culminated in the creation of the Gupta Empire, relations with ancient Greece and Rome were not infrequent. The appearance of similar fundamental institutions of international law in various parts of the world show that they are inherent in international society, irrespective of culture and tradition. Inter-State relations in the pre-Islamic period resulted in clear-cut rules of warfare of a high humanitarian standard, in rules of neutrality, of treaty law, of customary law embodied in religious charters, in exchange of embassies of a temporary or semi-permanent character.\n\nWhen India became part of the British Empire, there was a break in tradition, and Hindu and Islamic law were supplanted by the common law. After the failed rebellion against the British in 1857, the British Parliament took over control of India from the British East India Company, and British India came under the direct rule of the Crown. The British Parliament passed the Government of India Act of 1858 to this effect, which set up the structure of British government in India. It established in Britain the office of the Secretary of State for India through whom the Parliament would exercise its rule, along with a Council of India to aid him. It also established the office of the Governor-General of India along with an Executive Council in India, which consisted of high officials of the British Government. As a result, the present judicial system of the country derives largely from the British system and has little correlation to the institutions of the pre-British era.\n\nPost-partition, India retained its common law system. Much of contemporary Indian law shows substantial European and American influence. Legislation first introduced by the British is still in effect in modified form today. During the drafting of the Indian Constitution, laws from Ireland, the United States, Britain, and France were all synthesized to produce a refined set of Indian laws. Indian laws also adhere to the United Nations guidelines on human rights law and environmental law. Certain international trade laws, such as those on intellectual property, are also enforced in India.\n\nThe exception to this rule is in the state of Goa, annexed in stages in the 1960s through 1980s. In Goa, a Portuguese uniform civil code is in place, in which all religions have a common law regarding marriages, divorces and adoption.\n\nPost-partition, Pakistan retained its common law system.\n\nPost-partition, Bangladesh retained its common law system.\n\nCanada has separate federal and provincial legal systems. The division of jurisdiction between the federal and provincial Parliaments is specified in the Canadian constitution.\n\nEach province and territory is considered a separate jurisdiction with respect to common law matters. As such, only the provincial legislature may enact legislation to amend private law. Each has its own procedural law, statutorily created provincial courts and superior trial courts with inherent jurisdiction culminating in the Court of Appeal of the province. This is the highest court in provincial jurisdiction, only subject to the Supreme Court of Canada in terms of appeal of their decisions. All but one of the provinces of Canada use a common law system (the exception being Quebec, which uses a civil law system for issues arising within provincial jurisdiction, such as property ownership and contracts).\n\nCanadian Federal Courts operate under a separate system throughout Canada and deal with narrower subject matter than superior courts in provincial jurisdiction. They hear cases reserved for federal jurisdiction by the Canadian constitution, such as immigration, intellectual property, judicial review of federal government decisions, and admiralty. The Federal Court of Appeal is the appellate level court in federal jurisdiction and hears cases in multiple cities, and unlike the United States, the Canadian Federal Court of Appeal is not divided into appellate circuits.\n\nCriminal law is uniform throughout Canada. It is based on the constitution and federal statutory Criminal Code, as interpreted by the Supreme Court of Canada. The administration of justice and enforcement of the criminal code are the responsibilities of the provinces.\n\nCanadian federal statutes must use the terminology of both the common law and civil law for those matters; this is referred to as legislative bijuralism.\n\nNicaragua's legal system is also a mixture of the English Common Law and Civil Law. This situation was brought through the influence of British administration of the Eastern half of the Mosquito Coast from the mid-17th century until about 1894, the William Walker period from about 1855 through 1857, USA interventions/occupations during the period from 1909 to 1933, the influence of USA institutions during the Somoza family administrations (1933 through 1979) and the considerable importation between 1979 and the present of USA culture and institutions.\n\nIsrael has a common law legal system. Its basic principles are inherited from the law of the British Mandate of Palestine and thus resemble those of British and American law, namely: the role of courts in creating the body of law and the authority of the supreme court in reviewing and if necessary overturning legislative and executive decisions, as well as employing the adversarial system. One of the primary reasons that the Israeli constitution remains unwritten is the fear by whatever party holds power that creating a written constitution, combined with the common-law elements, would severely limit the powers of the Knesset (which, following the doctrine of parliamentary sovereignty, holds near-unlimited power).\n\nRoman Dutch Common law is a bijuridical or mixed system of law similar to the common law system in Scotland and Louisiana. Roman Dutch common law jurisdictions include South Africa, Botswana, Lesotho, Namibia, Swaziland, Sri-Lanka and Zimbabwe. Many of these jurisdictions recognise customary law, and in some, such as South Africa the Constitution requires that the common law be developed in accordance with the Bill of Rights. Roman Dutch common law is a development of Roman Dutch law by courts in the Roman Dutch common law jurisdictions. During the Napoleonic wars the Kingdom of the Netherlands adopted the French \"code civil\" in 1809, however the Dutch colonies in the Cape of Good Hope and Sri Lanka, at the time called Ceylon, were seized by the British to prevent them being used as bases by the French Navy. The system was developed by the courts and spread with the expansion of British colonies in Southern Africa. Roman Dutch common law relies on legal principles set out in Roman law sources such as Justinian's Institutes and Digest, and also on the writing of Dutch jurists of the 17th century such as Grotius and Voet. In practice, the majority of decisions rely on recent precedent.\n\nEdward Coke, a 17th-century Lord Chief Justice of the English Court of Common Pleas and a Member of Parliament, wrote several legal texts that collected and integrated centuries of case law. Lawyers in both England and America learned the law from his \"Institutes\" and \"Reports\" until the end of the 18th century. His works are still cited by common law courts around the world.\n\nThe next definitive historical treatise on the common law is \"Commentaries on the Laws of England\", written by Sir William Blackstone and first published in 1765–1769. Since 1979, a facsimile edition of that first edition has been available in four paper-bound volumes. Today it has been superseded in the English part of the United Kingdom by Halsbury's Laws of England that covers both common and statutory English law.\n\nWhile he was still on the Massachusetts Supreme Judicial Court, and before being named to the U.S. Supreme Court, Justice Oliver Wendell Holmes, Jr. published a short volume called \"The Common Law\", which remains a classic in the field. Unlike Blackstone and the Restatements, Holmes' book only briefly discusses what the law \"is\"; rather, Holmes describes the common law \"process\". Law professor John Chipman Gray's \"The Nature and Sources of the Law\", an examination and survey of the common law, is also still commonly read in U.S. law schools.\n\nIn the United States, Restatements of various subject matter areas (Contracts, Torts, Judgments, and so on.), edited by the American Law Institute, collect the common law for the area. The ALI Restatements are often cited by American courts and lawyers for propositions of uncodified common law, and are considered highly persuasive authority, just below binding precedential decisions. The Corpus Juris Secundum is an encyclopedia whose main content is a compendium of the common law and its variations throughout the various state jurisdictions.\n\nScots \"common law\" covers matters including murder and theft, and has sources in custom, in legal writings and previous court decisions. The legal writings used are called \"Institutional Texts\" and come mostly from the 17th, 18th and 19th centuries. Examples include Craig, \"Jus Feudale\" (1655) and Stair, \"The Institutions of the Law of Scotland\" (1681).\n\n\n\n\n\n\n\n\n\n"
            ],
            "good_message": false,
            "loggedData": [
                {
                    "curr_message": " A supreme court is the highest court within the hierarchy of courts in many legal jurisdictions.",
                    "last_key_pressed": "<<selected>>"
                }
            ],
            "offensive": null,
            "researchTask": true,
            "selectedData": [
                "A supreme court is the highest court within the hierarchy of courts in many legal jurisdictions."
            ],
            "shown_passages": [
                [
                    "Supreme court",
                    [
                        "A supreme court is the highest court within the hierarchy of courts in many legal jurisdictions.",
                        "Other descriptions for such courts include court of last resort, apex court, and highest (or final) court of appeal.",
                        "Broadly speaking, the decisions of a supreme court are not subject to further review by any other court.",
                        "Supreme courts typically function primarily as appellate courts, hearing appeals from decisions of lower trial courts, or from intermediate-level appellate courts.",
                        "However, not all highest courts are named as such.",
                        "Civil law states tend not to have a single highest court.",
                        "Additionally, the highest court in some jurisdictions is not named the \"Supreme Court\", for example, the High Court of Australia; this is because decisions by the High Court could formerly be appealed to the Privy Council."
                    ]
                ],
                [
                    "Precedent",
                    [
                        "In legal systems based on common law, a precedent, or authority, is a principle or rule established in a previous legal case that is either binding on or persuasive for a court or other tribunal when deciding subsequent cases with similar issues or facts .",
                        "Common law legal systems place great value on deciding cases according to consistent principled rules so that similar facts will yield similar and predictable outcomes, and observance of precedent is the mechanism by which that goal is attained.",
                        "The principle by which judges are bound to precedents is known as stare decisis.",
                        "Black's Law Dictionary defines \"precedent\" as a \"rule of law established for the first time by a court for a particular type of case and thereafter referred to in deciding similar cases\"."
                    ]
                ],
                [
                    "Common law",
                    [
                        "Common law (also known as judicial precedent or judge-made law, or case law) is that body of law derived from judicial decisions of courts and similar tribunals.",
                        "The defining characteristic of “common law” is that it arises as precedent.",
                        "In cases where the parties disagree on what the law is, a common law court looks to past precedential decisions of relevant courts, and synthesizes the principles of those past cases as applicable to the current facts.",
                        "If a similar dispute has been resolved in the past, the court is usually bound to follow the reasoning used in the prior decision (a principle known as \"stare decisis\")."
                    ]
                ]
            ],
            "speaker": "0_Wizard",
            "suggestionsConfig": "show",
            "suggestionsNumber": 5,
            "suggestionsShown": true,
            "text": " A supreme court is the highest court within the hierarchy of courts in many legal jurisdictions.",
            "textboxFilled": 0,
            "time": 1580233442.2309556,
            "timeTaken": 21.641,
            "totalTasks": 120,
            "turn": 22
        },
        {
            "checked_passage": {
                "no_passages_used": "no_passages_used"
            },
            "checked_sentence": {
                "no_passages_used": "no_passages_used"
            },
            "clicked_passages": {},
            "context": {
                "conversation": [
                    "i really love animals, do u know about Armadillo?",
                    "I know a little bit. I saw a giant armadillo in a museum. Apparently they eat a lot of ants and termites! "
                ],
                "index": 1
            },
            "contextCount": 0,
            "full_passages": [
                "Giant armadillo\n\nThe giant armadillo (\"Priodontes maximus\"), colloquially tatou, ocarro, tatu-canastra or tatú carreta, is the largest living species of armadillo (although their extinct relatives, the glyptodonts, were much larger). It lives in South America, ranging throughout as far south as northern Argentina. This species is considered vulnerable to extinction.\n\nThe giant armadillo prefers termites and some ants as prey, and often consumes the entire population of a termite mound. It also has been known to prey upon worms, larvae and larger creatures, such as spiders and snakes, and plants.\n\nAt least one zoo park, in Villavicencio, Colombia – \"Los Ocarros\" – is dedicated to this animal.\n\nThe giant armadillo is the largest living species of armadillo, with 11 to 13 hinged bands protecting the body and a further three or four on the neck. Its body is dark brown in color, with a lighter, yellowish band running along the sides, and a pale, yellow-white head. These armadillos have around 80 to 100 teeth, which is more than any other terrestrial mammal. The teeth are all similar in appearance, being reduced premolars and molars, grow constantly throughout life, and lack enamel. They also possess extremely long front claws, including a sickle-shaped third claw, which are proportionately the largest of any living mammal. The tail is covered in small rounded scales and does not have the heavy bony scutes that cover the upper body and top of the head. The animal is almost entirely hairless, with just a few beige colored hairs protruding between the scutes.\n\nGiant armadillos typically weigh around when fully grown, however a specimen has been weighed in the wild and captive specimens have been weighed up to . The typical length of the species is , with the tail adding another .\n\nGiant armadillos are found throughout much of northern South America east of the Andes, except for eastern Brazil and Paraguay. In the south, they reach the northernmost provinces of Argentina, including Salta, Formosa, Chaco, and Santiago del Estero. There are no recognised geographic subspecies. They primarily inhabit open habitats, with cerrado grasslands covering about 25% of their range, but they can also be found in lowland forests.\n\nGiant armadillos are solitary and nocturnal, spending the day in burrows. They also burrow to escape predators, being unable to completely roll into a protective ball. Compared with those of other armadillos, their burrows are unusually large, with entrances averaging wide, and typically opening to the west.\n\nGiant armadillos use their large front claws to dig for prey and rip open termite mounds. The diet is mainly composed of termites, although ants, worms, spiders and other invertebrates are also eaten. Little is currently known about this species' reproductive biology, and no juveniles have ever been discovered in the field. The average sleep time of a captive giant armadillo is said to be 18.1 hours.\n\nArmadillos have not been extensively studied in the wild; therefore, little is known about their natural ecology and behavior. In the only long term study on the species, that started in 2003 in the Peruvian Amazon, dozens of other species of mammals, reptiles and birds were found using the giant armadillos' burrows on the same day, including the rare short-eared dog (\"Atelocynus microtis\"). Because of this, the species is considered a habitat engineer, and the local extinction of \"Priodontes\" may have cascading effects in the mammalian community by impoverishing fossorial habitat.\n\nFemale giant armadillos have two teats and are thought to normally give birth to only a single young per year. Little is known with certainty about their life history, although it is thought that the young are weaned by about seven to eight months of age, and that the mother periodically seals up the entrance to burrows containing younger offspring, presumably to protect them from predators. Although they have never bred in captivity, a wild-born giant armadillo at San Antonio Zoo was estimated to have been around sixteen years old when it died.\n\nHunted throughout its range, a single giant armadillo supplies a great deal of meat, and is the primary source of protein for some indigenous peoples. In addition, live giant armadillos are frequently captured for trade on the black market, and invariably die during transportation or in captivity. Despite this species’ wide range, it is locally rare. This is further exacerbated by habitat loss resulting from deforestation. Current estimates indicate the giant armadillo may have undergone a worrying population decline of 30 to 50 percent over the past three decades. Without intervention, this trend is likely to continue.\n\nThe giant armadillo was classified as vulnerable on the World Conservation Union's Red List in 2002, and is listed under Appendix I (threatened with extinction) of the Convention on the International Trade in Endangered Species of Wild Flora and Fauna.\n\nThe giant armadillo is protected by law in Colombia, Guyana, Brazil, Argentina, Paraguay, Suriname and Peru, and international trade is banned by its listing on Appendix I of the Convention on International Trade in Endangered Species (CITES). However, hunting for food and sale in the black market continues to occur throughout its entire range. Some populations occur within protected reserves, including the Parque das Emas in Brazil, and the Central Suriname Nature Reserve, a massive 1.6-million-hectare site of pristine rainforest managed by Conservation International. Such protection helps to some degree to mitigate the threat of habitat loss, but targeted conservation action is required to prevent the further decline of this species.\n\n",
                "Termite\n\nTermites are eusocial insects that are classified at the taxonomic rank of infraorder Isoptera, or as epifamily Termitoidae within the cockroach order Blattodea. Termites were once classified in a separate order from cockroaches, but recent phylogenetic studies indicate that they evolved from close ancestors of cockroaches during the Jurassic or Triassic. However, the first termites possibly emerged during the Permian or even the Carboniferous. About 3,106 species are currently described, with a few hundred more left to be described. Although these insects are often called \"white ants\", they are not ants.\n\nLike ants and some bees and wasps from the separate order Hymenoptera, termites divide labour among castes consisting of sterile male and female \"workers\" and \"soldiers\". All colonies have fertile males called \"kings\" and one or more fertile females called \"queens\". Termites mostly feed on dead plant material and cellulose, generally in the form of wood, leaf litter, soil, or animal dung. Termites are major detritivores, particularly in the subtropical and tropical regions, and their recycling of wood and plant matter is of considerable ecological importance.\n\nTermites are among the most successful groups of insects on Earth, colonising most landmasses except for Antarctica. Their colonies range in size from a few hundred individuals to enormous societies with several million individuals. Termite queens have the longest lifespan of any insect in the world, with some queens reportedly living up to 30 to 50 years. Unlike ants, which undergo a complete metamorphosis, each individual termite goes through an incomplete metamorphosis that proceeds through egg, nymph, and adult stages. Colonies are described as superorganisms because the termites form part of a self-regulating entity: the colony itself.\n\nTermites are a delicacy in the diet of some human cultures and are used in many traditional medicines. Several hundred species are economically significant as pests that can cause serious damage to buildings, crops, or plantation forests. Some species, such as the West Indian drywood termite (\"Cryptotermes brevis\"), are regarded as invasive species.\n\nThe infraorder name Isoptera is derived from the Greek words \"iso\" (equal) and \"ptera\" (winged), which refers to the nearly equal size of the fore and hind wings. \"Termite\" derives from the Latin and Late Latin word \"termes\" (\"woodworm, white ant\"), altered by the influence of Latin \"terere\" (\"to rub, wear, erode\") from the earlier word \"tarmes\". Termite nests were commonly known as \"terminarium\" or \"termitaria\". In early English, termites were known as \"wood ants\" or \"white ants\". The modern term was first used in 1781.\n\nDNA analysis from 16S rRNA sequences has supported a hypothesis, originally suggested by Cleveland and colleagues in 1934, that these insects are most closely related to wood-eating cockroaches (genus \"Cryptocercus\", the woodroach). This earlier conclusion had been based on the similarity of the symbiotic gut flagellates in the wood-eating cockroaches to those in certain species of termites regarded as living fossils. In the 1960s additional evidence supporting that hypothesis emerged when F. A. McKittrick noted similar morphological characteristics between some termites and \"Cryptocercus\" nymphs. These similarities have led some authors to propose that termites be reclassified as a single family, the Termitidae, within the order Blattodea, which contains cockroaches. Other researchers advocate the more conservative measure of retaining the termites as the Termitoidae, an epifamily within the cockroach order, which preserves the classification of termites at family level and below.\n\nThe oldest unambiguous termite fossils date to the early Cretaceous, but given the diversity of Cretaceous termites and early fossil records showing mutualism between microorganisms and these insects, they likely originated earlier in the Jurassic or Triassic. Further evidence of a Jurassic origin is the assumption that the extinct \"Fruitafossor\" consumed termites, judging from its morphological similarity to modern termite-eating mammals. The oldest termite nest discovered is believed to be from the Upper Cretaceous in West Texas, where the oldest known faecal pellets were also discovered.\n\nClaims that termites emerged earlier have faced controversy. For example, F. M. Weesner indicated that the Mastotermitidae termites may go back to the Late Permian, 251 million years ago, and fossil wings that have a close resemblance to the wings of \"Mastotermes\" of the Mastotermitidae, the most primitive living termite, have been discovered in the Permian layers in Kansas. It is even possible that the first termites emerged during the Carboniferous. Termites are thought to be the descendants of the genus \"Cryptocercus\". The folded wings of the fossil wood roach \"Pycnoblattina\", arranged in a convex pattern between segments 1a and 2a, resemble those seen in \"Mastotermes\", the only living insect with the same pattern. Krishna \"et al.\", though, consider that all of the Paleozoic and Triassic insects tentatively classified as termites are in fact unrelated to termites and should be excluded from the Isoptera. Termites were the first social insects to evolve a caste system, evolving more than 100 million years ago.\n\nTermites have long been accepted to be closely related to cockroaches and mantids, and they are classified in the same superorder (Dictyoptera). Strong evidence suggests termites are highly specialised wood-eating cockroaches. The cockroach genus \"Cryptocercus\" shares the strongest phylogenetical similarity with termites and is considered to be a sister-group to termites. Termites and \"Cryptocercus\" share similar morphological and social features: for example, most cockroaches do not exhibit social characteristics, but \"Cryptocercus\" takes care of its young and exhibits other social behaviour such as trophallaxis and allogrooming. The primitive giant northern termite (\"Mastotermes darwiniensis\") exhibits numerous cockroach-like characteristics that are not shared with other termites, such as laying its eggs in rafts and having anal lobes on the wings. Cryptocercidae and Isoptera are united in the clade Xylophagodea. Although termites are sometimes called \"white ants\", they are actually not ants. Ants belong to the family Formicidae within the order Hymenoptera. The similarity of their social structure to that of termites is attributed to convergent evolution.\nAs of 2013, about 3,106 living and fossil termite species are recognised, classified in 12 families. The infraorder Isoptera is divided into the following clade and family groups, showing the subfamilies in their respective classification:\n\nOrder Blattaria\n\nTermites are found on all continents except Antarctica. The diversity of termite species is low in North America and Europe (10 species known in Europe and 50 in North America), but is high in South America, where over 400 species are known. Of the 3,000 termite species currently classified, 1,000 are found in Africa, where mounds are extremely abundant in certain regions. Approximately 1.1 million active termite mounds can be found in the northern Kruger National Park alone. In Asia, there are 435 species of termites, which are mainly distributed in China. Within China, termite species are restricted to mild tropical and subtropical habitats south of the Yangtze River. In Australia, all ecological groups of termites (dampwood, drywood, subterranean) are endemic to the country, with over 360 classified species.\n\nDue to their soft cuticles, termites do not inhabit cool or cold habitats. There are three ecological groups of termites: dampwood, drywood and subterranean. Dampwood termites are found only in coniferous forests, and drywood termites are found in hardwood forests; subterranean termites live in widely diverse areas. One species in the drywood group is the West Indian drywood termite \"(Cryptotermes brevis)\", which is an invasive species in Australia.\n\nTermites are usually small, measuring between in length. The largest of all extant termites are the queens of the species \"Macrotermes bellicosus\", measuring up to over 10 centimetres (4 in) in length. Another giant termite, the extinct \"Gyatermes styriensis\", flourished in Austria during the Miocene and had a wingspan of and a body length of .\n\nMost worker and soldier termites are completely blind as they do not have a pair of eyes. However, some species, such as \"Hodotermes mossambicus\", have compound eyes which they use for orientation and to distinguish sunlight from moonlight. The alates have eyes along with lateral ocelli. Lateral ocelli, however, are not found in all termites. Like other insects, termites have a small tongue-shaped labrum and a clypeus; the clypeus is divided into a postclypeus and anteclypeus. Termite antennae have a number of functions such as the sensing of touch, taste, odours (including pheromones), heat and vibration. The three basic segments of a termite antenna include a scape, a pedicel (typically shorter than the scape), and the flagellum (all segments beyond the scape and pedicel). The mouth parts contain a maxillae, a labium, and a set of mandibles. The maxillae and labium have palps that help termites sense food and handling.\n\nConsistent with all insects, the anatomy of the termite thorax consists of three segments: the prothorax, the mesothorax and the metathorax. Each segment contains a pair of legs. On alates, the wings are located at the mesothorax and metathorax. The mesothorax and metathorax have well-developed exoskeletal plates; the prothorax has smaller plates.\nTermites have a ten-segmented abdomen with two plates, the tergites and the sternites. The tenth abdominal segment has a pair of short cerci. There are ten tergites, of which nine are wide and one is elongated. The reproductive organs are similar to those in cockroaches but are more simplified. For example, the intromittent organ is not present in male alates, and the sperm is either immotile or aflagellate. However, Mastotermitidae termites have multiflagellate sperm with limited motility. The genitals in females are also simplified. Unlike in other termites, Mastotermitidae females have an ovipositor, a feature strikingly similar to that in female cockroaches.\n\nThe non-reproductive castes of termites are wingless and rely exclusively on their six legs for locomotion. The alates fly only for a brief amount of time, so they also rely on their legs. The appearance of the legs is similar in each caste, but the soldiers have larger and heavier legs. The structure of the legs is consistent with other insects: the parts of a leg include a coxa, trochanter, femur, tibia and the tarsus. The number of tibial spurs on an individual's leg varies. Some species of termite have an arolium, located between the claws, which is present in species that climb on smooth surfaces but is absent in most termites.\n\nUnlike in ants, the hind-wings and fore-wings are of equal length. Most of the time, the alates are poor flyers; their technique is to launch themselves in the air and fly in a random direction. Studies show that in comparison to larger termites, smaller termites cannot fly long distances. When a termite is in flight, its wings remain at a right angle, and when the termite is at rest, its wings remain parallel to the body.\n\nWorker termites undertake the most labour within the colony, being responsible for foraging, food storage, and brood and nest maintenance. Workers are tasked with the digestion of cellulose in food and are thus the most likely caste to be found in infested wood. The process of worker termites feeding other nestmates is known as trophallaxis. Trophallaxis is an effective nutritional tactic to convert and recycle nitrogenous components. It frees the parents from feeding all but the first generation of offspring, allowing for the group to grow much larger and ensuring that the necessary gut symbionts are transferred from one generation to another. Some termite species do not have a true worker caste, instead relying on nymphs that perform the same work without differentiating as a separate caste.\n\nThe soldier caste has anatomical and behavioural specialisations, and their sole purpose is to defend the colony. Many soldiers have large heads with highly modified powerful jaws so enlarged they cannot feed themselves. Instead, like juveniles, they are fed by workers. Fontanelles, simple holes in the forehead that exude defensive secretions, are a feature of the family Rhinotermitidae. Many species are readily identified using the characteristics of the soldiers' larger and darker head and large mandibles. Among certain termites, soldiers may use their globular (phragmotic) heads to block their narrow tunnels. Different sorts of soldiers include minor and major soldiers, and nasutes, which have a horn-like nozzle frontal projection (a nasus). These unique soldiers are able to spray noxious, sticky secretions containing diterpenes at their enemies. Nitrogen fixation plays an important role in nasute nutrition.\n\nThe reproductive caste of a mature colony includes a fertile female and male, known as the queen and king. The queen of the colony is responsible for egg production for the colony. Unlike in ants, the king mates with her for life. In some species, the abdomen of the queen swells up dramatically to increase fecundity, a characteristic known as physogastrism. Depending on the species, the queen starts producing reproductive winged alates at a certain time of the year, and huge swarms emerge from the colony when nuptial flight begins. These swarms attract a wide variety of predators.\n\nTermites are often compared with the social Hymenoptera (ants and various species of bees and wasps), but their differing evolutionary origins result in major differences in life cycle. In the eusocial Hymenoptera, the workers are exclusively female, males (drones) are haploid and develop from unfertilised eggs, while females (both workers and the queen) are diploid and develop from fertilised eggs. In contrast, worker termites, which constitute the majority in a colony, are diploid individuals of both sexes and develop from fertilised eggs. Depending on species, male and female workers may have different roles in a termite colony.\n\nThe life cycle of a termite begins with an egg, but is different from that of a bee or ant in that it goes through a developmental process called incomplete metamorphosis, with egg, nymph and adult stages. Nymphs resemble small adults, and go through a series of moults as they grow. In some species, eggs go through four moulting stages and nymphs go through three. Nymphs first moult into workers, and then some workers go through further moulting and become soldiers or alates; workers become alates only by moulting into alate nymphs.\n\nThe development of nymphs into adults can take months; the time period depends on food availability, temperature, and the general population of the colony. Since nymphs are unable to feed themselves, workers must feed them, but workers also take part in the social life of the colony and have certain other tasks to accomplish such as foraging, building or maintaining the nest or tending to the queen. Pheromones regulate the caste system in termite colonies, preventing all but a very few of the termites from becoming fertile queens.\n\nTermite alates only leave the colony when a nuptial flight takes place. Alate males and females pair up together and then land in search of a suitable place for a colony. A termite king and queen do not mate until they find such a spot. When they do, they excavate a chamber big enough for both, close up the entrance and proceed to mate. After mating, the pair never go outside and spend the rest of their lives in the nest. Nuptial flight time varies in each species. For example, alates in certain species emerge during the day in summer while others emerge during the winter. The nuptial flight may also begin at dusk, when the alates swarm around areas with lots of lights. The time when nuptial flight begins depends on the environmental conditions, the time of day, moisture, wind speed and precipitation. The number of termites in a colony also varies, with the larger species typically having 100–1,000 individuals. However, some termite colonies, including those with large individuals, can number in the millions.\n\nThe queen only lays 10–20 eggs in the very early stages of the colony, but lays as many as 1,000 a day when the colony is several years old. At maturity, a primary queen has a great capacity to lay eggs. In some species, the mature queen has a greatly distended abdomen and may produce 40,000 eggs a day. The two mature ovaries may have some 2,000 ovarioles each. The abdomen increases the queen's body length to several times more than before mating and reduces her ability to move freely; attendant workers provide assistance.\n\nThe king grows only slightly larger after initial mating and continues to mate with the queen for life (a termite queen can live between 30  to 50 years); this is very different from ant colonies, in which a queen mates once with the male(s) and stores the gametes for life, as the male ants die shortly after mating. If a queen is absent, a termite king produces pheromones which encourage the development of replacement termite queens. As the queen and king are monogamous, sperm competition does not occur.\n\nTermites going through incomplete metamorphosis on the path to becoming alates form a subcaste in certain species of termite, functioning as potential supplementary reproductives. These supplementary reproductives only mature into primary reproductives upon the death of a king or queen, or when the primary reproductives are separated from the colony. Supplementaries have the ability to replace a dead primary reproductive, and there may also be more than a single supplementary within a colony. Some queens have the ability to switch from sexual reproduction to asexual reproduction. Studies show that while termite queens mate with the king to produce colony workers, the queens reproduce their replacements (neotenic queens) parthenogenetically.\n\nTermites are detritivores, consuming dead plants at any level of decomposition. They also play a vital role in the ecosystem by recycling waste material such as dead wood, faeces and plants. Many species eat cellulose, having a specialised midgut that breaks down the fibre. Termites are considered to be a major source (11%) of atmospheric methane, one of the prime greenhouse gases, produced from the breakdown of cellulose. Termites rely primarily upon symbiotic protozoa (metamonads) and other microbes such as flagellate protists in their guts to digest the cellulose for them, allowing them to absorb the end products for their own use. Gut protozoa, such as \"Trichonympha\", in turn, rely on symbiotic bacteria embedded on their surfaces to produce some of the necessary digestive enzymes. Most higher termites, especially in the family Termitidae, can produce their own cellulase enzymes, but they rely primarily upon the bacteria. The flagellates have been lost in Termitidae. Scientists' understanding of the relationship between the termite digestive tract and the microbial endosymbionts is still rudimentary; what is true in all termite species, however, is that the workers feed the other members of the colony with substances derived from the digestion of plant material, either from the mouth or anus. Judging from closely related bacterial species, it is strongly presumed that the termites' and cockroach's gut microbiota derives from their dictyopteran ancestors.\n\nCertain species such as \"Gnathamitermes tubiformans\" have seasonal food habits. For example, they may preferentially consume Red three-awn (\"Aristida longiseta\") during the summer, Buffalograss (\"Buchloe dactyloides\") from May to August, and blue grama \"Bouteloua gracilis\" during spring, summer and autumn. Colonies of \"G. tubiformans\" consume less food in spring than they do during autumn when their feeding activity is high.\n\nVarious woods differ in their susceptibility to termite attack; the differences are attributed to such factors as moisture content, hardness, and resin and lignin content. In one study, the drywood termite \"Cryptotermes brevis\" strongly preferred poplar and maple woods to other woods that were generally rejected by the termite colony. These preferences may in part have represented conditioned or learned behaviour.\n\nSome species of termite practice fungiculture. They maintain a \"garden\" of specialised fungi of genus \"Termitomyces\", which are nourished by the excrement of the insects. When the fungi are eaten, their spores pass undamaged through the intestines of the termites to complete the cycle by germinating in the fresh faecal pellets. Molecular evidence suggests that the family Macrotermitinae developed agriculture about 31 million years ago. It is assumed that more than 90 percent of dry wood in the semiarid savannah ecosystems of Africa and Asia are reprocessed by these termites. Originally living in the rainforest, fungus farming allowed them to colonise the African savannah and other new environments, eventually expanding into Asia.\n\nDepending on their feeding habits, termites are placed into two groups: the lower termites and higher termites. The lower termites predominately feed on wood. As wood is difficult to digest, termites prefer to consume fungus-infected wood because it is easier to digest and the fungi are high in protein. Meanwhile, the higher termites consume a wide variety of materials, including faeces, humus, grass, leaves and roots. The gut in the lower termites contains many species of bacteria along with protozoa, while the higher termites only have a few species of bacteria with no protozoa.\n\nTermites are consumed by a wide variety of predators. One termite species alone, \"Hodotermes mossambicus\", was found in the stomach contents of 65 birds and 19 mammals. Ants, arthropods, reptiles, and amphibians such as bees, centipedes, cockroaches, crickets, dragonflies, frogs, lizards, scorpions, spiders, and toads consume these insects, while 2 spiders in the family Ammoxenidae are specialist termite predators. Other predators include aardvarks, aardwolves, anteaters, bats, bears, bilbies, many birds, echidnas, foxes, galagos, numbats, mice and pangolins. The aardwolf is an insectivorous mammal that primarily feeds on termites; it locates its food by sound and also by detecting the scent secreted by the soldiers; a single aardwolf is capable of consuming thousands of termites in a single night by using its long, sticky tongue. Sloth bears break open mounds to consume the nestmates, while chimpanzees have developed tools to \"fish\" termites from their nest. Wear pattern analysis of bone tools used by the early hominin \"Paranthropus robustus\" suggests that they used these tools to dig into termite mounds.\nAmong all predators, ants are the greatest enemy to termites. Some ant genera are specialist predators of termites. For example, \"Megaponera\" is a strictly termite-eating (termitophagous) genus that perform raiding activities, some lasting several hours. \"Paltothyreus tarsatus\" is another termite-raiding species, with each individual stacking as many termites as possible in its mandibles before returning home, all the while recruiting additional nestmates to the raiding site through chemical trails. The Malaysian basicerotine ants \"Eurhopalothrix heliscata\" uses a different strategy of termite hunting by pressing themselves into tight spaces, as they hunt through rotting wood housing termite colonies. Once inside, the ants seize their prey by using their short but sharp mandibles. \"Tetramorium uelense\" is a specialised predator species that feeds on small termites. A scout recruits 10–30 workers to an area where termites are present, killing them by immobilising them with their stinger. \"Centromyrmex\" and \"Iridomyrmex\" colonies sometimes nest in termite mounds, and so the termites are preyed on by these ants. No evidence for any kind of relationship (other than a predatory one) is known. Other ants, including \"Acanthostichus\", \"Camponotus\", \"Crematogaster\", \"Cylindromyrmex\", \"Leptogenys\", \"Odontomachus\", \"Ophthalmopone\", \"Pachycondyla\", \"Rhytidoponera\", \"Solenopsis\" and \"Wasmannia\", also prey on termites. In contrast to all these ant species, and despite their enormous diversity of prey, \"Dorylus ants\" rarely consume termites.\n\nAnts are not the only invertebrates that perform raids. Many sphecoid wasps and several species including \"Polybia Lepeletier\" and \"Angiopolybia Araujo\" are known to raid termite mounds during the termites' nuptial flight.\n\nTermites are less likely to be attacked by parasites than bees, wasps and ants, as they are usually well protected in their mounds. Nevertheless, termites are infected by a variety of parasites. Some of these include dipteran flies, \"Pyemotes\" mites, and a large number of nematode parasites. Most nematode parasites are in the order Rhabditida; others are in the genus \"Mermis\", \"Diplogaster aerivora\" and \"Harteria gallinarum\". Under imminent threat of an attack by parasites, a colony may migrate to a new location. Fungi pathogens such as \"Aspergillus nomius\" and \"Metarhizium anisopliae\" are, however, major threats to a termite colony as they are not host-specific and may infect large portions of the colony; transmission usually occurs via direct physical contact. \"M. anispliae\" is known to weaken the termite immune system. Infection with \"A. nomius\" only occurs when a colony is under great stress.\n\nTermites are infected by viruses including Entomopoxvirinae and the Nuclear Polyhedrosis Virus.\n\nBecause the worker and soldier castes lack wings and thus never fly, and the reproductives use their wings for just a brief amount of time, termites predominantly rely upon their legs to move about.\n\nForaging behaviour depends on the type of termite. For example, certain species feed on the wood structures they inhabit, and others harvest food that is near the nest. Most workers are rarely found out in the open, and do not forage unprotected; they rely on sheeting and runways to protect them from predators. Subterranean termites construct tunnels and galleries to look for food, and workers who manage to find food sources recruit additional nestmates by depositing a phagostimulant pheromone that attracts workers. Foraging workers use semiochemicals to communicate with each other, and workers who begin to forage outside of their nest release trail pheromones from their sternal glands. In one species, \"Nasutitermes costalis\", there are three phases in a foraging expedition: first, soldiers scout an area. When they find a food source, they communicate to other soldiers and a small force of workers starts to emerge. In the second phase, workers appear in large numbers at the site. The third phase is marked by a decrease in the number of soldiers present and an increase in the number of workers. Isolated termite workers may engage in Lévy flight behaviour as an optimised strategy for finding their nestmates or foraging for food.\n\nCompetition between two colonies always results in agonistic behaviour towards each other, resulting in fights. These fights can cause mortality on both sides and, in some cases, the gain or loss of territory. \"Cemetery pits\" may be present, where the bodies of dead termites are buried.\n\nStudies show that when termites encounter each other in foraging areas, some of the termites deliberately block passages to prevent other termites from entering. Dead termites from other colonies found in exploratory tunnels leads to the isolation of the area and thus the need to construct new tunnels. Conflict between two competitors does not always occur. For example, though they might block each other's passages, colonies of \"Macrotermes bellicosus\" and \"Macrotermes subhyalinus\" are not always aggressive towards each other. Suicide cramming is known in \"Coptotermes formosanus\". Since \"C. formosanus\" colonies may get into physical conflict, some termites squeeze tightly into foraging tunnels and die, successfully blocking the tunnel and ending all agonistic activities.\n\nAmong the reproductive caste, neotenic queens may compete with each other to become the dominant queen when there are no primary reproductives. This struggle among the queens leads to the elimination of all but a single queen, which, with the king, takes over the colony.\n\nAnts and termites may compete with each other for nesting space. In particular, ants that prey on termites usually have a negative impact on arboreal nesting species.\n\nMost termites are blind, so communication primarily occurs through chemical, mechanical and pheromonal cues. These methods of communication are used in a variety of activities, including foraging, locating reproductives, construction of nests, recognition of nestmates, nuptial flight, locating and fighting enemies, and defending the nests. The most common way of communicating is through antennation. A number of pheromones are known, including contact pheromones (which are transmitted when workers are engaged in trophallaxis or grooming) and alarm, trail and sex pheromones. The alarm pheromone and other defensive chemicals are secreted from the frontal gland. Trail pheromones are secreted from the sternal gland, and sex pheromones derive from two glandular sources: the sternal and tergal glands. When termites go out to look for food, they forage in columns along the ground through vegetation. A trail can be identified by the faecal deposits or runways that are covered by objects. Workers leave pheromones on these trails, which are detected by other nestmates through olfactory receptors. Termites can also communicate through mechanical cues, vibrations, and physical contact. These signals are frequently used for alarm communication or for evaluating a food source.\n\nWhen termites construct their nests, they use predominantly indirect communication. No single termite would be in charge of any particular construction project. Individual termites react rather than think, but at a group level, they exhibit a sort of collective cognition. Specific structures or other objects such as pellets of soil or pillars cause termites to start building. The termite adds these objects onto existing structures, and such behaviour encourages building behaviour in other workers. The result is a self-organised process whereby the information that directs termite activity results from changes in the environment rather than from direct contact among individuals.\n\nTermites can distinguish nestmates and non-nestmates through chemical communication and gut symbionts: chemicals consisting of hydrocarbons released from the cuticle allow the recognition of alien termite species. Each colony has its own distinct odour. This odour is a result of genetic and environmental factors such as the termites' diet and the composition of the bacteria within the termites' intestines.\n\nTermites rely on alarm communication to defend a colony. Alarm pheromones can be released when the nest has been breached or is being attacked by enemies or potential pathogens. Termites always avoid nestmates infected with \"Metarhizium anisopliae\" spores, through vibrational signals released by infected nestmates. Other methods of defence include intense jerking and secretion of fluids from the frontal gland and defecating faeces containing alarm pheromones.\n\nIn some species, some soldiers block tunnels to prevent their enemies from entering the nest, and they may deliberately rupture themselves as an act of defence. In cases where the intrusion is coming from a breach that is larger than the soldier's head, defence requires a special formations where soldiers form a phalanx-like formation around the breach and bite at intruders. If an invasion carried out by \"Megaponera analis\" is successful, an entire colony may be destroyed, although this scenario is rare.\n\nTo termites, any breach of their tunnels or nests is a cause for alarm. When termites detect a potential breach, the soldiers usually bang their heads, apparently to attract other soldiers for defence and to recruit additional workers to repair any breach. Additionally, an alarmed termite bumps into other termites which causes them to be alarmed and to leave pheromone trails to the disturbed area, which is also a way to recruit extra workers.\nThe pantropical subfamily Nasutitermitinae has a specialised caste of soldiers, known as nasutes, that have the ability to exude noxious liquids through a horn-like frontal projection that they use for defence. Nasutes have lost their mandibles through the course of evolution and must be fed by workers. A wide variety of monoterpene hydrocarbon solvents have been identified in the liquids that nasutes secrete.\n\nSoldiers of the species \"Globitermes sulphureus\" commit suicide by autothysis – rupturing a large gland just beneath the surface of their cuticles. The thick, yellow fluid in the gland becomes very sticky on contact with the air, entangling ants or other insects which are trying to invade the nest. Another termite, \"Neocapriterme taracua\", also engages in suicidal defence. Workers physically unable to use their mandibles while in a fight form a pouch full of chemicals, then deliberately rupture themselves, releasing toxic chemicals that paralyse and kill their enemies. The soldiers of the neotropical termite family Serritermitidae have a defence strategy which involves front gland autothysis, with the body rupturing between the head and abdomen. When soldiers guarding nest entrances are attacked by intruders, they engage in autothysis, creating a block that denies entry to any attacker.\n\nWorkers use several different strategies to deal with their dead, including burying, cannibalism, and avoiding a corpse altogether. To avoid pathogens, termites occasionally engage in necrophoresis, in which a nestmate carries away a corpse from the colony to dispose of it elsewhere. Which strategy is used depends on the nature of the corpse a worker is dealing with (i.e. the age of the carcass).\n\nA species of fungus is known to mimic termite eggs, successfully avoiding its natural predators. These small brown balls, known as \"termite balls\", rarely kill the eggs, and in some cases the workers tend to them. This fungus mimics these eggs by producing a cellulose-digesting enzyme known as glucosidases. A unique mimicking behaviour exists between various species of \"Trichopsenius\" beetles and certain termite species within \"Reticulitermes\". The beetles share the same cuticle hydrocarbons as the termites and even biosynthesize them. This chemical mimicry allows the beetles to integrate themselves within the termite colonies. The developed appendages on the physogastric abdomen of \"Austrospirachtha mimetes\" allows the beetle to mimic a termite worker.\n\nSome species of ant are known to capture termites to use as a fresh food source later on, rather than killing them. For example, \"Formica nigra\" captures termites, and those who try to escape are immediately seized and driven underground. Certain species of ants in the subfamily Ponerinae conduct these raids although other ant species go in alone to steal the eggs or nymphs. Ants such as \"Megaponera analis\" attack the outside the mounds and Dorylinae ants attack underground. Despite this, some termites and ants can coexist peacefully. Some species of termite, including \"Nasutitermes corniger\", form associations with certain ant species to keep away predatory ant species. The earliest known association between \"Azteca\" ants and \"Nasutitermes\" termites date back to the Oligocene to Miocene period.\n54 species of ants are known to inhabit \"Nasutitermes\" mounds, both occupied and abandoned ones. One reason many ants live in \"Nasutitermes\" mounds is due to the termites' frequent occurrence in their geographical range; another is to protect themselves from floods. \"Iridomyrmex\" also inhabits termite mounds although no evidence for any kind of relationship (other than a predatory one) is known. In rare cases, certain species of termites live inside active ant colonies. Some invertebrate organisms such as beetles, caterpillars, flies and millipedes are termitophiles and dwell inside termite colonies (they are unable to survive independently). As a result, certain beetles and flies have evolved with their hosts. They have developed a gland that secrete a substance that attracts the workers by licking them. Mounds may also provide shelter and warmth to birds, lizards, snakes and scorpions.\n\nTermites are known to carry pollen and regularly visit flowers, so are regarded as potential pollinators for a number of flowering plants. One flower in particular, \"Rhizanthella gardneri\", is regularly pollinated by foraging workers, and it is perhaps the only Orchidaceae flower in the world to be pollinated by termites.\n\nMany plants have developed effective defences against termites. However, seedlings are vulnerable to termite attacks and need additional protection, as their defence mechanisms only develop when they have passed the seedling stage. Defence is typically achieved by secreting antifeedant chemicals into the woody cell walls. This reduces the ability of termites to efficiently digest the cellulose. A commercial product, \"Blockaid\", has been developed in Australia that uses a range of plant extracts to create a paint-on nontoxic termite barrier for buildings. An extract of a species of Australian figwort, \"Eremophila\", has been shown to repel termites; tests have shown that termites are strongly repelled by the toxic material to the extent that they will starve rather than consume the food. When kept close to the extract, they become disoriented and eventually die.\n\nTermite populations can be substantially impacted by environmental changes including those caused by human intervention. A Brazilian study investigated the termite assemblages of three sites of Caatinga under different levels of anthropogenic disturbance in the semi-arid region of northeastern Brazil were sampled using 65 x 2 m transects. A total of 26 species of termites were present in the three sites, and 196 encounters were recorded in the transects. The termite assemblages were considerably different among sites, with a conspicuous reduction in both diversity and abundance with increased disturbance, related to the reduction of tree density and soil cover, and with the intensity of trampling by cattle and goats. The wood-feeders were the most severely affected feeding group.\n\nA termite nest can be considered as being composed of two parts, the inanimate and the animate. The animate is all of the termites living inside the colony, and the inanimate part is the structure itself, which is constructed by the termites. Nests can be broadly separated into three main categories: subterranean (completely below ground), epigeal (protruding above the soil surface), and arboreal (built above ground, but always connected to the ground via shelter tubes). Epigeal nests (mounds) protrude from the earth with ground contact and are made out of earth and mud. A nest has many functions such as providing a protected living space and providing shelter against predators. Most termites construct underground colonies rather than multifunctional nests and mounds. Primitive termites of today nest in wooden structures such as logs, stumps and the dead parts of trees, as did termites millions of years ago.\n\nTo build their nests, termites primarily use faeces, which have many desirable properties as a construction material. Other building materials include partly digested plant material, used in carton nests (arboreal nests built from faecal elements and wood), and soil, used in subterranean nest and mound construction. Not all nests are visible, as many nests in tropical forests are located underground. Species in the subfamily Apicotermitinae are good examples of subterranean nest builders, as they only dwell inside tunnels. Other termites live in wood, and tunnels are constructed as they feed on the wood. Nests and mounds protect the termites' soft bodies against desiccation, light, pathogens and parasites, as well as providing a fortification against predators. Nests made out of carton are particularly weak, and so the inhabitants use counter-attack strategies against invading predators.\n\nArboreal carton nests of mangrove swamp-dwelling \"Nasutitermes\" are enriched in lignin and depleted in cellulose and xylans. This change is caused by bacterial decay in the gut of the termites: they use their faeces as a carton building material. Arboreal termites nests can account for as much as 2% of above ground carbon storage in Puerto Rican mangrove swamps. These \"Nasutitermes\" nests are mainly composed of partially biodegraded wood material from the stems and branches of mangrove trees, namely, \"Rhizophora mangle\" (red mangrove), \"Avicennia germinans\" (black mangrove) and \"Laguncularia racemose\" (white mangrove).\n\nSome species build complex nests called polycalic nests; this habitat is called polycalism. Polycalic species of termites form multiple nests, or calies, connected by subterranean chambers. The termite genera \"Apicotermes\" and \"Trinervitermes\" are known to have polycalic species. Polycalic nests appear to be less frequent in mound-building species although polycalic arboreal nests have been observed in a few species of \"Nasutitermes\".\n\nNests are considered mounds if they protrude from the earth's surface. A mound provides termites the same protection as a nest but is stronger. Mounds located in areas with torrential and continuous rainfall are at risk of mound erosion due to their clay-rich construction. Those made from carton can provide protection from the rain, and in fact can withstand high precipitation. Certain areas in mounds are used as strong points in case of a breach. For example, \"Cubitermes\" colonies build narrow tunnels used as strong points, as the diameter of the tunnels is small enough for soldiers to block. A highly protected chamber, known as the \"queens cell\", houses the queen and king and is used as a last line of defence.\n\nSpecies in the genus \"Macrotermes\" arguably build the most complex structures in the insect world, constructing enormous mounds. These mounds are among the largest in the world, reaching a height of 8 to 9 metres (26 to 29 feet), and consist of chimneys, pinnacles and ridges. Another termite species, \"Amitermes meridionalis\", can build nests 3 to 4 metres (9 to 13 feet) high and 2.5 metres (8 feet) wide. The tallest mound ever recorded was 12.8 metres (42 ft) long found in the Democratic Republic of the Congo.\n\nThe sculptured mounds sometimes have elaborate and distinctive forms, such as those of the compass termite (\"Amitermes meridionalis\" and \"A. laurensis\"), which builds tall, wedge-shaped mounds with the long axis oriented approximately north–south, which gives them their common name. This orientation has been experimentally shown to assist thermoregulation. The north-south orientation causes the internal temperature of a mound to increase rapidly during the morning while avoiding overheating from the midday sun. The temperature then remains at a plateau for the rest of the day until the evening.\n\nTermites construct shelter tubes, also known as earthen tubes or mud tubes, that start from the ground. These shelter tubes can be found on walls and other structures. Constructed by termites during the night, a time of higher humidity, these tubes provide protection to termites from potential predators, especially ants. Shelter tubes also provide high humidity and darkness and allow workers to collect food sources that cannot be accessed in any other way. These passageways are made from soil and faeces and are normally brown in colour. The size of these shelter tubes depends on the amount of food sources that are available. They range from less than 1 cm to several cm in width, but may extend dozens of metres in length.\n\nOwing to their wood-eating habits, many termite species can do great damage to unprotected buildings and other wooden structures. Their habit of remaining concealed often results in their presence being undetected until the timbers are severely damaged, leaving a thin layer of a wall that protects them from the environment. Of the 3,106 species known, only 183 species cause damage; 83 species cause significant damage to wooden structures. In North America, nine subterranean species are pests; in Australia, 16 species have an economic impact; in the Indian subcontinent 26 species are considered pests, and in tropical Africa, 24. In Central America and the West Indies, there are 17 pest species. Among the termite genera, \"Coptotermes\" has the highest number of pest species of any genus, with 28 species known to cause damage. Less than 10% of drywood termites are pests, but they infect wooden structures and furniture in tropical, subtropical and other regions. Dampwood termites only attack lumber material exposed to rainfall or soil.\n\nDrywood termites thrive in warm climates, and human activities can enable them to invade homes since they can be transported through contaminated goods, containers and ships. Colonies of termites have been seen thriving in warm buildings located in cold regions. Some termites are considered invasive species. \"Cryptotermes brevis\", the most widely introduced invasive termite species in the world, has been introduced to all the islands in the West Indies and to Australia.\nIn addition to causing damage to buildings, termites can also damage food crops. Termites may attack trees whose resistance to damage is low but generally ignore fast-growing plants. Most attacks occur at harvest time; crops and trees are attacked during the dry season.\n\nThe damage caused by termites costs the southwestern United States approximately $1.5 billion each year in wood structure damage, but the true cost of damage worldwide cannot be determined. Drywood termites are responsible for a large proportion of the damage caused by termites.\n\nTo better control the population of termites, various methods have been developed to track termite movements. One early method involved distributing termite bait laced with immunoglobulin G (IgG) marker proteins from rabbits or chickens. Termites collected from the field could be tested for the rabbit-IgG markers using a rabbit-IgG-specific assay. More recently developed, less expensive alternatives include tracking the termites using egg white, cow milk, or soy milk proteins, which can be sprayed on termites in the field. Termites bearing these proteins can be traced using a protein-specific ELISA test.\n\n43 termite species are used as food by humans or are fed to livestock. These insects are particularly important in less developed countries where malnutrition is common, as the protein from termites can help improve the human diet. Termites are consumed in many regions globally, but this practice has only become popular in developed nations in recent years.\n\nTermites are consumed by people in many different cultures around the world. In Africa, the alates are an important factor in the diets of native populations. Tribes have different ways of collecting or cultivating insects; sometimes tribes collect soldiers from several species. Though harder to acquire, queens are regarded as a delicacy. Termite alates are high in nutrition with adequate levels of fat and protein. They are regarded as pleasant in taste, having a nut-like flavour after they are cooked.\n\nAlates are collected when the rainy season begins. During a nuptial flight, they are typically seen around lights to which they are attracted, and so nets are set up on lamps and captured alates are later collected. The wings are removed through a technique that is similar to winnowing. The best result comes when they are lightly roasted on a hot plate or fried until crisp. Oil is not required as their bodies usually contain sufficient amounts of oil. Termites are typically eaten when livestock is lean and tribal crops have not yet developed or produced any food, or if food stocks from a previous growing season are limited.\n\nIn addition to Africa, termites are consumed in local or tribal areas in Asia and North and South America. In Australia, Indigenous Australians are aware that termites are edible but do not consume them even in times of scarcity; there are few explanations as to why. Termite mounds are the main sources of soil consumption (geophagy) in many countries including Kenya, Tanzania, Zambia, Zimbabwe and South Africa. Researchers have suggested that termites are suitable candidates for human consumption and space agriculture, as they are high in protein and can be used to convert inedible waste to consumable products for humans.\n\nTermites can be major agricultural pests, particularly in East Africa and North Asia, where crop losses can be severe (3–100% in crop loss in Africa). Counterbalancing this is the greatly improved water infiltration where termite tunnels in the soil allow rainwater to soak in deeply, which helps reduce runoff and consequent soil erosion through bioturbation. In South America, cultivated plants such as eucalyptus, upland rice and sugarcane can be severely damaged by termite infestations, with attacks on leaves, roots and woody tissue. Termites can also attack other plants, including cassava, coffee, cotton, fruit trees, maize, peanuts, soybeans and vegetables. Mounds can disrupt farming activities, making it difficult for farmers to operate farming machinery; however, despite farmers' dislike of the mounds, it is often the case that no net loss of production occurs. Termites can be beneficial to agriculture, such as by boosting crop yields and enriching the soil. Termites and ants can re-colonise untilled land that contains crop stubble, which colonies use for nourishment when they establish their nests. The presence of nests in fields enables larger amounts of rainwater to soak into the ground and increases the amount of nitrogen in the soil, both essential for the growth of crops.\n\nThe termite gut has inspired various research efforts aimed at replacing fossil fuels with cleaner, renewable energy sources. Termites are efficient bioreactors, capable of producing two litres of hydrogen from a single sheet of paper. Approximately 200 species of microbes live inside the termite hindgut, releasing the hydrogen that was trapped inside wood and plants that they digest. Through the action of unidentified enzymes in the termite gut, lignocellulose polymers are broken down into sugars and are transformed into hydrogen. The bacteria within the gut turns the sugar and hydrogen into cellulose acetate, an acetate ester of cellulose on which termites rely for energy. Community DNA sequencing of the microbes in the termite hindgut has been employed to provide a better understanding of the metabolic pathway. Genetic engineering may enable hydrogen to be generated in bioreactors from woody biomass.\n\nThe development of autonomous robots capable of constructing intricate structures without human assistance has been inspired by the complex mounds that termites build. These robots work independently and can move by themselves on a tracked grid, capable of climbing and lifting up bricks. Such robots may be useful for future projects on Mars, or for building levees to prevent flooding.\n\nTermites use sophisticated means to control the temperatures of their mounds. As discussed above, the shape and orientation of the mounds of the Australian compass termite stabilises their internal temperatures during the day. As the towers heat up, the solar chimney effect (stack effect) creates an updraft of air within the mound. Wind blowing across the tops of the towers enhances the circulation of air through the mounds, which also include side vents in their construction. The solar chimney effect has been in use for centuries in the Middle East and Near East for passive cooling, as well as in Europe by the Romans. It is only relatively recently, however, that climate responsive construction techniques have become incorporated into modern architecture. Especially in Africa, the stack effect has become a popular means to achieve natural ventilation and passive cooling in modern buildings.\n\nThe Eastgate Centre is a shopping centre and office block in central Harare, Zimbabwe, whose architect, Mick Pearce, used passive cooling inspired by that used by the local termites. It was the first major building exploiting termite-inspired cooling techniques to attract international attention. Other such buildings include the Learning Resource Center at the Catholic University of Eastern Africa and the Council House 2 building in Melbourne, Australia.\n\nFew zoos hold termites, due to the difficulty in keeping them captive and to the reluctance of authorities to permit potential pests. One of the few that do, the Zoo Basel in Switzerland, has two thriving \"Macrotermes bellicosus\" populations – resulting in an event very rare in captivity: the mass migrations of young flying termites. This happened in September 2008, when thousands of male termites left their mound each night, died, and covered the floors and water pits of the house holding their exhibit.\n\nAfrican tribes in several countries have termites as totems, and for this reason tribe members are forbidden to eat the reproductive alates. Termites are widely used in traditional popular medicine; they are used as treatments for diseases and other conditions such as asthma, bronchitis, hoarseness, influenza, sinusitis, tonsillitis and whooping cough. In Nigeria, \"Macrotermes nigeriensis\" is used for spiritual protection and to treat wounds and sick pregnant women. In Southeast Asia, termites are used in ritual practices. In Malaysia, Singapore and Thailand, termite mounds are commonly worshiped among the populace. Abandoned mounds are viewed as structures created by spirits, believing a local guardian dwells within the mound; this is known as Keramat and Datok Kong. In urban areas, local residents construct red-painted shrines over mounds that have been abandoned, where they pray for good health, protection and luck.\n\n\n\n",
                "Nine-banded armadillo\n\nThe nine-banded armadillo (\"Dasypus novemcinctus\"), or the nine-banded, long-nosed armadillo, is a medium-sized mammal found in North, Central, and South America, making it the most widespread of the armadillos. Its ancestors originated in South America, and remained there until the formation of the Isthmus of Panama allowed them to enter North America as part of the Great American Interchange.\nThe nine-banded armadillo is a solitary, mainly nocturnal animal, found in many kinds of habitats, from mature and secondary rainforests to grassland and dry scrub. It is an insectivore, feeding chiefly on ants, termites, and other small invertebrates. The armadillo can jump straight in the air if sufficiently frightened, making it a particular danger on roads. It is the state small mammal of Texas.\n\nThe nine-banded armadillo evolved in a warm, rainy environment, and is still most commonly found in regions resembling its ancestral home. As a very adaptable animal, though, it can also be found in scrublands, open prairies, and tropical rainforests. It cannot thrive in particularly cold or dry environments, as its large surface area, which is not well insulated by fat, makes it especially susceptible to heat and water loss.\n\nThe nine-banded armadillo has been rapidly expanding its range both north and east within the United States, where it is the only regularly occurring species of armadillo. The armadillo crossed the Rio Grande from Mexico in the late 19th century, and was introduced in Florida at about the same time by humans. By 1995, the species had become well established in Texas, Oklahoma, Louisiana, Arkansas, Mississippi, Alabama, and Florida, and had been sighted as far afield as Kansas, Missouri, Tennessee, Georgia and South Carolina. A decade later, the armadillo had become established in all of those areas and continued its migration, being sighted as far north as southern Nebraska, southern Illinois, and southern Indiana.\nThe primary cause of this rapid expansion is explained simply by the species having few natural predators within the United States, little desire on the part of Americans to hunt or eat the armadillo, and the animals' high reproductive rate. The northern expansion of the armadillo is expected to continue until the species reaches as far north as Ohio, Pennsylvania, New Jersey and Connecticut, and all points southward on the East Coast of the United States. Further northward and westward expansion will probably be limited by the armadillo's poor tolerance of harsh winters, due to its lack of insulating fat and its inability to hibernate.\nAs of 2009, newspaper reports indicated the nine-banded armadillo seems to have expanded its range northward as far as Omaha, Nebraska in the west, and Kentucky Dam and Evansville, Indiana, in the east. In 1995, armadillos were only seen in the southern tip of South Carolina, and within two to three years, they had swept across most of the state. In late 2009, North Carolina began considering the establishment of a hunting season for armadillo, following reports that the species has been moving into the southern reaches of the state (roughly between the areas of Charlotte and Wilmington).\nOutside the United States, the nine-banded armadillo ranges southward through Central and South America into northern Argentina and Uruguay, where it is still expanding its range.\n\nNine-banded armadillos are generally insectivores. They forage for meals by thrusting their snouts into loose soil and leaf litter and frantically digging in erratic patterns, stopping occasionally to dig up grubs, beetles (perhaps the main portion of this species' prey selection), ants, termites, and worms, which their sensitive noses can detect through of soil. They then lap up the insects with their sticky tongues. Nine-banded armadillos have been observed to roll about on ant hills to dislodge and consume the resident ants. They supplement their diets with amphibians and small reptiles, especially in more wintery months when such prey tends to be more sluggish, and occasionally bird eggs and baby mammals. Carrion is also eaten, although perhaps the species is most attracted to the maggots borne by carcasses rather than the meat itself. Less than 10% of the diet of this species is composed by nonanimal matter, though fungi, tubers, fruits, and seeds are occasionally eaten.\n\nNine-banded armadillos generally weigh from , though the largest specimens can scale up to . They are one of the largest species of armadillos. Head and body length is , which combines with the tail, for a total length of . They stand tall at the top of the shell. The outer shell is composed of ossified dermal scutes covered by nonoverlapping, keratinized epidermal scales, which are connected by flexible bands of skin. This armor covers the back, sides, head, tail, and outside surfaces of the legs. The underside of the body and the inner surfaces of the legs have no armored protection. Instead, they are covered by tough skin and a layer of coarse hair. The vertebrae attach to the carapace.\nThe claws on the middle toes of the forefeet are elongated for digging, though not to the same degree as those of the much larger giant armadillo of South America.\nTheir low metabolic rate and poor thermoregulation make them best suited for semitropical environments.\nUnlike the South American three-banded armadillos, the nine-banded armadillo cannot roll itself into a ball. It is, however, capable of floating across rivers by inflating its intestines, or by sinking and running across riverbeds. The second is possible due to its ability to hold its breath for up to six minutes, an adaptation originally developed for allowing the animal to keep its snout submerged in soil for extended periods while foraging. Although nine is the typical number of bands on the nine-banded armadillo, the actual number varies by geographic range.\nArmadillos possess the teeth typical of all sloths and anteaters. The teeth are all small, peg-like molars with open roots and no enamel. Incisors do form in the embryos, but quickly degenerate and are usually absent by birth.\n\nNine-banded armadillos are solitary, largely nocturnal animals that come out to forage around dusk. They are extensive burrowers, with a single animal sometimes maintaining up to 12 burrows on its range. These burrows are roughly wide, deep, and long. Armadillos mark their territory with urine, feces, and excretions from scent glands found on the eyelids, nose, and feet. Males hold breeding territories and may become aggressive in order to keep other males out of their home range to increase chances of pairing with a female. Territorial disputes are settled by kicking and chasing. When they are not foraging, armadillos shuffle along fairly slowly, stopping occasionally to sniff the air for signs of danger.\n\nIf alarmed, nine-banded armadillos can flee with surprising speed. Occasionally, a large predator may be able to ambush the armadillo before it can clear a distance, and breach the hard carapace with a well-placed bite or swipe. If the fleeing escape fails, the armadillo may quickly dig a shallow trench and lodge itself inside. Predators are rarely able to dislodge the animal once it has burrowed itself, and abandon their prey when they cannot breach the armadillo’s armor or grasp its tapered tail. Due to their softer carapaces, juvenile armadillos are more likely to fall victim to natural predation and their cautious behavior generally reflects this. Young nine-banded armadillos tend to forage earlier in the day and are more wary of the approach of an unknown animal (including humans) than are adults. Their known natural predators include cougars (perhaps the leading predator), maned wolves, coyotes, black bears, red wolves, jaguars, alligators, bobcats, and large raptors. By far the leading predator of nine-banded armadillos today is humans, as armadillos are locally harvested for their meat and shells and many thousands fall victim to auto accidents every year.\n\nMating takes place during a two-to-three month long mating season, which occurs from July–August in the Northern Hemisphere and November–January in the Southern Hemisphere. A single egg is fertilized, but implantation is delayed for three to four months to ensure the young will not be born during an unfavorable time. Once the zygote does implant in the uterus, a gestation period of four months occurs, during which the zygote splits into four identical embryos, each of which develops its own placenta, so blood and nutrients are not mixed between them. They are born in March and weigh 3 oz (85 g). After birth, the quadruplets remain in the burrow, living off the mother’s milk for about three months. They then begin to forage with the mother, eventually leaving after six months to a year.\n\nNine-banded armadillos reach sexual maturity at the age of one year, and reproduce every year for the rest of their 12–to-15 year lifespans. A single female can produce up to 56 young over the course of her life. This high reproductive rate is a major cause of the species’ rapid expansion.\n\nThe foraging of nine-banded armadillo can cause mild damage to the root systems of certain plants. Skunks, cotton rats, burrowing owls, pine snakes, and rattlesnakes can be found living in abandoned armadillo burrows. Occasionally, the armadillo may threaten the endangered gopher tortoise by aggressively displacing them from their burrows and claiming the burrows for themselves. Studies have shown the fan-tailed warbler habitually follows armadillos to feed on insects and other invertebrates displaced by them.\n\nThey are typically hunted for their meat, which is said to taste like pork, but are more frequently killed as a result of their tendency to steal the eggs of poultry and game birds. This has caused certain populations of the nine-banded armadillo to become threatened, although the species as a whole is under no immediate threat.\nThey are also valuable for use in medical research, as they are among the few mammals other than humans susceptible to leprosy.\nIn Texas, nine-banded armadillos are raised to participate in armadillo racing, a small-scale, but well-established sport in which the animals scurry down a 40-foot track.\n\nDuring the Great Depression, the species was hunted for its meat in East Texas, where it was known as poor man’s pork, or the \"Hoover hog\" by those who considered President Herbert Hoover to be responsible for the Depression. Earlier, German settlers in Texas would often refer to the armadillo as \"Panzerschwein\" (\"armored pig\"). In 1995, the nine-banded armadillo was, with some resistance, made the state small mammal of Texas, where it is considered a pest and is often seen dead on the roadside. They first forayed into Texas across the Rio Grande from Mexico in the 19th century, eventually spreading across the southeast United States.\n\nNorth American subspecies exhibit reduced genetic variability compared with the subspecies of South America, indicating the armadillos of North America are descended from a relatively small number of individuals that migrated from south of the Rio Grande.\n\n\n"
            ],
            "good_message": false,
            "loggedData": [],
            "offensive": null,
            "researchTask": true,
            "selectedData": [],
            "shown_passages": [
                [
                    "Giant armadillo",
                    [
                        "The giant armadillo (\"Priodontes maximus\"), colloquially tatou, ocarro, tatu-canastra or tatú carreta, is the largest living species of armadillo (although their extinct relatives, the glyptodonts, were much larger).",
                        "It lives in South America, ranging throughout as far south as northern Argentina.",
                        "This species is considered vulnerable to extinction.",
                        "The giant armadillo prefers termites and some ants as prey, and often consumes the entire population of a termite mound.",
                        "It also has been known to prey upon worms, larvae and larger creatures, such as spiders and snakes, and plants.",
                        "At least one zoo park, in Villavicencio, Colombia – \"Los Ocarros\" – is dedicated to this animal."
                    ]
                ],
                [
                    "Termite",
                    [
                        "Termites are eusocial insects that are classified at the taxonomic rank of infraorder Isoptera, or as epifamily Termitoidae within the cockroach order Blattodea.",
                        "Termites were once classified in a separate order from cockroaches, but recent phylogenetic studies indicate that they evolved from close ancestors of cockroaches during the Jurassic or Triassic.",
                        "However, the first termites possibly emerged during the Permian or even the Carboniferous.",
                        "About 3,106 species are currently described, with a few hundred more left to be described.",
                        "Although these insects are often called \"white ants\", they are not ants."
                    ]
                ],
                [
                    "Nine-banded armadillo",
                    [
                        "The nine-banded armadillo (\"Dasypus novemcinctus\"), or the nine-banded, long-nosed armadillo, is a medium-sized mammal found in North, Central, and South America, making it the most widespread of the armadillos.",
                        "Its ancestors originated in South America, and remained there until the formation of the Isthmus of Panama allowed them to enter North America as part of the Great American Interchange.",
                        "The nine-banded armadillo is a solitary, mainly nocturnal animal, found in many kinds of habitats, from mature and secondary rainforests to grassland and dry scrub.",
                        "It is an insectivore, feeding chiefly on ants, termites, and other small invertebrates."
                    ]
                ]
            ],
            "speaker": "0_Wizard",
            "suggestionsConfig": "show",
            "suggestionsNumber": 5,
            "suggestionsShown": true,
            "text": "I know a little bit. I saw a giant armadillo in a museum. Apparently they eat a lot of ants and termites! ",
            "textboxFilled": 0,
            "time": 1580233445.733886,
            "timeTaken": 3.418,
            "totalTasks": 120,
            "turn": 23
        },
        {
            "checked_passage": {
                "no_passages_used": "no_passages_used"
            },
            "checked_sentence": {
                "no_passages_used": "no_passages_used"
            },
            "clicked_passages": {},
            "context": {
                "conversation": [
                    "i really love animals, do u know about Armadillo?",
                    "I know a little bit. I saw a giant armadillo in a museum. Apparently they eat a lot of ants and termites! "
                ],
                "index": 1
            },
            "contextCount": 0,
            "full_passages": [
                "Giant armadillo\n\nThe giant armadillo (\"Priodontes maximus\"), colloquially tatou, ocarro, tatu-canastra or tatú carreta, is the largest living species of armadillo (although their extinct relatives, the glyptodonts, were much larger). It lives in South America, ranging throughout as far south as northern Argentina. This species is considered vulnerable to extinction.\n\nThe giant armadillo prefers termites and some ants as prey, and often consumes the entire population of a termite mound. It also has been known to prey upon worms, larvae and larger creatures, such as spiders and snakes, and plants.\n\nAt least one zoo park, in Villavicencio, Colombia – \"Los Ocarros\" – is dedicated to this animal.\n\nThe giant armadillo is the largest living species of armadillo, with 11 to 13 hinged bands protecting the body and a further three or four on the neck. Its body is dark brown in color, with a lighter, yellowish band running along the sides, and a pale, yellow-white head. These armadillos have around 80 to 100 teeth, which is more than any other terrestrial mammal. The teeth are all similar in appearance, being reduced premolars and molars, grow constantly throughout life, and lack enamel. They also possess extremely long front claws, including a sickle-shaped third claw, which are proportionately the largest of any living mammal. The tail is covered in small rounded scales and does not have the heavy bony scutes that cover the upper body and top of the head. The animal is almost entirely hairless, with just a few beige colored hairs protruding between the scutes.\n\nGiant armadillos typically weigh around when fully grown, however a specimen has been weighed in the wild and captive specimens have been weighed up to . The typical length of the species is , with the tail adding another .\n\nGiant armadillos are found throughout much of northern South America east of the Andes, except for eastern Brazil and Paraguay. In the south, they reach the northernmost provinces of Argentina, including Salta, Formosa, Chaco, and Santiago del Estero. There are no recognised geographic subspecies. They primarily inhabit open habitats, with cerrado grasslands covering about 25% of their range, but they can also be found in lowland forests.\n\nGiant armadillos are solitary and nocturnal, spending the day in burrows. They also burrow to escape predators, being unable to completely roll into a protective ball. Compared with those of other armadillos, their burrows are unusually large, with entrances averaging wide, and typically opening to the west.\n\nGiant armadillos use their large front claws to dig for prey and rip open termite mounds. The diet is mainly composed of termites, although ants, worms, spiders and other invertebrates are also eaten. Little is currently known about this species' reproductive biology, and no juveniles have ever been discovered in the field. The average sleep time of a captive giant armadillo is said to be 18.1 hours.\n\nArmadillos have not been extensively studied in the wild; therefore, little is known about their natural ecology and behavior. In the only long term study on the species, that started in 2003 in the Peruvian Amazon, dozens of other species of mammals, reptiles and birds were found using the giant armadillos' burrows on the same day, including the rare short-eared dog (\"Atelocynus microtis\"). Because of this, the species is considered a habitat engineer, and the local extinction of \"Priodontes\" may have cascading effects in the mammalian community by impoverishing fossorial habitat.\n\nFemale giant armadillos have two teats and are thought to normally give birth to only a single young per year. Little is known with certainty about their life history, although it is thought that the young are weaned by about seven to eight months of age, and that the mother periodically seals up the entrance to burrows containing younger offspring, presumably to protect them from predators. Although they have never bred in captivity, a wild-born giant armadillo at San Antonio Zoo was estimated to have been around sixteen years old when it died.\n\nHunted throughout its range, a single giant armadillo supplies a great deal of meat, and is the primary source of protein for some indigenous peoples. In addition, live giant armadillos are frequently captured for trade on the black market, and invariably die during transportation or in captivity. Despite this species’ wide range, it is locally rare. This is further exacerbated by habitat loss resulting from deforestation. Current estimates indicate the giant armadillo may have undergone a worrying population decline of 30 to 50 percent over the past three decades. Without intervention, this trend is likely to continue.\n\nThe giant armadillo was classified as vulnerable on the World Conservation Union's Red List in 2002, and is listed under Appendix I (threatened with extinction) of the Convention on the International Trade in Endangered Species of Wild Flora and Fauna.\n\nThe giant armadillo is protected by law in Colombia, Guyana, Brazil, Argentina, Paraguay, Suriname and Peru, and international trade is banned by its listing on Appendix I of the Convention on International Trade in Endangered Species (CITES). However, hunting for food and sale in the black market continues to occur throughout its entire range. Some populations occur within protected reserves, including the Parque das Emas in Brazil, and the Central Suriname Nature Reserve, a massive 1.6-million-hectare site of pristine rainforest managed by Conservation International. Such protection helps to some degree to mitigate the threat of habitat loss, but targeted conservation action is required to prevent the further decline of this species.\n\n",
                "Armadillo\n\nArmadillos are New World placental mammals in the order Cingulata with a leathery armour shell. The Chlamyphoridae and Dasypodidae are the only surviving families in the order, which is part of the superorder Xenarthra, along with the anteaters and sloths. The word \"armadillo\" means \"little armoured one\" in Spanish. The Aztecs called them \"āyōtōchtli\" , Nahuatl for \"turtle-rabbit\": \"āyōtl\" (turtle) and \"tōchtli\" (rabbit). The Portuguese word for \"armadillo\" is \"tatu\" which derives from the Tupi language. Similar names are also found in other, especially European, languages.\n\nAbout nine extant genera and 21 extant species of armadillo have been described, some of which are distinguished by the number of bands on their armour. Their average length is about , including tail. The giant armadillo grows up to and weighs up to , while the pink fairy armadillo is a diminutive species, with an overall length of . All species are native to the Americas, where they inhabit a variety of different environments.\n\nRecent genetic research suggests that an extinct group of giant armoured mammals, the glyptodonts, should be included within the lineage of armadillos, having diverged some 35 million years ago, much more recently than previously assumed.\n\nLike all of the Xenarthra lineages, armadillos originated in South America. Due to the continent's former isolation, they were confined there for most of the Cenozoic. The recent formation of the Isthmus of Panama allowed a few members of the family to migrate northward into southern North America by the early Pleistocene, as part of the Great American Interchange. (Some of their much larger cingulate relatives, the pampatheres and chlamyphorid glyptodonts, made the same journey.)\n\nToday, all extant armadillo species are still present in South America. They are particularly diverse in Paraguay (where 11 species exist) and surrounding areas. Many species are endangered. Some, including four species of \"Dasypus\", are widely distributed over the Americas, whereas others, such as Yepes's mulita, are restricted to small ranges. Two species, the northern naked-tailed armadillo and nine-banded armadillo, are found in Central America; the latter has also reached the United States, primarily in the south-central states (notably Texas), but with a range that extends as far east as South Carolina and Florida, and as far north as Nebraska and central Indiana. Their range has consistently expanded in North America over the last century due to a lack of natural predators.\n\nArmadillos are small to medium-sized mammals. The smallest species, the pink fairy armadillo, is roughly chipmunk-sized at and in total length. The largest species, the giant armadillo, can be the size of a small pig and weigh up to , and can be long. They are prolific diggers. Many species use their sharp claws to dig for food, such as grubs, and to dig dens. The nine-banded armadillo prefers to build burrows in moist soil near the creeks, streams, and arroyos around which it lives and feeds. The diets of different armadillo species vary, but consist mainly of insects, grubs, and other invertebrates. Some species, however, feed almost entirely on ants and termites.\n\nIn common with other xenarthrans, armadillos, in general, have low body temperatures of and low basal metabolic rates (40–60% of that expected in placental mammals of their mass). This is particularly true of types that specialize in using termites as their primary food source (for example, \"Priodontes\" and \"Tolypeutes\").\n\nThe armour is formed by plates of dermal bone covered in relatively small, overlapping epidermal scales called \"scutes\", composed of bone with a covering of horn. Most species have rigid shields over the shoulders and hips, with a number of bands separated by flexible skin covering the back and flanks. Additional armour covers the top of the head, the upper parts of the limbs, and the tail. The underside of the animal is never armoured, and is simply covered with soft skin and fur.\n\nThis armour-like skin appears to be the main defense of many armadillos, although most escape predators by fleeing (often into thorny patches, from which their armour protects them) or digging to safety. Only the South American three-banded armadillos (\"Tolypeutes\") rely heavily on their armour for protection. When threatened by a predator, \"Tolypeutes\" species frequently roll up into a ball. Other armadillo species cannot roll up because they have too many plates. The North American nine-banded armadillo tends to jump straight in the air when surprised, so consequently often collides with the undercarriage or fenders of passing vehicles.\n\nArmadillos have short legs, but can move quite quickly. The nine-banded armadillo is noted for its movement through water which is accomplished via two different methods: it can walk underwater for short distances, holding its breath for as long as six minutes; also, to cross larger bodies of water, it is capable of increasing its buoyancy by swallowing air, inflating its stomach and intestines.\n\nArmadillos have very poor eyesight, and use their keen sense of smell to hunt for food. They use their claws for digging and finding food, as well as for making their homes in burrows. They dig their burrows with their claws, making only a single corridor the width of the animal's body. They have five clawed toes on their hind feet, and three to five toes with heavy digging claws on their fore feet. Armadillos have a large number of cheek teeth which are not divided into premolars and molars, but usually have no incisors or canines. The dentition of the nine-banded armadillo is P 7/7, M 1/1 = 32.\n\nGestation lasts from 60 to 120 days, depending on species, although the nine-banded armadillo also exhibits delayed implantation, so the young are not typically born for eight months after mating. Most members of the genus \"Dasypus\" give birth to four monozygotic young (that is, identical quadruplets), but other species may have typical litter sizes that range from one to eight. The young are born with soft, leathery skin which hardens within a few weeks. They reach sexual maturity in three to 12 months, depending on the species. Armadillos are solitary animals that do not share their burrows with other adults.\n\nFamily Dasypodidae\n\nFamily Chlamyphoridae\n\n† indicates extinct taxon\n\nArmadillos are often used in the study of leprosy, since they, along with mangabey monkeys, rabbits, and mice (on their footpads), are among the few known species that can contract the disease systemically. They are particularly susceptible due to their unusually low body temperature, which is hospitable to the leprosy bacterium, \"Mycobacterium leprae\". (The leprosy bacterium is difficult to culture and armadillos have a body temperature of , similar to human skin.) Humans can acquire a leprosy infection from armadillos by handling them or consuming armadillo meat. Armadillos are a presumed vector and natural reservoir for the disease in Texas and Louisiana and Florida. Prior to the arrival of Europeans in the late 15th century, leprosy was unknown in the New World. Given that armadillos are native to the New World, at some point they must have acquired the disease from old-world humans.\n\nThe armadillo is also a natural reservoir for Chagas disease.\n\nThe nine-banded armadillo also serves science through its unusual reproductive system, in which four genetically identical offspring are born, the result of one original egg. Because they are always genetically identical, the group of four young provides a good subject for scientific, behavioral, or medical tests that need consistent biological and genetic makeup in the test subjects. This is the only reliable manifestation of polyembryony in the class Mammalia, and exists only within the genus \"Dasypus\" and not in all armadillos, as is commonly believed. Other species that display this trait include parasitoid wasps, certain flatworms, and various aquatic invertebrates.\n\nArmadillos (mainly \"Dasypus\") are common roadkill due to their habit of jumping 3–4 ft vertically when startled, which puts them into collision with the underside of vehicles. Wildlife enthusiasts are using the northward march of the armadillo as an opportunity to educate others about the animals, which can be a burrowing nuisance to property owners and managers.\n\nArmadillo shells have traditionally been used to make the back of the \"charango\", an Andean lute instrument.\n\n\n",
                "Serra da Canastra National Park\n\nSerra da Canastra National Park () is a national park in the Canastra Mountains of the state of Minas Gerais, Brazil.\n\nSerra da Canastra National Park is in the south west of Minas Gerais to the north of Rio Grande.\nIt is in the Cerrado biome.\nThe park was created by decree 70.355 on 3 April 1972, with .\nIt is administered by the Chico Mendes Institute for Biodiversity Conservation (ICMBio).\nIt lies in the municipalities of São Roque de Minas, Sacramento, Capitólio, Vargem Bonita, São João Batista do Glória and Delfinópolis in the state of Minas Gerais.\nAs of 2016 only of the park in the tableland of Canastra had been regularized, with former owners indemnified.\n\nThe park lies on the watershed between the São Francisco and Paraná rivers.\nIt preserves the headwaters of the São Francisco River, which flows east from the park.\nIn the south it feeds the Rio Grande and in the north the Araguari River, which rises in the park and is a tributary of the Paranaíba River.\nThe Rio Grande and Paranaíba flow west and join to form the Paraná.\nAltitude ranges from .\nA road cuts through the highest part of the park from east to west for more than .\nThe park includes areas of scenic beauty such as cliffs with dramatic waterfalls, including the Casca D'anta, the first waterfall of the São Francisco River, with a drop of .\nOther attractions are the Garagem de Pedras and the Curral de Pedras.\nLookout points can be accessed by car via dirt roads in good weather.\nThe highest peaks are almost .\n\nTemperatures range from and average . Average annual rainfall is .\nVegetation is mostly rocky fields and cerrado, with clumps of riparian forest in the valleys and ravines.\nThere are 45 endemic species of flora.\nThese include members of the genusus Hololepis, Inulopsis, Aspilia, Senecio, Stomatanthes, Campuloclinium, Stevia, Chinolaena, Chrysolaena, Ichtthyothere, Lessigianthus, Sinningia, Eriope, Habranthus, Ilex, Agalinis and Eryngium.\nFlora also include Barbacenia fulva, Barbacenia lymansmithii, Chaetostoma canastrensis, Miconia angelana, Microlicia canastrensis, Microlicia flava, Microlicia scoparia, Microlicia sp. nov, Svitramia sp. and Tibouchina sp. nov..\n\nFauna include bush dog, pampas deer, maned wolf, giant anteater, lontra, titi, giant armadillo (Priodontes maximus), cougar, Brazilian merganser, king vulture, toco toucan (Ramphastos toco), guan, southern crested caracara (Caracara plancus), greater rhea and seriema.\nEndemic herpetofauna are Hyla ibitiguara, Cinax canastrensis and Odontophrynus sp.\n\nThe park is classified as IUCN protected area category II (national park).\nIt has the objectives of preserving natural ecosystems of great ecological relevance and scenic beauty, enabling scientific research, environmental education, outdoors recreation and eco-tourism.\n\nProtected birds include the cock-tailed tyrant (Alectrurus tricolor), vinaceous-breasted amazon (Amazona vinacea), black-masked finch (Coryphaspiza melanotis), Brazilian merganser (Mergus octosetaceus), lesser nothura (Nothura minor) and the dwarf tinamou (Taoniscus nanus).\nOther protected species include the maned wolf (Chrysocyon brachyurus), cougar (Puma concolor), oncilla (Leopardus tigrinus), ocelot (Leopardus pardalis), colocolo (Leopardus colocolo), giant armadillo (Priodontes maximus), giant anteater (Myrmecophaga tridactyla) and the frog Phyllomedusa ayeaye.\n\n"
            ],
            "good_message": false,
            "loggedData": [
                {
                    "curr_message": " The giant armadillo (\"Priodontes maximus\"), colloquially tatou, ocarro, tatu-canastra or tatú carreta, is the largest living species of armadillo (although their extinct relatives, the glyptodonts, were much larger).",
                    "last_key_pressed": "<<selected>>"
                }
            ],
            "offensive": null,
            "researchTask": true,
            "selectedData": [
                "The giant armadillo (\"Priodontes maximus\"), colloquially tatou, ocarro, tatu-canastra or tatú carreta, is the largest living species of armadillo (although their extinct relatives, the glyptodonts, were much larger)."
            ],
            "shown_passages": [
                [
                    "Giant armadillo",
                    [
                        "The giant armadillo (\"Priodontes maximus\"), colloquially tatou, ocarro, tatu-canastra or tatú carreta, is the largest living species of armadillo (although their extinct relatives, the glyptodonts, were much larger).",
                        "It lives in South America, ranging throughout as far south as northern Argentina.",
                        "This species is considered vulnerable to extinction.",
                        "The giant armadillo prefers termites and some ants as prey, and often consumes the entire population of a termite mound.",
                        "It also has been known to prey upon worms, larvae and larger creatures, such as spiders and snakes, and plants.",
                        "At least one zoo park, in Villavicencio, Colombia – \"Los Ocarros\" – is dedicated to this animal."
                    ]
                ],
                [
                    "Armadillo",
                    [
                        "Armadillos are New World placental mammals in the order Cingulata with a leathery armour shell.",
                        "The Chlamyphoridae and Dasypodidae are the only surviving families in the order, which is part of the superorder Xenarthra, along with the anteaters and sloths.",
                        "The word \"armadillo\" means \"little armoured one\" in Spanish.",
                        "The Aztecs called them \"āyōtōchtli\" , Nahuatl for \"turtle-rabbit\": \"āyōtl\" (turtle) and \"tōchtli\" (rabbit).",
                        "The Portuguese word for \"armadillo\" is \"tatu\" which derives from the Tupi language.",
                        "Similar names are also found in other, especially European, languages.",
                        "About nine extant genera and 21 extant species of armadillo have been described, some of which are distinguished by the number of bands on their armour."
                    ]
                ],
                [
                    "Serra da Canastra National Park",
                    [
                        "Serra da Canastra National Park () is a national park in the Canastra Mountains of the state of Minas Gerais, Brazil.",
                        "Serra da Canastra National Park is in the south west of Minas Gerais to the north of Rio Grande.",
                        "It is in the Cerrado biome.",
                        "The park was created by decree 70.355 on 3 April 1972, with .",
                        "It is administered by the Chico Mendes Institute for Biodiversity Conservation (ICMBio).",
                        "It lies in the municipalities of São Roque de Minas, Sacramento, Capitólio, Vargem Bonita, São João Batista do Glória and Delfinópolis in the state of Minas Gerais.",
                        "As of 2016 only of the park in the tableland of Canastra had been regularized, with former owners indemnified."
                    ]
                ]
            ],
            "speaker": "0_Wizard",
            "suggestionsConfig": "show",
            "suggestionsNumber": 5,
            "suggestionsShown": true,
            "text": " The giant armadillo (\"Priodontes maximus\"), colloquially tatou, ocarro, tatu-canastra or tatú carreta, is the largest living species of armadillo (although their extinct relatives, the glyptodonts, were much larger).",
            "textboxFilled": 0,
            "time": 1580233452.5661755,
            "timeTaken": 6.973,
            "totalTasks": 120,
            "turn": 24
        },
        {
            "checked_passage": {
                "no_passages_used": "no_passages_used"
            },
            "checked_sentence": {
                "no_passages_used": "no_passages_used"
            },
            "clicked_passages": {},
            "context": {
                "conversation": [
                    "I was thinking of starting Krav Maga, do you know anything about it?",
                    "Of course ive been practicing it for a long time, I use Krav Maga for its focus on real-world situations and its extremely good efficiency. ",
                    "Interesting, where did Krav Maga originate?",
                    "Well it first derived from the street-fighting of Hungarian-Israeli martial artist Imi Lichtenfeld, who made use of his training as a boxer and wrestler as a means of defending the Jewish quarter against fascist groups in Bratislava, Czechoslovakia, in the mid-to-late 1930s.",
                    "Interesting, can you tell me anything else about it?",
                    "Well Krav Maga is pretty lit. It can be used as an advanced form of self defense that is even as good as doing karate in some cases. "
                ],
                "index": 17
            },
            "contextCount": 0,
            "full_passages": [
                "Krav Maga\n\nKrav Maga (; , \"lit.\" \"contact-combat\") is a military self-defence and fighting system developed for the Israel Defense Forces (IDF) and Israeli security forces (Shin Bet and Mossad) that derived from a combination of techniques sourced from Boxing, Wrestling, Aikido, Judo, Karate along with realistic fight training. \n\nKrav Maga is known for its focus on real-world situations and its extreme efficiency. It was derived from the street-fighting experience of Hungarian-Israeli martial artist Imi Lichtenfeld, who made use of his training as a boxer and wrestler as a means of defending the Jewish quarter against fascist groups in Bratislava, Czechoslovakia, in the mid-to-late 1930s. In the late 1940s, following his migration to Israel, he began to provide lessons on combat training to what was to become the IDF.\n\nFrom the outset, the original concept of Krav Maga was to take the most simple and practical techniques of other fighting styles (originally European boxing, wrestling and street fighting) and to make them rapidly teachable to military conscripts.\n\nKrav Maga has a philosophy emphasizing aggression, and simultaneous defensive and offensive maneuvers. Krav Maga has been used by the Israel Defense Forces' special forces units, the security apparatus, and by regular infantry units. Closely related variations have been developed and adopted by Israeli law enforcement and intelligence organizations. There are several organizations teaching variations of Krav Maga internationally such as the British SAS.\n\nThe name in Hebrew can be translated as \"contact combat\". The root word \"krav\" () means \"combat\" and \"maga\" () means \"contact\".\n\nLike most martial arts, Krav Maga encourages students to avoid physical confrontation. If this is impossible or unsafe, it promotes finishing a fight as quickly and aggressively as possible. Attacks are aimed at the most vulnerable parts of the body, and training is not limited to techniques that avoid severe injury; some even permanently injure or cause death to the opponent.\n\nStudents learn to defend against all variety of attacks and are taught to counter in the quickest and most efficient way.\n\nIdeas in Krav Maga include:\n\n\nTraining can also cover the study and development of situational awareness to develop an understanding of one's surroundings, learning to understand the psychology of a street confrontation, and identifying potential threats before an attack occurs. It may also cover ways to deal with physical and verbal methods to avoid violence whenever possible. It also teaches mental toughness, using controlled scenarios to strengthen mental fortitude in order for students to control the impulse and not do something rash, but instead attack only when it necessary and as a last resort.\n\nKrav Maga is a continuously evolving system (reflecting real-world experience) and so it is not clear cut to specify a universal curriculum, as may be the case for example within some eastern martial arts. However, of the major Krav Maga organizations worldwide, techniques are largely similar.\n\nAdopted techniques<br>\nSome of the key focuses of techniques in Krav Maga are—as described above—effectiveness and instinctive response under stress. To that end, Krav Maga is an eclectic system that has not sought to replace existing effective techniques, taking what is useful from available systems, for example:\nTechniques taken from such systems have in some cases been modified to reflect the fact that their genesis is in a sport with rules, which limits effectiveness in real fight situations. Beyond this, Krav Maga has developed several supplementary techniques, as necessary.\n\nExamples of techniques that were developed within the system include<br>\nEscapes from chokes and holds:\nEmpty-hand weapon defences (based on the premise that the individual who is attacked in e.g. a mugging situation, is most likely to be unarmed), including:\n\nAs there is no universal authority on the system, students may find that different schools advocate different approaches.\n\nImre Lichtenfeld (also known as Imi Sde-Or) was born in 1910 in Pozsony, Austro-Hungary and grew up in Bratislava (Slovakia). Lichtenfeld became active in a wide range of sports, including gymnastics, wrestling, and boxing. In 1928, Lichtenfeld won the Slovak Youth Wrestling Championship, and in 1929 the adult championship (light and middle weight divisions). That same year, he also won the national boxing championship and an international gymnastics championship. During the ensuing decade, Imi's athletic activities focused mainly on wrestling, both as a contestant and a trainer.\n\nIn the mid-1930s, anti-Semitic riots began to threaten the Jews of Bratislava, Czechoslovakia. Lichtenfeld became the leader of a group of Jewish boxers and wrestlers who took to the streets to defend Jewish neighborhoods against the growing numbers of national socialist party and anti-Semitic thugs. Lichtenfeld quickly discovered, however, that actual fighting was very different from competition fighting, and although boxing and wrestling were good sports, they were not always practical for the aggressive and brutal nature of street combat. It was then that he started to re-evaluate his ideas about fighting and started developing the skills and techniques that would eventually become Krav Maga. Having become a thorn in the side of the equally anti-Semitic local authorities, in 1940 Lichtenfeld left his home with his family and friends on the last refugee ship to escape Europe.\n\nAfter making his way to Mandatory Palestine, Lichtenfeld joined the Haganah paramilitary organization to protect Jewish refugees from Arabs. In 1944 Lichtenfeld began training fighters in his areas of expertise: physical fitness, swimming, wrestling, use of the knife, and defence against knife attacks. During this period, Lichtenfeld trained several elite units of the Haganah including Palmach (striking force of the Haganah and forerunner of the special units of the Israel Defense Forces) and the Pal-Yam, as well as groups of police officers.\n\nIn 1948, when the State of Israel was founded and the IDF was formed, Lichtenfeld became Chief Instructor for Physical Fitness and Krav Maga at the IDF School of Combat Fitness. He served in the IDF for about 20 years, during which time he developed and refined his unique method for self-defense and hand-to-hand combat. Self-defense was not a new concept, since nearly all martial arts had developed some form of defensive techniques in their quest for tournament or sport dominance. However, self-defense was based strictly upon the scientific and dynamic principles of the human body. In 1965 judo training was added as part of the Krav Maga training, and until 1968 there were no grades in Krav Maga. Then a trainee's grades were determined largely by his knowledge in judo.\n\nIn 1968 Eli Avikzar, Imi's principal student and first black belt, began learning aikido and in 1971 left for France where he received a brown belt in aikido. Upon his return, Eli started working as an instructor alongside Imi where they worked together to improve Krav Maga by incorporating aikido and counter defenses into Krav Maga. Then in 1974 Imi retired and handed Eli Avikzar the Krav Maga training center in Netanya. Shortly after, in 1976, Eli joined the permanent force of IDF, as head of the Krav Maga section. The role of Krav Maga in the army advanced greatly after Eli's appointment. More courses were given and every P.E. instructor was obliged to learn Krav Maga. Eli continued to develop Krav Maga within the IDF until his retirement in 1987. Up to this date, Eli had trained 80,000 male soldiers and 12,000 female soldiers.\nFurther pursuing excellence as a student of martial arts, Eli went to Germany in 1977 and received a black belt in aikido from the European Federation. Then in 1978 the Krav Maga association was established, and in 1989, as an active member of the judo association, Eli Avikzar helped to establish the professional and rank committees by founding the Israeli Krav Maga Association (IKMA or KAMI). Eli retired as the Chief Krav Maga instructor in 1987 and Boaz Aviram became the third person to hold the position, being the last head instructor to have studied directly with both Lichtenfeld and Avikzar.\n\nThe IDF Krav Maga instructor course is five weeks long.\n\nThe IDF has had an annual Krav Maga competition since May 2013.\n\nUpon Lichtenfeld's retirement from the IDF, he decided to open a school and teach Krav Maga to civilians.\nThe first Krav Maga course took place at the Wingate Institute, Netanya, Israel, in 1971, under the direct supervision of Imi Lichtenfeld. Some of the first students to receive a black belt in Lichtenfeld's civilian Krav Maga Association of 1st Dan, were: Haim Gidon, James Rubenis (UK), Eli Avikzar, Ami Niv Krav Maga Aiki, Eyal Yanilov, Richard Douieb, Raphy Elgrissy, Meni Ganis, Haim Zut, Shmuel Kurzviel, Haim Hakani, Shlomo Avisira, Vicktor Bracha, Yaron Lichtenstein, Avner Hazan and Miki Asulin.\n\nIn 1978, Lichtenfeld founded the non-profit Israeli Krav Maga Association (IKMA) with several senior instructors. \nUpon his retirement Imi nominated Haim Gidon as his successor to be Grand Master and the president of the IKMA. Lichtenfeld died in January 1998 in Netanya, Israel.\n\nWhen Krav Maga started to spread beyond the borders of Israel, there arose a need to found an international civilian organization. A few of Lichtenfeld's first- and second-generation students, among these being Arviat Zagal, Asaf Halevi, and Dan Levy, eventually formed a new, civilian, international Krav Maga federation.\n\nSome of the Krav Maga organizations in Israel, such as the Krav Maga Aiki Ami Niv Federation, IKMA (Israeli Krav Maga Association, by Haim Gidon), KMF (Krav Maga Federation, by Haim Zut) and Bukan (by Yaron Lichtenstein), KAMI (Israeli Krav Magen Association) (by Eli Avikzar), TKM (Traditional Krav Maga by Erez Sharabi), as well as Ultimate Survivor krav Maga International USKMI and international KMW, Alpha Krav Maga, by Sam Sade, Krav Maga Worldwide, by Darren Levine, use Imi Lichtenfeld's original colored belt grading system which is based upon the judo ranking system. It starts with White belt, and then Yellow, Orange, Green, Blue, Brown and Black belts. Black belt students can move up the ranks from 1st to 9th Dan. The time and requirements for advancing have some differences between organizations.\n\nOther organizations that teach Krav Maga in and outside of Israel, like the Krav Maga Aiki Ami Niv Federation, International Krav Maga Federation (IKMF), Krav Maga Global (KMG) and International Krav Maga (IKM), use the same grading system awarding a series of patches. The patch system was developed by Imi Lichtenfeld after the belt system in the late 1980s. The grades are divided into 3 main categories; Practitioner, Graduate and Expert. Each of the categories, which are often abbreviated to their initials, has 5 ranks. Grades P1 through to P5 are the student levels and make up the majority of the Krav Maga community. After P5 are G1-G5, and in order to achieve Graduate level the student has to demonstrate a proficiency in all of the P level techniques before advancing. The majority of instructors hold a G level grade and are civilian instructors. However, passing the instructor's training course is a requirement, and holding a Graduate rank does not necessarily make one an instructor. The Graduate syllabus also builds on the Practitioner syllabus by focusing more on developing fighting skills. The Expert grades cover more advanced military and third-party protection techniques as well as advanced sparring and fighting skills. People who hold these ranks tend to teach in other sectors such as military and law enforcement in addition to civilian. In order to progress to Expert level, one has to demonstrate proficiency in all of the Practitioner and Graduate syllabi and have excellent fighting skills. Beyond Expert 5 there is the rank of Master. However, this rank is held by only a small number of individuals and reserved only for those who have dedicated a lifetime to Krav Maga and made valuable contributions in teaching and promoting the style.\n\nKrav Maga organizations in the United States, South America and Europe such as Krav Maga Worldwide, Krav Maga Alliance, Fit to Fight, National Krav Maga Association (NKMA), Apolaki Krav Maga, United States Krav Maga Association (USKMA), Krav Maga Street Defence, South American Federation of Krav Maga, European Federation of Krav Maga, Hagana System and Krav Maga Academy Slovenia (KMAS, by Karli Zaniug) also use a belt ranking system like that of the IKMA, KMF and Bukan. Although there are some subtle differences, the various organizations teach the same core techniques and principles. Some other organizations such as Pure Krav Maga (founded by Boaz Aviram) and Urban Krav Maga have less formal grading ranks without belts or patches but do have levels by which students can monitor their progress.\n\nIn some organizations like Krav Maga Global (KMG), sparring is slow and light until the student reaches G2 level. This takes approximately four to six years, because rising one level in the Practitioner and Graduate categories takes at minimum half a year of consistent training. (It is, however, more common to observe regular trainees grading only once a year from P3 and up.)\n\nOnce in G2, students also do simulated \"real\" fighting with protective gear.\n\nSome organizations encourage sparring as soon as students start training. For example, the International Kapap Association starts from beginner levels, and will train full contact with minimal gear in both stand-up and ground fighting, using semi-professional MMA rules for safety. Sparring should always be supervised and monitored carefully by a qualified instructor.\n\nSome Krav Maga organizations do not support a competition component, taking the stance that Krav Maga is not a sport. So-called \"fighting\" sports tend to operate under principles of using safe techniques, doing minimal harm, and consequently wearing down opponents and using other tactics supported by the \"rules\" of safe competition. In its role as self-defence and as a hand-to-hand combat system, Krav Maga operates under a completely different set of principles in which techniques may indeed cause significant damage and fights are to be ended as quickly as possible when the conflict cannot be avoided. Krav Maga organizations that involve competition are usually founded and named specifically to focus on using Krav Maga-based techniques specifically under a set of sporting principles.\n\n",
                "Urban Krav Maga\n\nUrban Krav Maga is a self-defence system which has its roots in Israeli Krav Maga as its name suggests. It additionally draws on a variety of other fighting systems such as Brazilian jiu-jitsu, Boxing, Muay Thai and Mixed Martial Arts (MMA) among others .\n\nThe main focus of Urban Krav Maga is on defending the most common street attacks, pre-emptive moves to end a confrontation before it begins and a range of stand-up and ground fighting techniques. As well as hand-to-hand fighting, techniques cover defences against weaponry (chokes, pushes, broken bottle or knife attacks, etc.).\n\nContrary to many other fighting systems and martial arts, the focus is on techniques that can be implemented regardless of strength or size based on leverage and the \"surprise effect\". As a result, Urban Krav Maga's suitability for women and is considered to be higher than that of other self-defence systems; the senior instructors have actually made a whole DVD dedicated to female self defence and fighting larger, more powerful attackers.\n\nUltimately aiming to maintain safety under all circumstances, Urban Krav Maga is based on:\nThe discipline has a grading system though the focus on such tends to be less than other system; on the most basic level, students will learn to defend themselves from the \"10 most common street attacks\" (as registered by law enforcement statistics), how to get up quickly if taken to the ground and pre-emptive moves and strikes.\n\nUrban Krav Maga was originally developed in 2008 by Stewart McGill, a 3rd Dan in Goju Ryu karate, Senior Instructor with the British Combat Association and Civilian/Law Enforcement Instructor with the International Krav Maga Federation (IKMF) under Eyal Yanilov. He has also trained with the Israel Krav Maga Association in Israel.\n\nSince then, McGill has been joined as Chief Instructor by Leo Negao, an experienced Vale Tudo and MMA fighter, and a Brazilian Ju Jitsu black belt under Master Fábio Gurgel.\n\nLeo Negao started BJJ at Carlson Gracie’s academy under Carlson himself and trained with Murilo Bustamante, Amauri Bitteti, Mário Sperry, Vitor Belfort and others. He later moved to Sao Paulo to train at the Alliance Jiu Jitsu together with Fábio Gurgel, and Romero “Jacaré” Cavalcanti. Leo is a 4-Time BJJ World champion and has been MMA training partner to UFC champions Antoñio ‘Minotauro’ Nogueira and Vitor Belfort.\n\nWell established practitioners from other Krav Maga organisations and fighting schools decided to join as Urban Krav Maga instructors almost since its inception. As a result, the organisation has grown in popularity, with over 50 active Urban Krav Maga instructors operating across the UK and international branches in: \nUnited States;\nCanada;\nBrazil;\nItaly;\nGreece and\nColombia.\nBranches will soon be starting up in Russia and Germany.\n\nIn 2008, McGill was asked to produce a DVD boxset by the UK producer NAP. This 6-set has been the top rated and top selling Krav Maga DVD product on Amazon in the UK since its release in December 2008. The 6-set covers:\n\ndefending the 10 most common attacks, knife threats and attacks, ground fighting, gun threats and third party protection. \n\nOther DVDs include:\n\n\nUrban Krav Maga has been criticised for being too much like MMA, partly because of the prominent Vale Tudo, Jiu Jitsu and MMA background of Joint Chief Instructor, Leo Negao. In this respect, the reason why Urban Krav Maga draws from these techniques is so as to be able to counter attacks on the ground from those trained in said martial arts; given a large percentage of fights end up on the ground, focus is placed on getting up quickly and safely.\n\nBecause of the move away from \"pure\" Krav Maga principles - such as the introduction of pre-emptive moves and striking with the heel of the hand to protect the knuckles rather than punching - the Urban Krav Maga organisation has alienated some in the wider Krav Maga community so such controversies will continue.\n\n",
                "Eyal Yanilov\n\nEyal Yanilov (born 30 May 1959) is an Israeli Krav Maga instructor and currently Chief Instructor of Krav Maga Global. He was a co-founder and Chief Instructor of the International Krav Maga Federation from 1996 to 2010.\n\nEyal Yanilov is a graduate of the School for Trainers and Instructors at the Wingate Institute for Sport and Physical Education and holds a degree in electrical engineering. He studied with the founder of Krav Maga, Imi Lichtenfeld (also known as Imi Sde-Or) beginning in 1974. He is one of two Krav Maga practitioners who holds the highest level granted by Imi Lichtenfeld, Master level 3 and the Founder's Diploma of Excellence, along with Darren Levine.Yanilov and Levine were the driving force in popularizing Krav Maga in the 1990s.\n\nIn 1981 Yanilov directed the first Krav Maga training seminar outside Israel and has introduced Krav Maga in 40 countries. He is the co-author of \"Krav Maga: How to Protect Yourself Against Armed Assault\" with a foreword written by former president of Israel, Shimon Peres. The book has been translated into six languages. From 1984 until Lichtenfeld's death in January 1998, Yanilov and Lichtenfeld worked together to compile a comprehensive manual on the principles of Krav Maga and its various techniques.\n\nUntil 2010, Yanilov served as head of the Professional Committee of the Israeli Krav Maga Association, established and served as the Chairman and Chief Instructor of the IKMF. He now heads the KMG — Krav Maga Global. Since the early 1980s, he has trained generations of instructors in the United States, Europe, South East Asia, and Australia.\n\nYanilov was featured on an episode of Human Weapon, where he demonstrated disarming techniques of Krav Maga. In 2010 Eyal founded MaxKravMaga.com — an online Krav Maga training program where members can learn Krav Maga directly from Eyal via streaming video.\n\n\n"
            ],
            "good_message": false,
            "loggedData": [],
            "offensive": null,
            "researchTask": true,
            "selectedData": [],
            "shown_passages": [
                [
                    "Krav Maga",
                    [
                        "Krav Maga (; , \"lit.\"",
                        "\"contact-combat\") is a military self-defence and fighting system developed for the Israel Defense Forces (IDF) and Israeli security forces (Shin Bet and Mossad) that derived from a combination of techniques sourced from Boxing, Wrestling, Aikido, Judo, Karate along with realistic fight training.",
                        "Krav Maga is known for its focus on real-world situations and its extreme efficiency.",
                        "It was derived from the street-fighting experience of Hungarian-Israeli martial artist Imi Lichtenfeld, who made use of his training as a boxer and wrestler as a means of defending the Jewish quarter against fascist groups in Bratislava, Czechoslovakia, in the mid-to-late 1930s."
                    ]
                ],
                [
                    "Urban Krav Maga",
                    [
                        "Urban Krav Maga is a self-defence system which has its roots in Israeli Krav Maga as its name suggests.",
                        "It additionally draws on a variety of other fighting systems such as Brazilian jiu-jitsu, Boxing, Muay Thai and Mixed Martial Arts (MMA) among others .",
                        "The main focus of Urban Krav Maga is on defending the most common street attacks, pre-emptive moves to end a confrontation before it begins and a range of stand-up and ground fighting techniques.",
                        "As well as hand-to-hand fighting, techniques cover defences against weaponry (chokes, pushes, broken bottle or knife attacks, etc.).",
                        "Contrary to many other fighting systems and martial arts, the focus is on techniques that can be implemented regardless of strength or size based on leverage and the \"surprise effect\"."
                    ]
                ],
                [
                    "Eyal Yanilov",
                    [
                        "Eyal Yanilov (born 30 May 1959) is an Israeli Krav Maga instructor and currently Chief Instructor of Krav Maga Global.",
                        "He was a co-founder and Chief Instructor of the International Krav Maga Federation from 1996 to 2010.",
                        "Eyal Yanilov is a graduate of the School for Trainers and Instructors at the Wingate Institute for Sport and Physical Education and holds a degree in electrical engineering.",
                        "He studied with the founder of Krav Maga, Imi Lichtenfeld (also known as Imi Sde-Or) beginning in 1974.",
                        "He is one of two Krav Maga practitioners who holds the highest level granted by Imi Lichtenfeld, Master level 3 and the Founder's Diploma of Excellence, along with Darren Levine.Yanilov and Levine were the driving force in popularizing Krav Maga in the 1990s."
                    ]
                ]
            ],
            "speaker": "0_Wizard",
            "suggestionsConfig": "show",
            "suggestionsNumber": 5,
            "suggestionsShown": true,
            "text": "Well Krav Maga is pretty lit. It can be used as an advanced form of self defense that is even as good as doing karate in some cases. ",
            "textboxFilled": 0,
            "time": 1580233456.1071794,
            "timeTaken": 3.331,
            "totalTasks": 120,
            "turn": 25
        }
    ],
    "good_wizard_worker": "A30HUZHJBOX1LK",
    "hit_ids": [
        "39O6Z4JLX43XHFPZFMMG2E75HJ6VX6"
    ],
    "n_turn": 2,
    "persona": {
        "i am a veterinarian.": [
            "Veterinary medicine"
        ],
        "i grew up on a farm.": [],
        "i have two nephews.": [],
        "running is my way to relax.": [
            "Running"
        ]
    },
    "relevant_topics": [
        "Running (running is my way to relax.)",
        "Veterinary medicine (i am a veterinarian.)"
    ],
    "speaker_with_persona": "A30HUZHJBOX1LK",
    "wizard_eval": 0,
    "wizard_good": true,
    "workers": [
        "A30HUZHJBOX1LK"
    ]
}